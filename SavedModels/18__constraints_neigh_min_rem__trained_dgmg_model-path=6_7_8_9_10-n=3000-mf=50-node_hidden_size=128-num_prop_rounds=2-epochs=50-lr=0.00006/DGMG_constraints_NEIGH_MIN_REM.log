nohup: ignoring input
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:2859: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  metadata = torch.load(load_path_metadata)
/home/nschmitz/.pyenv/versions/3.12.9/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
‚úÖ Using device: cuda:2
/home/nschmitz/GNNs/kirigami-database/data/cleaned
Model without constraints:  False
üíø Folder that model data gets saved into:  18__constraints_neigh_min_rem__
Metrics---
Amount unfoldings:  3000
Max files per dir:  50
Batch size:  1
Num prop rounds:  2
Learning rate:  6e-05
Num epochs:  50
Accum steps:  4
Training node ordering strategy:  NodeOrder.NEIGH_MIN_REM
Add edge is deterministic:  True
Add edge is deterministic threshold:  0.5
Calc PR-Curve:  False
---------------
6_7_8_9_10
Found existing dataset files; skipping save.
Loading dataset from disk‚Ä¶
6_7_8_9_10
Max amount of features per node in a graph:  128
Loaded 46571 graphs with actions!
Train size: 32599, Validation size: 6985, Test size: 6987
Graph(num_nodes=9, num_edges=16,
      ndata_schemes={'a': Scheme(shape=(256,), dtype=torch.float32), 'hv': Scheme(shape=(128,), dtype=torch.float32), 'deg_rem': Scheme(shape=(1,), dtype=torch.int16), 'deg_target': Scheme(shape=(1,), dtype=torch.int16)}
      edata_schemes={'he': Scheme(shape=(1,), dtype=torch.float32)})
[[0. 0. 1. 0. 0. 0. 1. 0. 1.]
 [0. 0. 1. 0. 0. 0. 0. 1. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 1. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0.]]
--------
Graph(num_nodes=9, num_edges=30,
      ndata_schemes={'deg_target': Scheme(shape=(1,), dtype=torch.int16), 'hv': Scheme(shape=(128,), dtype=torch.float32)}
      edata_schemes={})
[[0. 0. 1. 0. 1. 0. 1. 0. 1.]
 [0. 0. 1. 0. 0. 1. 0. 1. 1.]
 [1. 1. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 1. 0. 0. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0.]
 [0. 1. 0. 1. 1. 0. 1. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 1. 0.]
 [0. 1. 0. 0. 0. 0. 1. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 1. 0.]]
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM
Device for model: cuda:2
Dir_name to save to:  6_7_8_9_10
Training started at: 2025-07-14 11:31:08
grad AddEdge W: 158.6933135986328
grad ChooseDest W: 2.776397705078125
grad AddEdge W: 3.7199840545654297
grad ChooseDest W: 3.8802740573883057
grad AddEdge W: 3.7162046432495117
grad ChooseDest W: 2.3107752799987793
grad AddEdge W: 1.3167550563812256
grad ChooseDest W: 2.7602949142456055
grad AddEdge W: 0.4492017924785614
grad ChooseDest W: 3.067453622817993
grad AddEdge W: 0.8089677095413208
grad ChooseDest W: 2.9018728733062744
grad AddEdge W: 0.18085461854934692
grad ChooseDest W: 3.731152057647705
grad AddEdge W: 0.22159406542778015
grad ChooseDest W: 3.636143684387207
grad AddEdge W: 0.15732119977474213
grad ChooseDest W: 5.032867431640625
grad AddEdge W: 0.18756940960884094
grad ChooseDest W: 2.9563491344451904
grad AddEdge W: 0.06449027359485626
grad ChooseDest W: 2.6765048503875732
grad AddEdge W: 0.30964621901512146
grad ChooseDest W: 3.9006125926971436
grad AddEdge W: 0.08655132353305817
grad ChooseDest W: 3.42691969871521
grad AddEdge W: 0.3943069577217102
grad ChooseDest W: 2.7317423820495605
grad AddEdge W: 0.10967129468917847
grad ChooseDest W: 3.0215632915496826
grad AddEdge W: 0.05405861511826515
grad ChooseDest W: 3.586299180984497
grad AddEdge W: 0.04756665974855423
grad ChooseDest W: 3.873495578765869
grad AddEdge W: 0.09422940760850906
grad ChooseDest W: 6.062608242034912
grad AddEdge W: 0.045342110097408295
grad ChooseDest W: 3.3262975215911865
grad AddEdge W: 0.03123234398663044
grad ChooseDest W: 3.3773598670959473
grad AddEdge W: 0.05744766816496849
grad ChooseDest W: 2.753363609313965
grad AddEdge W: 0.05969403311610222
grad ChooseDest W: 4.972805976867676
grad AddEdge W: 0.03128613531589508
grad ChooseDest W: 3.014301300048828
grad AddEdge W: 0.022775869816541672
grad ChooseDest W: 2.7485122680664062
grad AddEdge W: 0.030560635030269623
grad ChooseDest W: 4.576787948608398
grad AddEdge W: 0.013353606685996056
grad ChooseDest W: 3.864283561706543
grad AddEdge W: 0.008845530450344086
grad ChooseDest W: 3.487722635269165
grad AddEdge W: 0.012889966368675232
grad ChooseDest W: 9.25133991241455
grad AddEdge W: 0.026432469487190247
grad ChooseDest W: 3.3004047870635986
grad AddEdge W: 0.007468491792678833
grad ChooseDest W: 1.1654691696166992
grad AddEdge W: 0.008249469101428986
grad ChooseDest W: 2.3727707862854004
grad AddEdge W: 0.00609690323472023
grad ChooseDest W: 2.8462636470794678
grad AddEdge W: 0.007937532849609852
grad ChooseDest W: 2.491321086883545
grad AddEdge W: 0.00842382200062275
grad ChooseDest W: 2.825563669204712
grad AddEdge W: 0.01939745433628559
grad ChooseDest W: 2.516122817993164
grad AddEdge W: 0.012804062105715275
grad ChooseDest W: 3.9370927810668945
grad AddEdge W: 0.010225560516119003
grad ChooseDest W: 4.563064098358154
grad AddEdge W: 0.014736214652657509
grad ChooseDest W: 2.6981070041656494
grad AddEdge W: 0.04638228192925453
grad ChooseDest W: 3.236978054046631
grad AddEdge W: 0.004969161469489336
grad ChooseDest W: 5.197442531585693
grad AddEdge W: 0.0041964356787502766
grad ChooseDest W: 3.7577457427978516
grad AddEdge W: 0.003368301782757044
grad ChooseDest W: 2.8963632583618164
grad AddEdge W: 0.006055499892681837
grad ChooseDest W: 3.6378984451293945
grad AddEdge W: 0.0028110051061958075
grad ChooseDest W: 2.8421638011932373
grad AddEdge W: 0.010701854713261127
grad ChooseDest W: 3.3200464248657227
grad AddEdge W: 0.0025207269936800003
grad ChooseDest W: 3.6448183059692383
grad AddEdge W: 0.0047434596344828606
grad ChooseDest W: 4.3543314933776855
grad AddEdge W: 0.003954314161092043
grad ChooseDest W: 3.2397656440734863
grad AddEdge W: 0.0017586220055818558
grad ChooseDest W: 3.7346127033233643
grad AddEdge W: 0.014358876273036003
grad ChooseDest W: 2.2887251377105713
grad AddEdge W: 0.0020212852396070957
grad ChooseDest W: 2.937966823577881
grad AddEdge W: 0.0040094368159770966
grad ChooseDest W: 2.7561144828796387
grad AddEdge W: 0.00191977818030864
grad ChooseDest W: 3.0203123092651367
grad AddEdge W: 0.009794550016522408
grad ChooseDest W: 2.3182387351989746
grad AddEdge W: 0.0032013466116040945
grad ChooseDest W: 1.4623664617538452
grad AddEdge W: 0.0028394449036568403
grad ChooseDest W: 3.548572540283203
grad AddEdge W: 0.00320769683457911
grad ChooseDest W: 2.8791072368621826
grad AddEdge W: 0.003425187896937132
grad ChooseDest W: 3.254411458969116
grad AddEdge W: 0.007705270778387785
grad ChooseDest W: 2.826964855194092
grad AddEdge W: 0.0019249322358518839
grad ChooseDest W: 3.7125165462493896
grad AddEdge W: 0.00815299991518259
grad ChooseDest W: 2.0944125652313232
grad AddEdge W: 0.0008283596253022552
grad ChooseDest W: 3.579988479614258
grad AddEdge W: 0.002131605986505747
grad ChooseDest W: 3.3370883464813232
grad AddEdge W: 0.0006116274744272232
grad ChooseDest W: 3.2323150634765625
grad AddEdge W: 0.0007741344161331654
grad ChooseDest W: 2.946685791015625
grad AddEdge W: 0.0019181463867425919
grad ChooseDest W: 1.8516076803207397
=== Epoch 1: Train Loss: 6.5148, Train Log Prob: 0.0036 ===
Total mismatches: 100726
Predicted valid destination but wrong order: 7861
Epoch 1: Validation Loss: 6.5135, Validation Log Prob: 0.0026
Epoch 1: Edge Precision: 0.3572, Recall: 0.3531, F1: 0.3550, Jaccard: 0.2329
Epoch 1: TP: 2.4734430923407302, FP: 4.461417322834646, FN: 4.548031496062992
Epoch 1: warmup, skipping learning rate scheduler
Epoch 1: Current Learning Rate: 6e-05
[Epoch 1] ‚è±Ô∏è Total: 4714.74s | Current time: 2025-07-14 12:49:43 | üèãÔ∏è Train: 3920.48s | ‚úÖ Val: 794.26s
grad AddEdge W: 0.009239459410309792
grad ChooseDest W: 5.706532955169678
grad AddEdge W: 0.0005664019263349473
grad ChooseDest W: 5.322624683380127
grad AddEdge W: 0.004650108981877565
grad ChooseDest W: 1.4820529222488403
grad AddEdge W: 0.0006075045675970614
grad ChooseDest W: 2.888202667236328
grad AddEdge W: 0.003562389872968197
grad ChooseDest W: 2.407238483428955
grad AddEdge W: 0.00041357052396051586
grad ChooseDest W: 3.2220451831817627
grad AddEdge W: 0.00024626284721307456
grad ChooseDest W: 2.232525587081909
grad AddEdge W: 0.0010732526425272226
grad ChooseDest W: 3.4078352451324463
grad AddEdge W: 0.0009729426819831133
grad ChooseDest W: 2.0776846408843994
grad AddEdge W: 0.001035152468830347
grad ChooseDest W: 1.709490418434143
grad AddEdge W: 0.0008649761439301074
grad ChooseDest W: 2.600630521774292
grad AddEdge W: 0.00023344621877186
grad ChooseDest W: 2.6129095554351807
grad AddEdge W: 0.00020582639263011515
grad ChooseDest W: 2.579695224761963
grad AddEdge W: 0.0007844563806429505
grad ChooseDest W: 1.853269100189209
grad AddEdge W: 0.0007561433594673872
grad ChooseDest W: 4.072120189666748
grad AddEdge W: 0.00020886256243102252
grad ChooseDest W: 2.4334161281585693
grad AddEdge W: 0.00024855605443008244
grad ChooseDest W: 4.200918197631836
grad AddEdge W: 0.00011107086902484298
grad ChooseDest W: 4.717132091522217
grad AddEdge W: 0.000566403497941792
grad ChooseDest W: 3.6136038303375244
grad AddEdge W: 0.0005718800239264965
grad ChooseDest W: 2.4986732006073
grad AddEdge W: 0.0004107537679374218
grad ChooseDest W: 3.5198585987091064
grad AddEdge W: 0.00016193838382605463
grad ChooseDest W: 2.6514692306518555
grad AddEdge W: 0.00011139916023239493
grad ChooseDest W: 2.411329984664917
grad AddEdge W: 0.00012266858539078385
grad ChooseDest W: 2.4319074153900146
grad AddEdge W: 0.00015083503967616707
grad ChooseDest W: 3.0523934364318848
grad AddEdge W: 0.0006187327089719474
grad ChooseDest W: 2.505467414855957
grad AddEdge W: 9.279424557462335e-05
grad ChooseDest W: 2.586350202560425
grad AddEdge W: 7.720511348452419e-05
grad ChooseDest W: 5.26726770401001
grad AddEdge W: 7.318965799640864e-05
grad ChooseDest W: 3.7318994998931885
grad AddEdge W: 7.725612522335723e-05
grad ChooseDest W: 1.4761451482772827
grad AddEdge W: 0.00010888765973504633
grad ChooseDest W: 3.3114116191864014
grad AddEdge W: 0.00024196768936235458
grad ChooseDest W: 2.820566415786743
grad AddEdge W: 0.0003391775244381279
grad ChooseDest W: 4.320816516876221
grad AddEdge W: 2.779436545097269e-05
grad ChooseDest W: 2.9139411449432373
grad AddEdge W: 4.0068251109914854e-05
grad ChooseDest W: 2.535506010055542
grad AddEdge W: 0.00015410409832838923
grad ChooseDest W: 2.708174228668213
grad AddEdge W: 2.6566387532511726e-05
grad ChooseDest W: 2.9084432125091553
grad AddEdge W: 0.00017078699602279812
grad ChooseDest W: 2.9765920639038086
grad AddEdge W: 2.3594089725520462e-05
grad ChooseDest W: 3.615530014038086
grad AddEdge W: 9.17115539778024e-05
grad ChooseDest W: 1.6169884204864502
grad AddEdge W: 0.00012065776536474004
grad ChooseDest W: 2.9856762886047363
grad AddEdge W: 3.5063709219684824e-05
grad ChooseDest W: 3.637986183166504
grad AddEdge W: 2.0482411855482496e-05
grad ChooseDest W: 3.364264726638794
grad AddEdge W: 6.30169379292056e-05
grad ChooseDest W: 1.2019314765930176
grad AddEdge W: 6.769656465621665e-05
grad ChooseDest W: 3.462681293487549
grad AddEdge W: 6.41240258119069e-05
grad ChooseDest W: 2.3203694820404053
grad AddEdge W: 7.188317977124825e-05
grad ChooseDest W: 2.0042150020599365
grad AddEdge W: 1.3862253581464756e-05
grad ChooseDest W: 2.862746000289917
grad AddEdge W: 5.933292050031014e-05
grad ChooseDest W: 1.9723784923553467
grad AddEdge W: 1.0413225936645176e-05
grad ChooseDest W: 1.8451534509658813
grad AddEdge W: 0.00022459957108367234
grad ChooseDest W: 1.4495742321014404
grad AddEdge W: 1.054951917467406e-05
grad ChooseDest W: 3.607539415359497
grad AddEdge W: 7.248298970807809e-06
grad ChooseDest W: 3.1124725341796875
grad AddEdge W: 8.439270459348336e-06
grad ChooseDest W: 2.035888195037842
grad AddEdge W: 5.974917257844936e-06
grad ChooseDest W: 2.212029218673706
grad AddEdge W: 4.812398401554674e-05
grad ChooseDest W: 2.6346657276153564
grad AddEdge W: 6.852631941001164e-06
grad ChooseDest W: 3.802199363708496
grad AddEdge W: 5.907863669563085e-05
grad ChooseDest W: 2.422555446624756
grad AddEdge W: 4.375150183477672e-06
grad ChooseDest W: 2.2301254272460938
grad AddEdge W: 0.00015222921501845121
grad ChooseDest W: 2.2784149646759033
grad AddEdge W: 5.7494080465403385e-06
grad ChooseDest W: 5.7387919425964355
grad AddEdge W: 3.9536218537250534e-05
grad ChooseDest W: 3.33095645904541
grad AddEdge W: 5.388726549426792e-06
grad ChooseDest W: 4.365478992462158
grad AddEdge W: 1.6935575331444852e-05
grad ChooseDest W: 3.0655734539031982
grad AddEdge W: 5.335212335921824e-06
grad ChooseDest W: 3.7245466709136963
grad AddEdge W: 1.740136394801084e-05
grad ChooseDest W: 2.340071439743042
=== Epoch 2: Train Loss: 6.2190, Train Log Prob: 0.0050 ===
Total mismatches: 92378
Predicted valid destination but wrong order: 7892
Epoch 2: Validation Loss: 6.1143, Validation Log Prob: 0.0039
Epoch 2: Edge Precision: 0.3622, Recall: 0.3606, F1: 0.3613, Jaccard: 0.2380
Epoch 2: TP: 2.525984251968504, FP: 4.46284896206156, FN: 4.495490336435219
Epoch 2: warmup, skipping learning rate scheduler
Epoch 2: Current Learning Rate: 6e-05
[Epoch 2] ‚è±Ô∏è Total: 4870.25s | Current time: 2025-07-14 14:10:53 | üèãÔ∏è Train: 4069.28s | ‚úÖ Val: 800.97s
grad AddEdge W: 5.11257785547059e-05
grad ChooseDest W: 5.663808822631836
grad AddEdge W: 1.6533260350115597e-05
grad ChooseDest W: 2.381593942642212
grad AddEdge W: 1.0689333066693507e-05
grad ChooseDest W: 3.052377939224243
grad AddEdge W: 1.7610620488994755e-05
grad ChooseDest W: 2.5896315574645996
grad AddEdge W: 1.5461010889339377e-06
grad ChooseDest W: 3.097321033477783
grad AddEdge W: 2.752496357061318e-06
grad ChooseDest W: 3.3283064365386963
grad AddEdge W: 2.6185866772721056e-06
grad ChooseDest W: 3.971743106842041
grad AddEdge W: 0.00025058345636352897
grad ChooseDest W: 1.4200249910354614
grad AddEdge W: 2.8588256100192666e-06
grad ChooseDest W: 2.476203441619873
grad AddEdge W: 1.8075631942338077e-06
grad ChooseDest W: 4.989048480987549
grad AddEdge W: 1.7482564089732477e-06
grad ChooseDest W: 2.504148244857788
grad AddEdge W: 0.0002937904791906476
grad ChooseDest W: 1.3503738641738892
grad AddEdge W: 1.481192384744645e-06
grad ChooseDest W: 3.1617085933685303
grad AddEdge W: 6.156847121019382e-06
grad ChooseDest W: 3.1551427841186523
grad AddEdge W: 9.60478064371273e-06
grad ChooseDest W: 2.049968957901001
grad AddEdge W: 4.131166406295961e-06
grad ChooseDest W: 3.347501754760742
grad AddEdge W: 5.760396106779808e-07
grad ChooseDest W: 1.7144519090652466
grad AddEdge W: 1.3137799896867364e-06
grad ChooseDest W: 8.19943904876709
grad AddEdge W: 4.570387318381108e-06
grad ChooseDest W: 2.994725465774536
grad AddEdge W: 3.6766930406884057e-06
grad ChooseDest W: 1.5300593376159668
grad AddEdge W: 5.208376478549326e-06
grad ChooseDest W: 3.8470427989959717
grad AddEdge W: 1.1673187145788688e-06
grad ChooseDest W: 2.355879783630371
grad AddEdge W: 6.785762707295362e-07
grad ChooseDest W: 3.032055377960205
grad AddEdge W: 3.1335814583144384e-06
grad ChooseDest W: 3.3575568199157715
grad AddEdge W: 7.695870749557798e-07
grad ChooseDest W: 4.695847034454346
grad AddEdge W: 3.5974096590507543e-07
grad ChooseDest W: 1.5626511573791504
grad AddEdge W: 2.7420171591074904e-06
grad ChooseDest W: 2.9509634971618652
grad AddEdge W: 5.161741682968568e-07
grad ChooseDest W: 3.4427850246429443
grad AddEdge W: 4.078715107880271e-07
grad ChooseDest W: 5.096570014953613
grad AddEdge W: 4.3165397300981567e-07
grad ChooseDest W: 9.117513656616211
grad AddEdge W: 1.7893160020321375e-06
grad ChooseDest W: 2.8747942447662354
grad AddEdge W: 3.000712354150892e-07
grad ChooseDest W: 2.552446126937866
grad AddEdge W: 2.1411349280242575e-06
grad ChooseDest W: 3.7599306106567383
grad AddEdge W: 1.0358203326177318e-05
grad ChooseDest W: 3.0007805824279785
grad AddEdge W: 3.13552789066307e-07
grad ChooseDest W: 2.956296920776367
grad AddEdge W: 5.393596325120598e-07
grad ChooseDest W: 2.997911214828491
grad AddEdge W: 1.1610684396146098e-06
grad ChooseDest W: 2.532541275024414
grad AddEdge W: 9.013335215968254e-07
grad ChooseDest W: 2.0272905826568604
grad AddEdge W: 1.3262688014492596e-07
grad ChooseDest W: 3.3042874336242676
grad AddEdge W: 9.240301892532443e-07
grad ChooseDest W: 2.270334005355835
grad AddEdge W: 1.060806198438513e-06
grad ChooseDest W: 2.4351491928100586
grad AddEdge W: 1.3008768746658461e-06
grad ChooseDest W: 3.567373514175415
grad AddEdge W: 6.206371381267672e-07
grad ChooseDest W: 3.721714735031128
grad AddEdge W: 5.203989985602675e-06
grad ChooseDest W: 4.885590076446533
grad AddEdge W: 8.174715304676283e-08
grad ChooseDest W: 3.0265636444091797
grad AddEdge W: 6.942811978660757e-07
grad ChooseDest W: 3.891852378845215
grad AddEdge W: 3.3919911857083207e-07
grad ChooseDest W: 3.4768521785736084
grad AddEdge W: 6.230617088931467e-08
grad ChooseDest W: 2.021793842315674
grad AddEdge W: 3.024243540039606e-07
grad ChooseDest W: 2.605482816696167
grad AddEdge W: 5.839477523750247e-08
grad ChooseDest W: 2.452322244644165
grad AddEdge W: 2.5822109250839276e-07
grad ChooseDest W: 1.6569786071777344
grad AddEdge W: 1.633157444302924e-05
grad ChooseDest W: 1.8788938522338867
grad AddEdge W: 6.26376817081109e-08
grad ChooseDest W: 3.650695323944092
grad AddEdge W: 3.0373141157724604e-07
grad ChooseDest W: 1.709792137145996
grad AddEdge W: 4.0667856637810473e-07
grad ChooseDest W: 2.265134811401367
grad AddEdge W: 3.6838681438666754e-08
grad ChooseDest W: 2.711878776550293
grad AddEdge W: 3.484890100935445e-07
grad ChooseDest W: 2.2985079288482666
grad AddEdge W: 2.2604451643815082e-08
grad ChooseDest W: 3.743208885192871
grad AddEdge W: 3.3455084036404514e-08
grad ChooseDest W: 2.152024507522583
grad AddEdge W: 2.1614343381770595e-07
grad ChooseDest W: 2.393436908721924
grad AddEdge W: 1.194997730635805e-05
grad ChooseDest W: 2.4835617542266846
grad AddEdge W: 1.316940796414201e-07
grad ChooseDest W: 2.659125328063965
grad AddEdge W: 1.2278184158276417e-07
grad ChooseDest W: 2.4809701442718506
grad AddEdge W: 9.285855071539117e-07
grad ChooseDest W: 2.1883888244628906
grad AddEdge W: 1.3174643598290459e-08
grad ChooseDest W: 5.901209831237793
grad AddEdge W: 1.0612256318154323e-07
grad ChooseDest W: 4.031963348388672
=== Epoch 3: Train Loss: 6.0890, Train Log Prob: 0.0061 ===
Total mismatches: 89842
Predicted valid destination but wrong order: 7823
Epoch 3: Validation Loss: 5.7479, Validation Log Prob: 0.0058
Epoch 3: Edge Precision: 0.3679, Recall: 0.3673, F1: 0.3676, Jaccard: 0.2430
Epoch 3: TP: 2.574087329992842, FP: 4.4386542591267, FN: 4.44738725841088
Epoch 3: warmup, skipping learning rate scheduler
Epoch 3: Current Learning Rate: 6e-05
[Epoch 3] ‚è±Ô∏è Total: 4883.34s | Current time: 2025-07-14 15:32:17 | üèãÔ∏è Train: 4070.39s | ‚úÖ Val: 812.95s
grad AddEdge W: 2.8845295219070977e-07
grad ChooseDest W: 4.8080363273620605
grad AddEdge W: 1.4290874261746467e-08
grad ChooseDest W: 3.638098955154419
grad AddEdge W: 1.113817127418315e-08
grad ChooseDest W: 2.094855546951294
grad AddEdge W: 1.1501753327536335e-08
grad ChooseDest W: 2.0597622394561768
grad AddEdge W: 6.547326680816923e-09
grad ChooseDest W: 2.487490653991699
grad AddEdge W: 8.660287598161176e-09
grad ChooseDest W: 3.52103328704834
grad AddEdge W: 3.84540532749611e-09
grad ChooseDest W: 2.664858102798462
grad AddEdge W: 3.382940505503029e-08
grad ChooseDest W: 2.5036048889160156
grad AddEdge W: 9.282210555738857e-08
grad ChooseDest W: 2.7312557697296143
grad AddEdge W: 5.1243040921633565e-09
grad ChooseDest W: 2.708578586578369
grad AddEdge W: 3.3104959584306926e-05
grad ChooseDest W: 0.8628069162368774
grad AddEdge W: 3.4460800435454075e-08
grad ChooseDest W: 5.534616947174072
grad AddEdge W: 6.101212335352102e-08
grad ChooseDest W: 2.716092824935913
grad AddEdge W: 6.83815670754484e-09
grad ChooseDest W: 2.1936094760894775
grad AddEdge W: 5.323184559813399e-09
grad ChooseDest W: 4.255504131317139
grad AddEdge W: 3.537646620088708e-08
grad ChooseDest W: 2.8475148677825928
grad AddEdge W: 4.302160849789516e-09
grad ChooseDest W: 3.3267838954925537
grad AddEdge W: 2.29161289944102e-09
grad ChooseDest W: 2.4271066188812256
grad AddEdge W: 3.1595352112390174e-08
grad ChooseDest W: 2.8805840015411377
grad AddEdge W: 1.9690438168851188e-09
grad ChooseDest W: 3.0754597187042236
grad AddEdge W: 6.150281706140959e-08
grad ChooseDest W: 3.297942638397217
grad AddEdge W: 2.53057645949184e-08
grad ChooseDest W: 1.9809174537658691
grad AddEdge W: 2.2848649638973484e-09
grad ChooseDest W: 3.211841583251953
grad AddEdge W: 2.0669233435910428e-08
grad ChooseDest W: 3.1630308628082275
grad AddEdge W: 1.6635118837982077e-09
grad ChooseDest W: 3.3536810874938965
grad AddEdge W: 1.5008112530523476e-09
grad ChooseDest W: 1.9659428596496582
grad AddEdge W: 1.918561975955413e-09
grad ChooseDest W: 2.6347954273223877
grad AddEdge W: 1.1626974938394596e-09
grad ChooseDest W: 4.377962112426758
grad AddEdge W: 1.1647088626887125e-08
grad ChooseDest W: 2.1331865787506104
grad AddEdge W: 1.1833510837888639e-09
grad ChooseDest W: 2.5815417766571045
grad AddEdge W: 1.0730648369872142e-08
grad ChooseDest W: 3.5853078365325928
grad AddEdge W: 7.4318577958365495e-09
grad ChooseDest W: 1.5325316190719604
grad AddEdge W: 1.1102148533836953e-08
grad ChooseDest W: 2.1988143920898438
grad AddEdge W: 9.54263779107123e-10
grad ChooseDest W: 2.2813961505889893
grad AddEdge W: 9.958851521219003e-09
grad ChooseDest W: 2.5506415367126465
grad AddEdge W: 1.0583600840163854e-07
grad ChooseDest W: 2.8513472080230713
grad AddEdge W: 5.901971134036899e-10
grad ChooseDest W: 3.124922275543213
grad AddEdge W: 6.393716667218996e-10
grad ChooseDest W: 5.3608551025390625
grad AddEdge W: 4.7198600583442385e-09
grad ChooseDest W: 2.8110015392303467
grad AddEdge W: 2.617654981662554e-10
grad ChooseDest W: 3.613600015640259
grad AddEdge W: 3.4079103428119595e-10
grad ChooseDest W: 3.48158860206604
grad AddEdge W: 6.654829465269074e-10
grad ChooseDest W: 2.759164333343506
grad AddEdge W: 3.768064471554311e-10
grad ChooseDest W: 4.733075141906738
grad AddEdge W: 2.394784093251445e-10
grad ChooseDest W: 2.5821051597595215
grad AddEdge W: 2.2300314928003218e-09
grad ChooseDest W: 4.905177116394043
grad AddEdge W: 1.7974476074211765e-10
grad ChooseDest W: 2.738097667694092
grad AddEdge W: 2.107073404644666e-09
grad ChooseDest W: 2.3990883827209473
grad AddEdge W: 4.1802525885259456e-08
grad ChooseDest W: 2.8627898693084717
grad AddEdge W: 1.5049689272572664e-10
grad ChooseDest W: 2.8026671409606934
grad AddEdge W: 1.4905379153162812e-09
grad ChooseDest W: 2.946103811264038
grad AddEdge W: 1.6114067025174705e-10
grad ChooseDest W: 2.8273799419403076
grad AddEdge W: 1.5938604602805384e-10
grad ChooseDest W: 3.7152881622314453
grad AddEdge W: 2.9605262685805656e-09
grad ChooseDest W: 2.6552908420562744
grad AddEdge W: 2.3140591665082866e-09
grad ChooseDest W: 2.829896926879883
grad AddEdge W: 2.2285817635747662e-09
grad ChooseDest W: 5.054691314697266
grad AddEdge W: 4.354685015361959e-11
grad ChooseDest W: 1.9104726314544678
grad AddEdge W: 4.408770917785354e-11
grad ChooseDest W: 2.522048234939575
grad AddEdge W: 2.9612156282610158e-08
grad ChooseDest W: 1.8849037885665894
grad AddEdge W: 6.558346921092806e-11
grad ChooseDest W: 2.871769666671753
grad AddEdge W: 5.2305645925621036e-11
grad ChooseDest W: 2.417036294937134
grad AddEdge W: 7.169915772209379e-10
grad ChooseDest W: 2.4761099815368652
grad AddEdge W: 2.029639477318934e-11
grad ChooseDest W: 2.883976936340332
grad AddEdge W: 3.901165154807096e-11
grad ChooseDest W: 3.176466464996338
grad AddEdge W: 6.954822273641525e-10
grad ChooseDest W: 3.225508689880371
grad AddEdge W: 4.4122313441752326e-11
grad ChooseDest W: 2.2239363193511963
grad AddEdge W: 5.092854471477892e-10
grad ChooseDest W: 1.8823617696762085
=== Epoch 4: Train Loss: 6.0427, Train Log Prob: 0.0067 ===
Total mismatches: 88815
Predicted valid destination but wrong order: 7769
Epoch 4: Validation Loss: 5.5131, Validation Log Prob: 0.0072
Epoch 4: Edge Precision: 0.3684, Recall: 0.3679, F1: 0.3681, Jaccard: 0.2432
Epoch 4: TP: 2.576521116678597, FP: 4.437079455977094, FN: 4.444953471725126
Epoch 4: warmup, skipping learning rate scheduler
Epoch 4: Current Learning Rate: 6e-05
[Epoch 4] ‚è±Ô∏è Total: 4839.23s | Current time: 2025-07-14 16:52:56 | üèãÔ∏è Train: 4048.66s | ‚úÖ Val: 790.58s
grad AddEdge W: 1.3817513799807557e-10
grad ChooseDest W: 5.147280216217041
grad AddEdge W: 3.071750492855685e-11
grad ChooseDest W: 2.9297823905944824
grad AddEdge W: 4.177190260357122e-10
grad ChooseDest W: 1.5746798515319824
grad AddEdge W: 5.585182316636406e-10
grad ChooseDest W: 2.1368978023529053
grad AddEdge W: 1.5518954177284883e-11
grad ChooseDest W: 3.6163365840911865
grad AddEdge W: 2.0371766773497058e-11
grad ChooseDest W: 2.9871251583099365
grad AddEdge W: 8.199232404138002e-09
grad ChooseDest W: 3.0203890800476074
grad AddEdge W: 5.554738891078159e-09
grad ChooseDest W: 2.111504554748535
grad AddEdge W: 2.676614659080645e-11
grad ChooseDest W: 2.95200777053833
grad AddEdge W: 1.9889537239414778e-10
grad ChooseDest W: 3.03090238571167
grad AddEdge W: 1.2091797181590369e-11
grad ChooseDest W: 4.03330135345459
grad AddEdge W: 1.3517773140847567e-06
grad ChooseDest W: 0.5692973136901855
grad AddEdge W: 1.7909384045222687e-11
grad ChooseDest W: 2.19014048576355
grad AddEdge W: 1.407262847918922e-11
grad ChooseDest W: 4.212857246398926
grad AddEdge W: 3.8239891253510905e-09
grad ChooseDest W: 2.7083497047424316
grad AddEdge W: 7.72279722810465e-12
grad ChooseDest W: 4.101319789886475
grad AddEdge W: 1.0230058467008973e-10
grad ChooseDest W: 1.7623499631881714
grad AddEdge W: 5.391270208043863e-10
grad ChooseDest W: 3.317272901535034
grad AddEdge W: 1.7888357461970372e-09
grad ChooseDest W: 3.401745080947876
grad AddEdge W: 1.2120104919688401e-10
grad ChooseDest W: 2.139430046081543
grad AddEdge W: 7.513983359130894e-12
grad ChooseDest W: 2.6037418842315674
grad AddEdge W: 1.7622987791288125e-10
grad ChooseDest W: 3.340954303741455
grad AddEdge W: 3.8267275632997144e-12
grad ChooseDest W: 3.9676194190979004
grad AddEdge W: 9.761295938215397e-11
grad ChooseDest W: 2.532360315322876
grad AddEdge W: 1.0063779809499351e-09
grad ChooseDest W: 1.9927430152893066
grad AddEdge W: 3.893729522835843e-12
grad ChooseDest W: 2.825469970703125
grad AddEdge W: 4.616672495683094e-12
grad ChooseDest W: 4.668472766876221
grad AddEdge W: 2.035394422450487e-12
grad ChooseDest W: 3.0305986404418945
grad AddEdge W: 3.4777764001958644e-11
grad ChooseDest W: 2.851841449737549
grad AddEdge W: 1.7557165573819922e-12
grad ChooseDest W: 5.817886829376221
grad AddEdge W: 8.17427514565594e-11
grad ChooseDest W: 4.18598747253418
grad AddEdge W: 1.7178937309139664e-12
grad ChooseDest W: 4.349512577056885
grad AddEdge W: 8.238831283868819e-10
grad ChooseDest W: 3.2166826725006104
grad AddEdge W: 2.219184125351603e-08
grad ChooseDest W: 2.12419056892395
grad AddEdge W: 1.6131463569449278e-12
grad ChooseDest W: 1.6125751733779907
grad AddEdge W: 1.601989049228314e-12
grad ChooseDest W: 3.8550798892974854
grad AddEdge W: 2.2383154357763502e-11
grad ChooseDest W: 1.673717975616455
grad AddEdge W: 1.785704180010117e-12
grad ChooseDest W: 3.1068475246429443
grad AddEdge W: 1.2466302946531616e-12
grad ChooseDest W: 3.914066791534424
grad AddEdge W: 2.1890191482382138e-12
grad ChooseDest W: 2.778987407684326
grad AddEdge W: 4.936619147320742e-13
grad ChooseDest W: 2.9577550888061523
grad AddEdge W: 2.5843649442691685e-10
grad ChooseDest W: 1.1482502222061157
grad AddEdge W: 1.1746421977459898e-12
grad ChooseDest W: 3.6444101333618164
grad AddEdge W: 1.0996825195244697e-12
grad ChooseDest W: 2.5813968181610107
grad AddEdge W: 1.3281155689104374e-11
grad ChooseDest W: 3.025664806365967
grad AddEdge W: 2.384635891528042e-11
grad ChooseDest W: 3.0356955528259277
grad AddEdge W: 1.3594667058747234e-11
grad ChooseDest W: 3.2467403411865234
grad AddEdge W: 3.663261371762011e-13
grad ChooseDest W: 3.072131872177124
grad AddEdge W: 4.750990182864068e-13
grad ChooseDest W: 2.373110294342041
grad AddEdge W: 7.976608956683506e-12
grad ChooseDest W: 3.163037061691284
grad AddEdge W: 9.932467175122195e-12
grad ChooseDest W: 3.3594400882720947
grad AddEdge W: 7.0229217406792e-12
grad ChooseDest W: 2.4961695671081543
grad AddEdge W: 8.225551316465296e-12
grad ChooseDest W: 1.7628270387649536
grad AddEdge W: 3.999371634647725e-13
grad ChooseDest W: 2.8399085998535156
grad AddEdge W: 2.965643952201197e-13
grad ChooseDest W: 2.4026846885681152
grad AddEdge W: 3.5190277883519194e-13
grad ChooseDest W: 5.433511734008789
grad AddEdge W: 5.518046956864975e-12
grad ChooseDest W: 3.384446859359741
grad AddEdge W: 4.700636039266237e-13
grad ChooseDest W: 2.611414909362793
grad AddEdge W: 1.211356275704345e-11
grad ChooseDest W: 4.439007759094238
grad AddEdge W: 5.060475693000055e-13
grad ChooseDest W: 1.7313992977142334
grad AddEdge W: 1.3035789624398636e-13
grad ChooseDest W: 2.1249265670776367
grad AddEdge W: 1.0085069037475344e-13
grad ChooseDest W: 3.1482691764831543
grad AddEdge W: 3.094569045458684e-09
grad ChooseDest W: 5.802530765533447
grad AddEdge W: 1.734276784681743e-13
grad ChooseDest W: 3.5019633769989014
grad AddEdge W: 1.9081005493085806e-13
grad ChooseDest W: 3.7568037509918213
grad AddEdge W: 3.023133176086046e-12
grad ChooseDest W: 2.775343894958496
=== Epoch 5: Train Loss: 6.0256, Train Log Prob: 0.0071 ===
Total mismatches: 88330
Predicted valid destination but wrong order: 7767
Epoch 5: Validation Loss: 5.5107, Validation Log Prob: 0.0073
Epoch 5: Edge Precision: 0.3661, Recall: 0.3656, F1: 0.3658, Jaccard: 0.2419
Epoch 5: TP: 2.5606299212598427, FP: 4.452541159627774, FN: 4.4608446671438795
Epoch 5: warmup, skipping learning rate scheduler
Epoch 5: Current Learning Rate: 6e-05
[Epoch 5] ‚è±Ô∏è Total: 4850.06s | Current time: 2025-07-14 18:13:46 | üèãÔ∏è Train: 4045.45s | ‚úÖ Val: 804.61s
grad AddEdge W: 8.814584478988863e-12
grad ChooseDest W: 5.926079750061035
grad AddEdge W: 4.395231262819578e-14
grad ChooseDest W: 3.2055680751800537
grad AddEdge W: 5.640657686485466e-14
grad ChooseDest W: 2.8250479698181152
grad AddEdge W: 1.0680491864187291e-13
grad ChooseDest W: 2.7826783657073975
grad AddEdge W: 1.5013282500878078e-12
grad ChooseDest W: 3.668004274368286
grad AddEdge W: 5.986726549613366e-14
grad ChooseDest W: 2.757585048675537
grad AddEdge W: 5.606745197782835e-14
grad ChooseDest W: 4.489027500152588
grad AddEdge W: 4.345711683844018e-14
grad ChooseDest W: 4.514737606048584
grad AddEdge W: 4.49797466525563e-14
grad ChooseDest W: 3.3989250659942627
grad AddEdge W: 1.830762971777311e-12
grad ChooseDest W: 2.015049457550049
grad AddEdge W: 5.683584299812777e-14
grad ChooseDest W: 3.6332006454467773
grad AddEdge W: 3.159464402884178e-14
grad ChooseDest W: 4.551417350769043
grad AddEdge W: 4.7043619000319836e-11
grad ChooseDest W: 1.5878558158874512
grad AddEdge W: 2.9136706936050416e-11
grad ChooseDest W: 2.9389755725860596
grad AddEdge W: 6.991869106257043e-13
grad ChooseDest W: 2.9210846424102783
grad AddEdge W: 3.368010351948586e-14
grad ChooseDest W: 2.0967159271240234
grad AddEdge W: 2.4924750848323574e-14
grad ChooseDest W: 3.055866241455078
grad AddEdge W: 1.6271827635305769e-12
grad ChooseDest W: 3.6864118576049805
grad AddEdge W: 3.756891309549329e-14
grad ChooseDest W: 3.0149409770965576
grad AddEdge W: 7.853888709091067e-14
grad ChooseDest W: 2.287520408630371
grad AddEdge W: 6.875767587666975e-14
grad ChooseDest W: 4.828329563140869
grad AddEdge W: 1.7054913525275245e-14
grad ChooseDest W: 3.520209312438965
grad AddEdge W: 2.2266903761374718e-14
grad ChooseDest W: 2.410672664642334
grad AddEdge W: 1.0279996255288526e-12
grad ChooseDest W: 3.8337912559509277
grad AddEdge W: 9.152162534775687e-13
grad ChooseDest W: 4.641730785369873
grad AddEdge W: 1.1519684505075217e-14
grad ChooseDest W: 2.9579555988311768
grad AddEdge W: 6.79057233620578e-13
grad ChooseDest W: 3.4858336448669434
grad AddEdge W: 1.500684884518655e-14
grad ChooseDest W: 2.498544216156006
grad AddEdge W: 2.1327156281779856e-14
grad ChooseDest W: 3.0015463829040527
grad AddEdge W: 1.6590182122500595e-14
grad ChooseDest W: 2.4456100463867188
grad AddEdge W: 1.859500238250017e-14
grad ChooseDest W: 3.7437925338745117
grad AddEdge W: 3.398720888059259e-11
grad ChooseDest W: 3.31943416595459
grad AddEdge W: 7.937081981240768e-13
grad ChooseDest W: 2.2586581707000732
grad AddEdge W: 1.994498655317513e-14
grad ChooseDest W: 3.6468632221221924
grad AddEdge W: 4.800527718938109e-14
grad ChooseDest W: 3.1457557678222656
grad AddEdge W: 1.8806059276033418e-14
grad ChooseDest W: 3.783660650253296
grad AddEdge W: 5.094850864979794e-13
grad ChooseDest W: 3.507594108581543
grad AddEdge W: 2.4035920213254063e-14
grad ChooseDest W: 5.326683044433594
grad AddEdge W: 6.23597344823288e-15
grad ChooseDest W: 2.2346394062042236
grad AddEdge W: 5.268564989661939e-13
grad ChooseDest W: 2.7137153148651123
grad AddEdge W: 5.459554791760468e-13
grad ChooseDest W: 3.7186319828033447
grad AddEdge W: 2.0963919754877006e-14
grad ChooseDest W: 1.776389241218567
grad AddEdge W: 6.282382383412943e-13
grad ChooseDest W: 3.1019387245178223
grad AddEdge W: 2.349359720437913e-14
grad ChooseDest W: 2.346734046936035
grad AddEdge W: 6.436003531333501e-13
grad ChooseDest W: 2.919741630554199
grad AddEdge W: 4.013087876331839e-13
grad ChooseDest W: 3.3234119415283203
grad AddEdge W: 7.067158273518781e-13
grad ChooseDest W: 3.0833470821380615
grad AddEdge W: 8.731952538961439e-15
grad ChooseDest W: 3.501004457473755
grad AddEdge W: 1.194306121826607e-14
grad ChooseDest W: 2.5086159706115723
grad AddEdge W: 5.839618068617658e-13
grad ChooseDest W: 2.274223804473877
grad AddEdge W: 1.7238545186042294e-14
grad ChooseDest W: 3.4621987342834473
grad AddEdge W: 5.518341339438848e-10
grad ChooseDest W: 1.613569736480713
grad AddEdge W: 9.753187840688682e-12
grad ChooseDest W: 4.900859832763672
grad AddEdge W: 1.297942635802244e-14
grad ChooseDest W: 4.230515480041504
grad AddEdge W: 1.0860714894968e-14
grad ChooseDest W: 4.343123435974121
grad AddEdge W: 7.176051270677171e-15
grad ChooseDest W: 2.781766176223755
grad AddEdge W: 1.9783799571444424e-14
grad ChooseDest W: 3.054506540298462
grad AddEdge W: 2.0700098807367034e-13
grad ChooseDest W: 2.2834596633911133
grad AddEdge W: 2.3622003333424735e-13
grad ChooseDest W: 2.910536050796509
grad AddEdge W: 7.232182450025819e-15
grad ChooseDest W: 4.968314170837402
grad AddEdge W: 4.574998691518617e-13
grad ChooseDest W: 2.76204252243042
grad AddEdge W: 9.084757008085906e-15
grad ChooseDest W: 1.6548595428466797
grad AddEdge W: 1.0756359588833322e-14
grad ChooseDest W: 3.286890983581543
grad AddEdge W: 6.086353643297571e-12
grad ChooseDest W: 1.1902713775634766
grad AddEdge W: 2.0263460776947378e-13
grad ChooseDest W: 4.124063491821289
grad AddEdge W: 4.951120624596683e-10
grad ChooseDest W: 2.6584672927856445
=== Epoch 6: Train Loss: 6.0148, Train Log Prob: 0.0072 ===
Total mismatches: 87999
Predicted valid destination but wrong order: 7794
Epoch 6: Validation Loss: 5.5316, Validation Log Prob: 0.0072
Epoch 6: Edge Precision: 0.3670, Recall: 0.3664, F1: 0.3667, Jaccard: 0.2423
Epoch 6: TP: 2.5662133142448105, FP: 4.446098783106657, FN: 4.455261274158912
Epoch 6: Current Learning Rate: 6e-05
[Epoch 6] ‚è±Ô∏è Total: 4834.34s | Current time: 2025-07-14 19:34:20 | üèãÔ∏è Train: 4046.41s | ‚úÖ Val: 787.93s
grad AddEdge W: 3.963578597276918e-13
grad ChooseDest W: 5.439125061035156
grad AddEdge W: 3.8598288519368917e-13
grad ChooseDest W: 3.8397891521453857
grad AddEdge W: 1.2750879081161338e-14
grad ChooseDest W: 2.6806249618530273
grad AddEdge W: 8.712858722264433e-15
grad ChooseDest W: 4.93369197845459
grad AddEdge W: 3.647709846850422e-13
grad ChooseDest W: 1.9430781602859497
grad AddEdge W: 1.995266744794083e-13
grad ChooseDest W: 2.1419248580932617
grad AddEdge W: 2.827655915405708e-13
grad ChooseDest W: 1.776611328125
grad AddEdge W: 6.99796089945106e-15
grad ChooseDest W: 3.3222570419311523
grad AddEdge W: 8.53615070880824e-15
grad ChooseDest W: 4.2996344566345215
grad AddEdge W: 8.083664861352046e-15
grad ChooseDest W: 1.7411394119262695
grad AddEdge W: 2.384081701587576e-13
grad ChooseDest W: 2.413555383682251
grad AddEdge W: 1.2865705408589972e-13
grad ChooseDest W: 3.4928250312805176
grad AddEdge W: 1.43477657326644e-14
grad ChooseDest W: 4.672165393829346
grad AddEdge W: 4.637676063356166e-15
grad ChooseDest W: 3.1179556846618652
grad AddEdge W: 5.327962599105743e-15
grad ChooseDest W: 3.9777581691741943
grad AddEdge W: 3.592900309950034e-15
grad ChooseDest W: 2.6478431224823
grad AddEdge W: 4.67691698222009e-15
grad ChooseDest W: 5.011384963989258
grad AddEdge W: 2.9556232136219995e-13
grad ChooseDest W: 4.142108917236328
grad AddEdge W: 4.2245506680534117e-13
grad ChooseDest W: 2.7462825775146484
grad AddEdge W: 2.0474015549349495e-13
grad ChooseDest W: 3.583022356033325
grad AddEdge W: 1.9993989103239684e-13
grad ChooseDest W: 3.707031488418579
grad AddEdge W: 2.0964167088497604e-13
grad ChooseDest W: 2.5980656147003174
grad AddEdge W: 5.87546224983782e-15
grad ChooseDest W: 2.180227518081665
grad AddEdge W: 1.9561223029828517e-13
grad ChooseDest W: 2.906806707382202
grad AddEdge W: 3.683569257722976e-15
grad ChooseDest W: 2.7731146812438965
grad AddEdge W: 1.6235496562606866e-13
grad ChooseDest W: 3.724902629852295
grad AddEdge W: 7.669125242899897e-15
grad ChooseDest W: 2.834300994873047
grad AddEdge W: 4.961958392047739e-15
grad ChooseDest W: 3.55403208732605
grad AddEdge W: 5.423598548081384e-15
grad ChooseDest W: 3.543156623840332
grad AddEdge W: 9.86027665766942e-15
grad ChooseDest W: 2.647982597351074
grad AddEdge W: 4.1369173847194754e-15
grad ChooseDest W: 4.857810020446777
grad AddEdge W: 2.4264371436578946e-13
grad ChooseDest W: 4.164114475250244
grad AddEdge W: 7.739433226239267e-12
grad ChooseDest W: 2.4447813034057617
grad AddEdge W: 1.9320100532425888e-13
grad ChooseDest W: 2.966189384460449
grad AddEdge W: 3.236782671452382e-15
grad ChooseDest W: 4.061486721038818
grad AddEdge W: 2.1227288047979964e-13
grad ChooseDest W: 3.496598720550537
grad AddEdge W: 1.727150594852539e-13
grad ChooseDest W: 2.960132598876953
grad AddEdge W: 5.0997511708082265e-15
grad ChooseDest W: 2.793043375015259
grad AddEdge W: 3.660368015634408e-12
grad ChooseDest W: 3.549111843109131
grad AddEdge W: 1.9558887929399527e-13
grad ChooseDest W: 2.7954013347625732
grad AddEdge W: 9.924013769367938e-16
grad ChooseDest W: 6.577149391174316
grad AddEdge W: 1.343289493310404e-13
grad ChooseDest W: 3.6111481189727783
grad AddEdge W: 4.9180803913140714e-15
grad ChooseDest W: 2.6883270740509033
grad AddEdge W: 3.1750661568495393e-15
grad ChooseDest W: 3.2029335498809814
grad AddEdge W: 5.784012591797394e-14
grad ChooseDest W: 2.042989492416382
grad AddEdge W: 1.0259587640169474e-13
grad ChooseDest W: 2.5879149436950684
grad AddEdge W: 4.356534393217676e-15
grad ChooseDest W: 7.260787487030029
grad AddEdge W: 4.3958117344983314e-15
grad ChooseDest W: 2.5229766368865967
grad AddEdge W: 4.821652466532747e-15
grad ChooseDest W: 3.608959197998047
grad AddEdge W: 3.407066295757488e-13
grad ChooseDest W: 2.2152369022369385
grad AddEdge W: 1.0148448108347058e-13
grad ChooseDest W: 3.86600661277771
grad AddEdge W: 5.695488924369963e-15
grad ChooseDest W: 4.088863849639893
grad AddEdge W: 1.9335141127063693e-13
grad ChooseDest W: 2.9512858390808105
grad AddEdge W: 1.4258525729473476e-13
grad ChooseDest W: 3.6018621921539307
grad AddEdge W: 3.2498894475199583e-15
grad ChooseDest W: 4.776479721069336
grad AddEdge W: 2.7853683206744643e-15
grad ChooseDest W: 3.0741844177246094
grad AddEdge W: 2.4504472581653777e-15
grad ChooseDest W: 9.050457954406738
grad AddEdge W: 7.51366590473479e-12
grad ChooseDest W: 2.0387377738952637
grad AddEdge W: 3.3114312073673213e-12
grad ChooseDest W: 3.0578293800354004
grad AddEdge W: 4.362328522093369e-15
grad ChooseDest W: 4.363804817199707
grad AddEdge W: 7.809229066353673e-14
grad ChooseDest W: 4.066810607910156
grad AddEdge W: 2.9707308932692272e-15
grad ChooseDest W: 2.6068499088287354
grad AddEdge W: 3.0087213797447667e-15
grad ChooseDest W: 2.9185612201690674
grad AddEdge W: 2.5841154099892107e-15
grad ChooseDest W: 3.5491156578063965
grad AddEdge W: 1.6370893085159571e-13
grad ChooseDest W: 3.03464412689209
grad AddEdge W: 8.837950580794021e-14
grad ChooseDest W: 3.3960189819335938
=== Epoch 7: Train Loss: 6.0058, Train Log Prob: 0.0073 ===
Total mismatches: 87872
Predicted valid destination but wrong order: 7741
Epoch 7: Validation Loss: 5.4096, Validation Log Prob: 0.0080
Epoch 7: Edge Precision: 0.3676, Recall: 0.3670, F1: 0.3673, Jaccard: 0.2427
Epoch 7: TP: 2.569935576234789, FP: 4.442233357193987, FN: 4.451539012168934
Epoch 7: Current Learning Rate: 6e-05
[Epoch 7] ‚è±Ô∏è Total: 4834.29s | Current time: 2025-07-14 20:54:55 | üèãÔ∏è Train: 4043.56s | ‚úÖ Val: 790.73s
grad AddEdge W: 1.4332522527745611e-13
grad ChooseDest W: 6.033319473266602
grad AddEdge W: 2.7728642085488595e-15
grad ChooseDest W: 9.503841400146484
grad AddEdge W: 2.0163909418173054e-15
grad ChooseDest W: 2.669811725616455
grad AddEdge W: 9.313593461995201e-14
grad ChooseDest W: 2.715675115585327
grad AddEdge W: 2.4023436218160955e-15
grad ChooseDest W: 4.9099297523498535
grad AddEdge W: 3.0571058075160636e-15
grad ChooseDest W: 3.497507095336914
grad AddEdge W: 9.965966040696023e-14
grad ChooseDest W: 2.4357750415802
grad AddEdge W: 2.6138257258888657e-15
grad ChooseDest W: 4.37957763671875
grad AddEdge W: 9.881605624907641e-14
grad ChooseDest W: 3.3486993312835693
grad AddEdge W: 3.3892848126166513e-15
grad ChooseDest W: 2.827136754989624
grad AddEdge W: 5.513438250598964e-15
grad ChooseDest W: 3.1842808723449707
grad AddEdge W: 7.086648743039492e-16
grad ChooseDest W: 4.125411033630371
grad AddEdge W: 1.3054315929020982e-13
grad ChooseDest W: 5.6168293952941895
grad AddEdge W: 1.0685260278515507e-15
grad ChooseDest W: 3.835645914077759
grad AddEdge W: 3.5268044232516496e-15
grad ChooseDest W: 3.0167860984802246
grad AddEdge W: 2.928993581962497e-13
grad ChooseDest W: 2.571629285812378
grad AddEdge W: 8.037913859013763e-16
grad ChooseDest W: 4.638561725616455
grad AddEdge W: 1.7102188128127402e-15
grad ChooseDest W: 4.29537296295166
grad AddEdge W: 1.7461631853250685e-15
grad ChooseDest W: 4.298026084899902
grad AddEdge W: 1.7450724567637244e-13
grad ChooseDest W: 3.144717216491699
grad AddEdge W: 1.7481246214045426e-13
grad ChooseDest W: 5.142242908477783
grad AddEdge W: 3.471299836525733e-15
grad ChooseDest W: 3.30145525932312
grad AddEdge W: 2.7628978071332286e-15
grad ChooseDest W: 4.1889119148254395
grad AddEdge W: 9.710514456331282e-14
grad ChooseDest W: 4.006721019744873
grad AddEdge W: 2.4677064014986313e-15
grad ChooseDest W: 2.2683236598968506
grad AddEdge W: 1.4414772815055793e-15
grad ChooseDest W: 2.562913179397583
grad AddEdge W: 1.356325533656555e-13
grad ChooseDest W: 3.1221680641174316
grad AddEdge W: 1.8173823719807441e-13
grad ChooseDest W: 3.2984352111816406
grad AddEdge W: 2.5939376040455715e-15
grad ChooseDest W: 4.073328018188477
grad AddEdge W: 2.849974699935102e-15
grad ChooseDest W: 2.726649522781372
grad AddEdge W: 1.6404874607047404e-15
grad ChooseDest W: 4.199990749359131
grad AddEdge W: 2.1429857332568546e-15
grad ChooseDest W: 3.4850521087646484
grad AddEdge W: 1.3571004121021974e-15
grad ChooseDest W: 2.8895864486694336
grad AddEdge W: 5.2905796470116215e-14
grad ChooseDest W: 2.71966814994812
grad AddEdge W: 1.2150931150335274e-15
grad ChooseDest W: 2.332058906555176
grad AddEdge W: 1.361627282280009e-13
grad ChooseDest W: 2.7701632976531982
grad AddEdge W: 2.1140839103053538e-15
grad ChooseDest W: 6.206937789916992
grad AddEdge W: 1.3451972825581637e-13
grad ChooseDest W: 2.629019260406494
grad AddEdge W: 5.82495409870952e-14
grad ChooseDest W: 2.6160504817962646
grad AddEdge W: 1.6980929008980844e-15
grad ChooseDest W: 2.9035236835479736
grad AddEdge W: 6.975417964592834e-14
grad ChooseDest W: 3.05366849899292
grad AddEdge W: 1.4494267873595146e-13
grad ChooseDest W: 2.4275403022766113
grad AddEdge W: 3.1515730627827308e-15
grad ChooseDest W: 2.7440624237060547
grad AddEdge W: 1.9769266180135435e-15
grad ChooseDest W: 3.1412253379821777
grad AddEdge W: 3.9247330903334314e-15
grad ChooseDest W: 3.1497745513916016
grad AddEdge W: 7.489703166474113e-14
grad ChooseDest W: 1.7944276332855225
grad AddEdge W: 7.210577350046793e-14
grad ChooseDest W: 2.3084418773651123
grad AddEdge W: 4.514733889743414e-15
grad ChooseDest W: 1.7825850248336792
grad AddEdge W: 3.2223562180529837e-15
grad ChooseDest W: 4.532991886138916
grad AddEdge W: 2.8765364656208048e-12
grad ChooseDest W: 3.987041473388672
grad AddEdge W: 1.5998857834110527e-15
grad ChooseDest W: 4.84152364730835
grad AddEdge W: 2.4876228989456585e-15
grad ChooseDest W: 1.7327595949172974
grad AddEdge W: 7.677309444642857e-14
grad ChooseDest W: 3.738539457321167
grad AddEdge W: 1.130117050288093e-13
grad ChooseDest W: 3.1271276473999023
grad AddEdge W: 1.7411511685563874e-13
grad ChooseDest W: 5.880871295928955
grad AddEdge W: 9.852274313900235e-16
grad ChooseDest W: 4.817800045013428
grad AddEdge W: 2.1446408356357895e-15
grad ChooseDest W: 4.504984378814697
grad AddEdge W: 6.43613715925126e-14
grad ChooseDest W: 3.3347327709198
grad AddEdge W: 1.2080155194845073e-15
grad ChooseDest W: 2.957714557647705
grad AddEdge W: 9.783861410926284e-14
grad ChooseDest W: 3.5645298957824707
grad AddEdge W: 1.4454346194351514e-15
grad ChooseDest W: 3.9914751052856445
grad AddEdge W: 2.2336119058659603e-15
grad ChooseDest W: 3.610790491104126
grad AddEdge W: 2.7108749493370008e-15
grad ChooseDest W: 4.276743412017822
grad AddEdge W: 2.1098279932618745e-15
grad ChooseDest W: 3.1732208728790283
grad AddEdge W: 2.5109578045273695e-12
grad ChooseDest W: 3.005542755126953
grad AddEdge W: 2.022423086951177e-15
grad ChooseDest W: 2.9244701862335205
=== Epoch 8: Train Loss: 5.9968, Train Log Prob: 0.0074 ===
Total mismatches: 87498
Predicted valid destination but wrong order: 7753
Epoch 8: Validation Loss: 5.4319, Validation Log Prob: 0.0080
Epoch 8: Edge Precision: 0.3661, Recall: 0.3655, F1: 0.3658, Jaccard: 0.2410
Epoch 8: TP: 2.559914101646385, FP: 4.450536864710093, FN: 4.461560486757337
Epoch 8: Current Learning Rate: 6e-05
[Epoch 8] ‚è±Ô∏è Total: 4868.82s | Current time: 2025-07-14 22:16:03 | üèãÔ∏è Train: 4070.49s | ‚úÖ Val: 798.33s
grad AddEdge W: 1.3208013242144157e-12
grad ChooseDest W: 8.27273941040039
grad AddEdge W: 6.484831389323015e-14
grad ChooseDest W: 2.9973740577697754
grad AddEdge W: 1.6564956422540178e-15
grad ChooseDest W: 2.961165428161621
grad AddEdge W: 4.8961485907272895e-14
grad ChooseDest W: 2.018287181854248
grad AddEdge W: 5.832869343774805e-12
grad ChooseDest W: 5.170749664306641
grad AddEdge W: 1.464705995413726e-15
grad ChooseDest W: 4.026567459106445
grad AddEdge W: 2.2182679040264486e-15
grad ChooseDest W: 6.627905368804932
grad AddEdge W: 6.918779920728549e-14
grad ChooseDest W: 2.582087278366089
grad AddEdge W: 5.1974127990772265e-14
grad ChooseDest W: 3.0506272315979004
grad AddEdge W: 2.346190516172575e-12
grad ChooseDest W: 2.4773921966552734
grad AddEdge W: 1.6539362934775426e-10
grad ChooseDest W: 2.547074317932129
grad AddEdge W: 6.277475690956089e-14
grad ChooseDest W: 3.1075406074523926
grad AddEdge W: 2.571078048271662e-14
grad ChooseDest W: 4.055631637573242
grad AddEdge W: 5.5700832401334166e-14
grad ChooseDest W: 2.6987345218658447
grad AddEdge W: 1.285058142355851e-15
grad ChooseDest W: 3.2614023685455322
grad AddEdge W: 1.9638988277682987e-15
grad ChooseDest W: 3.593766689300537
grad AddEdge W: 1.2097668786875682e-13
grad ChooseDest W: 3.592461347579956
grad AddEdge W: 3.647112126192731e-12
grad ChooseDest W: 2.0390331745147705
grad AddEdge W: 1.5026092374956983e-15
grad ChooseDest W: 2.679564952850342
grad AddEdge W: 5.0972159164937997e-14
grad ChooseDest W: 3.7938568592071533
grad AddEdge W: 1.2519943171388978e-15
grad ChooseDest W: 2.703394651412964
grad AddEdge W: 1.238686476625467e-15
grad ChooseDest W: 4.2528862953186035
grad AddEdge W: 5.536365569008654e-14
grad ChooseDest W: 10.94527816772461
grad AddEdge W: 1.3771744576774139e-15
grad ChooseDest W: 2.7280640602111816
grad AddEdge W: 4.632373044183291e-14
grad ChooseDest W: 5.484472274780273
grad AddEdge W: 3.5424215508650034e-14
grad ChooseDest W: 3.512289047241211
grad AddEdge W: 4.686163363278907e-14
grad ChooseDest W: 7.216955184936523
grad AddEdge W: 5.951944666293674e-14
grad ChooseDest W: 4.562983989715576
grad AddEdge W: 2.226857029869844e-15
grad ChooseDest W: 2.7732276916503906
grad AddEdge W: 8.240958773778051e-16
grad ChooseDest W: 4.474879264831543
grad AddEdge W: 2.0657308227531052e-15
grad ChooseDest W: 1.9455840587615967
grad AddEdge W: 7.743571815667025e-14
grad ChooseDest W: 2.8706905841827393
grad AddEdge W: 1.7154680877451119e-15
grad ChooseDest W: 4.8053460121154785
grad AddEdge W: 1.156895704689171e-15
grad ChooseDest W: 3.5788910388946533
grad AddEdge W: 1.7870702747474487e-12
grad ChooseDest W: 2.0791282653808594
grad AddEdge W: 2.023429362092515e-15
grad ChooseDest W: 5.211579322814941
grad AddEdge W: 1.196764804362602e-15
grad ChooseDest W: 5.568172454833984
grad AddEdge W: 2.115171969865881e-12
grad ChooseDest W: 5.322598934173584
grad AddEdge W: 2.1664762862248213e-15
grad ChooseDest W: 4.067601203918457
grad AddEdge W: 1.434970162646535e-15
grad ChooseDest W: 3.2441751956939697
grad AddEdge W: 4.9802250810717513e-14
grad ChooseDest W: 3.1270036697387695
grad AddEdge W: 1.399641688965979e-15
grad ChooseDest W: 2.1827950477600098
grad AddEdge W: 3.5823361150318783e-14
grad ChooseDest W: 3.287409543991089
grad AddEdge W: 2.701846992664292e-12
grad ChooseDest W: 2.7979090213775635
grad AddEdge W: 1.0605121432584593e-15
grad ChooseDest W: 2.3541178703308105
grad AddEdge W: 3.3778230592359376e-14
grad ChooseDest W: 2.9322853088378906
grad AddEdge W: 5.465338214139708e-16
grad ChooseDest W: 3.364577054977417
grad AddEdge W: 2.35504699610731e-15
grad ChooseDest W: 4.589076519012451
grad AddEdge W: 4.971118841614057e-16
grad ChooseDest W: 2.857423782348633
grad AddEdge W: 1.6554526270585925e-15
grad ChooseDest W: 6.803930759429932
grad AddEdge W: 2.316750347119978e-09
grad ChooseDest W: 1.1745067834854126
grad AddEdge W: 4.3276600564853135e-14
grad ChooseDest W: 3.8495218753814697
grad AddEdge W: 7.479581537461755e-11
grad ChooseDest W: 1.651490569114685
grad AddEdge W: 4.88436519776068e-16
grad ChooseDest W: 2.632535457611084
grad AddEdge W: 1.3058969866846376e-13
grad ChooseDest W: 2.8637144565582275
grad AddEdge W: 3.112639066307245e-14
grad ChooseDest W: 3.1221108436584473
grad AddEdge W: 1.6285338141157324e-15
grad ChooseDest W: 3.023061752319336
grad AddEdge W: 5.554985047255198e-14
grad ChooseDest W: 2.683953285217285
grad AddEdge W: 5.961931925895631e-16
grad ChooseDest W: 2.805387020111084
grad AddEdge W: 1.980098160089816e-12
grad ChooseDest W: 6.829017162322998
grad AddEdge W: 1.3841735967996946e-15
grad ChooseDest W: 5.0326924324035645
grad AddEdge W: 1.2519710237328483e-15
grad ChooseDest W: 3.486926555633545
grad AddEdge W: 6.289406510007168e-16
grad ChooseDest W: 3.443296194076538
grad AddEdge W: 1.5728260453615932e-15
grad ChooseDest W: 2.892622232437134
grad AddEdge W: 6.092078008845662e-16
grad ChooseDest W: 2.5786895751953125
grad AddEdge W: 8.532560453782185e-16
grad ChooseDest W: 3.497742176055908
=== Epoch 9: Train Loss: 5.9864, Train Log Prob: 0.0074 ===
Total mismatches: 87151
Predicted valid destination but wrong order: 7715
Epoch 9: Validation Loss: 5.3694, Validation Log Prob: 0.0083
Epoch 9: Edge Precision: 0.3671, Recall: 0.3663, F1: 0.3667, Jaccard: 0.2418
Epoch 9: TP: 2.565926986399427, FP: 4.441947029348604, FN: 4.455547602004295
Epoch 9: Current Learning Rate: 6e-05
[Epoch 9] ‚è±Ô∏è Total: 4860.16s | Current time: 2025-07-14 23:37:04 | üèãÔ∏è Train: 4068.09s | ‚úÖ Val: 792.07s
grad AddEdge W: 2.377423073945256e-13
grad ChooseDest W: 7.8664445877075195
grad AddEdge W: 5.5451655637042685e-14
grad ChooseDest W: 3.7410004138946533
grad AddEdge W: 5.374195351432777e-16
grad ChooseDest W: 2.964179039001465
grad AddEdge W: 9.330644955201435e-16
grad ChooseDest W: 3.734835147857666
grad AddEdge W: 1.9355846339239556e-14
grad ChooseDest W: 2.1302244663238525
grad AddEdge W: 1.6802733393911037e-15
grad ChooseDest W: 3.8217875957489014
grad AddEdge W: 9.962037078370184e-16
grad ChooseDest W: 3.61468243598938
grad AddEdge W: 5.452877830090005e-16
grad ChooseDest W: 2.9242265224456787
grad AddEdge W: 5.7818204705299e-14
grad ChooseDest W: 2.9013638496398926
grad AddEdge W: 7.352825140945012e-16
grad ChooseDest W: 2.2503459453582764
grad AddEdge W: 1.3964710328861693e-14
grad ChooseDest W: 2.1912922859191895
grad AddEdge W: 4.639664981419614e-14
grad ChooseDest W: 2.4508917331695557
grad AddEdge W: 7.424291957682818e-16
grad ChooseDest W: 3.8609654903411865
grad AddEdge W: 4.693355032990099e-16
grad ChooseDest W: 5.209570407867432
grad AddEdge W: 6.645636189787918e-14
grad ChooseDest W: 4.448760509490967
grad AddEdge W: 9.281872798098533e-16
grad ChooseDest W: 4.138631820678711
grad AddEdge W: 6.224397790096585e-16
grad ChooseDest W: 4.381975173950195
grad AddEdge W: 7.8636691232473045e-16
grad ChooseDest W: 4.032930850982666
grad AddEdge W: 1.0392659099633613e-15
grad ChooseDest W: 3.78611421585083
grad AddEdge W: 1.4500052092185356e-15
grad ChooseDest W: 3.32100772857666
grad AddEdge W: 1.4364753401938058e-15
grad ChooseDest W: 2.2566959857940674
grad AddEdge W: 9.259463482687736e-16
grad ChooseDest W: 4.63427209854126
grad AddEdge W: 5.690401750367872e-16
grad ChooseDest W: 2.7002506256103516
grad AddEdge W: 1.1930696231302051e-15
grad ChooseDest W: 4.411503314971924
grad AddEdge W: 7.101575581152481e-16
grad ChooseDest W: 3.913865566253662
grad AddEdge W: 6.664213824606868e-16
grad ChooseDest W: 3.5132205486297607
grad AddEdge W: 1.1878606822127158e-12
grad ChooseDest W: 2.902209520339966
grad AddEdge W: 1.0105740453036612e-15
grad ChooseDest W: 2.03633451461792
grad AddEdge W: 3.1453386038294076e-14
grad ChooseDest W: 3.3434088230133057
grad AddEdge W: 6.140858106882445e-16
grad ChooseDest W: 3.2618868350982666
grad AddEdge W: 2.896153708021633e-14
grad ChooseDest W: 2.6999945640563965
grad AddEdge W: 7.329110929266136e-11
grad ChooseDest W: 1.418817400932312
grad AddEdge W: 3.0534439993817886e-14
grad ChooseDest W: 5.290220260620117
grad AddEdge W: 6.634268562943468e-16
grad ChooseDest W: 2.4169187545776367
grad AddEdge W: 2.723986638195671e-14
grad ChooseDest W: 3.1177749633789062
grad AddEdge W: 6.787391558082251e-14
grad ChooseDest W: 3.577609062194824
grad AddEdge W: 2.769908529431063e-14
grad ChooseDest W: 2.5995566844940186
grad AddEdge W: 6.256917502524045e-16
grad ChooseDest W: 4.107565879821777
grad AddEdge W: 2.850167611688839e-14
grad ChooseDest W: 3.503851890563965
grad AddEdge W: 3.4895245444792973e-16
grad ChooseDest W: 3.1615116596221924
grad AddEdge W: 7.764029461288229e-16
grad ChooseDest W: 2.331132411956787
grad AddEdge W: 1.3350465328918924e-15
grad ChooseDest W: 6.459603786468506
grad AddEdge W: 1.1277848229948595e-12
grad ChooseDest W: 4.019299507141113
grad AddEdge W: 1.8965376856049532e-15
grad ChooseDest W: 2.036872148513794
grad AddEdge W: 1.3061332614430765e-12
grad ChooseDest W: 3.056401252746582
grad AddEdge W: 5.18813927061245e-16
grad ChooseDest W: 2.9366164207458496
grad AddEdge W: 1.29531242909443e-15
grad ChooseDest W: 3.1990652084350586
grad AddEdge W: 5.578693986492791e-16
grad ChooseDest W: 5.247095584869385
grad AddEdge W: 5.885408322462717e-16
grad ChooseDest W: 2.6536049842834473
grad AddEdge W: 6.694912416197732e-16
grad ChooseDest W: 3.7268550395965576
grad AddEdge W: 8.419153330056675e-16
grad ChooseDest W: 3.5073328018188477
grad AddEdge W: 2.590349403074413e-14
grad ChooseDest W: 3.6108176708221436
grad AddEdge W: 8.889312202319975e-16
grad ChooseDest W: 5.428780555725098
grad AddEdge W: 4.665807933358613e-16
grad ChooseDest W: 3.4737040996551514
grad AddEdge W: 4.136378195308989e-16
grad ChooseDest W: 2.8388500213623047
grad AddEdge W: 8.793950056744534e-16
grad ChooseDest W: 5.598798751831055
grad AddEdge W: 2.603104025194168e-14
grad ChooseDest W: 2.566779136657715
grad AddEdge W: 5.917189210401935e-14
grad ChooseDest W: 2.0418355464935303
grad AddEdge W: 8.478589102569917e-16
grad ChooseDest W: 2.7285103797912598
grad AddEdge W: 6.245688492621413e-16
grad ChooseDest W: 3.5326898097991943
grad AddEdge W: 5.913107189222527e-14
grad ChooseDest W: 4.245080471038818
grad AddEdge W: 1.338119039028939e-15
grad ChooseDest W: 4.356547832489014
grad AddEdge W: 2.4781119894974135e-15
grad ChooseDest W: 4.61365270614624
grad AddEdge W: 2.3335960984763315e-14
grad ChooseDest W: 3.109525203704834
grad AddEdge W: 2.0124593113376825e-14
grad ChooseDest W: 3.4872751235961914
grad AddEdge W: 1.567018363958744e-16
grad ChooseDest W: 3.9936819076538086
=== Epoch 10: Train Loss: 5.9714, Train Log Prob: 0.0075 ===
Total mismatches: 86709
Predicted valid destination but wrong order: 7728
Epoch 10: Validation Loss: 5.3244, Validation Log Prob: 0.0085
Epoch 10: Edge Precision: 0.3640, Recall: 0.3632, F1: 0.3636, Jaccard: 0.2398
Epoch 10: TP: 2.54201861130995, FP: 4.4642806012884755, FN: 4.479455977093773
Epoch 10: Current Learning Rate: 6e-05
[Epoch 10] ‚è±Ô∏è Total: 4851.96s | Current time: 2025-07-15 00:57:55 | üèãÔ∏è Train: 4040.22s | ‚úÖ Val: 811.74s
grad AddEdge W: 4.4044130999678147e-14
grad ChooseDest W: 9.854386329650879
grad AddEdge W: 4.925929252713214e-14
grad ChooseDest W: 3.821399688720703
grad AddEdge W: 8.817717271848897e-16
grad ChooseDest W: 5.119166851043701
grad AddEdge W: 8.468707506092965e-13
grad ChooseDest W: 1.7098231315612793
grad AddEdge W: 6.332905887013413e-16
grad ChooseDest W: 6.480667591094971
grad AddEdge W: 4.111453202632759e-14
grad ChooseDest W: 3.227004289627075
grad AddEdge W: 2.3115370311892246e-15
grad ChooseDest W: 4.506748676300049
grad AddEdge W: 4.816657089726315e-16
grad ChooseDest W: 4.594159126281738
grad AddEdge W: 1.378258184664416e-12
grad ChooseDest W: 2.552279233932495
grad AddEdge W: 1.2417486743715456e-12
grad ChooseDest W: 5.077980995178223
grad AddEdge W: 5.061309851046511e-16
grad ChooseDest W: 3.49334979057312
grad AddEdge W: 4.3465217755668604e-16
grad ChooseDest W: 2.6190342903137207
grad AddEdge W: 6.113980713850706e-14
grad ChooseDest W: 3.6232094764709473
grad AddEdge W: 4.051992167361865e-14
grad ChooseDest W: 2.8036234378814697
grad AddEdge W: 5.83345627662088e-14
grad ChooseDest W: 2.840726375579834
grad AddEdge W: 7.884905298026154e-16
grad ChooseDest W: 2.656393527984619
grad AddEdge W: 8.555852801040494e-16
grad ChooseDest W: 4.271676063537598
grad AddEdge W: 2.3864087721442266e-14
grad ChooseDest W: 2.9052085876464844
grad AddEdge W: 5.225499246937877e-16
grad ChooseDest W: 3.809731960296631
grad AddEdge W: 3.301535110956862e-16
grad ChooseDest W: 2.7910778522491455
grad AddEdge W: 1.6650055899986027e-12
grad ChooseDest W: 3.5912787914276123
grad AddEdge W: 2.246104265409422e-16
grad ChooseDest W: 4.207600116729736
grad AddEdge W: 6.180534464208728e-14
grad ChooseDest W: 5.205784797668457
grad AddEdge W: 4.0073229319763435e-16
grad ChooseDest W: 4.155460357666016
grad AddEdge W: 4.096278861492582e-16
grad ChooseDest W: 3.8962881565093994
grad AddEdge W: 7.447242315388673e-16
grad ChooseDest W: 2.1223647594451904
grad AddEdge W: 6.133871143858781e-16
grad ChooseDest W: 6.125091552734375
grad AddEdge W: 1.3201057187352738e-15
grad ChooseDest W: 4.983784198760986
grad AddEdge W: 6.629413475968925e-16
grad ChooseDest W: 2.0564095973968506
grad AddEdge W: 1.042909739823331e-15
grad ChooseDest W: 4.798956871032715
grad AddEdge W: 5.157659848796688e-16
grad ChooseDest W: 3.9371771812438965
grad AddEdge W: 2.608527068935669e-14
grad ChooseDest W: 4.241479873657227
grad AddEdge W: 7.798419528342346e-16
grad ChooseDest W: 4.485342502593994
grad AddEdge W: 1.3014388828766488e-12
grad ChooseDest W: 1.9244376420974731
grad AddEdge W: 4.572761364102728e-16
grad ChooseDest W: 3.7120604515075684
grad AddEdge W: 1.615463622248181e-12
grad ChooseDest W: 1.6141979694366455
grad AddEdge W: 3.9075351027789695e-14
grad ChooseDest W: 3.630297899246216
grad AddEdge W: 2.61242613099847e-14
grad ChooseDest W: 3.8771519660949707
grad AddEdge W: 1.6661517543252458e-14
grad ChooseDest W: 2.5395004749298096
grad AddEdge W: 1.1535677122394088e-15
grad ChooseDest W: 7.93883752822876
grad AddEdge W: 4.058368631388795e-14
grad ChooseDest W: 2.800611972808838
grad AddEdge W: 7.104736602232516e-16
grad ChooseDest W: 4.096675872802734
grad AddEdge W: 3.885623355697214e-16
grad ChooseDest W: 4.662827491760254
grad AddEdge W: 5.239341353482788e-16
grad ChooseDest W: 4.114757061004639
grad AddEdge W: 6.05900348983775e-16
grad ChooseDest W: 2.5405240058898926
grad AddEdge W: 3.0105573236579404e-16
grad ChooseDest W: 4.801411151885986
grad AddEdge W: 8.033327175604381e-16
grad ChooseDest W: 3.0395030975341797
grad AddEdge W: 2.7552475749602172e-14
grad ChooseDest W: 2.8409852981567383
grad AddEdge W: 2.689914060266009e-14
grad ChooseDest W: 3.888763189315796
grad AddEdge W: 9.47681213695319e-16
grad ChooseDest W: 4.226055145263672
grad AddEdge W: 2.8196459652806566e-14
grad ChooseDest W: 5.0527448654174805
grad AddEdge W: 2.669384353316817e-14
grad ChooseDest W: 3.4776692390441895
grad AddEdge W: 1.6853543300526797e-14
grad ChooseDest W: 5.806988716125488
grad AddEdge W: 3.3009175709987544e-16
grad ChooseDest W: 3.2310750484466553
grad AddEdge W: 2.957441962766344e-14
grad ChooseDest W: 2.4440207481384277
grad AddEdge W: 1.040031775978445e-12
grad ChooseDest W: 3.575517416000366
grad AddEdge W: 3.459275102696627e-14
grad ChooseDest W: 2.606419563293457
grad AddEdge W: 3.359699603483306e-14
grad ChooseDest W: 2.854475975036621
grad AddEdge W: 4.1359522965551975e-16
grad ChooseDest W: 3.0509026050567627
grad AddEdge W: 4.0051415574393677e-16
grad ChooseDest W: 4.302777290344238
grad AddEdge W: 5.386145927527351e-16
grad ChooseDest W: 2.814300775527954
grad AddEdge W: 2.5757340189761294e-16
grad ChooseDest W: 2.73602557182312
grad AddEdge W: 6.954061617014593e-16
grad ChooseDest W: 3.530719041824341
grad AddEdge W: 3.753764063908066e-14
grad ChooseDest W: 3.510385274887085
grad AddEdge W: 7.078424582517245e-16
grad ChooseDest W: 5.053213596343994
grad AddEdge W: 5.166752218089871e-16
grad ChooseDest W: 4.533074378967285
=== Epoch 11: Train Loss: 5.9540, Train Log Prob: 0.0077 ===
Total mismatches: 86061
Predicted valid destination but wrong order: 7812
Epoch 11: Validation Loss: 5.3092, Validation Log Prob: 0.0086
Epoch 11: Edge Precision: 0.3656, Recall: 0.3648, F1: 0.3652, Jaccard: 0.2408
Epoch 11: TP: 2.554187544738726, FP: 4.452541159627774, FN: 4.467287043664997
Epoch 11: Current Learning Rate: 6e-05
[Epoch 11] ‚è±Ô∏è Total: 4829.68s | Current time: 2025-07-15 02:18:25 | üèãÔ∏è Train: 4034.41s | ‚úÖ Val: 795.27s
grad AddEdge W: 4.105884808037509e-14
grad ChooseDest W: 7.134302616119385
grad AddEdge W: 1.183285782725696e-12
grad ChooseDest W: 3.3740787506103516
grad AddEdge W: 5.582997972656027e-16
grad ChooseDest W: 2.7619307041168213
grad AddEdge W: 7.015200984543e-16
grad ChooseDest W: 2.8109238147735596
grad AddEdge W: 5.603933980134193e-16
grad ChooseDest W: 3.504706621170044
grad AddEdge W: 1.492789521010708e-14
grad ChooseDest W: 5.3444719314575195
grad AddEdge W: 3.456147518242185e-14
grad ChooseDest W: 2.612788438796997
grad AddEdge W: 6.285790738113576e-16
grad ChooseDest W: 6.207354545593262
grad AddEdge W: 5.910940542470922e-16
grad ChooseDest W: 3.748850107192993
grad AddEdge W: 4.2195722149652656e-14
grad ChooseDest W: 3.02018141746521
grad AddEdge W: 8.833891192334964e-13
grad ChooseDest W: 2.7628164291381836
grad AddEdge W: 1.449205779522917e-14
grad ChooseDest W: 3.1018524169921875
grad AddEdge W: 3.1000159037009034e-14
grad ChooseDest W: 4.671616554260254
grad AddEdge W: 2.625605963657747e-14
grad ChooseDest W: 3.2860114574432373
grad AddEdge W: 4.4542708144959184e-14
grad ChooseDest W: 5.176663398742676
grad AddEdge W: 3.653633588707704e-14
grad ChooseDest W: 3.1099750995635986
grad AddEdge W: 7.910926150165806e-16
grad ChooseDest W: 3.1059751510620117
grad AddEdge W: 6.731124133484037e-16
grad ChooseDest W: 2.6633448600769043
grad AddEdge W: 9.719822713146896e-16
grad ChooseDest W: 4.364912986755371
grad AddEdge W: 5.252692180918292e-16
grad ChooseDest W: 5.1071648597717285
grad AddEdge W: 2.0942450857795898e-14
grad ChooseDest W: 6.197031497955322
grad AddEdge W: 3.5233958038532207e-16
grad ChooseDest W: 4.188304424285889
grad AddEdge W: 3.282738920461697e-16
grad ChooseDest W: 3.1109859943389893
grad AddEdge W: 5.716904352496275e-16
grad ChooseDest W: 4.496407985687256
grad AddEdge W: 3.172448062712871e-14
grad ChooseDest W: 3.777130126953125
grad AddEdge W: 4.624952887334578e-16
grad ChooseDest W: 2.8293888568878174
grad AddEdge W: 6.453957377208774e-14
grad ChooseDest W: 4.500919818878174
grad AddEdge W: 1.4083179744996469e-14
grad ChooseDest W: 3.0317299365997314
grad AddEdge W: 5.163432907727818e-16
grad ChooseDest W: 2.892594575881958
grad AddEdge W: 6.27111695109358e-16
grad ChooseDest W: 2.3348939418792725
grad AddEdge W: 5.146594888000115e-14
grad ChooseDest W: 2.943223237991333
grad AddEdge W: 1.031438033805487e-14
grad ChooseDest W: 5.026134490966797
grad AddEdge W: 1.1971749271277507e-16
grad ChooseDest W: 5.025809288024902
grad AddEdge W: 1.1511346346623763e-12
grad ChooseDest W: 3.6811745166778564
grad AddEdge W: 3.1563432558801355e-14
grad ChooseDest W: 6.432555675506592
grad AddEdge W: 4.972533386635972e-16
grad ChooseDest W: 4.481002330780029
grad AddEdge W: 2.6574649056830546e-14
grad ChooseDest W: 3.6683356761932373
grad AddEdge W: 4.674194088932022e-16
grad ChooseDest W: 2.961484909057617
grad AddEdge W: 4.788931583780314e-16
grad ChooseDest W: 3.5128471851348877
grad AddEdge W: 3.187019697559429e-16
grad ChooseDest W: 5.153846740722656
grad AddEdge W: 5.32221029248182e-16
grad ChooseDest W: 4.067127704620361
grad AddEdge W: 7.158162675984989e-16
grad ChooseDest W: 4.2477593421936035
grad AddEdge W: 7.114788359158242e-13
grad ChooseDest W: 3.3708560466766357
grad AddEdge W: 1.6122889292093448e-14
grad ChooseDest W: 3.907571792602539
grad AddEdge W: 1.4998786785594583e-14
grad ChooseDest W: 3.6398532390594482
grad AddEdge W: 3.14828085747499e-14
grad ChooseDest W: 3.195828914642334
grad AddEdge W: 2.518006182521907e-14
grad ChooseDest W: 6.466114044189453
grad AddEdge W: 2.807532208288794e-14
grad ChooseDest W: 2.806546449661255
grad AddEdge W: 3.418531801494158e-14
grad ChooseDest W: 5.014020919799805
grad AddEdge W: 2.806313857273287e-16
grad ChooseDest W: 4.085381031036377
grad AddEdge W: 7.125485733066694e-16
grad ChooseDest W: 3.2855875492095947
grad AddEdge W: 1.4009154147604126e-14
grad ChooseDest W: 5.087324142456055
grad AddEdge W: 8.931384328810096e-16
grad ChooseDest W: 5.600876331329346
grad AddEdge W: 9.66542519848304e-16
grad ChooseDest W: 6.862655162811279
grad AddEdge W: 3.3388228247839763e-16
grad ChooseDest W: 2.6734228134155273
grad AddEdge W: 2.987167567610697e-16
grad ChooseDest W: 3.0099637508392334
grad AddEdge W: 5.225122317276348e-16
grad ChooseDest W: 4.921020984649658
grad AddEdge W: 4.420780624657053e-16
grad ChooseDest W: 3.5199403762817383
grad AddEdge W: 2.5574128657336083e-14
grad ChooseDest W: 2.7527451515197754
grad AddEdge W: 1.3302019279500714e-15
grad ChooseDest W: 3.6631221771240234
grad AddEdge W: 2.60937552066311e-16
grad ChooseDest W: 3.7008790969848633
grad AddEdge W: 2.9859618691498396e-16
grad ChooseDest W: 3.320488452911377
grad AddEdge W: 5.19066925214678e-16
grad ChooseDest W: 5.071889400482178
grad AddEdge W: 6.823279729958529e-16
grad ChooseDest W: 3.210301637649536
grad AddEdge W: 1.947909471526274e-14
grad ChooseDest W: 4.139471054077148
grad AddEdge W: 3.074043840659013e-14
grad ChooseDest W: 3.9059906005859375
=== Epoch 12: Train Loss: 5.9332, Train Log Prob: 0.0078 ===
Total mismatches: 85775
Predicted valid destination but wrong order: 7782
Epoch 12: Validation Loss: 5.1799, Validation Log Prob: 0.0095
Epoch 12: Edge Precision: 0.3664, Recall: 0.3654, F1: 0.3659, Jaccard: 0.2412
Epoch 12: TP: 2.558339298496779, FP: 4.445955619183966, FN: 4.463135289906943
Epoch 12: Current Learning Rate: 6e-05
[Epoch 12] ‚è±Ô∏è Total: 4819.70s | Current time: 2025-07-15 03:38:45 | üèãÔ∏è Train: 4025.85s | ‚úÖ Val: 793.85s
grad AddEdge W: 3.009667642601792e-14
grad ChooseDest W: 7.518182754516602
grad AddEdge W: 6.647633832290722e-11
grad ChooseDest W: 5.014369964599609
grad AddEdge W: 3.093651255783587e-16
grad ChooseDest W: 3.3730738162994385
grad AddEdge W: 3.3610232936337155e-16
grad ChooseDest W: 3.4440274238586426
grad AddEdge W: 3.903583365854761e-16
grad ChooseDest W: 3.8376102447509766
grad AddEdge W: 1.463671344668655e-16
grad ChooseDest W: 5.173704624176025
grad AddEdge W: 1.8658136830256716e-14
grad ChooseDest W: 4.0549750328063965
grad AddEdge W: 6.88350641948627e-16
grad ChooseDest W: 1.9921754598617554
grad AddEdge W: 4.7324106439150526e-11
grad ChooseDest W: 3.369230031967163
grad AddEdge W: 3.7465029586710233e-16
grad ChooseDest W: 2.648566246032715
grad AddEdge W: 5.52080191458526e-13
grad ChooseDest W: 3.665140151977539
grad AddEdge W: 4.1757724800759247e-14
grad ChooseDest W: 5.077086448669434
grad AddEdge W: 7.613164952448358e-16
grad ChooseDest W: 3.581477642059326
grad AddEdge W: 4.093953491354573e-16
grad ChooseDest W: 3.420412540435791
grad AddEdge W: 2.3593270959414332e-14
grad ChooseDest W: 4.713792324066162
grad AddEdge W: 1.2323434068724822e-14
grad ChooseDest W: 4.811169147491455
grad AddEdge W: 7.494201821984451e-16
grad ChooseDest W: 5.596104145050049
grad AddEdge W: 1.62560723587363e-16
grad ChooseDest W: 3.5855605602264404
grad AddEdge W: 2.093799546449334e-14
grad ChooseDest W: 3.124833345413208
grad AddEdge W: 6.227559340572211e-16
grad ChooseDest W: 6.665050506591797
grad AddEdge W: 1.9012477711268384e-16
grad ChooseDest W: 3.6901204586029053
grad AddEdge W: 3.5719850547790756e-16
grad ChooseDest W: 1.9235738515853882
grad AddEdge W: 2.57299598497413e-16
grad ChooseDest W: 3.2889795303344727
grad AddEdge W: 3.2771495618010027e-16
grad ChooseDest W: 3.9970710277557373
grad AddEdge W: 3.030750970285309e-14
grad ChooseDest W: 2.99636173248291
grad AddEdge W: 2.1865366733937633e-16
grad ChooseDest W: 4.012226104736328
grad AddEdge W: 3.175415727346871e-14
grad ChooseDest W: 3.6221413612365723
grad AddEdge W: 5.57943037576131e-16
grad ChooseDest W: 3.3277671337127686
grad AddEdge W: 2.310500474620022e-16
grad ChooseDest W: 3.4052016735076904
grad AddEdge W: 3.1545674830699524e-16
grad ChooseDest W: 5.1276631355285645
grad AddEdge W: 1.0976679634677744e-14
grad ChooseDest W: 5.050115585327148
grad AddEdge W: 1.5897660955017484e-16
grad ChooseDest W: 6.396790504455566
grad AddEdge W: 2.17447307134029e-16
grad ChooseDest W: 4.844977378845215
grad AddEdge W: 7.023324559902761e-16
grad ChooseDest W: 2.945406436920166
grad AddEdge W: 3.229095529818694e-16
grad ChooseDest W: 2.4148683547973633
grad AddEdge W: 1.4070319243758306e-14
grad ChooseDest W: 2.3145031929016113
grad AddEdge W: 3.1293769879164055e-16
grad ChooseDest W: 3.9818546772003174
grad AddEdge W: 3.9465604941094643e-16
grad ChooseDest W: 4.04348611831665
grad AddEdge W: 1.5680133312102364e-14
grad ChooseDest W: 4.070314884185791
grad AddEdge W: 2.2110605271480414e-16
grad ChooseDest W: 5.591460704803467
grad AddEdge W: 9.099563991036858e-15
grad ChooseDest W: 3.257601499557495
grad AddEdge W: 1.7065919871391867e-14
grad ChooseDest W: 3.249310255050659
grad AddEdge W: 1.1998078549289078e-14
grad ChooseDest W: 3.1177990436553955
grad AddEdge W: 6.234470578680984e-11
grad ChooseDest W: 2.9526402950286865
grad AddEdge W: 2.2888874972345867e-14
grad ChooseDest W: 2.279892921447754
grad AddEdge W: 6.242322739442863e-13
grad ChooseDest W: 2.8010618686676025
grad AddEdge W: 7.657109021751911e-16
grad ChooseDest W: 3.698906183242798
grad AddEdge W: 3.106936438165679e-16
grad ChooseDest W: 3.5664098262786865
grad AddEdge W: 2.1690278406295268e-16
grad ChooseDest W: 3.6482510566711426
grad AddEdge W: 1.3469140150543409e-14
grad ChooseDest W: 5.4053473472595215
grad AddEdge W: 6.736955425930291e-16
grad ChooseDest W: 4.7639265060424805
grad AddEdge W: 2.3393194471613696e-16
grad ChooseDest W: 3.0724477767944336
grad AddEdge W: 8.418129373102563e-15
grad ChooseDest W: 3.617476224899292
grad AddEdge W: 7.192702032596059e-16
grad ChooseDest W: 4.403534412384033
grad AddEdge W: 8.773849329631479e-15
grad ChooseDest W: 4.293615818023682
grad AddEdge W: 2.935054659624264e-16
grad ChooseDest W: 3.9130470752716064
grad AddEdge W: 7.275204202093766e-13
grad ChooseDest W: 5.312428951263428
grad AddEdge W: 3.9884947130478605e-16
grad ChooseDest W: 3.086916446685791
grad AddEdge W: 3.0099966937259764e-16
grad ChooseDest W: 2.8899033069610596
grad AddEdge W: 2.0449367568210752e-14
grad ChooseDest W: 4.570927619934082
grad AddEdge W: 3.6749842611651985e-16
grad ChooseDest W: 3.83856201171875
grad AddEdge W: 7.0419134390840854e-15
grad ChooseDest W: 5.511659622192383
grad AddEdge W: 1.5560257821275146e-14
grad ChooseDest W: 4.826450347900391
grad AddEdge W: 1.3022678401408087e-14
grad ChooseDest W: 3.8118810653686523
grad AddEdge W: 1.6694323129299617e-14
grad ChooseDest W: 2.733017683029175
grad AddEdge W: 2.9644181155320185e-16
grad ChooseDest W: 4.156864643096924
=== Epoch 13: Train Loss: 5.9088, Train Log Prob: 0.0080 ===
Total mismatches: 85139
Predicted valid destination but wrong order: 7847
Epoch 13: Validation Loss: 5.1316, Validation Log Prob: 0.0098
Epoch 13: Edge Precision: 0.3610, Recall: 0.3597, F1: 0.3603, Jaccard: 0.2371
Epoch 13: TP: 2.5183965640658554, FP: 4.4781675017895495, FN: 4.503078024337867
Epoch 13: Current Learning Rate: 6e-05
[Epoch 13] ‚è±Ô∏è Total: 4839.37s | Current time: 2025-07-15 04:59:24 | üèãÔ∏è Train: 4043.33s | ‚úÖ Val: 796.05s
grad AddEdge W: 1.2381374933965279e-15
grad ChooseDest W: 11.81049919128418
grad AddEdge W: 2.3154373532135346e-16
grad ChooseDest W: 4.283419132232666
grad AddEdge W: 4.368417047857792e-16
grad ChooseDest W: 5.4643707275390625
grad AddEdge W: 5.881184428346842e-11
grad ChooseDest W: 3.304408073425293
grad AddEdge W: 3.4393476361729326e-16
grad ChooseDest W: 7.244472503662109
grad AddEdge W: 7.018149188595037e-16
grad ChooseDest W: 1.9728666543960571
grad AddEdge W: 4.563415414320961e-16
grad ChooseDest W: 3.874917507171631
grad AddEdge W: 9.083165433178015e-15
grad ChooseDest W: 4.461626052856445
grad AddEdge W: 4.596826099529815e-16
grad ChooseDest W: 2.9244320392608643
grad AddEdge W: 4.462474497996666e-16
grad ChooseDest W: 4.021847724914551
grad AddEdge W: 9.675845397800163e-15
grad ChooseDest W: 3.378974199295044
grad AddEdge W: 2.279363543478954e-16
grad ChooseDest W: 4.048777103424072
grad AddEdge W: 4.2074098488701424e-16
grad ChooseDest W: 4.42720365524292
grad AddEdge W: 3.4653507535602516e-16
grad ChooseDest W: 5.768280029296875
grad AddEdge W: 3.0829873756754516e-16
grad ChooseDest W: 7.310773849487305
grad AddEdge W: 2.3319821618986332e-14
grad ChooseDest W: 3.7467970848083496
grad AddEdge W: 1.9627446130287667e-16
grad ChooseDest W: 4.118078231811523
grad AddEdge W: 3.343064892838768e-14
grad ChooseDest W: 3.9983747005462646
grad AddEdge W: 1.6113915083896407e-16
grad ChooseDest W: 4.619109153747559
grad AddEdge W: 2.969213116106866e-16
grad ChooseDest W: 9.650856971740723
grad AddEdge W: 1.7499568341375837e-16
grad ChooseDest W: 4.906681060791016
grad AddEdge W: 3.4222997746204597e-16
grad ChooseDest W: 5.399901390075684
grad AddEdge W: 2.4947840668310128e-11
grad ChooseDest W: 2.6030445098876953
grad AddEdge W: 2.0066152922213962e-14
grad ChooseDest W: 5.165554046630859
grad AddEdge W: 2.2111248487124735e-16
grad ChooseDest W: 4.359673976898193
grad AddEdge W: 3.1685165275244546e-16
grad ChooseDest W: 5.300992012023926
grad AddEdge W: 3.268638468867873e-16
grad ChooseDest W: 4.039827346801758
grad AddEdge W: 2.1115439496641137e-16
grad ChooseDest W: 3.355052947998047
grad AddEdge W: 1.3331614187161306e-14
grad ChooseDest W: 4.22359561920166
grad AddEdge W: 1.3654060995456178e-14
grad ChooseDest W: 4.082922458648682
grad AddEdge W: 1.4120505098850177e-14
grad ChooseDest W: 3.3987693786621094
grad AddEdge W: 4.2761741413446797e-13
grad ChooseDest W: 2.8595356941223145
grad AddEdge W: 2.9712359366640276e-16
grad ChooseDest W: 3.973604917526245
grad AddEdge W: 1.1612916998063024e-16
grad ChooseDest W: 4.441654682159424
grad AddEdge W: 1.3071989271465443e-14
grad ChooseDest W: 3.7950282096862793
grad AddEdge W: 1.0640958126033502e-14
grad ChooseDest W: 3.892181396484375
grad AddEdge W: 2.18339987745602e-14
grad ChooseDest W: 4.6814703941345215
grad AddEdge W: 1.9953938950273778e-16
grad ChooseDest W: 3.6933743953704834
grad AddEdge W: 1.2617295209115757e-14
grad ChooseDest W: 3.0362088680267334
grad AddEdge W: 2.095401089876223e-16
grad ChooseDest W: 5.092700481414795
grad AddEdge W: 2.7939696861439475e-14
grad ChooseDest W: 2.588688611984253
grad AddEdge W: 3.795122385145624e-16
grad ChooseDest W: 5.046081066131592
grad AddEdge W: 5.185426647598868e-16
grad ChooseDest W: 3.609527111053467
grad AddEdge W: 1.7567514148031024e-14
grad ChooseDest W: 2.2204701900482178
grad AddEdge W: 4.873378121643607e-16
grad ChooseDest W: 6.096401691436768
grad AddEdge W: 4.1807486927619253e-16
grad ChooseDest W: 3.933568239212036
grad AddEdge W: 3.4561138592704435e-16
grad ChooseDest W: 3.819770336151123
grad AddEdge W: 3.692167647989232e-16
grad ChooseDest W: 4.9259233474731445
grad AddEdge W: 1.0759332335665006e-13
grad ChooseDest W: 4.270683288574219
grad AddEdge W: 5.66737674788354e-16
grad ChooseDest W: 3.5326547622680664
grad AddEdge W: 1.9108221551065682e-16
grad ChooseDest W: 4.9038896560668945
grad AddEdge W: 3.1988368659648103e-14
grad ChooseDest W: 3.8622052669525146
grad AddEdge W: 1.5046091087539453e-16
grad ChooseDest W: 3.942610740661621
grad AddEdge W: 1.3350352250020465e-14
grad ChooseDest W: 4.051368236541748
grad AddEdge W: 1.1001381656518521e-14
grad ChooseDest W: 5.095745086669922
grad AddEdge W: 2.9030018741089774e-16
grad ChooseDest W: 4.2078166007995605
grad AddEdge W: 3.9443796489680805e-16
grad ChooseDest W: 2.171304702758789
grad AddEdge W: 1.9149307942963436e-16
grad ChooseDest W: 3.8743972778320312
grad AddEdge W: 2.5616254993934828e-14
grad ChooseDest W: 2.7900779247283936
grad AddEdge W: 1.5046017342733482e-14
grad ChooseDest W: 3.506164073944092
grad AddEdge W: 1.3756714612399822e-14
grad ChooseDest W: 2.1489779949188232
grad AddEdge W: 1.1450813103166917e-14
grad ChooseDest W: 5.054391860961914
grad AddEdge W: 9.893114973069661e-13
grad ChooseDest W: 4.228079319000244
grad AddEdge W: 1.9406156125839521e-16
grad ChooseDest W: 4.01168155670166
grad AddEdge W: 2.3532036406558477e-16
grad ChooseDest W: 4.353878974914551
grad AddEdge W: 1.525365915824208e-16
grad ChooseDest W: 3.5026633739471436
=== Epoch 14: Train Loss: 5.8776, Train Log Prob: 0.0082 ===
Total mismatches: 84499
Predicted valid destination but wrong order: 7920
Epoch 14: Validation Loss: 5.0942, Validation Log Prob: 0.0103
Epoch 14: Edge Precision: 0.3639, Recall: 0.3624, F1: 0.3631, Jaccard: 0.2399
Epoch 14: TP: 2.537580529706514, FP: 4.453972798854688, FN: 4.483894058697208
Epoch 14: Current Learning Rate: 6e-05
[Epoch 14] ‚è±Ô∏è Total: 4822.59s | Current time: 2025-07-15 06:19:47 | üèãÔ∏è Train: 4028.57s | ‚úÖ Val: 794.02s
grad AddEdge W: 5.192340698026432e-13
grad ChooseDest W: 7.706140041351318
grad AddEdge W: 5.542453152448923e-16
grad ChooseDest W: 4.5929670333862305
grad AddEdge W: 3.1365667094518184e-16
grad ChooseDest W: 5.391894817352295
grad AddEdge W: 3.5957554462569914e-16
grad ChooseDest W: 4.673867225646973
grad AddEdge W: 9.343284169082127e-15
grad ChooseDest W: 3.761956214904785
grad AddEdge W: 3.4480040481960756e-16
grad ChooseDest W: 6.331212520599365
grad AddEdge W: 1.7545293085692755e-14
grad ChooseDest W: 3.212061882019043
grad AddEdge W: 1.1037305832589013e-16
grad ChooseDest W: 3.8120505809783936
grad AddEdge W: 1.643201698374878e-16
grad ChooseDest W: 4.227733135223389
grad AddEdge W: 1.139995866802912e-12
grad ChooseDest W: 2.7277064323425293
grad AddEdge W: 1.4511182105112277e-14
grad ChooseDest W: 3.107389450073242
grad AddEdge W: 1.3707151697101098e-16
grad ChooseDest W: 4.414013862609863
grad AddEdge W: 3.8260764101096445e-16
grad ChooseDest W: 5.30989408493042
grad AddEdge W: 2.6722934032708673e-14
grad ChooseDest W: 3.762714385986328
grad AddEdge W: 4.982316087781167e-16
grad ChooseDest W: 4.5341386795043945
grad AddEdge W: 2.1948902711382628e-16
grad ChooseDest W: 3.6155266761779785
grad AddEdge W: 3.7746021586126943e-11
grad ChooseDest W: 1.8269052505493164
grad AddEdge W: 1.1831548483727094e-14
grad ChooseDest W: 4.128526210784912
grad AddEdge W: 1.127319029412769e-14
grad ChooseDest W: 6.828686237335205
grad AddEdge W: 1.7352762496781853e-14
grad ChooseDest W: 3.4177916049957275
grad AddEdge W: 2.420469703370864e-16
grad ChooseDest W: 7.671755313873291
grad AddEdge W: 4.477856616318804e-16
grad ChooseDest W: 5.332052707672119
grad AddEdge W: 2.4980346279333083e-16
grad ChooseDest W: 6.437173843383789
grad AddEdge W: 2.0354426715647452e-16
grad ChooseDest W: 3.4227449893951416
grad AddEdge W: 1.1582813023601132e-14
grad ChooseDest W: 3.6105172634124756
grad AddEdge W: 2.4074264547743317e-16
grad ChooseDest W: 2.7319395542144775
grad AddEdge W: 6.003275074655521e-16
grad ChooseDest W: 6.968743324279785
grad AddEdge W: 2.1496661232931716e-16
grad ChooseDest W: 4.486868858337402
grad AddEdge W: 1.2657939926271077e-16
grad ChooseDest W: 4.306219577789307
grad AddEdge W: 1.2342631223441394e-14
grad ChooseDest W: 3.5960333347320557
grad AddEdge W: 1.4108502377289787e-16
grad ChooseDest W: 6.129281044006348
grad AddEdge W: 1.2458635773699659e-14
grad ChooseDest W: 4.675288677215576
grad AddEdge W: 2.0698152685643247e-16
grad ChooseDest W: 3.9766852855682373
grad AddEdge W: 5.260287106075012e-13
grad ChooseDest W: 2.0150325298309326
grad AddEdge W: 2.4828166222447007e-16
grad ChooseDest W: 6.280829906463623
grad AddEdge W: 7.498010187994425e-15
grad ChooseDest W: 4.08167028427124
grad AddEdge W: 3.6231315511122504e-16
grad ChooseDest W: 6.99003791809082
grad AddEdge W: 1.6895534588310014e-16
grad ChooseDest W: 4.590417861938477
grad AddEdge W: 1.3096577790891288e-14
grad ChooseDest W: 4.679353713989258
grad AddEdge W: 2.604785046781289e-14
grad ChooseDest W: 3.2701234817504883
grad AddEdge W: 2.983982723729021e-16
grad ChooseDest W: 5.839147567749023
grad AddEdge W: 1.6260071942434117e-16
grad ChooseDest W: 3.557684898376465
grad AddEdge W: 7.330142657408726e-15
grad ChooseDest W: 6.020299434661865
grad AddEdge W: 1.4091936371605183e-14
grad ChooseDest W: 3.243607521057129
grad AddEdge W: 1.1908105015565832e-14
grad ChooseDest W: 5.370672702789307
grad AddEdge W: 2.762858155403385e-16
grad ChooseDest W: 5.852026462554932
grad AddEdge W: 2.1705882341370469e-16
grad ChooseDest W: 5.122191905975342
grad AddEdge W: 1.8238628410656855e-16
grad ChooseDest W: 4.791118144989014
grad AddEdge W: 2.1262771613393162e-16
grad ChooseDest W: 3.2781097888946533
grad AddEdge W: 4.387464171863581e-16
grad ChooseDest W: 4.114156246185303
grad AddEdge W: 1.3883163820051561e-16
grad ChooseDest W: 6.140809535980225
grad AddEdge W: 1.8412973308069326e-14
grad ChooseDest W: 3.895141839981079
grad AddEdge W: 3.297885722443176e-16
grad ChooseDest W: 3.7708306312561035
grad AddEdge W: 1.4255908609424697e-16
grad ChooseDest W: 3.7049152851104736
grad AddEdge W: 2.5541142970908514e-16
grad ChooseDest W: 4.559598922729492
grad AddEdge W: 1.385489806590389e-16
grad ChooseDest W: 5.1520209312438965
grad AddEdge W: 8.030214541281458e-15
grad ChooseDest W: 3.9147489070892334
grad AddEdge W: 2.455900297461123e-16
grad ChooseDest W: 5.609127044677734
grad AddEdge W: 2.756976570375888e-16
grad ChooseDest W: 4.015316486358643
grad AddEdge W: 3.069560577939612e-11
grad ChooseDest W: 1.809403657913208
grad AddEdge W: 2.506563985014363e-16
grad ChooseDest W: 4.641444206237793
grad AddEdge W: 2.2663157950198896e-16
grad ChooseDest W: 2.7804393768310547
grad AddEdge W: 3.6958573679767015e-13
grad ChooseDest W: 2.562943935394287
grad AddEdge W: 1.5134561084841763e-14
grad ChooseDest W: 6.93344783782959
grad AddEdge W: 3.719447402846741e-16
grad ChooseDest W: 6.395553112030029
grad AddEdge W: 1.5390411304087306e-16
grad ChooseDest W: 3.6368138790130615
=== Epoch 15: Train Loss: 5.8398, Train Log Prob: 0.0085 ===
Total mismatches: 83513
Predicted valid destination but wrong order: 7919
Epoch 15: Validation Loss: 4.9851, Validation Log Prob: 0.0113
Epoch 15: Edge Precision: 0.3650, Recall: 0.3635, F1: 0.3642, Jaccard: 0.2405
Epoch 15: TP: 2.5458840372226197, FP: 4.447100930565497, FN: 4.475590551181102
Epoch 15: Current Learning Rate: 6e-05
[Epoch 15] ‚è±Ô∏è Total: 4841.64s | Current time: 2025-07-15 07:40:28 | üèãÔ∏è Train: 4029.95s | ‚úÖ Val: 811.69s
grad AddEdge W: 9.361088293413644e-13
grad ChooseDest W: 8.119503021240234
grad AddEdge W: 2.7112325560594197e-16
grad ChooseDest W: 4.276987552642822
grad AddEdge W: 2.7337046048478723e-16
grad ChooseDest W: 2.976119041442871
grad AddEdge W: 1.7681321494824112e-14
grad ChooseDest W: 3.8228886127471924
grad AddEdge W: 1.7441242182023497e-16
grad ChooseDest W: 2.1727921962738037
grad AddEdge W: 1.6753266669791386e-15
grad ChooseDest W: 3.8712687492370605
grad AddEdge W: 1.8567506157785078e-16
grad ChooseDest W: 5.012546062469482
grad AddEdge W: 4.4199656201431166e-16
grad ChooseDest W: 5.500365257263184
grad AddEdge W: 1.0387990227558568e-16
grad ChooseDest W: 4.555872440338135
grad AddEdge W: 1.383055778007115e-16
grad ChooseDest W: 4.055103778839111
grad AddEdge W: 2.5806018114448814e-16
grad ChooseDest W: 3.515465259552002
grad AddEdge W: 3.7301016442281565e-11
grad ChooseDest W: 2.0786750316619873
grad AddEdge W: 9.399795666224092e-15
grad ChooseDest W: 3.492368221282959
grad AddEdge W: 9.63627398362088e-13
grad ChooseDest W: 3.6479835510253906
grad AddEdge W: 1.2051272430134885e-16
grad ChooseDest W: 3.8923768997192383
grad AddEdge W: 4.702898447327694e-16
grad ChooseDest W: 3.8702778816223145
grad AddEdge W: 9.846086341173848e-17
grad ChooseDest W: 4.453786849975586
grad AddEdge W: 3.850387843882619e-16
grad ChooseDest W: 3.373185873031616
grad AddEdge W: 1.1094909713182943e-12
grad ChooseDest W: 5.143192768096924
grad AddEdge W: 1.0980781815241297e-14
grad ChooseDest W: 5.144827365875244
grad AddEdge W: 2.454230319066052e-16
grad ChooseDest W: 4.170272350311279
grad AddEdge W: 4.4814537958392034e-14
grad ChooseDest W: 5.0720696449279785
grad AddEdge W: 3.6136611933663553e-16
grad ChooseDest W: 4.343709945678711
grad AddEdge W: 1.4955049604088396e-16
grad ChooseDest W: 5.7262091636657715
grad AddEdge W: 2.8300286622140393e-16
grad ChooseDest W: 5.402749061584473
grad AddEdge W: 8.151622314710259e-15
grad ChooseDest W: 3.1096301078796387
grad AddEdge W: 1.9616758956773482e-16
grad ChooseDest W: 4.365099906921387
grad AddEdge W: 3.3441787199885837e-16
grad ChooseDest W: 4.962403297424316
grad AddEdge W: 4.2719942582137107e-16
grad ChooseDest W: 6.181168556213379
grad AddEdge W: 1.96837857326809e-16
grad ChooseDest W: 4.305988788604736
grad AddEdge W: 2.72078099965514e-16
grad ChooseDest W: 5.303089141845703
grad AddEdge W: 2.654582331729216e-09
grad ChooseDest W: 3.922672986984253
grad AddEdge W: 2.586769799487669e-16
grad ChooseDest W: 4.160576343536377
grad AddEdge W: 6.432770118582629e-15
grad ChooseDest W: 5.767347812652588
grad AddEdge W: 6.999713308974975e-13
grad ChooseDest W: 5.093148231506348
grad AddEdge W: 4.927964630533819e-16
grad ChooseDest W: 4.834585666656494
grad AddEdge W: 9.22488929184671e-15
grad ChooseDest W: 2.709575891494751
grad AddEdge W: 3.979564603503636e-16
grad ChooseDest W: 4.842074871063232
grad AddEdge W: 1.9635974693275334e-16
grad ChooseDest W: 3.2403275966644287
grad AddEdge W: 2.450933772714457e-16
grad ChooseDest W: 4.283637046813965
grad AddEdge W: 6.652795952826778e-17
grad ChooseDest W: 3.592956304550171
grad AddEdge W: 2.212380310358982e-16
grad ChooseDest W: 4.850086212158203
grad AddEdge W: 4.2067358711067593e-13
grad ChooseDest W: 4.118709087371826
grad AddEdge W: 4.7344398367010765e-15
grad ChooseDest W: 3.8482162952423096
grad AddEdge W: 1.9388490194933349e-16
grad ChooseDest W: 6.026163101196289
grad AddEdge W: 1.0073515508074796e-14
grad ChooseDest W: 2.745112895965576
grad AddEdge W: 1.847236053500678e-16
grad ChooseDest W: 5.535046100616455
grad AddEdge W: 1.9185133466165352e-16
grad ChooseDest W: 5.713793754577637
grad AddEdge W: 9.18705963575203e-17
grad ChooseDest W: 4.843532085418701
grad AddEdge W: 1.807334316588998e-16
grad ChooseDest W: 3.9705488681793213
grad AddEdge W: 1.3153628000987707e-14
grad ChooseDest W: 4.991026401519775
grad AddEdge W: 1.1945638157189413e-16
grad ChooseDest W: 5.805951118469238
grad AddEdge W: 1.7371139723605482e-14
grad ChooseDest W: 4.480636119842529
grad AddEdge W: 2.1466358629243694e-16
grad ChooseDest W: 5.697342395782471
grad AddEdge W: 3.7949865951762674e-16
grad ChooseDest W: 4.303765773773193
grad AddEdge W: 3.1694368817612056e-16
grad ChooseDest W: 5.2198262214660645
grad AddEdge W: 1.3029414007404653e-14
grad ChooseDest W: 5.9669389724731445
grad AddEdge W: 1.9165831702879795e-16
grad ChooseDest W: 4.037255764007568
grad AddEdge W: 5.111730546023007e-16
grad ChooseDest W: 3.8490664958953857
grad AddEdge W: 2.5194076514726985e-16
grad ChooseDest W: 5.260948181152344
grad AddEdge W: 3.277921420574188e-16
grad ChooseDest W: 8.727449417114258
grad AddEdge W: 8.607685182302708e-15
grad ChooseDest W: 4.997635841369629
grad AddEdge W: 1.189965925004876e-14
grad ChooseDest W: 3.2695300579071045
grad AddEdge W: 7.858087440822695e-17
grad ChooseDest W: 3.109149694442749
grad AddEdge W: 1.7395659898060395e-16
grad ChooseDest W: 5.296327114105225
grad AddEdge W: 3.209843265020992e-16
grad ChooseDest W: 4.643764019012451
=== Epoch 16: Train Loss: 5.7969, Train Log Prob: 0.0089 ===
Total mismatches: 82701
Predicted valid destination but wrong order: 7999
Epoch 16: Validation Loss: 4.9158, Validation Log Prob: 0.0119
Epoch 16: Edge Precision: 0.3664, Recall: 0.3648, F1: 0.3656, Jaccard: 0.2416
Epoch 16: TP: 2.5549033643521835, FP: 4.4346456692913385, FN: 4.466571224051539
Epoch 16: Current Learning Rate: 6e-05
[Epoch 16] ‚è±Ô∏è Total: 4823.52s | Current time: 2025-07-15 09:00:52 | üèãÔ∏è Train: 4028.68s | ‚úÖ Val: 794.84s
grad AddEdge W: 2.1849611285843992e-14
grad ChooseDest W: 10.27964973449707
grad AddEdge W: 6.246634611482837e-13
grad ChooseDest W: 2.967036247253418
grad AddEdge W: 2.0419387684075634e-14
grad ChooseDest W: 9.657027244567871
grad AddEdge W: 5.40811182119026e-15
grad ChooseDest W: 3.6275475025177
grad AddEdge W: 1.365775247091943e-16
grad ChooseDest W: 4.194484710693359
grad AddEdge W: 1.5040366997700586e-16
grad ChooseDest W: 2.7640457153320312
grad AddEdge W: 1.6489207589556206e-16
grad ChooseDest W: 5.906896591186523
grad AddEdge W: 3.313435394470193e-16
grad ChooseDest W: 4.938611030578613
grad AddEdge W: 1.1359900315784284e-16
grad ChooseDest W: 6.323166847229004
grad AddEdge W: 1.5535229691749676e-14
grad ChooseDest W: 2.994520902633667
grad AddEdge W: 4.378918085881818e-15
grad ChooseDest W: 4.675787448883057
grad AddEdge W: 1.702649048769718e-14
grad ChooseDest W: 5.2517571449279785
grad AddEdge W: 7.539876485478363e-15
grad ChooseDest W: 4.18663215637207
grad AddEdge W: 2.665204521007825e-16
grad ChooseDest W: 3.881291389465332
grad AddEdge W: 1.2397585821086746e-16
grad ChooseDest W: 3.3569419384002686
grad AddEdge W: 6.446280055863319e-17
grad ChooseDest W: 4.585727691650391
grad AddEdge W: 2.693955932083717e-14
grad ChooseDest W: 6.015112400054932
grad AddEdge W: 1.685039699664422e-16
grad ChooseDest W: 5.204269886016846
grad AddEdge W: 1.4007600689178862e-14
grad ChooseDest W: 5.734255313873291
grad AddEdge W: 1.0187619289929643e-16
grad ChooseDest W: 4.3248162269592285
grad AddEdge W: 1.7886987976560153e-16
grad ChooseDest W: 3.9981539249420166
grad AddEdge W: 3.041531200956662e-16
grad ChooseDest W: 5.690859794616699
grad AddEdge W: 1.4445516934667572e-16
grad ChooseDest W: 3.977114677429199
grad AddEdge W: 3.0281864615704665e-16
grad ChooseDest W: 4.66315221786499
grad AddEdge W: 9.546749953342083e-15
grad ChooseDest W: 4.221184730529785
grad AddEdge W: 1.0574998558700066e-16
grad ChooseDest W: 4.218025207519531
grad AddEdge W: 7.30373725231102e-15
grad ChooseDest W: 3.6776862144470215
grad AddEdge W: 2.26569507868823e-16
grad ChooseDest W: 4.956563472747803
grad AddEdge W: 6.351100895874264e-15
grad ChooseDest W: 3.8820528984069824
grad AddEdge W: 2.591953514069923e-14
grad ChooseDest W: 7.501162052154541
grad AddEdge W: 2.4051752000192074e-16
grad ChooseDest W: 6.859556674957275
grad AddEdge W: 3.854180963299542e-16
grad ChooseDest W: 5.607954502105713
grad AddEdge W: 1.216179302439483e-16
grad ChooseDest W: 3.9565491676330566
grad AddEdge W: 3.607188908737267e-15
grad ChooseDest W: 4.941340446472168
grad AddEdge W: 1.8263590577491558e-14
grad ChooseDest W: 4.533414840698242
grad AddEdge W: 8.552646980387879e-13
grad ChooseDest W: 3.512240171432495
grad AddEdge W: 9.160463462952735e-17
grad ChooseDest W: 4.912209510803223
grad AddEdge W: 1.6469905826270648e-16
grad ChooseDest W: 4.836419582366943
grad AddEdge W: 5.99900200519486e-15
grad ChooseDest W: 3.126490354537964
grad AddEdge W: 1.2362317116168531e-14
grad ChooseDest W: 4.131216526031494
grad AddEdge W: 1.0057396576967665e-16
grad ChooseDest W: 4.3795647621154785
grad AddEdge W: 6.9095365889883416e-15
grad ChooseDest W: 6.583777904510498
grad AddEdge W: 2.172586570148077e-16
grad ChooseDest W: 4.897077560424805
grad AddEdge W: 1.7581068369252988e-14
grad ChooseDest W: 5.051828384399414
grad AddEdge W: 2.4459810122531833e-16
grad ChooseDest W: 4.526610851287842
grad AddEdge W: 2.1396340768241285e-16
grad ChooseDest W: 5.1625189781188965
grad AddEdge W: 2.0071366833520786e-16
grad ChooseDest W: 6.592834949493408
grad AddEdge W: 1.2462726625197542e-16
grad ChooseDest W: 5.568789482116699
grad AddEdge W: 5.8639282021950585e-15
grad ChooseDest W: 3.8983068466186523
grad AddEdge W: 1.5379826039224587e-16
grad ChooseDest W: 2.527571201324463
grad AddEdge W: 9.98741037382166e-15
grad ChooseDest W: 2.887467622756958
grad AddEdge W: 1.8301161367612062e-12
grad ChooseDest W: 3.916249990463257
grad AddEdge W: 1.6813920316864102e-16
grad ChooseDest W: 3.827348470687866
grad AddEdge W: 6.661178270282145e-17
grad ChooseDest W: 3.107774019241333
grad AddEdge W: 9.811101312883624e-15
grad ChooseDest W: 4.6239118576049805
grad AddEdge W: 3.919254004774558e-16
grad ChooseDest W: 4.385791778564453
grad AddEdge W: 1.2338322631536507e-16
grad ChooseDest W: 4.652470111846924
grad AddEdge W: 1.0574073439902987e-16
grad ChooseDest W: 6.788670063018799
grad AddEdge W: 7.511954891405073e-15
grad ChooseDest W: 5.816217422485352
grad AddEdge W: 9.17209965747498e-15
grad ChooseDest W: 4.19815731048584
grad AddEdge W: 1.3590395458041703e-14
grad ChooseDest W: 3.9905951023101807
grad AddEdge W: 2.3879274916307417e-16
grad ChooseDest W: 2.1211934089660645
grad AddEdge W: 5.034245030799368e-15
grad ChooseDest W: 8.497493743896484
grad AddEdge W: 1.8200905004257497e-16
grad ChooseDest W: 3.6839146614074707
grad AddEdge W: 1.2073275434428796e-16
grad ChooseDest W: 3.9461917877197266
grad AddEdge W: 6.3869066726205976e-15
grad ChooseDest W: 4.733111381530762
=== Epoch 17: Train Loss: 5.7482, Train Log Prob: 0.0093 ===
Total mismatches: 81612
Predicted valid destination but wrong order: 8051
Epoch 17: Validation Loss: 4.8778, Validation Log Prob: 0.0125
Epoch 17: Edge Precision: 0.3628, Recall: 0.3611, F1: 0.3619, Jaccard: 0.2389
Epoch 17: TP: 2.5285612025769506, FP: 4.458267716535433, FN: 4.492913385826772
Epoch 17: Current Learning Rate: 6e-05
[Epoch 17] ‚è±Ô∏è Total: 4826.68s | Current time: 2025-07-15 10:21:19 | üèãÔ∏è Train: 4032.03s | ‚úÖ Val: 794.64s
grad AddEdge W: 9.708400262094935e-15
grad ChooseDest W: 9.393009185791016
grad AddEdge W: 2.6544337029900985e-16
grad ChooseDest W: 5.236154079437256
grad AddEdge W: 2.0184960569192488e-16
grad ChooseDest W: 6.77678918838501
grad AddEdge W: 8.690603778608273e-15
grad ChooseDest W: 3.0875308513641357
grad AddEdge W: 3.2249395097234317e-16
grad ChooseDest W: 4.585564136505127
grad AddEdge W: 1.2834804376124715e-16
grad ChooseDest W: 5.102141380310059
grad AddEdge W: 1.1234542721843318e-14
grad ChooseDest W: 3.761852264404297
grad AddEdge W: 3.232761329595733e-16
grad ChooseDest W: 4.811868667602539
grad AddEdge W: 5.23223771167064e-15
grad ChooseDest W: 7.147041320800781
grad AddEdge W: 1.1608163025646559e-14
grad ChooseDest W: 4.0370049476623535
grad AddEdge W: 1.0042179524191123e-14
grad ChooseDest W: 3.5930936336517334
grad AddEdge W: 1.376994018483825e-14
grad ChooseDest W: 5.795999526977539
grad AddEdge W: 1.2810842608140279e-16
grad ChooseDest W: 5.672091007232666
grad AddEdge W: 7.00755185351282e-15
grad ChooseDest W: 4.37299919128418
grad AddEdge W: 2.6436872371696056e-16
grad ChooseDest W: 3.5364625453948975
grad AddEdge W: 8.972131642831152e-17
grad ChooseDest W: 3.8615756034851074
grad AddEdge W: 1.7239866716258688e-16
grad ChooseDest W: 4.704083442687988
grad AddEdge W: 9.668577008079774e-15
grad ChooseDest W: 4.700432777404785
grad AddEdge W: 1.1005415042124889e-16
grad ChooseDest W: 7.106250286102295
grad AddEdge W: 2.8599066906372507e-16
grad ChooseDest W: 5.053963661193848
grad AddEdge W: 7.197970180481287e-17
grad ChooseDest W: 4.468512058258057
grad AddEdge W: 2.1065639941512775e-14
grad ChooseDest W: 6.664062023162842
grad AddEdge W: 1.6176407586558053e-16
grad ChooseDest W: 5.016598224639893
grad AddEdge W: 1.4919453044480034e-16
grad ChooseDest W: 4.557247161865234
grad AddEdge W: 1.7210507760213466e-16
grad ChooseDest W: 4.399295806884766
grad AddEdge W: 3.4921266826328217e-15
grad ChooseDest W: 4.294619560241699
grad AddEdge W: 1.8439819911453433e-16
grad ChooseDest W: 3.757291078567505
grad AddEdge W: 9.765466565817352e-15
grad ChooseDest W: 4.41765022277832
grad AddEdge W: 6.2202182118974766e-15
grad ChooseDest W: 3.1331026554107666
grad AddEdge W: 1.174685143586965e-16
grad ChooseDest W: 4.45504903793335
grad AddEdge W: 2.3734628158695985e-16
grad ChooseDest W: 2.4673118591308594
grad AddEdge W: 1.0997104193074446e-16
grad ChooseDest W: 2.8167569637298584
grad AddEdge W: 1.7200437332564e-16
grad ChooseDest W: 5.744326114654541
grad AddEdge W: 1.517633505573445e-14
grad ChooseDest W: 3.435250997543335
grad AddEdge W: 4.473234781042111e-15
grad ChooseDest W: 4.135934829711914
grad AddEdge W: 1.8841482935041422e-16
grad ChooseDest W: 3.679051160812378
grad AddEdge W: 3.560242107635697e-13
grad ChooseDest W: 3.732163906097412
grad AddEdge W: 9.140218713768868e-17
grad ChooseDest W: 4.278051376342773
grad AddEdge W: 3.354992419049265e-16
grad ChooseDest W: 4.657546520233154
grad AddEdge W: 6.148769659307596e-17
grad ChooseDest W: 4.207595348358154
grad AddEdge W: 1.7224660933679026e-11
grad ChooseDest W: 2.9686238765716553
grad AddEdge W: 5.241868158643566e-16
grad ChooseDest W: 4.885305404663086
grad AddEdge W: 9.484486255455313e-15
grad ChooseDest W: 3.242629051208496
grad AddEdge W: 1.331651497784355e-14
grad ChooseDest W: 8.701896667480469
grad AddEdge W: 9.851048233709084e-15
grad ChooseDest W: 2.229685068130493
grad AddEdge W: 7.208119192671225e-13
grad ChooseDest W: 6.339822292327881
grad AddEdge W: 1.3447075524602585e-16
grad ChooseDest W: 3.416944980621338
grad AddEdge W: 1.20705503706188e-16
grad ChooseDest W: 5.123812675476074
grad AddEdge W: 7.831946548232549e-17
grad ChooseDest W: 5.486904144287109
grad AddEdge W: 9.953955325262193e-17
grad ChooseDest W: 3.6396687030792236
grad AddEdge W: 4.0076485102654444e-16
grad ChooseDest W: 6.026438236236572
grad AddEdge W: 1.1614244722207845e-14
grad ChooseDest W: 3.382798194885254
grad AddEdge W: 1.6986193742264503e-14
grad ChooseDest W: 4.321695804595947
grad AddEdge W: 4.230330278784389e-13
grad ChooseDest W: 2.404315710067749
grad AddEdge W: 1.620022641773264e-16
grad ChooseDest W: 3.2904696464538574
grad AddEdge W: 8.857232285280596e-17
grad ChooseDest W: 7.622852802276611
grad AddEdge W: 2.2562501319307524e-15
grad ChooseDest W: 3.119551420211792
grad AddEdge W: 4.824511558483568e-13
grad ChooseDest W: 4.743759632110596
grad AddEdge W: 3.4516053210031927e-13
grad ChooseDest W: 3.222276210784912
grad AddEdge W: 1.2015683811460404e-16
grad ChooseDest W: 3.785646438598633
grad AddEdge W: 9.289228220454252e-15
grad ChooseDest W: 3.6802256107330322
grad AddEdge W: 1.201253401356692e-14
grad ChooseDest W: 2.5298779010772705
grad AddEdge W: 1.7591274216294895e-16
grad ChooseDest W: 5.712625980377197
grad AddEdge W: 1.0451379446943781e-14
grad ChooseDest W: 3.301593542098999
grad AddEdge W: 9.327419241980054e-15
grad ChooseDest W: 4.605078220367432
grad AddEdge W: 2.581685484221775e-16
grad ChooseDest W: 3.5659189224243164
=== Epoch 18: Train Loss: 5.6948, Train Log Prob: 0.0097 ===
Total mismatches: 80639
Predicted valid destination but wrong order: 8049
Epoch 18: Validation Loss: 4.7779, Validation Log Prob: 0.0136
Epoch 18: Edge Precision: 0.3646, Recall: 0.3622, F1: 0.3633, Jaccard: 0.2398
Epoch 18: TP: 2.536721546170365, FP: 4.44151753758053, FN: 4.484753042233357
Epoch 18: Current Learning Rate: 6e-05
[Epoch 18] ‚è±Ô∏è Total: 4832.62s | Current time: 2025-07-15 11:41:51 | üèãÔ∏è Train: 4042.29s | ‚úÖ Val: 790.33s
grad AddEdge W: 4.106888033860237e-14
grad ChooseDest W: 8.894819259643555
grad AddEdge W: 1.9446488129018627e-16
grad ChooseDest W: 4.0726518630981445
grad AddEdge W: 4.616269423196682e-15
grad ChooseDest W: 5.267213821411133
grad AddEdge W: 4.455704163649262e-15
grad ChooseDest W: 1.978697419166565
grad AddEdge W: 4.901635670159603e-15
grad ChooseDest W: 3.064917802810669
grad AddEdge W: 5.0286592720286995e-15
grad ChooseDest W: 3.943148136138916
grad AddEdge W: 8.686400801123997e-15
grad ChooseDest W: 4.23310661315918
grad AddEdge W: 8.818302465736332e-15
grad ChooseDest W: 4.529969215393066
grad AddEdge W: 7.640075056833729e-17
grad ChooseDest W: 3.85619854927063
grad AddEdge W: 1.5403531050346887e-16
grad ChooseDest W: 2.972673177719116
grad AddEdge W: 5.688605775821897e-17
grad ChooseDest W: 3.313138246536255
grad AddEdge W: 7.713425913074244e-15
grad ChooseDest W: 3.3031487464904785
grad AddEdge W: 1.862713727727178e-16
grad ChooseDest W: 3.3962769508361816
grad AddEdge W: 6.9046750433875755e-15
grad ChooseDest W: 6.036992073059082
grad AddEdge W: 5.143976911092807e-11
grad ChooseDest W: 2.5176751613616943
grad AddEdge W: 1.3487680166511588e-16
grad ChooseDest W: 5.170580863952637
grad AddEdge W: 5.872199478924997e-15
grad ChooseDest W: 4.476596832275391
grad AddEdge W: 6.1971187763929046e-15
grad ChooseDest W: 4.902258396148682
grad AddEdge W: 4.65739075520351e-15
grad ChooseDest W: 5.106049060821533
grad AddEdge W: 5.750516605076796e-17
grad ChooseDest W: 4.742018222808838
grad AddEdge W: 4.091558346877534e-15
grad ChooseDest W: 2.8826751708984375
grad AddEdge W: 1.0191137785383198e-16
grad ChooseDest W: 4.086786270141602
grad AddEdge W: 5.0464541637010914e-15
grad ChooseDest W: 3.9698710441589355
grad AddEdge W: 1.4282147784306504e-14
grad ChooseDest W: 4.907920837402344
grad AddEdge W: 1.6472167668937613e-16
grad ChooseDest W: 5.5353264808654785
grad AddEdge W: 9.561446213204961e-17
grad ChooseDest W: 5.392858028411865
grad AddEdge W: 8.163246941938817e-17
grad ChooseDest W: 4.599442958831787
grad AddEdge W: 6.468906423466849e-17
grad ChooseDest W: 5.820775508880615
grad AddEdge W: 3.0581187530418614e-16
grad ChooseDest W: 2.596538543701172
grad AddEdge W: 2.7987895577237087e-15
grad ChooseDest W: 2.616284132003784
grad AddEdge W: 3.4056243068680825e-13
grad ChooseDest W: 5.664371490478516
grad AddEdge W: 7.669552147505313e-15
grad ChooseDest W: 4.083436965942383
grad AddEdge W: 1.329147821916993e-16
grad ChooseDest W: 4.941031455993652
grad AddEdge W: 2.2175898270471173e-11
grad ChooseDest W: 2.8213140964508057
grad AddEdge W: 8.582565308521138e-17
grad ChooseDest W: 3.3166465759277344
grad AddEdge W: 3.3467477920934297e-13
grad ChooseDest W: 3.053741931915283
grad AddEdge W: 8.658922681738153e-17
grad ChooseDest W: 3.600754499435425
grad AddEdge W: 3.3818796270752806e-15
grad ChooseDest W: 3.79122257232666
grad AddEdge W: 1.281509233125533e-16
grad ChooseDest W: 3.142409563064575
grad AddEdge W: 1.3204939245229123e-16
grad ChooseDest W: 3.8796417713165283
grad AddEdge W: 2.048263838710419e-16
grad ChooseDest W: 3.975435972213745
grad AddEdge W: 1.0525559627848997e-16
grad ChooseDest W: 6.656957626342773
grad AddEdge W: 8.535474776516332e-15
grad ChooseDest W: 3.2554125785827637
grad AddEdge W: 9.118133653158192e-17
grad ChooseDest W: 4.850793361663818
grad AddEdge W: 5.170922284108981e-13
grad ChooseDest W: 5.97399377822876
grad AddEdge W: 5.607409726893692e-17
grad ChooseDest W: 4.792339324951172
grad AddEdge W: 3.015595314691949e-13
grad ChooseDest W: 4.167030334472656
grad AddEdge W: 2.9339834276437835e-15
grad ChooseDest W: 2.8553261756896973
grad AddEdge W: 3.863317272426864e-15
grad ChooseDest W: 3.336080551147461
grad AddEdge W: 1.9830327726511813e-16
grad ChooseDest W: 4.897143363952637
grad AddEdge W: 5.176993243468869e-17
grad ChooseDest W: 3.3570363521575928
grad AddEdge W: 2.722577751353844e-13
grad ChooseDest W: 5.3798699378967285
grad AddEdge W: 4.340697841719336e-15
grad ChooseDest W: 4.832686424255371
grad AddEdge W: 9.065358206588329e-17
grad ChooseDest W: 4.752400875091553
grad AddEdge W: 6.441435689149514e-15
grad ChooseDest W: 4.729113578796387
grad AddEdge W: 3.2048311487259395e-13
grad ChooseDest W: 6.475567817687988
grad AddEdge W: 2.361956116803121e-11
grad ChooseDest W: 2.277604341506958
grad AddEdge W: 1.1305368599337848e-16
grad ChooseDest W: 3.4326975345611572
grad AddEdge W: 3.49223842480741e-17
grad ChooseDest W: 6.364783763885498
grad AddEdge W: 2.352222935321605e-16
grad ChooseDest W: 4.015944004058838
grad AddEdge W: 2.298822753248747e-13
grad ChooseDest W: 4.079061031341553
grad AddEdge W: 3.004769124012878e-15
grad ChooseDest W: 4.678830623626709
grad AddEdge W: 1.4918482927057632e-16
grad ChooseDest W: 4.854874610900879
grad AddEdge W: 8.663894368091842e-17
grad ChooseDest W: 7.183283805847168
grad AddEdge W: 1.6054175438313337e-16
grad ChooseDest W: 4.371643543243408
grad AddEdge W: 5.146252856096014e-15
grad ChooseDest W: 6.5122389793396
=== Epoch 19: Train Loss: 5.6396, Train Log Prob: 0.0103 ===
Total mismatches: 79673
Predicted valid destination but wrong order: 8045
Epoch 19: Validation Loss: 4.6332, Validation Log Prob: 0.0153
Epoch 19: Edge Precision: 0.3631, Recall: 0.3605, F1: 0.3617, Jaccard: 0.2382
Epoch 19: TP: 2.5258410880458126, FP: 4.444953471725126, FN: 4.4956335003579095
Epoch 19: Current Learning Rate: 6e-05
[Epoch 19] ‚è±Ô∏è Total: 4826.27s | Current time: 2025-07-15 13:02:18 | üèãÔ∏è Train: 4032.36s | ‚úÖ Val: 793.91s
grad AddEdge W: 1.7557364999257023e-14
grad ChooseDest W: 9.593400955200195
grad AddEdge W: 1.4924723177598732e-16
grad ChooseDest W: 3.699298620223999
grad AddEdge W: 5.4770609516063603e-17
grad ChooseDest W: 3.8166563510894775
grad AddEdge W: 1.2951737460298318e-09
grad ChooseDest W: 3.0062122344970703
grad AddEdge W: 1.404802951881175e-16
grad ChooseDest W: 4.863248825073242
grad AddEdge W: 7.39489747919337e-15
grad ChooseDest W: 5.67888069152832
grad AddEdge W: 1.863836972824576e-16
grad ChooseDest W: 6.468587398529053
grad AddEdge W: 6.886488822493552e-15
grad ChooseDest W: 6.3249664306640625
grad AddEdge W: 1.1947914717053837e-14
grad ChooseDest W: 4.240930080413818
grad AddEdge W: 1.0090343511637897e-14
grad ChooseDest W: 3.946932554244995
grad AddEdge W: 1.426983303698417e-16
grad ChooseDest W: 3.9091129302978516
grad AddEdge W: 1.6748096243190957e-09
grad ChooseDest W: 1.1352986097335815
grad AddEdge W: 3.666069980549176e-15
grad ChooseDest W: 3.8859713077545166
grad AddEdge W: 3.9552554990712665e-15
grad ChooseDest W: 5.528666973114014
grad AddEdge W: 5.970645883219628e-15
grad ChooseDest W: 6.52924108505249
grad AddEdge W: 9.97204434912552e-15
grad ChooseDest W: 3.4047749042510986
grad AddEdge W: 3.6850998462586645e-15
grad ChooseDest W: 2.726012706756592
grad AddEdge W: 4.88381028530111e-15
grad ChooseDest W: 2.7594656944274902
grad AddEdge W: 1.161197890907394e-14
grad ChooseDest W: 3.1266427040100098
grad AddEdge W: 2.420017136314124e-17
grad ChooseDest W: 3.283876419067383
grad AddEdge W: 1.8077332161675955e-16
grad ChooseDest W: 5.755641460418701
grad AddEdge W: 8.656352307320151e-15
grad ChooseDest W: 4.836454391479492
grad AddEdge W: 3.6630464964439516e-15
grad ChooseDest W: 2.6285648345947266
grad AddEdge W: 1.3148545379787708e-16
grad ChooseDest W: 3.9743003845214844
grad AddEdge W: 7.712150281455679e-17
grad ChooseDest W: 5.020993232727051
grad AddEdge W: 1.385802944083077e-16
grad ChooseDest W: 3.7656829357147217
grad AddEdge W: 4.238260641693716e-15
grad ChooseDest W: 4.077291011810303
grad AddEdge W: 2.567253050769769e-13
grad ChooseDest W: 3.9741783142089844
grad AddEdge W: 5.834274266925955e-17
grad ChooseDest W: 3.955988883972168
grad AddEdge W: 7.824019510986331e-17
grad ChooseDest W: 5.657197952270508
grad AddEdge W: 6.141274741213376e-17
grad ChooseDest W: 5.91808557510376
grad AddEdge W: 6.120031419594034e-17
grad ChooseDest W: 4.6839375495910645
grad AddEdge W: 4.868718275763406e-15
grad ChooseDest W: 3.1417222023010254
grad AddEdge W: 2.356740638485345e-15
grad ChooseDest W: 5.565706253051758
grad AddEdge W: 7.977737462067265e-17
grad ChooseDest W: 4.908450603485107
grad AddEdge W: 1.196898714977607e-16
grad ChooseDest W: 3.4353034496307373
grad AddEdge W: 1.5026832999390238e-16
grad ChooseDest W: 4.3532867431640625
grad AddEdge W: 1.2312969876634002e-16
grad ChooseDest W: 6.293440341949463
grad AddEdge W: 5.425466718951223e-17
grad ChooseDest W: 3.736391067504883
grad AddEdge W: 9.007106824363364e-17
grad ChooseDest W: 4.69052791595459
grad AddEdge W: 4.907413758348857e-17
grad ChooseDest W: 4.522544860839844
grad AddEdge W: 1.0621458977601455e-16
grad ChooseDest W: 5.160024166107178
grad AddEdge W: 1.0297728940861272e-16
grad ChooseDest W: 4.244849681854248
grad AddEdge W: 1.0604287528648022e-14
grad ChooseDest W: 3.230037212371826
grad AddEdge W: 4.4459846193981966e-17
grad ChooseDest W: 5.624032497406006
grad AddEdge W: 5.141346841265517e-15
grad ChooseDest W: 9.625410079956055
grad AddEdge W: 6.172153750822005e-15
grad ChooseDest W: 6.030066967010498
grad AddEdge W: 1.3315921737143117e-16
grad ChooseDest W: 3.879516363143921
grad AddEdge W: 5.789672211841063e-15
grad ChooseDest W: 3.685913324356079
grad AddEdge W: 8.786126516336558e-17
grad ChooseDest W: 5.52887487411499
grad AddEdge W: 5.122313958001154e-17
grad ChooseDest W: 4.170191764831543
grad AddEdge W: 1.3674633474181075e-11
grad ChooseDest W: 2.5365872383117676
grad AddEdge W: 2.820927627773806e-13
grad ChooseDest W: 3.346045970916748
grad AddEdge W: 8.34876436274425e-17
grad ChooseDest W: 4.705406665802002
grad AddEdge W: 3.3844544462986736e-13
grad ChooseDest W: 4.986194133758545
grad AddEdge W: 8.681086489943144e-17
grad ChooseDest W: 4.60330867767334
grad AddEdge W: 1.038646689174249e-16
grad ChooseDest W: 4.5196967124938965
grad AddEdge W: 5.710864875233454e-17
grad ChooseDest W: 4.296837329864502
grad AddEdge W: 6.716811791304501e-17
grad ChooseDest W: 4.455363750457764
grad AddEdge W: 7.740877271512911e-17
grad ChooseDest W: 4.79002571105957
grad AddEdge W: 1.9043721315621247e-16
grad ChooseDest W: 8.032258033752441
grad AddEdge W: 1.5677096222530424e-16
grad ChooseDest W: 4.1222734451293945
grad AddEdge W: 1.0296208252023154e-16
grad ChooseDest W: 4.9785943031311035
grad AddEdge W: 4.0336405548145604e-13
grad ChooseDest W: 4.57175874710083
grad AddEdge W: 9.954077747992851e-17
grad ChooseDest W: 2.782684564590454
grad AddEdge W: 4.9118555461846996e-15
grad ChooseDest W: 2.7080984115600586
=== Epoch 20: Train Loss: 5.5808, Train Log Prob: 0.0108 ===
Total mismatches: 78515
Predicted valid destination but wrong order: 8085
Epoch 20: Validation Loss: 4.5774, Validation Log Prob: 0.0162
Epoch 20: Edge Precision: 0.3636, Recall: 0.3614, F1: 0.3624, Jaccard: 0.2387
Epoch 20: TP: 2.5305654974946314, FP: 4.44824624194703, FN: 4.490909090909091
Epoch 20: Current Learning Rate: 6e-05
[Epoch 20] ‚è±Ô∏è Total: 4838.19s | Current time: 2025-07-15 14:22:56 | üèãÔ∏è Train: 4025.59s | ‚úÖ Val: 812.60s
grad AddEdge W: 4.476754414696189e-16
grad ChooseDest W: 10.608753204345703
grad AddEdge W: 3.0607171324866823e-15
grad ChooseDest W: 3.7043893337249756
grad AddEdge W: 1.9239425048771241e-13
grad ChooseDest W: 4.552349090576172
grad AddEdge W: 1.3562367296635736e-16
grad ChooseDest W: 4.6878581047058105
grad AddEdge W: 2.693859645613438e-16
grad ChooseDest W: 4.6612043380737305
grad AddEdge W: 6.178883851104414e-15
grad ChooseDest W: 3.680020570755005
grad AddEdge W: 3.2748790370463283e-15
grad ChooseDest W: 3.4445641040802
grad AddEdge W: 3.967310895493063e-15
grad ChooseDest W: 5.763833045959473
grad AddEdge W: 7.384446469052791e-17
grad ChooseDest W: 4.870024681091309
grad AddEdge W: 1.1315971069557308e-16
grad ChooseDest W: 3.84989070892334
grad AddEdge W: 4.84937924302817e-15
grad ChooseDest W: 5.462094306945801
grad AddEdge W: 8.345241340958383e-15
grad ChooseDest W: 4.41804838180542
grad AddEdge W: 3.818225341130883e-17
grad ChooseDest W: 2.9286422729492188
grad AddEdge W: 3.7673003280315476e-13
grad ChooseDest W: 3.171668767929077
grad AddEdge W: 1.1906846695183127e-16
grad ChooseDest W: 5.141937255859375
grad AddEdge W: 8.382012391157452e-17
grad ChooseDest W: 2.825406312942505
grad AddEdge W: 6.698919543782734e-17
grad ChooseDest W: 6.575525283813477
grad AddEdge W: 3.1894136244266062e-15
grad ChooseDest W: 7.967926025390625
grad AddEdge W: 2.0786123674660506e-16
grad ChooseDest W: 4.665699005126953
grad AddEdge W: 3.1182120348804097e-15
grad ChooseDest W: 4.576902389526367
grad AddEdge W: 2.9307617378289018e-15
grad ChooseDest W: 2.944897174835205
grad AddEdge W: 4.059894984759747e-15
grad ChooseDest W: 4.743928909301758
grad AddEdge W: 1.0974594954245653e-16
grad ChooseDest W: 4.8858771324157715
grad AddEdge W: 8.652784604546316e-15
grad ChooseDest W: 4.989238262176514
grad AddEdge W: 2.418566106936193e-13
grad ChooseDest W: 3.3827342987060547
grad AddEdge W: 3.5569077624388306e-15
grad ChooseDest W: 5.562799453735352
grad AddEdge W: 1.5429574666496996e-16
grad ChooseDest W: 7.009200096130371
grad AddEdge W: 9.63458022901098e-17
grad ChooseDest W: 7.062885284423828
grad AddEdge W: 4.286664339464563e-15
grad ChooseDest W: 3.6177878379821777
grad AddEdge W: 1.901600348591133e-16
grad ChooseDest W: 5.59807825088501
grad AddEdge W: 3.633325380874219e-15
grad ChooseDest W: 2.6912057399749756
grad AddEdge W: 3.208525838679227e-13
grad ChooseDest W: 5.446129322052002
grad AddEdge W: 7.270625093276515e-17
grad ChooseDest W: 6.807018756866455
grad AddEdge W: 9.525144233970214e-17
grad ChooseDest W: 4.895282745361328
grad AddEdge W: 4.966807973308125e-17
grad ChooseDest W: 4.382070541381836
grad AddEdge W: 4.0532776457384415e-17
grad ChooseDest W: 3.6868879795074463
grad AddEdge W: 4.472780347865909e-15
grad ChooseDest W: 3.9741947650909424
grad AddEdge W: 4.5808676679312985e-17
grad ChooseDest W: 3.166396379470825
grad AddEdge W: 8.934274828742601e-15
grad ChooseDest W: 3.489590644836426
grad AddEdge W: 6.943987351247085e-17
grad ChooseDest W: 5.790215015411377
grad AddEdge W: 2.2621113881675153e-15
grad ChooseDest W: 2.8471574783325195
grad AddEdge W: 2.0883404868669159e-13
grad ChooseDest W: 3.5395636558532715
grad AddEdge W: 4.8567201599446675e-17
grad ChooseDest W: 4.666259288787842
grad AddEdge W: 8.994314641626354e-17
grad ChooseDest W: 4.241003513336182
grad AddEdge W: 5.228168922734055e-17
grad ChooseDest W: 4.608327865600586
grad AddEdge W: 4.459531852598345e-17
grad ChooseDest W: 6.960442066192627
grad AddEdge W: 1.6838120312864953e-16
grad ChooseDest W: 3.1318373680114746
grad AddEdge W: 3.6373931210781363e-13
grad ChooseDest W: 6.263176441192627
grad AddEdge W: 6.678919640060182e-17
grad ChooseDest W: 3.7327301502227783
grad AddEdge W: 7.55742449351641e-17
grad ChooseDest W: 4.337650775909424
grad AddEdge W: 1.1727795841534388e-16
grad ChooseDest W: 4.033276557922363
grad AddEdge W: 4.728512300136191e-15
grad ChooseDest W: 3.504983901977539
grad AddEdge W: 4.024120124715988e-15
grad ChooseDest W: 5.174401760101318
grad AddEdge W: 1.0138137725157481e-11
grad ChooseDest W: 1.9855347871780396
grad AddEdge W: 6.201884580800851e-17
grad ChooseDest W: 3.058511972427368
grad AddEdge W: 1.1182914104944478e-16
grad ChooseDest W: 4.57901668548584
grad AddEdge W: 7.253947146893975e-17
grad ChooseDest W: 6.506923675537109
grad AddEdge W: 3.0398832983577788e-15
grad ChooseDest W: 3.5052576065063477
grad AddEdge W: 6.631043087950103e-17
grad ChooseDest W: 5.2795281410217285
grad AddEdge W: 4.995073992153593e-15
grad ChooseDest W: 5.86337423324585
grad AddEdge W: 1.402776160857073e-16
grad ChooseDest W: 6.764092445373535
grad AddEdge W: 3.775053051191177e-15
grad ChooseDest W: 3.0132265090942383
grad AddEdge W: 6.286504495720536e-17
grad ChooseDest W: 4.266129970550537
grad AddEdge W: 5.630086122380669e-15
grad ChooseDest W: 4.2053608894348145
grad AddEdge W: 5.956573251364165e-17
grad ChooseDest W: 7.558457851409912
grad AddEdge W: 2.1503896482488044e-15
grad ChooseDest W: 2.4995594024658203
=== Epoch 21: Train Loss: 5.5180, Train Log Prob: 0.0115 ===
Total mismatches: 77460
Predicted valid destination but wrong order: 8153
Epoch 21: Validation Loss: 4.4907, Validation Log Prob: 0.0174
Epoch 21: Edge Precision: 0.3605, Recall: 0.3576, F1: 0.3590, Jaccard: 0.2358
Epoch 21: TP: 2.5056549749463137, FP: 4.458410880458125, FN: 4.515819613457409
Epoch 21: Current Learning Rate: 6e-05
[Epoch 21] ‚è±Ô∏è Total: 4785.18s | Current time: 2025-07-15 15:42:41 | üèãÔ∏è Train: 3986.07s | ‚úÖ Val: 799.11s
grad AddEdge W: 6.99349237713782e-15
grad ChooseDest W: 9.183694839477539
grad AddEdge W: 4.789369499814044e-15
grad ChooseDest W: 6.393576145172119
grad AddEdge W: 2.8815052557570886e-13
grad ChooseDest W: 4.232801914215088
grad AddEdge W: 2.223066769189118e-15
grad ChooseDest W: 2.8882811069488525
grad AddEdge W: 4.144831213545672e-15
grad ChooseDest W: 4.337628364562988
grad AddEdge W: 3.115149197624456e-13
grad ChooseDest W: 4.911499500274658
grad AddEdge W: 1.379046532839744e-16
grad ChooseDest W: 6.079639434814453
grad AddEdge W: 2.165895221623005e-15
grad ChooseDest W: 3.539382219314575
grad AddEdge W: 1.4032613519171722e-16
grad ChooseDest W: 4.402055263519287
grad AddEdge W: 2.876730776672971e-15
grad ChooseDest W: 3.0023064613342285
grad AddEdge W: 5.617667772340974e-15
grad ChooseDest W: 2.8701727390289307
grad AddEdge W: 4.5490103485034804e-15
grad ChooseDest W: 7.805904388427734
grad AddEdge W: 8.890739717533894e-17
grad ChooseDest W: 3.0454094409942627
grad AddEdge W: 3.4179990177703352e-15
grad ChooseDest W: 4.069482803344727
grad AddEdge W: 7.979473217864646e-17
grad ChooseDest W: 3.6896066665649414
grad AddEdge W: 5.2849561952748e-15
grad ChooseDest W: 6.198896408081055
grad AddEdge W: 4.282500643133216e-17
grad ChooseDest W: 4.38435697555542
grad AddEdge W: 4.1508773479580633e-17
grad ChooseDest W: 3.3433151245117188
grad AddEdge W: 1.0057185480475341e-16
grad ChooseDest W: 5.3136067390441895
grad AddEdge W: 5.001723200789539e-15
grad ChooseDest W: 3.7357425689697266
grad AddEdge W: 2.807322787862915e-13
grad ChooseDest W: 5.979437828063965
grad AddEdge W: 6.974082167165234e-17
grad ChooseDest W: 5.591739654541016
grad AddEdge W: 8.386929152718467e-17
grad ChooseDest W: 6.012922763824463
grad AddEdge W: 4.025859255410269e-17
grad ChooseDest W: 4.559264183044434
grad AddEdge W: 3.2519928420862276e-15
grad ChooseDest W: 5.578253269195557
grad AddEdge W: 5.685359257353749e-17
grad ChooseDest W: 4.741344928741455
grad AddEdge W: 7.123440545545769e-17
grad ChooseDest W: 5.024731636047363
grad AddEdge W: 2.5598449051129453e-17
grad ChooseDest W: 4.4994354248046875
grad AddEdge W: 1.8197331584011268e-16
grad ChooseDest W: 6.929222106933594
grad AddEdge W: 3.880866074558299e-17
grad ChooseDest W: 4.740687847137451
grad AddEdge W: 1.2594016202626367e-15
grad ChooseDest W: 4.275369644165039
grad AddEdge W: 6.4461172402489886e-15
grad ChooseDest W: 4.672267913818359
grad AddEdge W: 4.44476558016842e-15
grad ChooseDest W: 3.7657620906829834
grad AddEdge W: 6.969594216033766e-17
grad ChooseDest W: 4.69802713394165
grad AddEdge W: 6.614042872000913e-17
grad ChooseDest W: 4.224051475524902
grad AddEdge W: 4.7935844416387e-17
grad ChooseDest W: 3.58910870552063
grad AddEdge W: 4.023635463051481e-17
grad ChooseDest W: 2.8453497886657715
grad AddEdge W: 7.789667692763739e-17
grad ChooseDest W: 5.093966960906982
grad AddEdge W: 4.733246367278395e-15
grad ChooseDest W: 5.270022869110107
grad AddEdge W: 3.0960648761667884e-15
grad ChooseDest W: 4.822957992553711
grad AddEdge W: 7.444906754385517e-17
grad ChooseDest W: 3.7907567024230957
grad AddEdge W: 3.552804589258543e-17
grad ChooseDest W: 6.600793361663818
grad AddEdge W: 3.5098027779343613e-15
grad ChooseDest W: 5.041737079620361
grad AddEdge W: 5.3907221553737905e-17
grad ChooseDest W: 5.25909948348999
grad AddEdge W: 8.740544894117946e-17
grad ChooseDest W: 5.06642484664917
grad AddEdge W: 3.773625085991004e-17
grad ChooseDest W: 4.301182746887207
grad AddEdge W: 3.419123877524289e-15
grad ChooseDest W: 3.233022689819336
grad AddEdge W: 1.5989295361531618e-16
grad ChooseDest W: 3.832763195037842
grad AddEdge W: 2.791277963668339e-17
grad ChooseDest W: 3.7138376235961914
grad AddEdge W: 5.641907128904093e-17
grad ChooseDest W: 6.124849319458008
grad AddEdge W: 4.236965951833838e-15
grad ChooseDest W: 3.582152843475342
grad AddEdge W: 1.1589069050130875e-16
grad ChooseDest W: 5.382131576538086
grad AddEdge W: 4.284645026153199e-17
grad ChooseDest W: 4.212801456451416
grad AddEdge W: 1.0134484516101687e-16
grad ChooseDest W: 3.743720769882202
grad AddEdge W: 2.1645868733568521e-16
grad ChooseDest W: 5.172683238983154
grad AddEdge W: 1.9193389655120918e-15
grad ChooseDest W: 6.187295913696289
grad AddEdge W: 2.807142626072616e-17
grad ChooseDest W: 5.670435428619385
grad AddEdge W: 2.6684744366435587e-13
grad ChooseDest W: 3.8397557735443115
grad AddEdge W: 5.1331976696288815e-17
grad ChooseDest W: 4.7065887451171875
grad AddEdge W: 2.4299537195230335e-15
grad ChooseDest W: 3.1931822299957275
grad AddEdge W: 3.53965704973038e-17
grad ChooseDest W: 4.72921895980835
grad AddEdge W: 6.675787603388812e-17
grad ChooseDest W: 4.330955505371094
grad AddEdge W: 4.8010492503586236e-17
grad ChooseDest W: 4.222657680511475
grad AddEdge W: 8.798211955958203e-17
grad ChooseDest W: 3.9657747745513916
grad AddEdge W: 4.580017988006084e-17
grad ChooseDest W: 3.679734706878662
grad AddEdge W: 6.745521576004992e-17
grad ChooseDest W: 7.530975818634033
=== Epoch 22: Train Loss: 5.4547, Train Log Prob: 0.0122 ===
Total mismatches: 76403
Predicted valid destination but wrong order: 8207
Epoch 22: Validation Loss: 4.4343, Validation Log Prob: 0.0184
Epoch 22: Edge Precision: 0.3599, Recall: 0.3559, F1: 0.3577, Jaccard: 0.2356
Epoch 22: TP: 2.4937723693629206, FP: 4.448818897637795, FN: 4.527702219040802
Epoch 22: Current Learning Rate: 6e-05
[Epoch 22] ‚è±Ô∏è Total: 4791.10s | Current time: 2025-07-15 17:02:32 | üèãÔ∏è Train: 3985.27s | ‚úÖ Val: 805.83s
grad AddEdge W: 3.64653020100544e-15
grad ChooseDest W: 10.037606239318848
grad AddEdge W: 9.525411578744191e-17
grad ChooseDest W: 3.729886531829834
grad AddEdge W: 2.0845893760490118e-15
grad ChooseDest W: 3.950861692428589
grad AddEdge W: 9.381812362660495e-17
grad ChooseDest W: 4.978037357330322
grad AddEdge W: 6.944322961582655e-15
grad ChooseDest W: 3.5156302452087402
grad AddEdge W: 7.607563550037944e-17
grad ChooseDest W: 3.642920970916748
grad AddEdge W: 3.928481211125032e-15
grad ChooseDest W: 4.078579902648926
grad AddEdge W: 3.4458534447778867e-17
grad ChooseDest W: 3.3659894466400146
grad AddEdge W: 1.8297785975198572e-13
grad ChooseDest W: 3.2670421600341797
grad AddEdge W: 6.96248046276581e-17
grad ChooseDest W: 5.668149948120117
grad AddEdge W: 5.135690355243753e-15
grad ChooseDest W: 6.362567901611328
grad AddEdge W: 4.155073138897177e-17
grad ChooseDest W: 2.8177974224090576
grad AddEdge W: 1.5873611561474654e-13
grad ChooseDest W: 5.232001781463623
grad AddEdge W: 2.6659178286285315e-15
grad ChooseDest W: 4.879282474517822
grad AddEdge W: 6.380519197165353e-15
grad ChooseDest W: 3.6406302452087402
grad AddEdge W: 6.763346325588775e-17
grad ChooseDest W: 5.19460916519165
grad AddEdge W: 2.7447852781056162e-15
grad ChooseDest W: 4.6484856605529785
grad AddEdge W: 2.046573055068581e-15
grad ChooseDest W: 3.550579786300659
grad AddEdge W: 5.501121650392058e-17
grad ChooseDest W: 3.920515537261963
grad AddEdge W: 1.9832928160011531e-13
grad ChooseDest W: 2.136810064315796
grad AddEdge W: 6.998862210269574e-13
grad ChooseDest W: 4.053018569946289
grad AddEdge W: 4.427504081024782e-17
grad ChooseDest W: 9.201753616333008
grad AddEdge W: 1.6686369066408425e-16
grad ChooseDest W: 8.455613136291504
grad AddEdge W: 3.2652559042029017e-13
grad ChooseDest W: 3.569622755050659
grad AddEdge W: 2.9915844209141545e-15
grad ChooseDest W: 2.6017403602600098
grad AddEdge W: 4.044405637360443e-17
grad ChooseDest W: 5.446791172027588
grad AddEdge W: 4.917884303186782e-15
grad ChooseDest W: 3.295924663543701
grad AddEdge W: 2.5840614447260477e-17
grad ChooseDest W: 4.041022777557373
grad AddEdge W: 8.014823608522712e-17
grad ChooseDest W: 2.8616156578063965
grad AddEdge W: 5.163861417460669e-13
grad ChooseDest W: 2.286813497543335
grad AddEdge W: 4.462959424358969e-15
grad ChooseDest W: 4.268304347991943
grad AddEdge W: 6.146765235247258e-17
grad ChooseDest W: 6.53653621673584
grad AddEdge W: 2.3216642034287113e-17
grad ChooseDest W: 4.25154447555542
grad AddEdge W: 4.095617712572581e-17
grad ChooseDest W: 5.272817611694336
grad AddEdge W: 5.534287622488494e-17
grad ChooseDest W: 5.640728950500488
grad AddEdge W: 3.592283246447959e-15
grad ChooseDest W: 5.176166534423828
grad AddEdge W: 9.767717979391154e-15
grad ChooseDest W: 6.816779136657715
grad AddEdge W: 5.620739537324192e-15
grad ChooseDest W: 6.903817176818848
grad AddEdge W: 1.1861957390049997e-15
grad ChooseDest W: 2.0877034664154053
grad AddEdge W: 4.721803375677463e-15
grad ChooseDest W: 2.383793354034424
grad AddEdge W: 5.007403350794267e-17
grad ChooseDest W: 8.588790893554688
grad AddEdge W: 4.1341094043248784e-17
grad ChooseDest W: 5.533007621765137
grad AddEdge W: 6.206954205339066e-17
grad ChooseDest W: 4.517931938171387
grad AddEdge W: 8.75010699611994e-15
grad ChooseDest W: 6.960089206695557
grad AddEdge W: 4.4627303019467365e-15
grad ChooseDest W: 6.271335124969482
grad AddEdge W: 1.7286773547993059e-15
grad ChooseDest W: 6.545264720916748
grad AddEdge W: 1.6051711419469775e-14
grad ChooseDest W: 3.452441453933716
grad AddEdge W: 4.961451866345281e-15
grad ChooseDest W: 7.4169182777404785
grad AddEdge W: 3.616557093133899e-15
grad ChooseDest W: 2.817431688308716
grad AddEdge W: 3.664164937276352e-17
grad ChooseDest W: 5.1610260009765625
grad AddEdge W: 5.2870684836870156e-17
grad ChooseDest W: 6.425942420959473
grad AddEdge W: 3.6503252130116047e-17
grad ChooseDest W: 5.023138523101807
grad AddEdge W: 6.066423498455697e-17
grad ChooseDest W: 5.0427656173706055
grad AddEdge W: 5.9277538288365645e-15
grad ChooseDest W: 7.778348922729492
grad AddEdge W: 4.942305890947569e-17
grad ChooseDest W: 4.347708702087402
grad AddEdge W: 1.193312192190475e-16
grad ChooseDest W: 4.686699867248535
grad AddEdge W: 4.929789774776915e-15
grad ChooseDest W: 6.392116546630859
grad AddEdge W: 4.87610265605801e-17
grad ChooseDest W: 6.3169708251953125
grad AddEdge W: 2.29417094994638e-15
grad ChooseDest W: 4.595898151397705
grad AddEdge W: 3.4541177731906795e-15
grad ChooseDest W: 4.277185916900635
grad AddEdge W: 2.4165523877480496e-15
grad ChooseDest W: 4.9424920082092285
grad AddEdge W: 5.299432186866723e-17
grad ChooseDest W: 4.933476448059082
grad AddEdge W: 4.6696102510239475e-17
grad ChooseDest W: 4.537320613861084
grad AddEdge W: 1.4414312029132487e-13
grad ChooseDest W: 4.865350723266602
grad AddEdge W: 8.420702150976553e-14
grad ChooseDest W: 3.724884271621704
grad AddEdge W: 4.6576144777807035e-17
grad ChooseDest W: 6.119025230407715
=== Epoch 23: Train Loss: 5.3909, Train Log Prob: 0.0129 ===
Total mismatches: 75413
Predicted valid destination but wrong order: 8213
Epoch 23: Validation Loss: 4.3481, Validation Log Prob: 0.0200
Epoch 23: Edge Precision: 0.3601, Recall: 0.3565, F1: 0.3582, Jaccard: 0.2359
Epoch 23: TP: 2.49663564781675, FP: 4.455833929849678, FN: 4.524838940586972
Epoch 23: Current Learning Rate: 6e-05
[Epoch 23] ‚è±Ô∏è Total: 4797.29s | Current time: 2025-07-15 18:22:29 | üèãÔ∏è Train: 3997.33s | ‚úÖ Val: 799.96s
grad AddEdge W: 3.1916313684407548e-15
grad ChooseDest W: 9.508383750915527
grad AddEdge W: 2.668345636813599e-15
grad ChooseDest W: 5.21328592300415
grad AddEdge W: 7.579329559625794e-17
grad ChooseDest W: 5.7575883865356445
grad AddEdge W: 3.2904907012971723e-15
grad ChooseDest W: 3.9370737075805664
grad AddEdge W: 4.2398237483536454e-17
grad ChooseDest W: 3.037196397781372
grad AddEdge W: 6.383892108831099e-17
grad ChooseDest W: 5.284885406494141
grad AddEdge W: 3.93882856560869e-15
grad ChooseDest W: 6.260268211364746
grad AddEdge W: 3.4044934215335e-17
grad ChooseDest W: 4.391907215118408
grad AddEdge W: 3.1933557157631277e-15
grad ChooseDest W: 4.562594413757324
grad AddEdge W: 3.978939916705036e-17
grad ChooseDest W: 5.35223913192749
grad AddEdge W: 1.804571769792318e-11
grad ChooseDest W: 3.9885106086730957
grad AddEdge W: 3.5301491048974506e-17
grad ChooseDest W: 6.208985328674316
grad AddEdge W: 3.37242892314074e-17
grad ChooseDest W: 9.092723846435547
grad AddEdge W: 6.178118689185468e-17
grad ChooseDest W: 10.388916969299316
grad AddEdge W: 1.7066785790526867e-17
grad ChooseDest W: 3.362854480743408
grad AddEdge W: 4.152526746099494e-17
grad ChooseDest W: 6.366523265838623
grad AddEdge W: 3.985769450714519e-17
grad ChooseDest W: 3.4089653491973877
grad AddEdge W: 2.3439938514203517e-15
grad ChooseDest W: 2.6158177852630615
grad AddEdge W: 6.797336487212669e-15
grad ChooseDest W: 5.146859169006348
grad AddEdge W: 3.751287424273589e-15
grad ChooseDest W: 3.764321804046631
grad AddEdge W: 4.5838805905944617e-17
grad ChooseDest W: 6.00549840927124
grad AddEdge W: 2.342357946249073e-17
grad ChooseDest W: 5.031635761260986
grad AddEdge W: 2.226354951090359e-15
grad ChooseDest W: 4.718156814575195
grad AddEdge W: 4.865634189097784e-17
grad ChooseDest W: 5.5060553550720215
grad AddEdge W: 5.183623592386849e-17
grad ChooseDest W: 4.849130153656006
grad AddEdge W: 5.983051104248641e-15
grad ChooseDest W: 4.634222507476807
grad AddEdge W: 4.618655594418436e-17
grad ChooseDest W: 3.816558837890625
grad AddEdge W: 2.984707863341209e-17
grad ChooseDest W: 4.441555500030518
grad AddEdge W: 5.7433738144606165e-15
grad ChooseDest W: 5.190846920013428
grad AddEdge W: 2.5757941583153844e-15
grad ChooseDest W: 4.621410846710205
grad AddEdge W: 2.780822731597914e-17
grad ChooseDest W: 7.8662261962890625
grad AddEdge W: 2.483550474649543e-13
grad ChooseDest W: 4.850724220275879
grad AddEdge W: 4.370839932663854e-15
grad ChooseDest W: 4.109148025512695
grad AddEdge W: 5.3083568039316803e-17
grad ChooseDest W: 4.009868621826172
grad AddEdge W: 5.550087566281645e-16
grad ChooseDest W: 5.119607448577881
grad AddEdge W: 2.6446731041108708e-17
grad ChooseDest W: 4.6690216064453125
grad AddEdge W: 2.202236873763419e-13
grad ChooseDest W: 6.6567463874816895
grad AddEdge W: 6.0814424514017e-17
grad ChooseDest W: 6.2143964767456055
grad AddEdge W: 3.5026656783207965e-15
grad ChooseDest W: 5.520738124847412
grad AddEdge W: 1.582134460995885e-17
grad ChooseDest W: 6.724091053009033
grad AddEdge W: 5.742859559582515e-17
grad ChooseDest W: 2.6399009227752686
grad AddEdge W: 6.99523482978944e-17
grad ChooseDest W: 5.235592842102051
grad AddEdge W: 3.674690234853383e-15
grad ChooseDest W: 3.3007054328918457
grad AddEdge W: 2.7629208887810413e-15
grad ChooseDest W: 4.431998252868652
grad AddEdge W: 1.842415443414066e-17
grad ChooseDest W: 5.13883113861084
grad AddEdge W: 2.0016371101837932e-13
grad ChooseDest W: 4.630119800567627
grad AddEdge W: 1.95202300233132e-15
grad ChooseDest W: 4.700623989105225
grad AddEdge W: 5.40264778370109e-17
grad ChooseDest W: 3.9217870235443115
grad AddEdge W: 3.32173732320491e-15
grad ChooseDest W: 2.2405707836151123
grad AddEdge W: 3.5050329236501355e-15
grad ChooseDest W: 3.9430015087127686
grad AddEdge W: 3.950050481610389e-15
grad ChooseDest W: 2.082167625427246
grad AddEdge W: 3.047505430212765e-17
grad ChooseDest W: 4.236310005187988
grad AddEdge W: 5.019953665920167e-17
grad ChooseDest W: 4.251422882080078
grad AddEdge W: 5.040096506452568e-17
grad ChooseDest W: 4.5969672203063965
grad AddEdge W: 2.077310398416782e-15
grad ChooseDest W: 3.9057364463806152
grad AddEdge W: 2.0613696552484847e-17
grad ChooseDest W: 3.229139566421509
grad AddEdge W: 7.110733727847974e-17
grad ChooseDest W: 4.596850395202637
grad AddEdge W: 3.363038226196556e-15
grad ChooseDest W: 3.595088243484497
grad AddEdge W: 5.472234187295991e-17
grad ChooseDest W: 3.71313214302063
grad AddEdge W: 1.7193497221050231e-15
grad ChooseDest W: 3.2632248401641846
grad AddEdge W: 3.7823849286779406e-17
grad ChooseDest W: 5.364604473114014
grad AddEdge W: 4.3426829957801243e-17
grad ChooseDest W: 3.865662097930908
grad AddEdge W: 2.774954685862683e-15
grad ChooseDest W: 4.643980026245117
grad AddEdge W: 5.787577605241621e-17
grad ChooseDest W: 6.083029747009277
grad AddEdge W: 2.3093116638785507e-15
grad ChooseDest W: 3.314227342605591
grad AddEdge W: 2.9267025441874223e-15
grad ChooseDest W: 4.474100589752197
=== Epoch 24: Train Loss: 5.3264, Train Log Prob: 0.0137 ===
Total mismatches: 74307
Predicted valid destination but wrong order: 8273
Epoch 24: Validation Loss: 4.2345, Validation Log Prob: 0.0220
Epoch 24: Edge Precision: 0.3612, Recall: 0.3573, F1: 0.3591, Jaccard: 0.2361
Epoch 24: TP: 2.5035075161059415, FP: 4.442806012884753, FN: 4.517967072297781
Epoch 24: Current Learning Rate: 6e-05
[Epoch 24] ‚è±Ô∏è Total: 4811.19s | Current time: 2025-07-15 19:42:41 | üèãÔ∏è Train: 4013.31s | ‚úÖ Val: 797.88s
grad AddEdge W: 2.509641520409534e-15
grad ChooseDest W: 9.333208084106445
grad AddEdge W: 4.134791332021867e-17
grad ChooseDest W: 5.70164680480957
grad AddEdge W: 3.8495685380294975e-17
grad ChooseDest W: 4.968875885009766
grad AddEdge W: 3.357163470372196e-17
grad ChooseDest W: 4.633930206298828
grad AddEdge W: 1.120904064011441e-11
grad ChooseDest W: 2.6603024005889893
grad AddEdge W: 6.719550090004297e-17
grad ChooseDest W: 5.53115701675415
grad AddEdge W: 2.709905394397415e-17
grad ChooseDest W: 3.4789416790008545
grad AddEdge W: 2.6729219837740467e-17
grad ChooseDest W: 5.20211935043335
grad AddEdge W: 5.470865947765358e-12
grad ChooseDest W: 1.0674713850021362
grad AddEdge W: 4.488578134372022e-17
grad ChooseDest W: 5.515021324157715
grad AddEdge W: 2.3305171866526214e-17
grad ChooseDest W: 3.814791679382324
grad AddEdge W: 4.9612509739529936e-17
grad ChooseDest W: 3.8477189540863037
grad AddEdge W: 5.4649123153859165e-17
grad ChooseDest W: 4.091672897338867
grad AddEdge W: 7.29675804493278e-17
grad ChooseDest W: 4.6077375411987305
grad AddEdge W: 4.742945768027184e-17
grad ChooseDest W: 6.250261306762695
grad AddEdge W: 2.8101533517440724e-15
grad ChooseDest W: 2.09071683883667
grad AddEdge W: 5.615596829724496e-17
grad ChooseDest W: 6.052934646606445
grad AddEdge W: 5.724336669561737e-17
grad ChooseDest W: 4.647202491760254
grad AddEdge W: 2.3907552912728155e-17
grad ChooseDest W: 4.0846848487854
grad AddEdge W: 5.1163834038813936e-17
grad ChooseDest W: 2.734785556793213
grad AddEdge W: 3.2873975487320364e-15
grad ChooseDest W: 3.466233015060425
grad AddEdge W: 2.914822682109054e-17
grad ChooseDest W: 5.548975467681885
grad AddEdge W: 3.510801350369835e-17
grad ChooseDest W: 5.414616107940674
grad AddEdge W: 2.5377566615112724e-15
grad ChooseDest W: 3.563387393951416
grad AddEdge W: 3.474781274287846e-17
grad ChooseDest W: 3.836264133453369
grad AddEdge W: 6.538506935454278e-15
grad ChooseDest W: 3.324625253677368
grad AddEdge W: 7.715228055078866e-17
grad ChooseDest W: 5.136327266693115
grad AddEdge W: 5.599525041294836e-17
grad ChooseDest W: 3.630988359451294
grad AddEdge W: 4.322223629623246e-15
grad ChooseDest W: 5.0601019859313965
grad AddEdge W: 5.300995187647534e-15
grad ChooseDest W: 5.800748348236084
grad AddEdge W: 3.6357208429886134e-17
grad ChooseDest W: 4.549866199493408
grad AddEdge W: 3.1310917002390034e-17
grad ChooseDest W: 7.313896179199219
grad AddEdge W: 3.451158981226802e-17
grad ChooseDest W: 2.840764284133911
grad AddEdge W: 3.6960464765525055e-15
grad ChooseDest W: 4.584920883178711
grad AddEdge W: 4.423031350016585e-17
grad ChooseDest W: 4.770602226257324
grad AddEdge W: 4.162464824850951e-17
grad ChooseDest W: 5.499216556549072
grad AddEdge W: 3.4035927872825525e-17
grad ChooseDest W: 5.965735912322998
grad AddEdge W: 3.609825722302069e-15
grad ChooseDest W: 5.983662128448486
grad AddEdge W: 3.358287853670117e-15
grad ChooseDest W: 5.485502243041992
grad AddEdge W: 3.2477436880714366e-17
grad ChooseDest W: 7.586560249328613
grad AddEdge W: 3.989772091671879e-15
grad ChooseDest W: 3.432299852371216
grad AddEdge W: 7.735607800138703e-17
grad ChooseDest W: 5.505837917327881
grad AddEdge W: 2.3702340983282325e-17
grad ChooseDest W: 3.8228533267974854
grad AddEdge W: 3.3318731312099916e-15
grad ChooseDest W: 3.283535957336426
grad AddEdge W: 3.928881262139042e-17
grad ChooseDest W: 4.574595928192139
grad AddEdge W: 5.8823293458409864e-12
grad ChooseDest W: 2.2723681926727295
grad AddEdge W: 2.6825477191262038e-17
grad ChooseDest W: 5.909026145935059
grad AddEdge W: 5.303517797348245e-17
grad ChooseDest W: 3.4461262226104736
grad AddEdge W: 6.775300078056901e-17
grad ChooseDest W: 6.846776962280273
grad AddEdge W: 4.009080062120753e-17
grad ChooseDest W: 4.434496879577637
grad AddEdge W: 3.098869706657613e-17
grad ChooseDest W: 4.453362941741943
grad AddEdge W: 8.693664929209871e-17
grad ChooseDest W: 4.563549995422363
grad AddEdge W: 1.7010306229173992e-15
grad ChooseDest W: 5.329069137573242
grad AddEdge W: 5.887492912897557e-14
grad ChooseDest W: 3.918691635131836
grad AddEdge W: 4.117841779230835e-15
grad ChooseDest W: 5.698148250579834
grad AddEdge W: 3.012144120770613e-17
grad ChooseDest W: 5.389827251434326
grad AddEdge W: 2.4599567250106343e-17
grad ChooseDest W: 4.977142810821533
grad AddEdge W: 1.1954794318651436e-15
grad ChooseDest W: 4.570844650268555
grad AddEdge W: 5.796015218066577e-15
grad ChooseDest W: 4.728157043457031
grad AddEdge W: 8.4563937953272e-17
grad ChooseDest W: 4.809497833251953
grad AddEdge W: 4.362148527592077e-15
grad ChooseDest W: 3.8155930042266846
grad AddEdge W: 1.3845668384629023e-17
grad ChooseDest W: 4.138265132904053
grad AddEdge W: 1.98446034080466e-15
grad ChooseDest W: 6.399099826812744
grad AddEdge W: 3.430115824080808e-15
grad ChooseDest W: 4.873706340789795
grad AddEdge W: 6.488697586507462e-15
grad ChooseDest W: 7.5323662757873535
grad AddEdge W: 2.4441041394433005e-17
grad ChooseDest W: 5.141304016113281
=== Epoch 25: Train Loss: 5.2688, Train Log Prob: 0.0145 ===
Total mismatches: 73631
Predicted valid destination but wrong order: 8213
Epoch 25: Validation Loss: 4.1874, Validation Log Prob: 0.0229
Epoch 25: Edge Precision: 0.3618, Recall: 0.3573, F1: 0.3594, Jaccard: 0.2367
Epoch 25: TP: 2.5035075161059415, FP: 4.432498210450967, FN: 4.517967072297781
Epoch 25: Current Learning Rate: 6e-05
üíæ Checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/checkpoint_epoch_25.pth
[Epoch 25] ‚è±Ô∏è Total: 4809.42s | Current time: 2025-07-15 21:02:50 | üèãÔ∏è Train: 4009.33s | ‚úÖ Val: 800.10s
grad AddEdge W: 2.978046293318189e-15
grad ChooseDest W: 11.274205207824707
grad AddEdge W: 2.829035621962502e-15
grad ChooseDest W: 7.032593250274658
grad AddEdge W: 2.2988596161226116e-13
grad ChooseDest W: 4.994232177734375
grad AddEdge W: 1.6925210122919272e-15
grad ChooseDest W: 3.762517213821411
grad AddEdge W: 3.2307110466422347e-17
grad ChooseDest W: 5.270447731018066
grad AddEdge W: 3.625467032706067e-15
grad ChooseDest W: 4.325875282287598
grad AddEdge W: 1.0500425534501825e-13
grad ChooseDest W: 4.495406150817871
grad AddEdge W: 1.1026903870950036e-16
grad ChooseDest W: 4.6521148681640625
grad AddEdge W: 2.475631135874024e-17
grad ChooseDest W: 4.8331193923950195
grad AddEdge W: 1.9378224949706015e-15
grad ChooseDest W: 4.804406642913818
grad AddEdge W: 2.5209088519355842e-17
grad ChooseDest W: 5.075255393981934
grad AddEdge W: 1.626682835367745e-15
grad ChooseDest W: 5.5156097412109375
grad AddEdge W: 5.769575508134507e-17
grad ChooseDest W: 2.6656064987182617
grad AddEdge W: 3.665771652898175e-17
grad ChooseDest W: 2.471031904220581
grad AddEdge W: 8.582813462704904e-17
grad ChooseDest W: 2.170987129211426
grad AddEdge W: 2.998103067891611e-13
grad ChooseDest W: 3.742703437805176
grad AddEdge W: 1.9045176822627095e-17
grad ChooseDest W: 4.499790668487549
grad AddEdge W: 3.0735981742738153e-15
grad ChooseDest W: 4.687891006469727
grad AddEdge W: 3.549215948889043e-17
grad ChooseDest W: 3.5949103832244873
grad AddEdge W: 1.809360631157067e-15
grad ChooseDest W: 5.125252723693848
grad AddEdge W: 4.331956885220145e-15
grad ChooseDest W: 4.1683197021484375
grad AddEdge W: 1.7376272107991132e-15
grad ChooseDest W: 7.6157965660095215
grad AddEdge W: 1.3948139188039847e-15
grad ChooseDest W: 3.4591779708862305
grad AddEdge W: 1.1753206914655312e-13
grad ChooseDest W: 6.225456714630127
grad AddEdge W: 4.1111495174983665e-17
grad ChooseDest W: 6.074878692626953
grad AddEdge W: 4.3903080849839874e-17
grad ChooseDest W: 5.640013217926025
grad AddEdge W: 1.6108543372998488e-17
grad ChooseDest W: 5.1754021644592285
grad AddEdge W: 2.2349358183625188e-15
grad ChooseDest W: 4.268025875091553
grad AddEdge W: 1.0834427545087318e-16
grad ChooseDest W: 5.14110803604126
grad AddEdge W: 4.840991499267984e-15
grad ChooseDest W: 2.905534267425537
grad AddEdge W: 2.3813479316023724e-17
grad ChooseDest W: 6.3114094734191895
grad AddEdge W: 5.41583337353743e-15
grad ChooseDest W: 4.32996129989624
grad AddEdge W: 4.091142334586424e-17
grad ChooseDest W: 5.751758098602295
grad AddEdge W: 7.743956368625078e-17
grad ChooseDest W: 3.8486366271972656
grad AddEdge W: 2.7842597663047452e-15
grad ChooseDest W: 4.098305702209473
grad AddEdge W: 8.023479888196957e-17
grad ChooseDest W: 5.52724027633667
grad AddEdge W: 4.8121847557648124e-17
grad ChooseDest W: 3.3844118118286133
grad AddEdge W: 1.4154031721693686e-15
grad ChooseDest W: 5.368145942687988
grad AddEdge W: 4.3362111346675094e-17
grad ChooseDest W: 3.4096555709838867
grad AddEdge W: 3.4104214610241983e-15
grad ChooseDest W: 5.13793420791626
grad AddEdge W: 4.798127648435086e-17
grad ChooseDest W: 4.273217678070068
grad AddEdge W: 6.401712226203452e-17
grad ChooseDest W: 4.230350971221924
grad AddEdge W: 4.057873792094031e-17
grad ChooseDest W: 4.507135391235352
grad AddEdge W: 1.1163308098012178e-13
grad ChooseDest W: 4.574796676635742
grad AddEdge W: 3.2210816452256028e-15
grad ChooseDest W: 5.329582691192627
grad AddEdge W: 5.420260444175814e-17
grad ChooseDest W: 4.300078392028809
grad AddEdge W: 1.0820272036351922e-15
grad ChooseDest W: 2.1453871726989746
grad AddEdge W: 4.2353043643589006e-17
grad ChooseDest W: 6.225857734680176
grad AddEdge W: 4.288623103155089e-15
grad ChooseDest W: 2.0098869800567627
grad AddEdge W: 2.4330625421976936e-15
grad ChooseDest W: 8.551803588867188
grad AddEdge W: 1.4123277014170066e-15
grad ChooseDest W: 3.390665292739868
grad AddEdge W: 2.4310032529016857e-17
grad ChooseDest W: 5.279813766479492
grad AddEdge W: 1.3739872844551328e-15
grad ChooseDest W: 5.11830472946167
grad AddEdge W: 2.702630870834999e-13
grad ChooseDest W: 3.6856813430786133
grad AddEdge W: 3.0730520762508527e-17
grad ChooseDest W: 4.168590068817139
grad AddEdge W: 2.9599428698947597e-15
grad ChooseDest W: 3.750547409057617
grad AddEdge W: 8.902815892732679e-17
grad ChooseDest W: 5.256676197052002
grad AddEdge W: 3.1758322452107716e-17
grad ChooseDest W: 4.568821907043457
grad AddEdge W: 5.876972668401452e-17
grad ChooseDest W: 4.4408183097839355
grad AddEdge W: 2.5449647001341696e-15
grad ChooseDest W: 7.628195285797119
grad AddEdge W: 3.232737877371006e-15
grad ChooseDest W: 5.960679054260254
grad AddEdge W: 4.7094191451836745e-17
grad ChooseDest W: 3.850146532058716
grad AddEdge W: 1.6665336841281407e-17
grad ChooseDest W: 4.100829601287842
grad AddEdge W: 5.573177684423797e-17
grad ChooseDest W: 4.538717746734619
grad AddEdge W: 4.925766368277457e-15
grad ChooseDest W: 2.8167176246643066
grad AddEdge W: 3.7067669490494295e-15
grad ChooseDest W: 3.173067331314087
=== Epoch 26: Train Loss: 5.2023, Train Log Prob: 0.0153 ===
Total mismatches: 72272
Predicted valid destination but wrong order: 8286
Epoch 26: Validation Loss: 4.0899, Validation Log Prob: 0.0251
Epoch 26: Edge Precision: 0.3576, Recall: 0.3523, F1: 0.3547, Jaccard: 0.2329
Epoch 26: TP: 2.4670007158196134, FP: 4.448389405869721, FN: 4.554473872584109
Epoch 26: Current Learning Rate: 6e-05
[Epoch 26] ‚è±Ô∏è Total: 4801.03s | Current time: 2025-07-15 22:22:51 | üèãÔ∏è Train: 4003.83s | ‚úÖ Val: 797.20s
grad AddEdge W: 2.6549159294348822e-15
grad ChooseDest W: 8.302251815795898
grad AddEdge W: 4.2621770002249093e-10
grad ChooseDest W: 5.053676128387451
grad AddEdge W: 4.2870299532953116e-17
grad ChooseDest W: 5.5971150398254395
grad AddEdge W: 1.3249662055448558e-15
grad ChooseDest W: 2.805751085281372
grad AddEdge W: 1.791267557417462e-17
grad ChooseDest W: 4.411727428436279
grad AddEdge W: 3.297983329755457e-17
grad ChooseDest W: 4.983898162841797
grad AddEdge W: 1.5638820590354125e-17
grad ChooseDest W: 3.5998969078063965
grad AddEdge W: 2.5819155727809626e-17
grad ChooseDest W: 5.554083824157715
grad AddEdge W: 3.387810842939531e-17
grad ChooseDest W: 6.643849849700928
grad AddEdge W: 4.70763111157158e-17
grad ChooseDest W: 3.1370575428009033
grad AddEdge W: 3.1773767568505306e-17
grad ChooseDest W: 4.14857816696167
grad AddEdge W: 3.5338765130865125e-15
grad ChooseDest W: 7.928574085235596
grad AddEdge W: 1.0190161050515896e-16
grad ChooseDest W: 3.2254199981689453
grad AddEdge W: 1.2725150794201667e-13
grad ChooseDest W: 4.6237969398498535
grad AddEdge W: 1.3662258156803232e-15
grad ChooseDest W: 3.680086851119995
grad AddEdge W: 1.6542555577458854e-15
grad ChooseDest W: 3.0521700382232666
grad AddEdge W: 1.3291746887432888e-17
grad ChooseDest W: 5.689229965209961
grad AddEdge W: 3.491567746766752e-17
grad ChooseDest W: 6.047252655029297
grad AddEdge W: 2.192506455726893e-15
grad ChooseDest W: 3.3923113346099854
grad AddEdge W: 2.5379110531182442e-17
grad ChooseDest W: 5.899781703948975
grad AddEdge W: 2.989133531081274e-15
grad ChooseDest W: 5.421014785766602
grad AddEdge W: 2.628194177383712e-17
grad ChooseDest W: 3.192366600036621
grad AddEdge W: 2.5687925839733804e-15
grad ChooseDest W: 3.8851301670074463
grad AddEdge W: 1.3861436895558897e-15
grad ChooseDest W: 5.3256635665893555
grad AddEdge W: 4.800848410905896e-17
grad ChooseDest W: 8.835649490356445
grad AddEdge W: 6.419020995079502e-14
grad ChooseDest W: 6.244095802307129
grad AddEdge W: 1.075777650554749e-13
grad ChooseDest W: 3.9164376258850098
grad AddEdge W: 1.4140117087952666e-15
grad ChooseDest W: 7.15446138381958
grad AddEdge W: 2.006419458203991e-15
grad ChooseDest W: 4.209650993347168
grad AddEdge W: 5.634784860930608e-10
grad ChooseDest W: 2.408123254776001
grad AddEdge W: 2.4626887371377744e-17
grad ChooseDest W: 4.64512300491333
grad AddEdge W: 2.3497147119461072e-15
grad ChooseDest W: 3.3407535552978516
grad AddEdge W: 5.062165023450993e-17
grad ChooseDest W: 6.786155700683594
grad AddEdge W: 2.812840352011e-15
grad ChooseDest W: 4.420214653015137
grad AddEdge W: 3.867994813354729e-17
grad ChooseDest W: 5.211436748504639
grad AddEdge W: 3.7362888001183205e-17
grad ChooseDest W: 4.411777496337891
grad AddEdge W: 2.7983651942171343e-15
grad ChooseDest W: 7.9253740310668945
grad AddEdge W: 8.796061752233686e-14
grad ChooseDest W: 1.9606716632843018
grad AddEdge W: 3.3951231195544995e-17
grad ChooseDest W: 5.6589179039001465
grad AddEdge W: 1.377391460239311e-17
grad ChooseDest W: 4.638469219207764
grad AddEdge W: 8.534619950865963e-14
grad ChooseDest W: 5.4147233963012695
grad AddEdge W: 1.0834448191515407e-15
grad ChooseDest W: 5.507308483123779
grad AddEdge W: 2.3003235855169986e-15
grad ChooseDest W: 2.880951166152954
grad AddEdge W: 2.415074566717997e-17
grad ChooseDest W: 4.493322372436523
grad AddEdge W: 2.943334459623234e-15
grad ChooseDest W: 2.6878793239593506
grad AddEdge W: 5.769405439800567e-17
grad ChooseDest W: 5.393023490905762
grad AddEdge W: 6.115388884949692e-16
grad ChooseDest W: 3.7616422176361084
grad AddEdge W: 4.603795168982468e-15
grad ChooseDest W: 5.4565510749816895
grad AddEdge W: 4.2029993218440097e-17
grad ChooseDest W: 3.7710626125335693
grad AddEdge W: 1.707148417640617e-17
grad ChooseDest W: 6.983073711395264
grad AddEdge W: 7.502846454955671e-17
grad ChooseDest W: 3.803663492202759
grad AddEdge W: 2.0849939335429992e-17
grad ChooseDest W: 2.74800705909729
grad AddEdge W: 1.9102240704364678e-17
grad ChooseDest W: 5.356204509735107
grad AddEdge W: 2.426651815688047e-13
grad ChooseDest W: 5.473466873168945
grad AddEdge W: 1.3268544537425224e-15
grad ChooseDest W: 4.785027503967285
grad AddEdge W: 2.5492197701447016e-15
grad ChooseDest W: 6.69888162612915
grad AddEdge W: 1.6858691963826902e-17
grad ChooseDest W: 5.235838413238525
grad AddEdge W: 4.497836932404451e-17
grad ChooseDest W: 5.713798999786377
grad AddEdge W: 1.824976159995733e-17
grad ChooseDest W: 6.918318748474121
grad AddEdge W: 1.8164702869031407e-13
grad ChooseDest W: 1.5246673822402954
grad AddEdge W: 3.760896430725038e-17
grad ChooseDest W: 7.796093463897705
grad AddEdge W: 2.953080116311877e-17
grad ChooseDest W: 5.254128932952881
grad AddEdge W: 2.7455471842416715e-15
grad ChooseDest W: 5.560262680053711
grad AddEdge W: 4.392579853818303e-17
grad ChooseDest W: 7.4576735496521
grad AddEdge W: 2.3461309155462743e-15
grad ChooseDest W: 4.265177249908447
grad AddEdge W: 2.9456338364376744e-17
grad ChooseDest W: 3.4691362380981445
=== Epoch 27: Train Loss: 5.1421, Train Log Prob: 0.0163 ===
Total mismatches: 71406
Predicted valid destination but wrong order: 8338
Epoch 27: Validation Loss: 3.9991, Validation Log Prob: 0.0270
Epoch 27: Edge Precision: 0.3621, Recall: 0.3568, F1: 0.3592, Jaccard: 0.2363
Epoch 27: TP: 2.499212598425197, FP: 4.416607015032212, FN: 4.522261989978525
Epoch 27: Current Learning Rate: 6e-05
[Epoch 27] ‚è±Ô∏è Total: 4815.18s | Current time: 2025-07-15 23:43:06 | üèãÔ∏è Train: 4003.14s | ‚úÖ Val: 812.04s
grad AddEdge W: 3.834174256843632e-15
grad ChooseDest W: 8.972391128540039
grad AddEdge W: 2.455060642964933e-17
grad ChooseDest W: 6.748983860015869
grad AddEdge W: 4.9101428455653514e-15
grad ChooseDest W: 5.2841668128967285
grad AddEdge W: 1.478829266075149e-13
grad ChooseDest W: 1.9472885131835938
grad AddEdge W: 1.643577436896324e-15
grad ChooseDest W: 3.051401138305664
grad AddEdge W: 9.247571140108286e-16
grad ChooseDest W: 7.0570831298828125
grad AddEdge W: 3.207501682142977e-17
grad ChooseDest W: 2.750588893890381
grad AddEdge W: 2.327251808466507e-17
grad ChooseDest W: 5.174095630645752
grad AddEdge W: 3.984120772551093e-14
grad ChooseDest W: 3.496166944503784
grad AddEdge W: 1.4966724894475113e-15
grad ChooseDest W: 2.189539909362793
grad AddEdge W: 1.8112429765938825e-17
grad ChooseDest W: 5.658542633056641
grad AddEdge W: 1.3760135990232019e-15
grad ChooseDest W: 3.823246717453003
grad AddEdge W: 5.380402720578381e-12
grad ChooseDest W: 1.809590220451355
grad AddEdge W: 1.4731823599960182e-15
grad ChooseDest W: 4.721768856048584
grad AddEdge W: 3.4703640768772538e-15
grad ChooseDest W: 1.7652283906936646
grad AddEdge W: 3.0917344466937304e-17
grad ChooseDest W: 4.616610527038574
grad AddEdge W: 3.757805091339805e-17
grad ChooseDest W: 6.512325763702393
grad AddEdge W: 2.918765024908482e-17
grad ChooseDest W: 6.337685585021973
grad AddEdge W: 3.486635579794009e-15
grad ChooseDest W: 5.089949131011963
grad AddEdge W: 3.345804295328373e-17
grad ChooseDest W: 3.9647328853607178
grad AddEdge W: 1.3827950506780382e-15
grad ChooseDest W: 3.3704497814178467
grad AddEdge W: 4.422547945666609e-17
grad ChooseDest W: 2.291335344314575
grad AddEdge W: 2.938031384863822e-17
grad ChooseDest W: 3.1610405445098877
grad AddEdge W: 1.6604370983158289e-15
grad ChooseDest W: 3.440110206604004
grad AddEdge W: 2.380615744411365e-15
grad ChooseDest W: 2.2794129848480225
grad AddEdge W: 2.22153812618223e-17
grad ChooseDest W: 3.1160709857940674
grad AddEdge W: 3.412729162584323e-17
grad ChooseDest W: 3.22529673576355
grad AddEdge W: 3.2961751129364163e-17
grad ChooseDest W: 6.145304203033447
grad AddEdge W: 1.5774588920666006e-15
grad ChooseDest W: 3.904998302459717
grad AddEdge W: 2.3023037103196627e-17
grad ChooseDest W: 5.473248481750488
grad AddEdge W: 2.0464995749604068e-15
grad ChooseDest W: 7.721312999725342
grad AddEdge W: 3.4479541526615264e-17
grad ChooseDest W: 4.0214314460754395
grad AddEdge W: 9.394954442796615e-18
grad ChooseDest W: 8.486132621765137
grad AddEdge W: 1.9679815265740645e-15
grad ChooseDest W: 2.296372652053833
grad AddEdge W: 1.0735914304599192e-17
grad ChooseDest W: 4.178357124328613
grad AddEdge W: 1.3945354567225748e-15
grad ChooseDest W: 2.6087148189544678
grad AddEdge W: 2.2563012186053836e-17
grad ChooseDest W: 2.758024215698242
grad AddEdge W: 4.986343332398667e-17
grad ChooseDest W: 8.096969604492188
grad AddEdge W: 3.2369425489211763e-15
grad ChooseDest W: 5.152003765106201
grad AddEdge W: 3.6550781928113344e-17
grad ChooseDest W: 1.7695101499557495
grad AddEdge W: 4.219430638219279e-10
grad ChooseDest W: 3.732879161834717
grad AddEdge W: 9.520958700070696e-17
grad ChooseDest W: 6.471271514892578
grad AddEdge W: 2.618464256184949e-13
grad ChooseDest W: 2.6213858127593994
grad AddEdge W: 7.533625911723219e-16
grad ChooseDest W: 5.025903701782227
grad AddEdge W: 1.2412229718431617e-13
grad ChooseDest W: 3.7152981758117676
grad AddEdge W: 2.4944877105539054e-17
grad ChooseDest W: 3.365962505340576
grad AddEdge W: 8.162349450974197e-18
grad ChooseDest W: 4.908735752105713
grad AddEdge W: 3.621814944274862e-17
grad ChooseDest W: 4.248961925506592
grad AddEdge W: 1.8617843737653625e-17
grad ChooseDest W: 5.072479724884033
grad AddEdge W: 1.2470902478372016e-17
grad ChooseDest W: 4.565145492553711
grad AddEdge W: 1.6922547263091342e-15
grad ChooseDest W: 3.59218692779541
grad AddEdge W: 2.0303483507221618e-15
grad ChooseDest W: 5.3538641929626465
grad AddEdge W: 9.471808289817285e-16
grad ChooseDest W: 7.242462158203125
grad AddEdge W: 1.9146896524865897e-13
grad ChooseDest W: 1.6219890117645264
grad AddEdge W: 2.7211262648428194e-17
grad ChooseDest W: 3.477843761444092
grad AddEdge W: 1.2221347116822892e-15
grad ChooseDest W: 5.029037952423096
grad AddEdge W: 2.3645313497491694e-17
grad ChooseDest W: 4.394296646118164
grad AddEdge W: 2.3880289767657346e-15
grad ChooseDest W: 1.919068455696106
grad AddEdge W: 3.561382121338473e-17
grad ChooseDest W: 3.3044772148132324
grad AddEdge W: 7.343149525940174e-12
grad ChooseDest W: 1.0888289213180542
grad AddEdge W: 2.71299295522161e-15
grad ChooseDest W: 3.240321397781372
grad AddEdge W: 3.820874635196768e-17
grad ChooseDest W: 4.75449275970459
grad AddEdge W: 7.474885765017909e-16
grad ChooseDest W: 3.608232259750366
grad AddEdge W: 3.1685007780055916e-17
grad ChooseDest W: 7.1832780838012695
grad AddEdge W: 1.3492016181108142e-15
grad ChooseDest W: 5.152334213256836
grad AddEdge W: 3.581460441783095e-17
grad ChooseDest W: 5.369050979614258
=== Epoch 28: Train Loss: 5.0862, Train Log Prob: 0.0171 ===
Total mismatches: 70585
Predicted valid destination but wrong order: 8323
Epoch 28: Validation Loss: 3.9600, Validation Log Prob: 0.0283
Epoch 28: Edge Precision: 0.3619, Recall: 0.3567, F1: 0.3591, Jaccard: 0.2361
Epoch 28: TP: 2.4982104509663565, FP: 4.423478883321403, FN: 4.523264137437366
Epoch 28: Current Learning Rate: 6e-05
[Epoch 28] ‚è±Ô∏è Total: 4805.61s | Current time: 2025-07-16 01:03:12 | üèãÔ∏è Train: 4007.18s | ‚úÖ Val: 798.43s
grad AddEdge W: 5.941434257967669e-15
grad ChooseDest W: 11.94337272644043
grad AddEdge W: 1.9900362461998718e-17
grad ChooseDest W: 4.460482120513916
grad AddEdge W: 2.9218467689986094e-17
grad ChooseDest W: 5.881235122680664
grad AddEdge W: 2.076878299117119e-17
grad ChooseDest W: 6.062312602996826
grad AddEdge W: 1.5745874503754085e-15
grad ChooseDest W: 5.100728511810303
grad AddEdge W: 2.4138442182748857e-17
grad ChooseDest W: 5.390239238739014
grad AddEdge W: 1.1085849687842851e-15
grad ChooseDest W: 6.829296112060547
grad AddEdge W: 4.287483128945224e-12
grad ChooseDest W: 1.9192407131195068
grad AddEdge W: 2.110977992689005e-17
grad ChooseDest W: 5.268737316131592
grad AddEdge W: 4.2006438423317037e-17
grad ChooseDest W: 3.0640318393707275
grad AddEdge W: 3.1923682474001966e-17
grad ChooseDest W: 4.103989124298096
grad AddEdge W: 2.666246689170303e-15
grad ChooseDest W: 3.389552354812622
grad AddEdge W: 5.559537145250553e-17
grad ChooseDest W: 4.5400824546813965
grad AddEdge W: 2.1560008180078905e-15
grad ChooseDest W: 3.1187684535980225
grad AddEdge W: 4.010197086819945e-17
grad ChooseDest W: 3.1614937782287598
grad AddEdge W: 1.4930634872000526e-17
grad ChooseDest W: 5.084758758544922
grad AddEdge W: 9.432315709523798e-18
grad ChooseDest W: 3.9142637252807617
grad AddEdge W: 4.359219487660472e-15
grad ChooseDest W: 2.837646007537842
grad AddEdge W: 6.527069873083977e-14
grad ChooseDest W: 3.400573968887329
grad AddEdge W: 2.2746768704771266e-17
grad ChooseDest W: 3.8701601028442383
grad AddEdge W: 1.4356077666975806e-15
grad ChooseDest W: 4.242258548736572
grad AddEdge W: 2.9752606137129064e-15
grad ChooseDest W: 2.5079452991485596
grad AddEdge W: 4.3564929415428196e-17
grad ChooseDest W: 4.087875843048096
grad AddEdge W: 1.9452356479156323e-17
grad ChooseDest W: 6.206486701965332
grad AddEdge W: 3.0794762917601845e-17
grad ChooseDest W: 5.360743999481201
grad AddEdge W: 3.640177692129049e-17
grad ChooseDest W: 4.596593856811523
grad AddEdge W: 4.949946061957354e-17
grad ChooseDest W: 3.3468515872955322
grad AddEdge W: 1.2388422248086435e-15
grad ChooseDest W: 2.306999921798706
grad AddEdge W: 1.747860109955774e-15
grad ChooseDest W: 4.283537864685059
grad AddEdge W: 1.7723011397696785e-15
grad ChooseDest W: 1.6155081987380981
grad AddEdge W: 8.6725570005131915e-16
grad ChooseDest W: 2.7191667556762695
grad AddEdge W: 9.961995024507841e-18
grad ChooseDest W: 4.252293586730957
grad AddEdge W: 1.9686226908104666e-17
grad ChooseDest W: 7.087153911590576
grad AddEdge W: 2.9659282164582954e-17
grad ChooseDest W: 5.670849800109863
grad AddEdge W: 6.515473780277828e-16
grad ChooseDest W: 2.825730323791504
grad AddEdge W: 1.622977165485181e-17
grad ChooseDest W: 4.798279762268066
grad AddEdge W: 2.335818289413453e-15
grad ChooseDest W: 5.153343200683594
grad AddEdge W: 1.1840543338352224e-15
grad ChooseDest W: 4.776504039764404
grad AddEdge W: 8.281340209270563e-18
grad ChooseDest W: 4.1425981521606445
grad AddEdge W: 2.0070027131800695e-17
grad ChooseDest W: 4.562320709228516
grad AddEdge W: 1.6352601358371033e-17
grad ChooseDest W: 6.65734338760376
grad AddEdge W: 3.177227864340271e-17
grad ChooseDest W: 3.484804391860962
grad AddEdge W: 2.834583184841205e-17
grad ChooseDest W: 5.493196487426758
grad AddEdge W: 1.9159044586692124e-15
grad ChooseDest W: 5.717866897583008
grad AddEdge W: 1.3248755002276057e-17
grad ChooseDest W: 5.602407455444336
grad AddEdge W: 1.919210229738999e-17
grad ChooseDest W: 4.909323215484619
grad AddEdge W: 8.686345108707715e-16
grad ChooseDest W: 6.031082630157471
grad AddEdge W: 1.6552921341674225e-17
grad ChooseDest W: 6.593636512756348
grad AddEdge W: 3.0231926067763614e-17
grad ChooseDest W: 3.853060245513916
grad AddEdge W: 9.535337878443725e-16
grad ChooseDest W: 3.209825277328491
grad AddEdge W: 5.130691974117336e-17
grad ChooseDest W: 3.231905937194824
grad AddEdge W: 8.057639138772947e-17
grad ChooseDest W: 4.941404819488525
grad AddEdge W: 3.0521621261891935e-17
grad ChooseDest W: 4.935807704925537
grad AddEdge W: 3.3349566487753525e-17
grad ChooseDest W: 4.32797384262085
grad AddEdge W: 2.3825620143659186e-15
grad ChooseDest W: 6.155834197998047
grad AddEdge W: 1.785817138629897e-15
grad ChooseDest W: 4.810019493103027
grad AddEdge W: 1.7410346986144642e-17
grad ChooseDest W: 6.0091986656188965
grad AddEdge W: 1.254775634524502e-13
grad ChooseDest W: 2.580052375793457
grad AddEdge W: 1.4520645157199003e-13
grad ChooseDest W: 6.098207950592041
grad AddEdge W: 2.292688854046922e-15
grad ChooseDest W: 4.569106578826904
grad AddEdge W: 4.143169679010294e-17
grad ChooseDest W: 5.291591644287109
grad AddEdge W: 2.6054822793048434e-17
grad ChooseDest W: 3.7932026386260986
grad AddEdge W: 2.1842684250401347e-15
grad ChooseDest W: 10.256166458129883
grad AddEdge W: 6.053018540320908e-17
grad ChooseDest W: 5.232892036437988
grad AddEdge W: 3.908792022910334e-17
grad ChooseDest W: 5.148229122161865
grad AddEdge W: 1.780849713910724e-17
grad ChooseDest W: 4.039604187011719
=== Epoch 29: Train Loss: 5.0251, Train Log Prob: 0.0180 ===
Total mismatches: 69588
Predicted valid destination but wrong order: 8316
Epoch 29: Validation Loss: 3.8926, Validation Log Prob: 0.0299
Epoch 29: Edge Precision: 0.3609, Recall: 0.3555, F1: 0.3580, Jaccard: 0.2356
Epoch 29: TP: 2.490050107372942, FP: 4.4259126700071585, FN: 4.53142448103078
Epoch 29: Current Learning Rate: 6e-05
[Epoch 29] ‚è±Ô∏è Total: 4801.26s | Current time: 2025-07-16 02:23:13 | üèãÔ∏è Train: 4002.26s | ‚úÖ Val: 799.00s
grad AddEdge W: 1.540101223905955e-14
grad ChooseDest W: 8.630431175231934
grad AddEdge W: 9.074807421597767e-18
grad ChooseDest W: 5.018815994262695
grad AddEdge W: 3.4432778690481926e-15
grad ChooseDest W: 3.5149519443511963
grad AddEdge W: 1.3569449815563762e-15
grad ChooseDest W: 4.612576484680176
grad AddEdge W: 1.419589103115581e-15
grad ChooseDest W: 4.8275651931762695
grad AddEdge W: 2.005621976684151e-15
grad ChooseDest W: 3.046325922012329
grad AddEdge W: 9.457660523492423e-18
grad ChooseDest W: 3.1387224197387695
grad AddEdge W: 2.4922543229000123e-17
grad ChooseDest W: 3.1876142024993896
grad AddEdge W: 9.329039827766388e-16
grad ChooseDest W: 4.368125915527344
grad AddEdge W: 1.1835956158634475e-17
grad ChooseDest W: 7.630090236663818
grad AddEdge W: 4.839300067116546e-17
grad ChooseDest W: 3.0028162002563477
grad AddEdge W: 2.8191102751750334e-17
grad ChooseDest W: 9.436619758605957
grad AddEdge W: 3.951611607377892e-12
grad ChooseDest W: 3.0770108699798584
grad AddEdge W: 1.0451063080137981e-17
grad ChooseDest W: 5.2136149406433105
grad AddEdge W: 1.5115408822871396e-15
grad ChooseDest W: 5.737491130828857
grad AddEdge W: 2.686247532570031e-17
grad ChooseDest W: 4.389735698699951
grad AddEdge W: 1.7189400757959073e-15
grad ChooseDest W: 4.167697906494141
grad AddEdge W: 2.032283821006638e-15
grad ChooseDest W: 4.02628755569458
grad AddEdge W: 9.513792536639128e-16
grad ChooseDest W: 4.161747455596924
grad AddEdge W: 2.8464552118648114e-17
grad ChooseDest W: 2.495215892791748
grad AddEdge W: 2.356686865128084e-17
grad ChooseDest W: 2.3207149505615234
grad AddEdge W: 1.4252495662217303e-17
grad ChooseDest W: 3.491939067840576
grad AddEdge W: 1.5285258781130585e-17
grad ChooseDest W: 3.2927393913269043
grad AddEdge W: 3.7867673315632465e-17
grad ChooseDest W: 4.188054084777832
grad AddEdge W: 1.4398901535206616e-15
grad ChooseDest W: 4.457057952880859
grad AddEdge W: 2.1044985215147546e-17
grad ChooseDest W: 3.3896114826202393
grad AddEdge W: 1.523622351441844e-17
grad ChooseDest W: 4.665223121643066
grad AddEdge W: 3.061843448078514e-17
grad ChooseDest W: 6.026505947113037
grad AddEdge W: 8.91879318404916e-16
grad ChooseDest W: 2.452993392944336
grad AddEdge W: 6.358233627974194e-17
grad ChooseDest W: 6.131722450256348
grad AddEdge W: 1.1892105410225146e-15
grad ChooseDest W: 4.867308139801025
grad AddEdge W: 2.909706404584291e-17
grad ChooseDest W: 5.666135311126709
grad AddEdge W: 2.206194752999984e-17
grad ChooseDest W: 5.028787612915039
grad AddEdge W: 3.1288460703120444e-17
grad ChooseDest W: 6.163010120391846
grad AddEdge W: 2.613675172399936e-17
grad ChooseDest W: 5.100642681121826
grad AddEdge W: 1.4460951794025566e-17
grad ChooseDest W: 5.989544868469238
grad AddEdge W: 6.371573407770725e-12
grad ChooseDest W: 4.668667793273926
grad AddEdge W: 2.5066724118490566e-17
grad ChooseDest W: 4.804185390472412
grad AddEdge W: 1.5100810210025468e-15
grad ChooseDest W: 5.232773303985596
grad AddEdge W: 3.816520025580044e-17
grad ChooseDest W: 3.639846086502075
grad AddEdge W: 9.664504050152902e-16
grad ChooseDest W: 5.908352375030518
grad AddEdge W: 3.896386298955509e-17
grad ChooseDest W: 4.169144630432129
grad AddEdge W: 2.1138896684451916e-17
grad ChooseDest W: 6.26348876953125
grad AddEdge W: 1.4245467935733052e-17
grad ChooseDest W: 4.253061294555664
grad AddEdge W: 1.6802880565885622e-15
grad ChooseDest W: 2.837291955947876
grad AddEdge W: 1.0571199880629427e-15
grad ChooseDest W: 3.098268508911133
grad AddEdge W: 8.781781454369336e-14
grad ChooseDest W: 5.179730415344238
grad AddEdge W: 1.906155665311687e-17
grad ChooseDest W: 3.907949924468994
grad AddEdge W: 1.3690552868889788e-13
grad ChooseDest W: 4.602829933166504
grad AddEdge W: 1.2409191241843226e-13
grad ChooseDest W: 4.7907280921936035
grad AddEdge W: 2.9967092614126186e-17
grad ChooseDest W: 4.476774215698242
grad AddEdge W: 8.717124168428569e-16
grad ChooseDest W: 3.4209160804748535
grad AddEdge W: 1.957555609784548e-15
grad ChooseDest W: 4.831355094909668
grad AddEdge W: 9.431355055047603e-16
grad ChooseDest W: 4.1757683753967285
grad AddEdge W: 2.0492089421910977e-17
grad ChooseDest W: 3.7964394092559814
grad AddEdge W: 3.987608438652347e-17
grad ChooseDest W: 4.533380031585693
grad AddEdge W: 9.169944128030807e-14
grad ChooseDest W: 1.2208682298660278
grad AddEdge W: 1.2060528382666007e-15
grad ChooseDest W: 4.295645713806152
grad AddEdge W: 2.960365261402754e-17
grad ChooseDest W: 3.9465038776397705
grad AddEdge W: 2.0458884406889628e-15
grad ChooseDest W: 4.730500221252441
grad AddEdge W: 1.6634950524268416e-13
grad ChooseDest W: 7.4120988845825195
grad AddEdge W: 1.5980171228502913e-15
grad ChooseDest W: 5.966747760772705
grad AddEdge W: 6.707044045584781e-14
grad ChooseDest W: 5.4877471923828125
grad AddEdge W: 1.0468686163088424e-17
grad ChooseDest W: 7.598752498626709
grad AddEdge W: 1.618310126442373e-15
grad ChooseDest W: 2.7536401748657227
grad AddEdge W: 1.8555002826517972e-17
grad ChooseDest W: 6.0470709800720215
=== Epoch 30: Train Loss: 4.9685, Train Log Prob: 0.0189 ===
Total mismatches: 68604
Predicted valid destination but wrong order: 8401
Epoch 30: Validation Loss: 3.8567, Validation Log Prob: 0.0308
Epoch 30: Edge Precision: 0.3625, Recall: 0.3563, F1: 0.3591, Jaccard: 0.2366
Epoch 30: TP: 2.495919828203293, FP: 4.40200429491768, FN: 4.525554760200429
Epoch 30: Current Learning Rate: 6e-05
[Epoch 30] ‚è±Ô∏è Total: 4809.47s | Current time: 2025-07-16 03:43:22 | üèãÔ∏è Train: 4013.33s | ‚úÖ Val: 796.13s
grad AddEdge W: 4.6958185224380694e-15
grad ChooseDest W: 9.392934799194336
grad AddEdge W: 1.975975333839328e-17
grad ChooseDest W: 4.466876983642578
grad AddEdge W: 1.0478844490258502e-13
grad ChooseDest W: 2.7597904205322266
grad AddEdge W: 2.3283834577189286e-15
grad ChooseDest W: 2.8718156814575195
grad AddEdge W: 2.079522133790961e-17
grad ChooseDest W: 4.5618062019348145
grad AddEdge W: 3.193776108802762e-17
grad ChooseDest W: 4.8235554695129395
grad AddEdge W: 1.5647156916567435e-17
grad ChooseDest W: 4.667750358581543
grad AddEdge W: 1.9960250338347557e-17
grad ChooseDest W: 4.470489978790283
grad AddEdge W: 1.3917829172813536e-15
grad ChooseDest W: 3.571606159210205
grad AddEdge W: 1.8439293493711604e-17
grad ChooseDest W: 4.541651725769043
grad AddEdge W: 1.0262025443343319e-15
grad ChooseDest W: 2.84123158454895
grad AddEdge W: 6.071957270579228e-16
grad ChooseDest W: 4.989207744598389
grad AddEdge W: 1.2909081754060628e-15
grad ChooseDest W: 6.7389421463012695
grad AddEdge W: 2.1081252452796566e-15
grad ChooseDest W: 3.870129108428955
grad AddEdge W: 7.931308939250297e-16
grad ChooseDest W: 5.221142292022705
grad AddEdge W: 3.687891455094578e-17
grad ChooseDest W: 6.5149335861206055
grad AddEdge W: 3.5661208736316666e-17
grad ChooseDest W: 6.101257801055908
grad AddEdge W: 1.8158593061564184e-17
grad ChooseDest W: 5.091348171234131
grad AddEdge W: 9.132333366425522e-16
grad ChooseDest W: 4.358765602111816
grad AddEdge W: 1.5813143742842654e-15
grad ChooseDest W: 4.0974650382995605
grad AddEdge W: 1.2108304548136946e-17
grad ChooseDest W: 5.646539211273193
grad AddEdge W: 1.0621657199616497e-11
grad ChooseDest W: 4.944637298583984
grad AddEdge W: 1.2136181130350024e-15
grad ChooseDest W: 5.288591384887695
grad AddEdge W: 4.2099393671833296e-17
grad ChooseDest W: 4.357525825500488
grad AddEdge W: 1.705337997212424e-15
grad ChooseDest W: 4.3374176025390625
grad AddEdge W: 2.501086461172486e-17
grad ChooseDest W: 6.202554225921631
grad AddEdge W: 1.797123546168084e-15
grad ChooseDest W: 3.906337022781372
grad AddEdge W: 3.222118956183524e-17
grad ChooseDest W: 2.9811909198760986
grad AddEdge W: 6.215650586729408e-16
grad ChooseDest W: 5.399503231048584
grad AddEdge W: 2.3988659626150205e-17
grad ChooseDest W: 4.771103858947754
grad AddEdge W: 4.622610454141363e-12
grad ChooseDest W: 4.469470500946045
grad AddEdge W: 6.0153860576144815e-16
grad ChooseDest W: 4.155026435852051
grad AddEdge W: 3.0442198688197044e-17
grad ChooseDest W: 3.416013717651367
grad AddEdge W: 1.3226440978071826e-17
grad ChooseDest W: 4.879831790924072
grad AddEdge W: 3.9262409016237727e-17
grad ChooseDest W: 3.9619038105010986
grad AddEdge W: 6.31667541225504e-17
grad ChooseDest W: 4.238161087036133
grad AddEdge W: 2.8182202817754855e-15
grad ChooseDest W: 4.443572044372559
grad AddEdge W: 1.576873592300048e-17
grad ChooseDest W: 4.684056758880615
grad AddEdge W: 2.7565016363553847e-17
grad ChooseDest W: 5.718752861022949
grad AddEdge W: 1.6224514094878422e-17
grad ChooseDest W: 3.823136568069458
grad AddEdge W: 3.5117866879155084e-17
grad ChooseDest W: 4.605506420135498
grad AddEdge W: 1.0495439136244634e-15
grad ChooseDest W: 7.557133197784424
grad AddEdge W: 1.4136361555622777e-15
grad ChooseDest W: 4.609511375427246
grad AddEdge W: 1.1117283080515458e-15
grad ChooseDest W: 4.716076850891113
grad AddEdge W: 2.9188358950963875e-14
grad ChooseDest W: 9.158214569091797
grad AddEdge W: 4.0888907489590546e-17
grad ChooseDest W: 4.294003963470459
grad AddEdge W: 4.9430152810408945e-17
grad ChooseDest W: 2.613656759262085
grad AddEdge W: 2.6479158175482012e-17
grad ChooseDest W: 4.185712814331055
grad AddEdge W: 1.8072232096891198e-17
grad ChooseDest W: 5.335179805755615
grad AddEdge W: 1.450874959854133e-17
grad ChooseDest W: 5.271980285644531
grad AddEdge W: 1.4073911934004086e-15
grad ChooseDest W: 6.739017963409424
grad AddEdge W: 2.0500979959134697e-17
grad ChooseDest W: 5.987697601318359
grad AddEdge W: 8.119270907729417e-14
grad ChooseDest W: 4.5063395500183105
grad AddEdge W: 9.970659741424334e-18
grad ChooseDest W: 3.053988456726074
grad AddEdge W: 2.0410405336421365e-17
grad ChooseDest W: 5.950026512145996
grad AddEdge W: 9.37399947633881e-18
grad ChooseDest W: 3.492577075958252
grad AddEdge W: 2.1996380496263008e-15
grad ChooseDest W: 3.5569396018981934
grad AddEdge W: 1.653140306521927e-17
grad ChooseDest W: 6.122749328613281
grad AddEdge W: 9.0510054979949e-16
grad ChooseDest W: 5.416873931884766
grad AddEdge W: 1.7319246276561727e-17
grad ChooseDest W: 5.1212334632873535
grad AddEdge W: 2.5607104669059208e-17
grad ChooseDest W: 2.794006586074829
grad AddEdge W: 1.4079539409147406e-15
grad ChooseDest W: 5.103933811187744
grad AddEdge W: 9.24487334017128e-16
grad ChooseDest W: 8.124361038208008
grad AddEdge W: 3.5783168245831485e-17
grad ChooseDest W: 2.7129647731781006
grad AddEdge W: 3.213943433881295e-17
grad ChooseDest W: 3.842116117477417
grad AddEdge W: 4.152280577149198e-17
grad ChooseDest W: 4.055765628814697
=== Epoch 31: Train Loss: 4.9147, Train Log Prob: 0.0199 ===
Total mismatches: 67916
Predicted valid destination but wrong order: 8431
Epoch 31: Validation Loss: 3.7822, Validation Log Prob: 0.0330
Epoch 31: Edge Precision: 0.3598, Recall: 0.3531, F1: 0.3562, Jaccard: 0.2336
Epoch 31: TP: 2.472870436649964, FP: 4.417752326413744, FN: 4.548604151753758
Epoch 31: Current Learning Rate: 6e-05
[Epoch 31] ‚è±Ô∏è Total: 4800.81s | Current time: 2025-07-16 05:03:23 | üèãÔ∏è Train: 4003.11s | ‚úÖ Val: 797.69s
grad AddEdge W: 9.093928892597012e-16
grad ChooseDest W: 10.894946098327637
grad AddEdge W: 1.9617157326956488e-17
grad ChooseDest W: 4.712996006011963
grad AddEdge W: 1.9260955355741025e-15
grad ChooseDest W: 2.8536736965179443
grad AddEdge W: 3.412736772645959e-17
grad ChooseDest W: 5.176301002502441
grad AddEdge W: 2.100325765901339e-15
grad ChooseDest W: 5.718019008636475
grad AddEdge W: 4.108751686338698e-17
grad ChooseDest W: 4.387816905975342
grad AddEdge W: 1.7529295558229767e-17
grad ChooseDest W: 7.36782169342041
grad AddEdge W: 1.7151599795105481e-15
grad ChooseDest W: 3.5011098384857178
grad AddEdge W: 4.95698536897018e-17
grad ChooseDest W: 6.689693450927734
grad AddEdge W: 2.227661499411499e-15
grad ChooseDest W: 5.738124847412109
grad AddEdge W: 9.644508282280473e-12
grad ChooseDest W: 2.1167125701904297
grad AddEdge W: 4.016485975581063e-17
grad ChooseDest W: 1.9748700857162476
grad AddEdge W: 2.7728976795851658e-17
grad ChooseDest W: 3.548992156982422
grad AddEdge W: 1.777827692260823e-17
grad ChooseDest W: 2.611698865890503
grad AddEdge W: 1.8345683118150203e-17
grad ChooseDest W: 3.9548351764678955
grad AddEdge W: 2.415274744426235e-17
grad ChooseDest W: 3.6485047340393066
grad AddEdge W: 1.556691444144938e-15
grad ChooseDest W: 3.2553012371063232
grad AddEdge W: 2.5861421348388636e-17
grad ChooseDest W: 3.6209073066711426
grad AddEdge W: 1.7286749659016968e-17
grad ChooseDest W: 4.104560852050781
grad AddEdge W: 2.5858504709548774e-17
grad ChooseDest W: 4.259997844696045
grad AddEdge W: 2.965427606751578e-17
grad ChooseDest W: 6.593796730041504
grad AddEdge W: 3.239977454736299e-17
grad ChooseDest W: 3.896169900894165
grad AddEdge W: 3.955545594620811e-17
grad ChooseDest W: 5.269002437591553
grad AddEdge W: 9.264103105656321e-16
grad ChooseDest W: 5.896889686584473
grad AddEdge W: 2.42744257123689e-17
grad ChooseDest W: 2.5560038089752197
grad AddEdge W: 3.314952589297403e-15
grad ChooseDest W: 3.120134115219116
grad AddEdge W: 5.542715944420809e-14
grad ChooseDest W: 2.8380565643310547
grad AddEdge W: 3.171454805409141e-17
grad ChooseDest W: 1.8336554765701294
grad AddEdge W: 2.7600623180201805e-17
grad ChooseDest W: 3.0668206214904785
grad AddEdge W: 1.9552501581556893e-17
grad ChooseDest W: 5.554081916809082
grad AddEdge W: 1.098799938298485e-13
grad ChooseDest W: 2.2472167015075684
grad AddEdge W: 7.10260504382075e-14
grad ChooseDest W: 4.312349319458008
grad AddEdge W: 2.3095564299305276e-17
grad ChooseDest W: 2.206737756729126
grad AddEdge W: 1.8319916574418074e-15
grad ChooseDest W: 2.4618210792541504
grad AddEdge W: 9.47142712499102e-16
grad ChooseDest W: 3.289417266845703
grad AddEdge W: 2.969324289181193e-17
grad ChooseDest W: 3.9802844524383545
grad AddEdge W: 1.0844146751848693e-17
grad ChooseDest W: 3.7714731693267822
grad AddEdge W: 5.260030265568372e-17
grad ChooseDest W: 2.5814144611358643
grad AddEdge W: 1.7340929989139192e-17
grad ChooseDest W: 4.769259452819824
grad AddEdge W: 6.204956795770322e-14
grad ChooseDest W: 3.0813968181610107
grad AddEdge W: 1.748176245151002e-17
grad ChooseDest W: 3.656339168548584
grad AddEdge W: 8.046474715132544e-16
grad ChooseDest W: 4.329339504241943
grad AddEdge W: 9.440976290537228e-16
grad ChooseDest W: 4.064838886260986
grad AddEdge W: 2.070039666120898e-17
grad ChooseDest W: 4.545444488525391
grad AddEdge W: 3.987651968204902e-15
grad ChooseDest W: 6.260453701019287
grad AddEdge W: 3.1218439003999994e-15
grad ChooseDest W: 5.523837566375732
grad AddEdge W: 2.7891332497761116e-17
grad ChooseDest W: 3.333839178085327
grad AddEdge W: 1.4515221459653945e-17
grad ChooseDest W: 4.508232593536377
grad AddEdge W: 1.2385475632221174e-15
grad ChooseDest W: 4.0797834396362305
grad AddEdge W: 1.3163551263280032e-17
grad ChooseDest W: 3.7247977256774902
grad AddEdge W: 2.1162918009440456e-17
grad ChooseDest W: 4.333846569061279
grad AddEdge W: 1.8965879980385312e-17
grad ChooseDest W: 1.982635259628296
grad AddEdge W: 1.7736330593745665e-17
grad ChooseDest W: 5.832984447479248
grad AddEdge W: 1.1258390530809124e-17
grad ChooseDest W: 9.119892120361328
grad AddEdge W: 5.42547845300452e-14
grad ChooseDest W: 3.380739212036133
grad AddEdge W: 1.243726005024275e-15
grad ChooseDest W: 6.175783634185791
grad AddEdge W: 6.706209651957287e-18
grad ChooseDest W: 3.0802745819091797
grad AddEdge W: 1.532946662178787e-17
grad ChooseDest W: 4.191565990447998
grad AddEdge W: 1.812265371830998e-17
grad ChooseDest W: 4.469533920288086
grad AddEdge W: 1.6837340777855683e-17
grad ChooseDest W: 8.175402641296387
grad AddEdge W: 8.373529869704424e-14
grad ChooseDest W: 2.351290225982666
grad AddEdge W: 3.0071274357916015e-17
grad ChooseDest W: 4.503762722015381
grad AddEdge W: 1.6039543275021764e-17
grad ChooseDest W: 3.8932180404663086
grad AddEdge W: 3.034911426715633e-15
grad ChooseDest W: 3.247380018234253
grad AddEdge W: 2.6367942087763032e-17
grad ChooseDest W: 4.1100358963012695
grad AddEdge W: 6.851649923930342e-18
grad ChooseDest W: 4.958124160766602
=== Epoch 32: Train Loss: 4.8645, Train Log Prob: 0.0206 ===
Total mismatches: 67051
Predicted valid destination but wrong order: 8463
Epoch 32: Validation Loss: 3.7417, Validation Log Prob: 0.0344
Epoch 32: Edge Precision: 0.3593, Recall: 0.3528, F1: 0.3558, Jaccard: 0.2336
Epoch 32: TP: 2.471295633500358, FP: 4.423908375089478, FN: 4.550178954903364
Epoch 32: Current Learning Rate: 6e-05
[Epoch 32] ‚è±Ô∏è Total: 4812.36s | Current time: 2025-07-16 06:23:36 | üèãÔ∏è Train: 4003.87s | ‚úÖ Val: 808.49s
grad AddEdge W: 6.657935192885345e-15
grad ChooseDest W: 7.895527362823486
grad AddEdge W: 4.400721296279295e-17
grad ChooseDest W: 5.750339508056641
grad AddEdge W: 5.5981244352588597e-14
grad ChooseDest W: 2.2218363285064697
grad AddEdge W: 2.5311657260951192e-17
grad ChooseDest W: 3.816226005554199
grad AddEdge W: 1.0905231578872687e-10
grad ChooseDest W: 1.793792486190796
grad AddEdge W: 2.6576189770056932e-17
grad ChooseDest W: 3.544861078262329
grad AddEdge W: 1.0766893045719916e-17
grad ChooseDest W: 2.397829532623291
grad AddEdge W: 1.5039693606507518e-15
grad ChooseDest W: 3.160628318786621
grad AddEdge W: 2.9612328084291993e-17
grad ChooseDest W: 3.769456148147583
grad AddEdge W: 1.546373629935315e-15
grad ChooseDest W: 1.6636106967926025
grad AddEdge W: 1.014359673772957e-17
grad ChooseDest W: 5.395371913909912
grad AddEdge W: 1.0731915912041458e-15
grad ChooseDest W: 3.5944955348968506
grad AddEdge W: 1.6187552422561552e-15
grad ChooseDest W: 6.929008483886719
grad AddEdge W: 4.2020340847872216e-18
grad ChooseDest W: 6.21453857421875
grad AddEdge W: 1.4787598800477857e-17
grad ChooseDest W: 5.344470024108887
grad AddEdge W: 2.480848752949992e-15
grad ChooseDest W: 2.840611219406128
grad AddEdge W: 1.436724215679066e-17
grad ChooseDest W: 4.6401286125183105
grad AddEdge W: 7.524137427472252e-14
grad ChooseDest W: 4.588221549987793
grad AddEdge W: 5.278299045704973e-16
grad ChooseDest W: 6.917780876159668
grad AddEdge W: 1.403586533159579e-17
grad ChooseDest W: 4.928366184234619
grad AddEdge W: 1.0885967349258149e-17
grad ChooseDest W: 4.954651832580566
grad AddEdge W: 1.372141675763682e-17
grad ChooseDest W: 4.424829006195068
grad AddEdge W: 7.652243876261119e-16
grad ChooseDest W: 5.075650691986084
grad AddEdge W: 3.079016855795638e-15
grad ChooseDest W: 2.476696729660034
grad AddEdge W: 1.225171450529649e-17
grad ChooseDest W: 4.176965713500977
grad AddEdge W: 1.4141674966831124e-17
grad ChooseDest W: 2.747765064239502
grad AddEdge W: 7.949822983070731e-14
grad ChooseDest W: 3.5483806133270264
grad AddEdge W: 1.607023306541091e-15
grad ChooseDest W: 8.65295124053955
grad AddEdge W: 1.2637211706562784e-15
grad ChooseDest W: 3.427557945251465
grad AddEdge W: 1.50803727608461e-17
grad ChooseDest W: 5.746213912963867
grad AddEdge W: 9.500611049618626e-18
grad ChooseDest W: 6.496371746063232
grad AddEdge W: 1.7617822081746417e-17
grad ChooseDest W: 3.9242825508117676
grad AddEdge W: 1.0156980101818161e-13
grad ChooseDest W: 2.390507936477661
grad AddEdge W: 7.308386384547928e-18
grad ChooseDest W: 4.101537227630615
grad AddEdge W: 2.0011034264874638e-17
grad ChooseDest W: 5.266630172729492
grad AddEdge W: 1.50655397581018e-17
grad ChooseDest W: 3.9400434494018555
grad AddEdge W: 1.072105152335284e-17
grad ChooseDest W: 3.2451796531677246
grad AddEdge W: 2.7963714110081957e-17
grad ChooseDest W: 6.461019039154053
grad AddEdge W: 2.2063037754047185e-17
grad ChooseDest W: 3.8734183311462402
grad AddEdge W: 2.666476096132666e-17
grad ChooseDest W: 9.1332368850708
grad AddEdge W: 1.8378634685031865e-17
grad ChooseDest W: 6.112996578216553
grad AddEdge W: 1.0227829664471479e-15
grad ChooseDest W: 2.036630868911743
grad AddEdge W: 1.8496451674039018e-17
grad ChooseDest W: 3.8792150020599365
grad AddEdge W: 1.3206357496020182e-15
grad ChooseDest W: 9.093560218811035
grad AddEdge W: 1.0795774784726144e-15
grad ChooseDest W: 2.3252179622650146
grad AddEdge W: 6.936037814688205e-16
grad ChooseDest W: 4.689805507659912
grad AddEdge W: 2.479997987763814e-17
grad ChooseDest W: 4.220573902130127
grad AddEdge W: 2.5460066697733006e-17
grad ChooseDest W: 5.158700942993164
grad AddEdge W: 9.87858709462167e-18
grad ChooseDest W: 3.660768508911133
grad AddEdge W: 1.9011662971452244e-15
grad ChooseDest W: 4.7930145263671875
grad AddEdge W: 1.5713981595707593e-15
grad ChooseDest W: 4.896948337554932
grad AddEdge W: 1.3180943331229e-15
grad ChooseDest W: 2.679021120071411
grad AddEdge W: 2.5965947199313287e-17
grad ChooseDest W: 4.766671657562256
grad AddEdge W: 2.3280077986068213e-15
grad ChooseDest W: 5.533120632171631
grad AddEdge W: 2.791776919013831e-17
grad ChooseDest W: 6.24350118637085
grad AddEdge W: 1.1126193867120574e-15
grad ChooseDest W: 3.182018995285034
grad AddEdge W: 1.722391311539495e-15
grad ChooseDest W: 5.970312595367432
grad AddEdge W: 1.1591584671809771e-17
grad ChooseDest W: 6.312708854675293
grad AddEdge W: 1.6335281850705398e-17
grad ChooseDest W: 2.5667295455932617
grad AddEdge W: 1.902143561408119e-15
grad ChooseDest W: 4.586572170257568
grad AddEdge W: 8.30604690302744e-16
grad ChooseDest W: 4.3840765953063965
grad AddEdge W: 2.790716804340783e-17
grad ChooseDest W: 5.508609771728516
grad AddEdge W: 1.0478025032204147e-17
grad ChooseDest W: 3.329913377761841
grad AddEdge W: 1.35655147188665e-17
grad ChooseDest W: 5.624128818511963
grad AddEdge W: 8.933734845238727e-16
grad ChooseDest W: 5.974352836608887
grad AddEdge W: 3.050364497281993e-17
grad ChooseDest W: 5.062973976135254
=== Epoch 33: Train Loss: 4.8124, Train Log Prob: 0.0217 ===
Total mismatches: 66367
Predicted valid destination but wrong order: 8403
Epoch 33: Validation Loss: 3.7005, Validation Log Prob: 0.0354
Epoch 33: Edge Precision: 0.3610, Recall: 0.3544, F1: 0.3575, Jaccard: 0.2350
Epoch 33: TP: 2.4833214030064426, FP: 4.407587687902648, FN: 4.53815318539728
Epoch 33: Current Learning Rate: 6e-05
[Epoch 33] ‚è±Ô∏è Total: 4800.26s | Current time: 2025-07-16 07:43:36 | üèãÔ∏è Train: 4001.72s | ‚úÖ Val: 798.54s
grad AddEdge W: 9.602578530170324e-16
grad ChooseDest W: 8.925790786743164
grad AddEdge W: 1.0953883840632428e-17
grad ChooseDest W: 4.741544723510742
grad AddEdge W: 1.9297504959763946e-17
grad ChooseDest W: 4.963649272918701
grad AddEdge W: 5.680286903119512e-18
grad ChooseDest W: 2.5081241130828857
grad AddEdge W: 6.479414264224233e-17
grad ChooseDest W: 6.091426849365234
grad AddEdge W: 9.043015859719924e-16
grad ChooseDest W: 3.8704469203948975
grad AddEdge W: 1.2028550275016421e-17
grad ChooseDest W: 2.901460886001587
grad AddEdge W: 1.9248425025298725e-17
grad ChooseDest W: 5.1604766845703125
grad AddEdge W: 1.6469559733902356e-17
grad ChooseDest W: 4.909219741821289
grad AddEdge W: 1.1532889325206437e-15
grad ChooseDest W: 2.9362714290618896
grad AddEdge W: 1.2582628109348329e-17
grad ChooseDest W: 2.382976770401001
grad AddEdge W: 5.411772379950938e-16
grad ChooseDest W: 3.0903947353363037
grad AddEdge W: 1.8767505525882845e-17
grad ChooseDest W: 3.3518834114074707
grad AddEdge W: 2.8314802649273964e-17
grad ChooseDest W: 4.491936683654785
grad AddEdge W: 3.3170806139935915e-17
grad ChooseDest W: 4.905192852020264
grad AddEdge W: 4.585002048981737e-16
grad ChooseDest W: 3.567460060119629
grad AddEdge W: 1.605591503222876e-15
grad ChooseDest W: 4.070467948913574
grad AddEdge W: 1.186097165545763e-15
grad ChooseDest W: 6.000368595123291
grad AddEdge W: 3.482605410265863e-17
grad ChooseDest W: 4.773202419281006
grad AddEdge W: 2.5478663372264424e-17
grad ChooseDest W: 4.41578483581543
grad AddEdge W: 1.0639232607423382e-17
grad ChooseDest W: 6.62741231918335
grad AddEdge W: 5.3730039466529046e-17
grad ChooseDest W: 2.0292718410491943
grad AddEdge W: 1.3910704633110392e-17
grad ChooseDest W: 3.879115581512451
grad AddEdge W: 2.4723040963363276e-15
grad ChooseDest W: 3.544790506362915
grad AddEdge W: 2.0647349568525955e-17
grad ChooseDest W: 3.6458537578582764
grad AddEdge W: 1.1979549749890005e-17
grad ChooseDest W: 5.2742791175842285
grad AddEdge W: 2.5870363170810334e-17
grad ChooseDest W: 3.6077029705047607
grad AddEdge W: 1.0782937040880994e-17
grad ChooseDest W: 2.988231897354126
grad AddEdge W: 1.349695633533688e-17
grad ChooseDest W: 5.397915363311768
grad AddEdge W: 1.1570177237556899e-17
grad ChooseDest W: 3.6861209869384766
grad AddEdge W: 1.268195833958386e-15
grad ChooseDest W: 3.854485034942627
grad AddEdge W: 3.939284216394754e-16
grad ChooseDest W: 5.402007102966309
grad AddEdge W: 9.066861954767501e-16
grad ChooseDest W: 3.339503526687622
grad AddEdge W: 6.807164398741424e-16
grad ChooseDest W: 6.281033039093018
grad AddEdge W: 1.3827348054596647e-15
grad ChooseDest W: 3.8430986404418945
grad AddEdge W: 1.155680212409861e-15
grad ChooseDest W: 5.846428871154785
grad AddEdge W: 2.744275576029606e-17
grad ChooseDest W: 5.171022415161133
grad AddEdge W: 2.164817160439387e-15
grad ChooseDest W: 3.793018102645874
grad AddEdge W: 8.909496997453045e-16
grad ChooseDest W: 2.0245285034179688
grad AddEdge W: 1.2284556057156683e-17
grad ChooseDest W: 7.507744312286377
grad AddEdge W: 1.2162483620944639e-15
grad ChooseDest W: 3.080165386199951
grad AddEdge W: 9.018833763907529e-18
grad ChooseDest W: 5.862461090087891
grad AddEdge W: 1.7676186291406934e-17
grad ChooseDest W: 6.95793342590332
grad AddEdge W: 1.6260747914430695e-17
grad ChooseDest W: 4.335494518280029
grad AddEdge W: 3.604676092855008e-17
grad ChooseDest W: 6.69000244140625
grad AddEdge W: 1.0869602176408828e-15
grad ChooseDest W: 8.200783729553223
grad AddEdge W: 2.3398403393367065e-17
grad ChooseDest W: 3.2018167972564697
grad AddEdge W: 1.3038107106542324e-15
grad ChooseDest W: 1.9653836488723755
grad AddEdge W: 8.623947367857043e-14
grad ChooseDest W: 5.260711193084717
grad AddEdge W: 1.8437670565349775e-17
grad ChooseDest W: 4.0886664390563965
grad AddEdge W: 1.8051018222901663e-17
grad ChooseDest W: 3.232792377471924
grad AddEdge W: 2.2242484661773213e-17
grad ChooseDest W: 2.831934690475464
grad AddEdge W: 1.518628827519984e-17
grad ChooseDest W: 6.521545886993408
grad AddEdge W: 1.735056498891421e-17
grad ChooseDest W: 8.396872520446777
grad AddEdge W: 2.2882414744701387e-17
grad ChooseDest W: 5.746264934539795
grad AddEdge W: 1.3698969557353968e-17
grad ChooseDest W: 3.754469633102417
grad AddEdge W: 2.070765434390352e-17
grad ChooseDest W: 5.876473903656006
grad AddEdge W: 2.0928776265251196e-17
grad ChooseDest W: 4.516627788543701
grad AddEdge W: 1.5535919388326978e-15
grad ChooseDest W: 3.0557117462158203
grad AddEdge W: 3.739584817074324e-16
grad ChooseDest W: 2.8371589183807373
grad AddEdge W: 2.198430180643516e-15
grad ChooseDest W: 3.8378219604492188
grad AddEdge W: 1.4983773550120974e-15
grad ChooseDest W: 7.452944278717041
grad AddEdge W: 8.728437947800375e-18
grad ChooseDest W: 4.046670913696289
grad AddEdge W: 1.6128164097128246e-17
grad ChooseDest W: 4.793595314025879
grad AddEdge W: 2.2556806422713294e-10
grad ChooseDest W: 3.254922866821289
grad AddEdge W: 1.3769167479032117e-15
grad ChooseDest W: 2.9341397285461426
=== Epoch 34: Train Loss: 4.7634, Train Log Prob: 0.0226 ===
Total mismatches: 65514
Predicted valid destination but wrong order: 8417
Epoch 34: Validation Loss: 3.5974, Validation Log Prob: 0.0386
Epoch 34: Edge Precision: 0.3598, Recall: 0.3528, F1: 0.3560, Jaccard: 0.2338
Epoch 34: TP: 2.4705798138869004, FP: 4.413314244810308, FN: 4.550894774516822
Epoch 34: Current Learning Rate: 6e-05
[Epoch 34] ‚è±Ô∏è Total: 4809.49s | Current time: 2025-07-16 09:03:45 | üèãÔ∏è Train: 3998.20s | ‚úÖ Val: 811.28s
grad AddEdge W: 4.9293188244582415e-15
grad ChooseDest W: 11.384993553161621
grad AddEdge W: 5.856026867104834e-16
grad ChooseDest W: 3.6858487129211426
grad AddEdge W: 1.6729186666595425e-13
grad ChooseDest W: 3.0037333965301514
grad AddEdge W: 1.2222472611851556e-15
grad ChooseDest W: 4.489057540893555
grad AddEdge W: 1.1088887359749942e-15
grad ChooseDest W: 3.298793077468872
grad AddEdge W: 3.586031441848063e-17
grad ChooseDest W: 4.392991065979004
grad AddEdge W: 9.99369916993855e-16
grad ChooseDest W: 5.043247699737549
grad AddEdge W: 2.7543417023398863e-17
grad ChooseDest W: 4.6469244956970215
grad AddEdge W: 4.8094635912709904e-12
grad ChooseDest W: 0.8407142758369446
grad AddEdge W: 1.2110336037546928e-15
grad ChooseDest W: 4.865398406982422
grad AddEdge W: 1.8274543931109418e-17
grad ChooseDest W: 3.2446300983428955
grad AddEdge W: 1.778218783254438e-17
grad ChooseDest W: 3.176182985305786
grad AddEdge W: 4.490066066857883e-16
grad ChooseDest W: 3.582568645477295
grad AddEdge W: 7.202220035945237e-16
grad ChooseDest W: 4.722231388092041
grad AddEdge W: 3.0416814169559016e-17
grad ChooseDest W: 6.445671081542969
grad AddEdge W: 3.531209206335609e-14
grad ChooseDest W: 5.2061567306518555
grad AddEdge W: 6.2869767331322425e-18
grad ChooseDest W: 3.0292258262634277
grad AddEdge W: 5.748599946758803e-12
grad ChooseDest W: 6.533149242401123
grad AddEdge W: 9.83722889117463e-18
grad ChooseDest W: 6.0341715812683105
grad AddEdge W: 1.8837700072664094e-17
grad ChooseDest W: 5.442686080932617
grad AddEdge W: 3.151435275933201e-12
grad ChooseDest W: 2.350240707397461
grad AddEdge W: 1.5910044841404138e-17
grad ChooseDest W: 6.184643745422363
grad AddEdge W: 3.00616658279206e-17
grad ChooseDest W: 4.3310112953186035
grad AddEdge W: 7.74008119289139e-16
grad ChooseDest W: 4.473052024841309
grad AddEdge W: 5.424892986478356e-17
grad ChooseDest W: 3.784628391265869
grad AddEdge W: 2.1854194501712246e-17
grad ChooseDest W: 5.685872554779053
grad AddEdge W: 2.1284215774465232e-17
grad ChooseDest W: 8.489187240600586
grad AddEdge W: 2.125101109031613e-17
grad ChooseDest W: 3.4937713146209717
grad AddEdge W: 1.6427760643188827e-17
grad ChooseDest W: 6.8160552978515625
grad AddEdge W: 8.522798399065079e-16
grad ChooseDest W: 6.276895046234131
grad AddEdge W: 5.214155887587365e-16
grad ChooseDest W: 5.1336822509765625
grad AddEdge W: 2.2530881182339827e-17
grad ChooseDest W: 6.369775295257568
grad AddEdge W: 1.8176283079969794e-15
grad ChooseDest W: 3.2254703044891357
grad AddEdge W: 2.2052385322118727e-17
grad ChooseDest W: 4.063356876373291
grad AddEdge W: 1.3404990957316051e-15
grad ChooseDest W: 3.062835693359375
grad AddEdge W: 6.171951230538272e-17
grad ChooseDest W: 5.482522964477539
grad AddEdge W: 1.881493440784541e-17
grad ChooseDest W: 6.18098258972168
grad AddEdge W: 6.947275824315902e-16
grad ChooseDest W: 2.4550678730010986
grad AddEdge W: 2.3019954167966417e-15
grad ChooseDest W: 4.993052005767822
grad AddEdge W: 2.1534221188442034e-17
grad ChooseDest W: 2.5988094806671143
grad AddEdge W: 1.1886846791460574e-17
grad ChooseDest W: 4.562230587005615
grad AddEdge W: 2.383735671158568e-17
grad ChooseDest W: 10.080804824829102
grad AddEdge W: 7.613410634354709e-14
grad ChooseDest W: 5.0641608238220215
grad AddEdge W: 4.267632237058698e-17
grad ChooseDest W: 6.845903396606445
grad AddEdge W: 1.3200869516615362e-17
grad ChooseDest W: 9.179296493530273
grad AddEdge W: 1.3301584116324062e-15
grad ChooseDest W: 5.741147518157959
grad AddEdge W: 1.0374730588513792e-15
grad ChooseDest W: 3.8177073001861572
grad AddEdge W: 8.779601318618046e-16
grad ChooseDest W: 2.2054519653320312
grad AddEdge W: 1.1173509127554199e-15
grad ChooseDest W: 2.312267541885376
grad AddEdge W: 1.573787546870735e-17
grad ChooseDest W: 4.012365341186523
grad AddEdge W: 1.175161765055918e-17
grad ChooseDest W: 2.650304079055786
grad AddEdge W: 1.3748577233049999e-17
grad ChooseDest W: 3.810854911804199
grad AddEdge W: 1.6441498194104311e-15
grad ChooseDest W: 3.2684624195098877
grad AddEdge W: 9.484634486221083e-16
grad ChooseDest W: 3.5235438346862793
grad AddEdge W: 1.1259668260157722e-15
grad ChooseDest W: 4.957143306732178
grad AddEdge W: 1.9372124922822355e-17
grad ChooseDest W: 5.514303684234619
grad AddEdge W: 4.581359807308543e-16
grad ChooseDest W: 6.134103775024414
grad AddEdge W: 1.1290573646901724e-17
grad ChooseDest W: 5.071878910064697
grad AddEdge W: 1.0100603198211515e-17
grad ChooseDest W: 7.125195026397705
grad AddEdge W: 1.3016269538370924e-15
grad ChooseDest W: 1.7934411764144897
grad AddEdge W: 1.3467516811899996e-15
grad ChooseDest W: 6.477441787719727
grad AddEdge W: 1.1003414323833695e-15
grad ChooseDest W: 4.559248924255371
grad AddEdge W: 8.527174978598873e-18
grad ChooseDest W: 4.071293354034424
grad AddEdge W: 9.331277053538324e-16
grad ChooseDest W: 5.718024730682373
grad AddEdge W: 6.847692172410463e-14
grad ChooseDest W: 5.892568111419678
grad AddEdge W: 2.250456360397084e-17
grad ChooseDest W: 4.940119743347168
=== Epoch 35: Train Loss: 4.7229, Train Log Prob: 0.0235 ===
Total mismatches: 64994
Predicted valid destination but wrong order: 8474
Epoch 35: Validation Loss: 3.5602, Validation Log Prob: 0.0402
Epoch 35: Edge Precision: 0.3597, Recall: 0.3526, F1: 0.3559, Jaccard: 0.2336
Epoch 35: TP: 2.468861846814603, FP: 4.418468146027201, FN: 4.55261274158912
Epoch 35: Current Learning Rate: 6e-05
[Epoch 35] ‚è±Ô∏è Total: 4794.80s | Current time: 2025-07-16 10:23:40 | üèãÔ∏è Train: 3997.03s | ‚úÖ Val: 797.77s
grad AddEdge W: 1.2445153338519976e-15
grad ChooseDest W: 8.65207576751709
grad AddEdge W: 2.0602612332276637e-17
grad ChooseDest W: 4.466505527496338
grad AddEdge W: 1.7346827786906695e-17
grad ChooseDest W: 3.2695581912994385
grad AddEdge W: 1.1109076158437177e-17
grad ChooseDest W: 6.913529872894287
grad AddEdge W: 2.7853067122624413e-17
grad ChooseDest W: 2.1022491455078125
grad AddEdge W: 2.0302966155379303e-17
grad ChooseDest W: 4.963446140289307
grad AddEdge W: 8.266322116592397e-16
grad ChooseDest W: 2.2506301403045654
grad AddEdge W: 6.091345854741879e-16
grad ChooseDest W: 8.32148551940918
grad AddEdge W: 6.950666338385083e-18
grad ChooseDest W: 5.707279682159424
grad AddEdge W: 2.6451340091481854e-17
grad ChooseDest W: 3.377389669418335
grad AddEdge W: 6.144157448442766e-14
grad ChooseDest W: 3.570958137512207
grad AddEdge W: 5.645458182186559e-16
grad ChooseDest W: 4.66217565536499
grad AddEdge W: 8.538315047395065e-14
grad ChooseDest W: 4.301501750946045
grad AddEdge W: 1.1746811731200247e-17
grad ChooseDest W: 3.301038980484009
grad AddEdge W: 5.0373171345957646e-14
grad ChooseDest W: 4.450850009918213
grad AddEdge W: 9.59220767052238e-16
grad ChooseDest W: 4.234471797943115
grad AddEdge W: 8.460531948321231e-16
grad ChooseDest W: 3.1547257900238037
grad AddEdge W: 8.687795123234296e-16
grad ChooseDest W: 3.3507156372070312
grad AddEdge W: 7.297413833722412e-16
grad ChooseDest W: 4.183623790740967
grad AddEdge W: 5.832795590922021e-14
grad ChooseDest W: 3.493220090866089
grad AddEdge W: 2.0924724999308707e-15
grad ChooseDest W: 2.339050531387329
grad AddEdge W: 2.8290205606579087e-17
grad ChooseDest W: 7.432552814483643
grad AddEdge W: 1.498224928786261e-17
grad ChooseDest W: 2.1102542877197266
grad AddEdge W: 1.0489124505622853e-15
grad ChooseDest W: 4.331260681152344
grad AddEdge W: 1.0662583916115754e-17
grad ChooseDest W: 4.805370807647705
grad AddEdge W: 2.3255459966073002e-17
grad ChooseDest W: 2.187183380126953
grad AddEdge W: 2.437932981644406e-15
grad ChooseDest W: 3.442899703979492
grad AddEdge W: 1.2116549884482875e-17
grad ChooseDest W: 4.535347938537598
grad AddEdge W: 1.2557528968021632e-17
grad ChooseDest W: 2.3955698013305664
grad AddEdge W: 1.3105699276942033e-15
grad ChooseDest W: 3.0614473819732666
grad AddEdge W: 7.245960287945857e-16
grad ChooseDest W: 3.7204298973083496
grad AddEdge W: 4.3549029018821457e-16
grad ChooseDest W: 5.473354816436768
grad AddEdge W: 1.045604006044759e-16
grad ChooseDest W: 4.569358825683594
grad AddEdge W: 1.5745053742063086e-17
grad ChooseDest W: 3.9396278858184814
grad AddEdge W: 1.7263651467592038e-17
grad ChooseDest W: 4.368772029876709
grad AddEdge W: 1.3728430422050657e-17
grad ChooseDest W: 4.76737642288208
grad AddEdge W: 2.4224260516940008e-17
grad ChooseDest W: 5.918820381164551
grad AddEdge W: 6.294815344770979e-16
grad ChooseDest W: 5.074756622314453
grad AddEdge W: 4.27000591454448e-17
grad ChooseDest W: 4.335861682891846
grad AddEdge W: 1.3420101330263354e-17
grad ChooseDest W: 5.360048294067383
grad AddEdge W: 8.344585611725755e-18
grad ChooseDest W: 3.017125129699707
grad AddEdge W: 8.908734074877453e-14
grad ChooseDest W: 4.579196453094482
grad AddEdge W: 2.7602615031116833e-17
grad ChooseDest W: 2.173401117324829
grad AddEdge W: 1.1066124408083667e-15
grad ChooseDest W: 6.868001461029053
grad AddEdge W: 7.168836879307169e-18
grad ChooseDest W: 4.947651386260986
grad AddEdge W: 2.1873808608397104e-17
grad ChooseDest W: 6.038591384887695
grad AddEdge W: 2.6183806720325053e-17
grad ChooseDest W: 9.513370513916016
grad AddEdge W: 1.423276025776742e-15
grad ChooseDest W: 3.514540910720825
grad AddEdge W: 3.5388636736733564e-14
grad ChooseDest W: 2.728574752807617
grad AddEdge W: 1.4185360029341274e-17
grad ChooseDest W: 5.078883171081543
grad AddEdge W: 7.902165182513236e-16
grad ChooseDest W: 4.688981056213379
grad AddEdge W: 1.6374585767644774e-15
grad ChooseDest W: 2.812990427017212
grad AddEdge W: 1.1517633897866918e-17
grad ChooseDest W: 5.7574357986450195
grad AddEdge W: 8.180956878853578e-18
grad ChooseDest W: 3.024923801422119
grad AddEdge W: 5.157039397162824e-16
grad ChooseDest W: 3.2740731239318848
grad AddEdge W: 5.7746085779070925e-15
grad ChooseDest W: 1.9741467237472534
grad AddEdge W: 2.0375860619679926e-17
grad ChooseDest W: 3.359097719192505
grad AddEdge W: 9.236137419547659e-18
grad ChooseDest W: 5.1931047439575195
grad AddEdge W: 5.944597926025664e-16
grad ChooseDest W: 1.8806809186935425
grad AddEdge W: 5.271409954691387e-17
grad ChooseDest W: 3.786630392074585
grad AddEdge W: 2.9090873426138565e-17
grad ChooseDest W: 4.274685382843018
grad AddEdge W: 1.2144052184012385e-15
grad ChooseDest W: 3.756211519241333
grad AddEdge W: 1.7811135845261285e-17
grad ChooseDest W: 3.5343258380889893
grad AddEdge W: 4.6906888095943344e-12
grad ChooseDest W: 3.4307451248168945
grad AddEdge W: 2.2985694861623533e-17
grad ChooseDest W: 5.915515422821045
grad AddEdge W: 3.5425930115093513e-16
grad ChooseDest W: 3.8376288414001465
=== Epoch 36: Train Loss: 4.6706, Train Log Prob: 0.0244 ===
Total mismatches: 64266
Predicted valid destination but wrong order: 8486
Epoch 36: Validation Loss: 3.5607, Validation Log Prob: 0.0402
Epoch 36: Edge Precision: 0.3568, Recall: 0.3493, F1: 0.3527, Jaccard: 0.2309
Epoch 36: TP: 2.4459556191839655, FP: 4.426485325697924, FN: 4.575518969219757
Epoch 36: Current Learning Rate: 6e-05
[Epoch 36] ‚è±Ô∏è Total: 4788.35s | Current time: 2025-07-16 11:43:29 | üèãÔ∏è Train: 3989.72s | ‚úÖ Val: 798.63s
grad AddEdge W: 5.415488207611424e-15
grad ChooseDest W: 9.690518379211426
grad AddEdge W: 1.5979387392154458e-17
grad ChooseDest W: 5.39341402053833
grad AddEdge W: 7.289836859311426e-18
grad ChooseDest W: 4.4518351554870605
grad AddEdge W: 7.430711408631821e-16
grad ChooseDest W: 5.59406042098999
grad AddEdge W: 1.0672606863598059e-17
grad ChooseDest W: 7.1388750076293945
grad AddEdge W: 8.529821956559042e-18
grad ChooseDest W: 5.213512420654297
grad AddEdge W: 8.430585331405116e-14
grad ChooseDest W: 5.052004814147949
grad AddEdge W: 1.1944641073679042e-17
grad ChooseDest W: 4.571570873260498
grad AddEdge W: 1.4456762951403598e-17
grad ChooseDest W: 4.25130033493042
grad AddEdge W: 7.846875636276804e-16
grad ChooseDest W: 3.6785390377044678
grad AddEdge W: 1.2047853361790959e-17
grad ChooseDest W: 4.582452774047852
grad AddEdge W: 6.896058388973394e-16
grad ChooseDest W: 3.2011733055114746
grad AddEdge W: 8.354826471668631e-16
grad ChooseDest W: 5.256012916564941
grad AddEdge W: 1.5360994379224756e-15
grad ChooseDest W: 5.6457929611206055
grad AddEdge W: 7.452166091441282e-18
grad ChooseDest W: 3.792419195175171
grad AddEdge W: 1.6190202576895274e-15
grad ChooseDest W: 4.563353538513184
grad AddEdge W: 1.2119792432484083e-17
grad ChooseDest W: 3.6140918731689453
grad AddEdge W: 6.069764067359493e-18
grad ChooseDest W: 4.247774600982666
grad AddEdge W: 1.3886147295084918e-17
grad ChooseDest W: 4.944779872894287
grad AddEdge W: 3.038735330486233e-17
grad ChooseDest W: 4.9035139083862305
grad AddEdge W: 1.2916725531577884e-17
grad ChooseDest W: 4.488648891448975
grad AddEdge W: 3.555051211802237e-17
grad ChooseDest W: 4.012601375579834
grad AddEdge W: 1.1944195223328876e-17
grad ChooseDest W: 2.7907450199127197
grad AddEdge W: 1.1394154030386227e-17
grad ChooseDest W: 7.045352935791016
grad AddEdge W: 3.0525684373060796e-17
grad ChooseDest W: 5.546247959136963
grad AddEdge W: 1.024974551701605e-17
grad ChooseDest W: 5.048131465911865
grad AddEdge W: 1.5707863701722702e-17
grad ChooseDest W: 5.361969947814941
grad AddEdge W: 1.051975050384316e-17
grad ChooseDest W: 5.2749342918396
grad AddEdge W: 8.555684585591125e-18
grad ChooseDest W: 4.881648063659668
grad AddEdge W: 7.786434012138697e-16
grad ChooseDest W: 3.468698501586914
grad AddEdge W: 2.849558462650865e-17
grad ChooseDest W: 2.2414748668670654
grad AddEdge W: 1.5443395861916022e-17
grad ChooseDest W: 4.6184611320495605
grad AddEdge W: 8.543077525875205e-18
grad ChooseDest W: 5.608295440673828
grad AddEdge W: 1.4019138085248743e-17
grad ChooseDest W: 4.396486759185791
grad AddEdge W: 1.7308321007380024e-15
grad ChooseDest W: 4.878253936767578
grad AddEdge W: 5.953821469620385e-18
grad ChooseDest W: 4.11837911605835
grad AddEdge W: 2.3585323050746898e-17
grad ChooseDest W: 4.201166152954102
grad AddEdge W: 9.409580650387778e-18
grad ChooseDest W: 4.044827461242676
grad AddEdge W: 8.602684160815776e-18
grad ChooseDest W: 5.162495136260986
grad AddEdge W: 1.1403181879591631e-17
grad ChooseDest W: 4.409610271453857
grad AddEdge W: 1.422092383259738e-17
grad ChooseDest W: 3.73633074760437
grad AddEdge W: 1.0759049719151688e-17
grad ChooseDest W: 4.885498523712158
grad AddEdge W: 9.442793176209089e-16
grad ChooseDest W: 6.090024471282959
grad AddEdge W: 5.249256932921584e-16
grad ChooseDest W: 5.281942844390869
grad AddEdge W: 1.629886439705714e-17
grad ChooseDest W: 3.5050344467163086
grad AddEdge W: 1.3660133162896808e-15
grad ChooseDest W: 4.8990888595581055
grad AddEdge W: 1.7623367500572973e-17
grad ChooseDest W: 7.703018665313721
grad AddEdge W: 1.6702188465489583e-15
grad ChooseDest W: 3.4832186698913574
grad AddEdge W: 2.5712767066145506e-17
grad ChooseDest W: 6.156105995178223
grad AddEdge W: 8.486605210624495e-16
grad ChooseDest W: 4.883994102478027
grad AddEdge W: 2.1406502516630376e-15
grad ChooseDest W: 5.431623935699463
grad AddEdge W: 2.5307241770841384e-17
grad ChooseDest W: 4.2931389808654785
grad AddEdge W: 2.4948745002083352e-17
grad ChooseDest W: 4.92403507232666
grad AddEdge W: 3.646402391674633e-17
grad ChooseDest W: 3.224022150039673
grad AddEdge W: 8.155244862867428e-16
grad ChooseDest W: 2.6151766777038574
grad AddEdge W: 9.453263231356091e-18
grad ChooseDest W: 4.7066650390625
grad AddEdge W: 9.23449295140268e-16
grad ChooseDest W: 4.92280387878418
grad AddEdge W: 1.0222409670048588e-13
grad ChooseDest W: 2.3464901447296143
grad AddEdge W: 9.281407988768727e-16
grad ChooseDest W: 4.813387393951416
grad AddEdge W: 5.163438400207085e-18
grad ChooseDest W: 7.470491409301758
grad AddEdge W: 7.328023486853814e-16
grad ChooseDest W: 4.733163356781006
grad AddEdge W: 9.373099305309005e-16
grad ChooseDest W: 1.5652077198028564
grad AddEdge W: 1.1539859347571156e-15
grad ChooseDest W: 7.788844108581543
grad AddEdge W: 7.859905649983535e-16
grad ChooseDest W: 6.436692714691162
grad AddEdge W: 2.0493834773003464e-17
grad ChooseDest W: 3.348414897918701
grad AddEdge W: 1.741244306181685e-17
grad ChooseDest W: 4.28756856918335
=== Epoch 37: Train Loss: 4.6283, Train Log Prob: 0.0255 ===
Total mismatches: 63458
Predicted valid destination but wrong order: 8460
Epoch 37: Validation Loss: 3.4852, Validation Log Prob: 0.0430
Epoch 37: Edge Precision: 0.3590, Recall: 0.3518, F1: 0.3551, Jaccard: 0.2330
Epoch 37: TP: 2.4642806012884755, FP: 4.414889047959914, FN: 4.557193987115247
Epoch 37: Current Learning Rate: 6e-05
[Epoch 37] ‚è±Ô∏è Total: 4796.97s | Current time: 2025-07-16 13:03:26 | üèãÔ∏è Train: 3990.03s | ‚úÖ Val: 806.94s
grad AddEdge W: 1.1592013283715972e-15
grad ChooseDest W: 10.349401473999023
grad AddEdge W: 2.983991194058493e-17
grad ChooseDest W: 2.018315076828003
grad AddEdge W: 4.079912332067057e-16
grad ChooseDest W: 4.625514984130859
grad AddEdge W: 2.250611870352244e-17
grad ChooseDest W: 4.559889316558838
grad AddEdge W: 3.1798455402544216e-15
grad ChooseDest W: 2.3693718910217285
grad AddEdge W: 8.368805691671879e-16
grad ChooseDest W: 3.911196231842041
grad AddEdge W: 1.5113130767464425e-17
grad ChooseDest W: 5.473793983459473
grad AddEdge W: 4.119561002003877e-14
grad ChooseDest W: 2.610218048095703
grad AddEdge W: 1.2587670602362452e-17
grad ChooseDest W: 5.407016277313232
grad AddEdge W: 1.3838221277574208e-17
grad ChooseDest W: 4.9300618171691895
grad AddEdge W: 7.650856266886927e-14
grad ChooseDest W: 3.463289737701416
grad AddEdge W: 9.81012300968188e-18
grad ChooseDest W: 8.91346549987793
grad AddEdge W: 9.680395447034577e-18
grad ChooseDest W: 5.382452964782715
grad AddEdge W: 2.0509753036711434e-17
grad ChooseDest W: 3.5761489868164062
grad AddEdge W: 2.4886764358784754e-17
grad ChooseDest W: 7.88424825668335
grad AddEdge W: 1.0594047535590428e-15
grad ChooseDest W: 2.104071617126465
grad AddEdge W: 9.043770777834165e-18
grad ChooseDest W: 3.0504424571990967
grad AddEdge W: 1.6808727210804045e-15
grad ChooseDest W: 5.1301116943359375
grad AddEdge W: 1.6027959437723572e-17
grad ChooseDest W: 2.7560627460479736
grad AddEdge W: 1.1022963844256324e-15
grad ChooseDest W: 5.546141147613525
grad AddEdge W: 8.100372116398049e-18
grad ChooseDest W: 4.060066223144531
grad AddEdge W: 1.083067363403143e-17
grad ChooseDest W: 5.6080322265625
grad AddEdge W: 6.520038229072345e-16
grad ChooseDest W: 3.997347831726074
grad AddEdge W: 1.009176311900516e-17
grad ChooseDest W: 5.021759986877441
grad AddEdge W: 2.7432790417671613e-15
grad ChooseDest W: 8.650956153869629
grad AddEdge W: 1.9469616429817854e-17
grad ChooseDest W: 2.7930116653442383
grad AddEdge W: 5.47972983330915e-16
grad ChooseDest W: 2.8858132362365723
grad AddEdge W: 3.741378442427359e-18
grad ChooseDest W: 4.69196891784668
grad AddEdge W: 1.7722023677870947e-17
grad ChooseDest W: 3.6107568740844727
grad AddEdge W: 3.702680120958046e-17
grad ChooseDest W: 4.7081780433654785
grad AddEdge W: 1.2290985930581404e-15
grad ChooseDest W: 4.742129802703857
grad AddEdge W: 1.5427398188869247e-17
grad ChooseDest W: 5.739742279052734
grad AddEdge W: 1.7031382460309598e-17
grad ChooseDest W: 2.8165204524993896
grad AddEdge W: 1.0729031334722138e-17
grad ChooseDest W: 2.8891637325286865
grad AddEdge W: 1.1202784141310859e-17
grad ChooseDest W: 6.539148807525635
grad AddEdge W: 3.497619267779292e-16
grad ChooseDest W: 4.077773571014404
grad AddEdge W: 1.4589101923244732e-17
grad ChooseDest W: 4.209323406219482
grad AddEdge W: 1.3622197270341697e-17
grad ChooseDest W: 2.1169774532318115
grad AddEdge W: 1.076837535337761e-17
grad ChooseDest W: 4.138446807861328
grad AddEdge W: 1.0368768535356306e-17
grad ChooseDest W: 4.063915729522705
grad AddEdge W: 8.430741435606828e-18
grad ChooseDest W: 3.4226391315460205
grad AddEdge W: 1.5941921338187277e-15
grad ChooseDest W: 3.7681941986083984
grad AddEdge W: 3.4898872466342895e-17
grad ChooseDest W: 5.3571457862854
grad AddEdge W: 7.810600821653373e-18
grad ChooseDest W: 2.674307107925415
grad AddEdge W: 7.62166386928287e-16
grad ChooseDest W: 4.6282124519348145
grad AddEdge W: 8.504139322028251e-16
grad ChooseDest W: 2.2055628299713135
grad AddEdge W: 1.871032749322073e-17
grad ChooseDest W: 4.89366340637207
grad AddEdge W: 2.7127593858864048e-14
grad ChooseDest W: 5.964056015014648
grad AddEdge W: 8.638886547504771e-18
grad ChooseDest W: 2.923840284347534
grad AddEdge W: 4.391856732526809e-18
grad ChooseDest W: 4.018829345703125
grad AddEdge W: 1.0156577683082365e-17
grad ChooseDest W: 3.793342113494873
grad AddEdge W: 4.153835924854982e-18
grad ChooseDest W: 6.005687713623047
grad AddEdge W: 4.666761374819866e-16
grad ChooseDest W: 5.367348670959473
grad AddEdge W: 4.640178833957066e-17
grad ChooseDest W: 4.400792121887207
grad AddEdge W: 6.6483975853556494e-18
grad ChooseDest W: 4.684800624847412
grad AddEdge W: 1.0365995307135348e-13
grad ChooseDest W: 5.4676594734191895
grad AddEdge W: 9.00921448056415e-18
grad ChooseDest W: 5.66595458984375
grad AddEdge W: 1.5508921271924431e-15
grad ChooseDest W: 1.719870686531067
grad AddEdge W: 1.409937374439855e-15
grad ChooseDest W: 2.107492685317993
grad AddEdge W: 1.655074585666321e-17
grad ChooseDest W: 1.9428470134735107
grad AddEdge W: 1.5460086712316117e-17
grad ChooseDest W: 4.6954851150512695
grad AddEdge W: 2.0301981810450365e-17
grad ChooseDest W: 7.459486961364746
grad AddEdge W: 1.581290405898836e-17
grad ChooseDest W: 6.647524833679199
grad AddEdge W: 1.095436582223175e-15
grad ChooseDest W: 3.377281904220581
grad AddEdge W: 6.748850944883293e-16
grad ChooseDest W: 4.842410564422607
grad AddEdge W: 5.384055492565175e-14
grad ChooseDest W: 3.71612286567688
=== Epoch 38: Train Loss: 4.5829, Train Log Prob: 0.0264 ===
Total mismatches: 62923
Predicted valid destination but wrong order: 8473
Epoch 38: Validation Loss: 3.4718, Validation Log Prob: 0.0437
Epoch 38: Edge Precision: 0.3576, Recall: 0.3497, F1: 0.3533, Jaccard: 0.2317
Epoch 38: TP: 2.449821045096636, FP: 4.416034359341446, FN: 4.571653543307087
Epoch 38: Current Learning Rate: 6e-05
[Epoch 38] ‚è±Ô∏è Total: 3966.97s | Current time: 2025-07-16 14:09:32 | üèãÔ∏è Train: 3671.23s | ‚úÖ Val: 295.74s
grad AddEdge W: 6.79076681497047e-14
grad ChooseDest W: 7.994470119476318
grad AddEdge W: 8.448581372226657e-16
grad ChooseDest W: 3.713698148727417
grad AddEdge W: 1.1891429372054118e-17
grad ChooseDest W: 2.996979236602783
grad AddEdge W: 9.43184633415701e-16
grad ChooseDest W: 4.382780075073242
grad AddEdge W: 1.9020657534909798e-17
grad ChooseDest W: 4.398009300231934
grad AddEdge W: 1.4578237976601433e-13
grad ChooseDest W: 3.4506285190582275
grad AddEdge W: 1.6011762381061946e-15
grad ChooseDest W: 4.657151222229004
grad AddEdge W: 1.7389372994532747e-17
grad ChooseDest W: 3.8038246631622314
grad AddEdge W: 5.9421642944890835e-16
grad ChooseDest W: 4.2053632736206055
grad AddEdge W: 1.2053631218369641e-17
grad ChooseDest W: 4.007295608520508
grad AddEdge W: 1.916869970349964e-17
grad ChooseDest W: 3.992966651916504
grad AddEdge W: 1.550316296989543e-17
grad ChooseDest W: 6.510532855987549
grad AddEdge W: 5.497127890045754e-16
grad ChooseDest W: 3.432065486907959
grad AddEdge W: 1.3468661795303892e-17
grad ChooseDest W: 3.9431564807891846
grad AddEdge W: 1.698736156247075e-17
grad ChooseDest W: 5.481400489807129
grad AddEdge W: 4.6982201123064416e-17
grad ChooseDest W: 3.2080929279327393
grad AddEdge W: 7.924996426210884e-16
grad ChooseDest W: 4.982326507568359
grad AddEdge W: 1.2545457921342646e-17
grad ChooseDest W: 5.489426612854004
grad AddEdge W: 1.525564406083996e-17
grad ChooseDest W: 6.31141996383667
grad AddEdge W: 9.40905055996403e-16
grad ChooseDest W: 4.689320087432861
grad AddEdge W: 1.2611600110302998e-17
grad ChooseDest W: 4.133619785308838
grad AddEdge W: 2.642445063352457e-15
grad ChooseDest W: 8.062609672546387
grad AddEdge W: 5.434687268152127e-16
grad ChooseDest W: 2.326902151107788
grad AddEdge W: 2.2770015788706457e-17
grad ChooseDest W: 4.503414630889893
grad AddEdge W: 6.721872945513244e-16
grad ChooseDest W: 4.480027198791504
grad AddEdge W: 8.881258738788934e-18
grad ChooseDest W: 6.466967582702637
grad AddEdge W: 1.3236331874441143e-15
grad ChooseDest W: 4.210732460021973
grad AddEdge W: 3.7233122553663846e-16
grad ChooseDest W: 4.364676475524902
grad AddEdge W: 3.024859541146778e-17
grad ChooseDest W: 4.375821113586426
grad AddEdge W: 1.1509961797685489e-17
grad ChooseDest W: 3.8289060592651367
grad AddEdge W: 1.622565064104007e-17
grad ChooseDest W: 3.0346574783325195
grad AddEdge W: 6.96326264475304e-18
grad ChooseDest W: 6.379842281341553
grad AddEdge W: 1.767162356314809e-17
grad ChooseDest W: 5.552239418029785
grad AddEdge W: 3.0314015471753376e-17
grad ChooseDest W: 6.210074424743652
grad AddEdge W: 7.51549752068018e-18
grad ChooseDest W: 3.4897215366363525
grad AddEdge W: 1.560090097671347e-17
grad ChooseDest W: 3.2998130321502686
grad AddEdge W: 2.1405923821073834e-17
grad ChooseDest W: 4.520549297332764
grad AddEdge W: 1.4064618129688134e-17
grad ChooseDest W: 4.595154285430908
grad AddEdge W: 3.484993182909283e-16
grad ChooseDest W: 3.2233612537384033
grad AddEdge W: 3.6769046436753016e-16
grad ChooseDest W: 4.469245433807373
grad AddEdge W: 1.502196222907128e-17
grad ChooseDest W: 3.6311562061309814
grad AddEdge W: 6.360186353246248e-18
grad ChooseDest W: 6.364311218261719
grad AddEdge W: 1.1321396878067288e-17
grad ChooseDest W: 2.834609031677246
grad AddEdge W: 1.7980798860702037e-17
grad ChooseDest W: 3.6144344806671143
grad AddEdge W: 1.5051772163986467e-17
grad ChooseDest W: 6.825073719024658
grad AddEdge W: 1.1112732777171161e-13
grad ChooseDest W: 4.813527584075928
grad AddEdge W: 1.0898596742850609e-17
grad ChooseDest W: 3.356834888458252
grad AddEdge W: 1.2126581930951918e-17
grad ChooseDest W: 3.353480815887451
grad AddEdge W: 4.72129653233765e-16
grad ChooseDest W: 2.5146875381469727
grad AddEdge W: 1.0071740550533824e-15
grad ChooseDest W: 6.2264275550842285
grad AddEdge W: 8.394639667167543e-16
grad ChooseDest W: 2.3059241771698
grad AddEdge W: 1.3876918440990664e-17
grad ChooseDest W: 2.353260040283203
grad AddEdge W: 2.812136480866721e-17
grad ChooseDest W: 4.380921840667725
grad AddEdge W: 1.3065952255892125e-15
grad ChooseDest W: 4.245255947113037
grad AddEdge W: 5.710585677292171e-14
grad ChooseDest W: 4.82246208190918
grad AddEdge W: 1.0689176118448108e-17
grad ChooseDest W: 5.0665507316589355
grad AddEdge W: 6.280252168342493e-18
grad ChooseDest W: 3.6554172039031982
grad AddEdge W: 2.7512634324083314e-17
grad ChooseDest W: 2.346266508102417
grad AddEdge W: 7.283180536922212e-18
grad ChooseDest W: 5.8519768714904785
grad AddEdge W: 1.1673687376863784e-15
grad ChooseDest W: 8.733514785766602
grad AddEdge W: 7.401770982153749e-14
grad ChooseDest W: 3.999525308609009
grad AddEdge W: 1.0835346377311741e-17
grad ChooseDest W: 4.12901496887207
grad AddEdge W: 1.1281235836577185e-15
grad ChooseDest W: 4.673977375030518
grad AddEdge W: 7.629973527193231e-18
grad ChooseDest W: 4.970397472381592
grad AddEdge W: 2.694990666208594e-17
grad ChooseDest W: 3.006345510482788
grad AddEdge W: 1.8803127231781828e-17
grad ChooseDest W: 5.058954238891602
=== Epoch 39: Train Loss: 4.5476, Train Log Prob: 0.0273 ===
Total mismatches: 62287
Predicted valid destination but wrong order: 8483
Epoch 39: Validation Loss: 3.4363, Validation Log Prob: 0.0449
Epoch 39: Edge Precision: 0.3573, Recall: 0.3493, F1: 0.3530, Jaccard: 0.2317
Epoch 39: TP: 2.445812455261274, FP: 4.416893342877595, FN: 4.5756621331424485
Epoch 39: Current Learning Rate: 6e-05
[Epoch 39] ‚è±Ô∏è Total: 2014.81s | Current time: 2025-07-16 14:43:07 | üèãÔ∏è Train: 1718.34s | ‚úÖ Val: 296.48s
grad AddEdge W: 3.792802203084417e-15
grad ChooseDest W: 11.944228172302246
grad AddEdge W: 5.1293064465913095e-18
grad ChooseDest W: 3.87514066696167
grad AddEdge W: 9.581429176268568e-16
grad ChooseDest W: 3.8530690670013428
grad AddEdge W: 8.900646561945422e-16
grad ChooseDest W: 4.875480651855469
grad AddEdge W: 1.970423099044525e-15
grad ChooseDest W: 3.9518179893493652
grad AddEdge W: 1.5992568614477164e-15
grad ChooseDest W: 3.6132569313049316
grad AddEdge W: 3.2451843912561975e-17
grad ChooseDest W: 3.950958728790283
grad AddEdge W: 9.565871464045997e-18
grad ChooseDest W: 3.20050048828125
grad AddEdge W: 6.854877235264666e-16
grad ChooseDest W: 6.7869672775268555
grad AddEdge W: 7.8353988692371e-18
grad ChooseDest W: 5.267879009246826
grad AddEdge W: 1.0812746148615568e-17
grad ChooseDest W: 3.8972108364105225
grad AddEdge W: 5.72188490622613e-16
grad ChooseDest W: 2.75840163230896
grad AddEdge W: 1.773499717859823e-17
grad ChooseDest W: 5.404290676116943
grad AddEdge W: 2.009296485018679e-17
grad ChooseDest W: 7.939164638519287
grad AddEdge W: 1.1233338538777343e-17
grad ChooseDest W: 4.2968621253967285
grad AddEdge W: 2.6487681444513758e-17
grad ChooseDest W: 2.050293445587158
grad AddEdge W: 1.1174114888460384e-17
grad ChooseDest W: 5.16091775894165
grad AddEdge W: 1.1531934295558408e-15
grad ChooseDest W: 3.143960952758789
grad AddEdge W: 8.051594499903104e-16
grad ChooseDest W: 5.8677239418029785
grad AddEdge W: 2.4256731605970527e-12
grad ChooseDest W: 1.9271340370178223
grad AddEdge W: 1.2452279334060997e-17
grad ChooseDest W: 2.8292691707611084
grad AddEdge W: 4.929108058838163e-18
grad ChooseDest W: 2.5431530475616455
grad AddEdge W: 3.5771448737148548e-12
grad ChooseDest W: 2.2659554481506348
grad AddEdge W: 2.2244509599912743e-17
grad ChooseDest W: 4.652824878692627
grad AddEdge W: 8.495873868649825e-16
grad ChooseDest W: 6.363178253173828
grad AddEdge W: 6.513255612747206e-16
grad ChooseDest W: 3.9486708641052246
grad AddEdge W: 7.78591414566732e-16
grad ChooseDest W: 3.5731561183929443
grad AddEdge W: 1.1520134464858666e-17
grad ChooseDest W: 7.503775119781494
grad AddEdge W: 1.6683885539337296e-17
grad ChooseDest W: 6.013195037841797
grad AddEdge W: 9.579714331597072e-18
grad ChooseDest W: 2.7523586750030518
grad AddEdge W: 5.961676790307495e-18
grad ChooseDest W: 4.133566379547119
grad AddEdge W: 3.7150343942787704e-18
grad ChooseDest W: 5.194849491119385
grad AddEdge W: 1.3051564243230153e-17
grad ChooseDest W: 3.5843498706817627
grad AddEdge W: 1.1374301464074383e-15
grad ChooseDest W: 7.368614196777344
grad AddEdge W: 1.6220902227277322e-15
grad ChooseDest W: 4.942798137664795
grad AddEdge W: 1.745711908670084e-17
grad ChooseDest W: 4.267794609069824
grad AddEdge W: 1.051166405234929e-15
grad ChooseDest W: 2.9747414588928223
grad AddEdge W: 1.622775002543473e-17
grad ChooseDest W: 4.951162338256836
grad AddEdge W: 1.480573721694992e-17
grad ChooseDest W: 2.9571900367736816
grad AddEdge W: 3.264437185449492e-17
grad ChooseDest W: 4.079564571380615
grad AddEdge W: 9.520228928247077e-16
grad ChooseDest W: 5.291221618652344
grad AddEdge W: 8.564101148323852e-18
grad ChooseDest W: 6.4510111808776855
grad AddEdge W: 2.5765299652487524e-17
grad ChooseDest W: 4.802909851074219
grad AddEdge W: 6.327091270528308e-18
grad ChooseDest W: 4.5011162757873535
grad AddEdge W: 1.4729790456188976e-17
grad ChooseDest W: 5.302925109863281
grad AddEdge W: 7.025009096676613e-16
grad ChooseDest W: 6.088932514190674
grad AddEdge W: 1.7794312645963182e-17
grad ChooseDest W: 3.5869767665863037
grad AddEdge W: 1.9228931157587708e-15
grad ChooseDest W: 2.5655667781829834
grad AddEdge W: 1.3026159045076812e-17
grad ChooseDest W: 4.369379043579102
grad AddEdge W: 8.671176866204759e-16
grad ChooseDest W: 5.260001182556152
grad AddEdge W: 1.3916573446469231e-15
grad ChooseDest W: 7.031664848327637
grad AddEdge W: 9.160362745441351e-16
grad ChooseDest W: 4.075583457946777
grad AddEdge W: 1.2225450693667319e-17
grad ChooseDest W: 5.752336025238037
grad AddEdge W: 1.0900339612401258e-17
grad ChooseDest W: 5.280172824859619
grad AddEdge W: 1.395270721025461e-17
grad ChooseDest W: 7.760644912719727
grad AddEdge W: 1.3915537022248927e-17
grad ChooseDest W: 6.193998336791992
grad AddEdge W: 8.298455403324898e-18
grad ChooseDest W: 5.868475914001465
grad AddEdge W: 3.9158309328764663e-16
grad ChooseDest W: 3.9011528491973877
grad AddEdge W: 8.149340513829474e-16
grad ChooseDest W: 5.6642560958862305
grad AddEdge W: 7.809551129456043e-18
grad ChooseDest W: 3.131580114364624
grad AddEdge W: 2.039675030504379e-15
grad ChooseDest W: 3.3200695514678955
grad AddEdge W: 1.0204421908974106e-15
grad ChooseDest W: 7.276325702667236
grad AddEdge W: 6.162487093190584e-16
grad ChooseDest W: 3.205003023147583
grad AddEdge W: 7.724163326230113e-16
grad ChooseDest W: 2.031848430633545
grad AddEdge W: 5.864210098730372e-18
grad ChooseDest W: 6.858402729034424
grad AddEdge W: 5.602109649836819e-18
grad ChooseDest W: 2.8386294841766357
=== Epoch 40: Train Loss: 4.5077, Train Log Prob: 0.0281 ===
Total mismatches: 61639
Predicted valid destination but wrong order: 8552
Epoch 40: Validation Loss: 3.4000, Validation Log Prob: 0.0466
Epoch 40: Edge Precision: 0.3581, Recall: 0.3503, F1: 0.3539, Jaccard: 0.2321
Epoch 40: TP: 2.4541159627773803, FP: 4.413027916964925, FN: 4.567358625626342
Epoch 40: Current Learning Rate: 6e-05
[Epoch 40] ‚è±Ô∏è Total: 2009.17s | Current time: 2025-07-16 15:16:36 | üèãÔ∏è Train: 1714.03s | ‚úÖ Val: 295.14s
grad AddEdge W: 6.111815020011166e-14
grad ChooseDest W: 10.476617813110352
grad AddEdge W: 2.2945781742706624e-17
grad ChooseDest W: 3.455164909362793
grad AddEdge W: 2.2143350374081183e-17
grad ChooseDest W: 6.825011730194092
grad AddEdge W: 1.0296986628979567e-17
grad ChooseDest W: 2.4513309001922607
grad AddEdge W: 7.512197566344461e-16
grad ChooseDest W: 4.594278812408447
grad AddEdge W: 1.6329891941834003e-17
grad ChooseDest W: 3.0244665145874023
grad AddEdge W: 2.164154456419834e-17
grad ChooseDest W: 4.8039870262146
grad AddEdge W: 1.1953896463727347e-15
grad ChooseDest W: 4.83760929107666
grad AddEdge W: 1.3665538225717025e-17
grad ChooseDest W: 7.486307621002197
grad AddEdge W: 1.1937162269063154e-15
grad ChooseDest W: 6.719493389129639
grad AddEdge W: 8.200762295870506e-16
grad ChooseDest W: 6.2867431640625
grad AddEdge W: 1.1425744057979627e-17
grad ChooseDest W: 3.34177827835083
grad AddEdge W: 7.513364883624896e-16
grad ChooseDest W: 7.169522762298584
grad AddEdge W: 2.7059225197479723e-17
grad ChooseDest W: 3.045142889022827
grad AddEdge W: 4.621620360717449e-12
grad ChooseDest W: 1.4854435920715332
grad AddEdge W: 1.0826691586562599e-17
grad ChooseDest W: 4.536074638366699
grad AddEdge W: 4.485267625211687e-16
grad ChooseDest W: 3.3614816665649414
grad AddEdge W: 1.0888273528805947e-17
grad ChooseDest W: 4.138652324676514
grad AddEdge W: 2.4643680792173796e-17
grad ChooseDest W: 5.066979885101318
grad AddEdge W: 1.3321038180104003e-17
grad ChooseDest W: 4.218827247619629
grad AddEdge W: 7.526008504723891e-18
grad ChooseDest W: 3.373025417327881
grad AddEdge W: 8.268968068848607e-18
grad ChooseDest W: 5.061511516571045
grad AddEdge W: 1.6091145250087435e-15
grad ChooseDest W: 1.7553057670593262
grad AddEdge W: 2.2662936133445834e-15
grad ChooseDest W: 7.715826988220215
grad AddEdge W: 1.0310406807237627e-17
grad ChooseDest W: 2.524519443511963
grad AddEdge W: 1.24170339622535e-15
grad ChooseDest W: 3.100407123565674
grad AddEdge W: 1.5132291579173604e-17
grad ChooseDest W: 3.5770363807678223
grad AddEdge W: 8.423227326922396e-18
grad ChooseDest W: 3.514125347137451
grad AddEdge W: 6.438120613952198e-16
grad ChooseDest W: 2.910339593887329
grad AddEdge W: 2.3653797061854038e-17
grad ChooseDest W: 3.601306200027466
grad AddEdge W: 1.200085702911431e-15
grad ChooseDest W: 3.5090837478637695
grad AddEdge W: 1.0805464477683263e-17
grad ChooseDest W: 4.050894260406494
grad AddEdge W: 7.589700584709862e-18
grad ChooseDest W: 6.312038898468018
grad AddEdge W: 9.315007668203937e-16
grad ChooseDest W: 8.048702239990234
grad AddEdge W: 5.89587235573486e-16
grad ChooseDest W: 2.3419792652130127
grad AddEdge W: 1.376127101438134e-15
grad ChooseDest W: 5.143970012664795
grad AddEdge W: 1.5578490999558183e-17
grad ChooseDest W: 2.233755111694336
grad AddEdge W: 3.949158301807046e-14
grad ChooseDest W: 1.8752223253250122
grad AddEdge W: 7.989571571131592e-16
grad ChooseDest W: 7.736265182495117
grad AddEdge W: 8.817888774844893e-14
grad ChooseDest W: 2.972951650619507
grad AddEdge W: 1.676678558032736e-17
grad ChooseDest W: 4.030698299407959
grad AddEdge W: 6.266968061295197e-18
grad ChooseDest W: 4.04048490524292
grad AddEdge W: 1.8496236607079754e-17
grad ChooseDest W: 4.5087995529174805
grad AddEdge W: 6.849150424332456e-14
grad ChooseDest W: 3.551290988922119
grad AddEdge W: 3.3814381111515242e-16
grad ChooseDest W: 3.5099995136260986
grad AddEdge W: 2.355468262649671e-17
grad ChooseDest W: 7.911941051483154
grad AddEdge W: 7.894752056037985e-16
grad ChooseDest W: 3.2960822582244873
grad AddEdge W: 1.0101695076620085e-17
grad ChooseDest W: 6.416607856750488
grad AddEdge W: 2.0983047452891902e-15
grad ChooseDest W: 4.350473880767822
grad AddEdge W: 5.061591489501473e-16
grad ChooseDest W: 3.5379772186279297
grad AddEdge W: 2.2848889114474613e-17
grad ChooseDest W: 5.84356689453125
grad AddEdge W: 6.133889376242971e-14
grad ChooseDest W: 2.125070571899414
grad AddEdge W: 7.65312300381614e-18
grad ChooseDest W: 4.348902225494385
grad AddEdge W: 4.451200489468711e-16
grad ChooseDest W: 1.968854308128357
grad AddEdge W: 1.445918381127152e-15
grad ChooseDest W: 4.268723011016846
grad AddEdge W: 1.2109790991697704e-17
grad ChooseDest W: 6.4466071128845215
grad AddEdge W: 7.808281109430754e-16
grad ChooseDest W: 8.612951278686523
grad AddEdge W: 1.5159927683439e-17
grad ChooseDest W: 4.332200050354004
grad AddEdge W: 1.469375019690004e-17
grad ChooseDest W: 6.342631816864014
grad AddEdge W: 7.096224483595427e-18
grad ChooseDest W: 3.8489317893981934
grad AddEdge W: 3.931982845328981e-14
grad ChooseDest W: 3.6533617973327637
grad AddEdge W: 5.169591896045541e-16
grad ChooseDest W: 8.54611587524414
grad AddEdge W: 1.8449730858680798e-17
grad ChooseDest W: 2.717970848083496
grad AddEdge W: 1.7301092970838638e-17
grad ChooseDest W: 3.1082191467285156
grad AddEdge W: 1.94430242274855e-17
grad ChooseDest W: 5.516520977020264
grad AddEdge W: 8.331683943362867e-16
grad ChooseDest W: 3.6471455097198486
=== Epoch 41: Train Loss: 4.4691, Train Log Prob: 0.0290 ===
Total mismatches: 61294
Predicted valid destination but wrong order: 8500
Epoch 41: Validation Loss: 3.3477, Validation Log Prob: 0.0488
Epoch 41: Edge Precision: 0.3560, Recall: 0.3483, F1: 0.3519, Jaccard: 0.2307
Epoch 41: TP: 2.439799570508232, FP: 4.426198997852541, FN: 4.58167501789549
Epoch 41: Current Learning Rate: 6e-05
[Epoch 41] ‚è±Ô∏è Total: 2009.49s | Current time: 2025-07-16 15:50:06 | üèãÔ∏è Train: 1714.52s | ‚úÖ Val: 294.97s
grad AddEdge W: 6.530235553692254e-12
grad ChooseDest W: 6.26630973815918
grad AddEdge W: 5.446914718141335e-16
grad ChooseDest W: 3.6149492263793945
grad AddEdge W: 9.600114259863855e-18
grad ChooseDest W: 3.5027270317077637
grad AddEdge W: 6.928400290656381e-18
grad ChooseDest W: 7.265824317932129
grad AddEdge W: 2.162933041527338e-17
grad ChooseDest W: 2.991710662841797
grad AddEdge W: 1.1980701185302679e-17
grad ChooseDest W: 3.853266716003418
grad AddEdge W: 1.7395854781812713e-17
grad ChooseDest W: 3.259986639022827
grad AddEdge W: 2.9753803100562653e-16
grad ChooseDest W: 4.028163433074951
grad AddEdge W: 4.9930823364796595e-12
grad ChooseDest W: 2.656904697418213
grad AddEdge W: 2.5059504486104203e-17
grad ChooseDest W: 5.405213356018066
grad AddEdge W: 6.496396480723293e-16
grad ChooseDest W: 7.1960248947143555
grad AddEdge W: 3.1497346968847235e-17
grad ChooseDest W: 4.443840503692627
grad AddEdge W: 7.673575342246534e-16
grad ChooseDest W: 3.3004133701324463
grad AddEdge W: 7.439203344061964e-18
grad ChooseDest W: 6.028674125671387
grad AddEdge W: 1.3886367325127857e-17
grad ChooseDest W: 5.530027866363525
grad AddEdge W: 1.9241471745069604e-17
grad ChooseDest W: 5.883462429046631
grad AddEdge W: 1.0423199931338051e-15
grad ChooseDest W: 2.4944710731506348
grad AddEdge W: 7.189914235408408e-16
grad ChooseDest W: 2.786144733428955
grad AddEdge W: 4.5942021888178974e-12
grad ChooseDest W: 3.629403591156006
grad AddEdge W: 9.688365332236525e-18
grad ChooseDest W: 4.883264541625977
grad AddEdge W: 2.3257484904212532e-17
grad ChooseDest W: 3.089172124862671
grad AddEdge W: 1.280653845654092e-17
grad ChooseDest W: 2.879054307937622
grad AddEdge W: 1.4087836096865762e-15
grad ChooseDest W: 4.397416114807129
grad AddEdge W: 8.949921644860756e-16
grad ChooseDest W: 9.617071151733398
grad AddEdge W: 1.2608660310405985e-17
grad ChooseDest W: 4.787670135498047
grad AddEdge W: 7.073231211759392e-16
grad ChooseDest W: 5.484304904937744
grad AddEdge W: 1.2766116356849495e-15
grad ChooseDest W: 6.77720308303833
grad AddEdge W: 7.963071615981149e-16
grad ChooseDest W: 3.4051449298858643
grad AddEdge W: 9.033140514346123e-16
grad ChooseDest W: 3.2690846920013428
grad AddEdge W: 1.1906889311528285e-15
grad ChooseDest W: 8.564565658569336
grad AddEdge W: 1.1644234717798771e-17
grad ChooseDest W: 6.589317321777344
grad AddEdge W: 1.6239746592771362e-15
grad ChooseDest W: 1.414057731628418
grad AddEdge W: 6.587803511830649e-16
grad ChooseDest W: 5.82248067855835
grad AddEdge W: 7.447462742478306e-18
grad ChooseDest W: 5.14608097076416
grad AddEdge W: 1.2052513697362082e-17
grad ChooseDest W: 4.286459445953369
grad AddEdge W: 8.833077769655057e-14
grad ChooseDest W: 1.9898492097854614
grad AddEdge W: 9.567598617165008e-18
grad ChooseDest W: 1.9278839826583862
grad AddEdge W: 6.241546733119911e-18
grad ChooseDest W: 5.583426475524902
grad AddEdge W: 1.749713312165248e-17
grad ChooseDest W: 4.236566543579102
grad AddEdge W: 8.570629290805345e-16
grad ChooseDest W: 2.1074509620666504
grad AddEdge W: 5.181517061151697e-16
grad ChooseDest W: 4.761130332946777
grad AddEdge W: 9.473254598574721e-16
grad ChooseDest W: 2.6260015964508057
grad AddEdge W: 9.057984222299663e-18
grad ChooseDest W: 4.479824542999268
grad AddEdge W: 3.5356598291486074e-12
grad ChooseDest W: 5.63383674621582
grad AddEdge W: 8.816384617707625e-18
grad ChooseDest W: 3.0863490104675293
grad AddEdge W: 5.03577074889561e-16
grad ChooseDest W: 3.2272300720214844
grad AddEdge W: 6.670017588482336e-16
grad ChooseDest W: 4.362383842468262
grad AddEdge W: 6.134606341987218e-18
grad ChooseDest W: 3.459399938583374
grad AddEdge W: 9.480551919764215e-18
grad ChooseDest W: 6.206686973571777
grad AddEdge W: 9.222052188077106e-18
grad ChooseDest W: 5.032491683959961
grad AddEdge W: 5.2000316131919e-16
grad ChooseDest W: 4.001156330108643
grad AddEdge W: 3.831877378114642e-18
grad ChooseDest W: 4.567213535308838
grad AddEdge W: 1.9033791508675915e-17
grad ChooseDest W: 4.99193811416626
grad AddEdge W: 4.249111663143636e-16
grad ChooseDest W: 3.916475534439087
grad AddEdge W: 1.9365467773252528e-17
grad ChooseDest W: 4.038506507873535
grad AddEdge W: 2.3142232175104293e-17
grad ChooseDest W: 3.651761293411255
grad AddEdge W: 4.71182466912304e-18
grad ChooseDest W: 4.455650806427002
grad AddEdge W: 1.1480648568184528e-15
grad ChooseDest W: 5.2501373291015625
grad AddEdge W: 1.0319979437594458e-15
grad ChooseDest W: 2.1943888664245605
grad AddEdge W: 1.4018685385843105e-15
grad ChooseDest W: 4.692413330078125
grad AddEdge W: 9.429890746840038e-16
grad ChooseDest W: 3.60060453414917
grad AddEdge W: 1.1407753706837211e-17
grad ChooseDest W: 4.364844799041748
grad AddEdge W: 1.6179978690698567e-17
grad ChooseDest W: 5.582362174987793
grad AddEdge W: 8.821119167923307e-16
grad ChooseDest W: 4.745945453643799
grad AddEdge W: 7.467224352009994e-16
grad ChooseDest W: 5.031946659088135
grad AddEdge W: 8.195212509530317e-18
grad ChooseDest W: 5.638674736022949
=== Epoch 42: Train Loss: 4.4389, Train Log Prob: 0.0298 ===
Total mismatches: 60765
Predicted valid destination but wrong order: 8557
Epoch 42: Validation Loss: 3.3211, Validation Log Prob: 0.0497
Epoch 42: Edge Precision: 0.3585, Recall: 0.3498, F1: 0.3538, Jaccard: 0.2320
Epoch 42: TP: 2.45025053686471, FP: 4.398425196850393, FN: 4.571224051539012
Epoch 42: Current Learning Rate: 6e-05
[Epoch 42] ‚è±Ô∏è Total: 2005.96s | Current time: 2025-07-16 16:23:32 | üèãÔ∏è Train: 1712.65s | ‚úÖ Val: 293.31s
grad AddEdge W: 4.3582904619239235e-14
grad ChooseDest W: 8.083322525024414
grad AddEdge W: 3.787290793477727e-12
grad ChooseDest W: 4.605183124542236
grad AddEdge W: 9.295648101537174e-18
grad ChooseDest W: 3.0169260501861572
grad AddEdge W: 1.8068423757351004e-17
grad ChooseDest W: 3.9576799869537354
grad AddEdge W: 8.205528444385588e-16
grad ChooseDest W: 5.1766486167907715
grad AddEdge W: 1.26980646710759e-15
grad ChooseDest W: 2.907928466796875
grad AddEdge W: 2.590498729689058e-17
grad ChooseDest W: 4.33524751663208
grad AddEdge W: 1.7743568424105504e-17
grad ChooseDest W: 1.2428302764892578
grad AddEdge W: 9.465935386877497e-14
grad ChooseDest W: 2.5747458934783936
grad AddEdge W: 2.2224537290935426e-15
grad ChooseDest W: 6.157415390014648
grad AddEdge W: 1.0190574806258295e-17
grad ChooseDest W: 5.066145420074463
grad AddEdge W: 3.008708409552762e-16
grad ChooseDest W: 5.031372547149658
grad AddEdge W: 1.6446890022034728e-17
grad ChooseDest W: 3.493602991104126
grad AddEdge W: 1.4168965309600473e-17
grad ChooseDest W: 5.700611591339111
grad AddEdge W: 1.201988638528054e-17
grad ChooseDest W: 4.912039756774902
grad AddEdge W: 3.775703096038635e-16
grad ChooseDest W: 4.841303825378418
grad AddEdge W: 1.4103196179096382e-17
grad ChooseDest W: 4.368858814239502
grad AddEdge W: 2.6296230817846045e-12
grad ChooseDest W: 4.214080810546875
grad AddEdge W: 1.03070641703823e-17
grad ChooseDest W: 4.593176364898682
grad AddEdge W: 8.1173406678733905e-16
grad ChooseDest W: 1.6341824531555176
grad AddEdge W: 1.2057038606923319e-15
grad ChooseDest W: 5.9141387939453125
grad AddEdge W: 5.770530008386945e-16
grad ChooseDest W: 7.233246803283691
grad AddEdge W: 9.307399029319103e-18
grad ChooseDest W: 4.761749267578125
grad AddEdge W: 9.386650376627196e-18
grad ChooseDest W: 3.8742520809173584
grad AddEdge W: 8.421267736051258e-18
grad ChooseDest W: 1.9004944562911987
grad AddEdge W: 1.1032310488869806e-17
grad ChooseDest W: 4.2756171226501465
grad AddEdge W: 2.8327908498899254e-17
grad ChooseDest W: 4.120607376098633
grad AddEdge W: 6.002563037584235e-16
grad ChooseDest W: 2.67399263381958
grad AddEdge W: 8.576643754126443e-16
grad ChooseDest W: 3.668273687362671
grad AddEdge W: 3.565677570997787e-16
grad ChooseDest W: 5.852888107299805
grad AddEdge W: 8.542604378564824e-18
grad ChooseDest W: 6.795992851257324
grad AddEdge W: 1.1582430164708973e-15
grad ChooseDest W: 3.1601014137268066
grad AddEdge W: 8.55855655667791e-18
grad ChooseDest W: 5.705974578857422
grad AddEdge W: 8.481954668871824e-18
grad ChooseDest W: 5.082360744476318
grad AddEdge W: 5.833697490428434e-16
grad ChooseDest W: 5.846636772155762
grad AddEdge W: 9.197887760842595e-18
grad ChooseDest W: 2.979896306991577
grad AddEdge W: 6.713887013007412e-16
grad ChooseDest W: 3.759451389312744
grad AddEdge W: 2.344004764592844e-12
grad ChooseDest W: 0.7436716556549072
grad AddEdge W: 1.1320858383488516e-17
grad ChooseDest W: 4.975297927856445
grad AddEdge W: 2.2740470551587288e-17
grad ChooseDest W: 6.741286754608154
grad AddEdge W: 6.188324691737331e-18
grad ChooseDest W: 4.419628143310547
grad AddEdge W: 5.862066212018757e-16
grad ChooseDest W: 5.5344414710998535
grad AddEdge W: 4.1541740928330057e-16
grad ChooseDest W: 3.792614698410034
grad AddEdge W: 6.82457357278546e-16
grad ChooseDest W: 2.15435528755188
grad AddEdge W: 7.684966622486036e-14
grad ChooseDest W: 4.726370811462402
grad AddEdge W: 1.2825655427677633e-17
grad ChooseDest W: 3.2235705852508545
grad AddEdge W: 3.2317811536570823e-16
grad ChooseDest W: 3.5392024517059326
grad AddEdge W: 4.7384793235916424e-18
grad ChooseDest W: 4.348896503448486
grad AddEdge W: 9.160232017817343e-18
grad ChooseDest W: 5.147656440734863
grad AddEdge W: 6.596083788285652e-16
grad ChooseDest W: 3.3508167266845703
grad AddEdge W: 2.7832923276240387e-14
grad ChooseDest W: 2.243061065673828
grad AddEdge W: 8.083096184607083e-16
grad ChooseDest W: 2.5232226848602295
grad AddEdge W: 1.3425019746185594e-17
grad ChooseDest W: 2.353097677230835
grad AddEdge W: 9.433419168460944e-18
grad ChooseDest W: 4.6162543296813965
grad AddEdge W: 1.3500602977610933e-15
grad ChooseDest W: 6.628467559814453
grad AddEdge W: 8.422831107408983e-18
grad ChooseDest W: 2.7822458744049072
grad AddEdge W: 7.473216183669532e-18
grad ChooseDest W: 4.60283899307251
grad AddEdge W: 7.210153558287458e-16
grad ChooseDest W: 3.6291494369506836
grad AddEdge W: 6.278676720147824e-16
grad ChooseDest W: 5.7075371742248535
grad AddEdge W: 1.0856248701995364e-15
grad ChooseDest W: 5.609562873840332
grad AddEdge W: 6.003846044345142e-18
grad ChooseDest W: 8.004366874694824
grad AddEdge W: 7.155014889794755e-16
grad ChooseDest W: 2.363903760910034
grad AddEdge W: 1.1445497131007394e-17
grad ChooseDest W: 3.195103645324707
grad AddEdge W: 4.549464781679682e-16
grad ChooseDest W: 5.281225204467773
grad AddEdge W: 8.196399546796555e-16
grad ChooseDest W: 4.498422622680664
grad AddEdge W: 6.801175362952356e-18
grad ChooseDest W: 5.093533992767334
=== Epoch 43: Train Loss: 4.3992, Train Log Prob: 0.0306 ===
Total mismatches: 60259
Predicted valid destination but wrong order: 8518
Epoch 43: Validation Loss: 3.2934, Validation Log Prob: 0.0509
Epoch 43: Edge Precision: 0.3549, Recall: 0.3462, F1: 0.3502, Jaccard: 0.2293
Epoch 43: TP: 2.4260558339298495, FP: 4.4224767358625625, FN: 4.595418754473872
Epoch 43: Current Learning Rate: 6e-05
[Epoch 43] ‚è±Ô∏è Total: 2009.77s | Current time: 2025-07-16 16:57:02 | üèãÔ∏è Train: 1716.66s | ‚úÖ Val: 293.10s
grad AddEdge W: 9.12822313904897e-16
grad ChooseDest W: 6.239161968231201
grad AddEdge W: 5.125914112724781e-16
grad ChooseDest W: 4.2129621505737305
grad AddEdge W: 3.230149979958907e-14
grad ChooseDest W: 3.1415090560913086
grad AddEdge W: 9.622049700045332e-16
grad ChooseDest W: 4.605441093444824
grad AddEdge W: 7.238929682873075e-18
grad ChooseDest W: 3.940373182296753
grad AddEdge W: 8.044340721501055e-16
grad ChooseDest W: 1.701602816581726
grad AddEdge W: 1.1607761015868858e-17
grad ChooseDest W: 4.067063808441162
grad AddEdge W: 1.126790036161385e-15
grad ChooseDest W: 4.301206111907959
grad AddEdge W: 8.913562623250968e-18
grad ChooseDest W: 3.633002996444702
grad AddEdge W: 1.436258817396664e-15
grad ChooseDest W: 6.095765590667725
grad AddEdge W: 1.138515725008463e-15
grad ChooseDest W: 6.0238261222839355
grad AddEdge W: 1.0887569598104664e-17
grad ChooseDest W: 6.25814151763916
grad AddEdge W: 1.1584078834931465e-17
grad ChooseDest W: 3.4046425819396973
grad AddEdge W: 6.111718119442e-14
grad ChooseDest W: 4.280866622924805
grad AddEdge W: 9.17993555927003e-16
grad ChooseDest W: 2.9724018573760986
grad AddEdge W: 1.1543218594213183e-17
grad ChooseDest W: 5.374321460723877
grad AddEdge W: 7.062750449586541e-14
grad ChooseDest W: 5.110576152801514
grad AddEdge W: 5.137204341933675e-14
grad ChooseDest W: 2.7126996517181396
grad AddEdge W: 1.2574312098690863e-15
grad ChooseDest W: 4.377742290496826
grad AddEdge W: 5.8181489599106626e-18
grad ChooseDest W: 5.746166229248047
grad AddEdge W: 1.4411511738208864e-15
grad ChooseDest W: 5.558405876159668
grad AddEdge W: 8.072591255525456e-18
grad ChooseDest W: 3.461207151412964
grad AddEdge W: 1.071886313432427e-15
grad ChooseDest W: 4.7990546226501465
grad AddEdge W: 1.3488516876244798e-15
grad ChooseDest W: 2.0889456272125244
grad AddEdge W: 1.4208330834951872e-17
grad ChooseDest W: 3.193100690841675
grad AddEdge W: 6.498946049894529e-16
grad ChooseDest W: 2.8921620845794678
grad AddEdge W: 8.53457454989999e-14
grad ChooseDest W: 3.406165838241577
grad AddEdge W: 7.376766924245849e-18
grad ChooseDest W: 4.344178199768066
grad AddEdge W: 7.440860451526704e-16
grad ChooseDest W: 4.415503978729248
grad AddEdge W: 1.1074962402794877e-17
grad ChooseDest W: 3.897217273712158
grad AddEdge W: 6.188335445085294e-18
grad ChooseDest W: 3.908759593963623
grad AddEdge W: 7.347081893603158e-18
grad ChooseDest W: 6.157613277435303
grad AddEdge W: 7.296060169193581e-16
grad ChooseDest W: 4.68721866607666
grad AddEdge W: 2.384804223073864e-17
grad ChooseDest W: 4.796949863433838
grad AddEdge W: 1.1391309356259657e-17
grad ChooseDest W: 3.3255512714385986
grad AddEdge W: 8.060204060416352e-16
grad ChooseDest W: 5.238813877105713
grad AddEdge W: 1.3352625262934422e-15
grad ChooseDest W: 2.3336455821990967
grad AddEdge W: 5.266070536924581e-16
grad ChooseDest W: 2.6844303607940674
grad AddEdge W: 5.54007794500988e-14
grad ChooseDest W: 2.0322484970092773
grad AddEdge W: 2.2968501085411006e-17
grad ChooseDest W: 4.613060474395752
grad AddEdge W: 9.246878690673905e-16
grad ChooseDest W: 3.8023338317871094
grad AddEdge W: 1.1220229347342378e-15
grad ChooseDest W: 6.880158424377441
grad AddEdge W: 4.431388107591025e-18
grad ChooseDest W: 4.0116047859191895
grad AddEdge W: 1.1058120178342685e-17
grad ChooseDest W: 3.569520950317383
grad AddEdge W: 7.227999483694922e-16
grad ChooseDest W: 3.0060505867004395
grad AddEdge W: 8.364515999189628e-16
grad ChooseDest W: 6.123990535736084
grad AddEdge W: 8.578644869464331e-16
grad ChooseDest W: 3.809316396713257
grad AddEdge W: 8.084764310117582e-16
grad ChooseDest W: 4.389258861541748
grad AddEdge W: 9.2888661138693e-16
grad ChooseDest W: 6.107834815979004
grad AddEdge W: 1.2797264934693588e-17
grad ChooseDest W: 3.101379632949829
grad AddEdge W: 6.77668444753007e-16
grad ChooseDest W: 2.5946500301361084
grad AddEdge W: 9.019497989939409e-18
grad ChooseDest W: 5.7560906410217285
grad AddEdge W: 1.2341230336825753e-17
grad ChooseDest W: 2.8674685955047607
grad AddEdge W: 1.2025694152710173e-15
grad ChooseDest W: 2.8249564170837402
grad AddEdge W: 4.749753662991842e-16
grad ChooseDest W: 3.294804573059082
grad AddEdge W: 9.993060718854556e-16
grad ChooseDest W: 4.21156644821167
grad AddEdge W: 4.886422429031324e-16
grad ChooseDest W: 4.340904712677002
grad AddEdge W: 5.783959930832621e-18
grad ChooseDest W: 4.193157196044922
grad AddEdge W: 6.945952236074143e-18
grad ChooseDest W: 5.372907638549805
grad AddEdge W: 1.1326484038834489e-17
grad ChooseDest W: 7.192543983459473
grad AddEdge W: 6.697245462571825e-16
grad ChooseDest W: 3.7080557346343994
grad AddEdge W: 3.3201557406388186e-17
grad ChooseDest W: 4.02311897277832
grad AddEdge W: 4.633514802244543e-16
grad ChooseDest W: 3.74082088470459
grad AddEdge W: 1.422013966537668e-17
grad ChooseDest W: 3.2017662525177
grad AddEdge W: 6.875966552397563e-18
grad ChooseDest W: 6.017312049865723
grad AddEdge W: 7.164039231754362e-18
grad ChooseDest W: 5.0537919998168945
=== Epoch 44: Train Loss: 4.3730, Train Log Prob: 0.0313 ===
Total mismatches: 59867
Predicted valid destination but wrong order: 8534
Epoch 44: Validation Loss: 3.2599, Validation Log Prob: 0.0525
Epoch 44: Edge Precision: 0.3577, Recall: 0.3491, F1: 0.3531, Jaccard: 0.2317
Epoch 44: TP: 2.446098783106657, FP: 4.405869720830351, FN: 4.575375805297065
Epoch 44: Current Learning Rate: 6e-05
[Epoch 44] ‚è±Ô∏è Total: 2011.46s | Current time: 2025-07-16 17:30:33 | üèãÔ∏è Train: 1717.57s | ‚úÖ Val: 293.89s
grad AddEdge W: 8.555512399674816e-16
grad ChooseDest W: 10.874518394470215
grad AddEdge W: 1.366861451041511e-17
grad ChooseDest W: 1.7384140491485596
grad AddEdge W: 7.169324088687963e-18
grad ChooseDest W: 4.532258033752441
grad AddEdge W: 9.002672309099468e-18
grad ChooseDest W: 3.7146456241607666
grad AddEdge W: 2.0741579535453383e-15
grad ChooseDest W: 4.455741882324219
grad AddEdge W: 7.269873682408071e-18
grad ChooseDest W: 3.9214649200439453
grad AddEdge W: 5.1122954111197075e-16
grad ChooseDest W: 3.010430097579956
grad AddEdge W: 6.394436710818394e-14
grad ChooseDest W: 5.050221920013428
grad AddEdge W: 1.843308864650072e-15
grad ChooseDest W: 3.512590169906616
grad AddEdge W: 1.072598317416488e-17
grad ChooseDest W: 4.401607990264893
grad AddEdge W: 5.6539862819530824e-18
grad ChooseDest W: 4.930351257324219
grad AddEdge W: 6.903569155848151e-18
grad ChooseDest W: 4.947700023651123
grad AddEdge W: 1.5320943352756122e-17
grad ChooseDest W: 2.4053876399993896
grad AddEdge W: 1.0546448519119472e-15
grad ChooseDest W: 4.280689716339111
grad AddEdge W: 1.4293369965005998e-17
grad ChooseDest W: 6.346368312835693
grad AddEdge W: 1.2129327575015553e-15
grad ChooseDest W: 6.877106666564941
grad AddEdge W: 1.198169959230203e-17
grad ChooseDest W: 6.416067123413086
grad AddEdge W: 7.252017102914318e-16
grad ChooseDest W: 4.706711292266846
grad AddEdge W: 8.935769941156179e-18
grad ChooseDest W: 5.461197376251221
grad AddEdge W: 9.685189785864934e-18
grad ChooseDest W: 3.31477952003479
grad AddEdge W: 4.6668115681394356e-18
grad ChooseDest W: 2.227653980255127
grad AddEdge W: 5.132023337856852e-16
grad ChooseDest W: 3.0926995277404785
grad AddEdge W: 1.4981203731568343e-17
grad ChooseDest W: 3.1882312297821045
grad AddEdge W: 8.614583981107964e-18
grad ChooseDest W: 4.6717352867126465
grad AddEdge W: 1.2022036227692566e-17
grad ChooseDest W: 7.0710320472717285
grad AddEdge W: 2.307876591542555e-17
grad ChooseDest W: 6.9363837242126465
grad AddEdge W: 2.3954141379188367e-17
grad ChooseDest W: 3.9955875873565674
grad AddEdge W: 1.0496958501593772e-15
grad ChooseDest W: 4.4769415855407715
grad AddEdge W: 6.425426766446408e-16
grad ChooseDest W: 5.492701053619385
grad AddEdge W: 6.665814716877178e-16
grad ChooseDest W: 4.422492504119873
grad AddEdge W: 4.586687644546773e-16
grad ChooseDest W: 3.9454829692840576
grad AddEdge W: 9.867750201416612e-18
grad ChooseDest W: 2.413259983062744
grad AddEdge W: 9.830971269840667e-18
grad ChooseDest W: 3.04345440864563
grad AddEdge W: 8.113252145716112e-18
grad ChooseDest W: 3.1827597618103027
grad AddEdge W: 5.817686979538552e-16
grad ChooseDest W: 3.361583709716797
grad AddEdge W: 5.26362403445767e-18
grad ChooseDest W: 4.496540546417236
grad AddEdge W: 1.6299284538633866e-15
grad ChooseDest W: 7.663462162017822
grad AddEdge W: 1.0117570988680692e-16
grad ChooseDest W: 3.186117649078369
grad AddEdge W: 1.1438868105578394e-17
grad ChooseDest W: 3.006018877029419
grad AddEdge W: 1.7014127472731742e-17
grad ChooseDest W: 4.4161906242370605
grad AddEdge W: 7.369998105293327e-18
grad ChooseDest W: 3.447070360183716
grad AddEdge W: 9.859046342313533e-16
grad ChooseDest W: 4.430091381072998
grad AddEdge W: 5.355201166986191e-16
grad ChooseDest W: 5.1310272216796875
grad AddEdge W: 4.445359138506209e-16
grad ChooseDest W: 3.835515022277832
grad AddEdge W: 1.1310244001868235e-17
grad ChooseDest W: 4.279565811157227
grad AddEdge W: 6.857036110488981e-16
grad ChooseDest W: 3.4325475692749023
grad AddEdge W: 7.795174730409872e-18
grad ChooseDest W: 5.0923895835876465
grad AddEdge W: 8.89898320099525e-16
grad ChooseDest W: 7.265622138977051
grad AddEdge W: 1.8800041848097005e-17
grad ChooseDest W: 3.330991506576538
grad AddEdge W: 7.711904112505383e-18
grad ChooseDest W: 4.334590911865234
grad AddEdge W: 1.24605163986008e-17
grad ChooseDest W: 5.810097694396973
grad AddEdge W: 8.300715260758393e-18
grad ChooseDest W: 3.008561611175537
grad AddEdge W: 2.409013152625331e-17
grad ChooseDest W: 4.581087112426758
grad AddEdge W: 7.973778741091709e-18
grad ChooseDest W: 4.2530412673950195
grad AddEdge W: 7.016917715186293e-18
grad ChooseDest W: 4.763381004333496
grad AddEdge W: 1.1843140222254498e-17
grad ChooseDest W: 4.072433948516846
grad AddEdge W: 7.308323518821374e-18
grad ChooseDest W: 4.737583637237549
grad AddEdge W: 6.325715911368116e-14
grad ChooseDest W: 2.695376396179199
grad AddEdge W: 3.60350103316404e-16
grad ChooseDest W: 4.138209342956543
grad AddEdge W: 1.3299944247303288e-17
grad ChooseDest W: 3.7953436374664307
grad AddEdge W: 9.421841816258754e-16
grad ChooseDest W: 3.058690309524536
grad AddEdge W: 1.5107051784055549e-15
grad ChooseDest W: 3.9330008029937744
grad AddEdge W: 1.1921259225478455e-15
grad ChooseDest W: 3.4046638011932373
grad AddEdge W: 2.306684167676168e-15
grad ChooseDest W: 5.183064937591553
grad AddEdge W: 7.663965753459934e-16
grad ChooseDest W: 4.46867561340332
grad AddEdge W: 1.9633630794291604e-14
grad ChooseDest W: 4.24871301651001
=== Epoch 45: Train Loss: 4.3474, Train Log Prob: 0.0320 ===
Total mismatches: 59583
Predicted valid destination but wrong order: 8508
Epoch 45: Validation Loss: 3.2555, Validation Log Prob: 0.0526
Epoch 45: Edge Precision: 0.3575, Recall: 0.3488, F1: 0.3528, Jaccard: 0.2314
Epoch 45: TP: 2.4438081603435933, FP: 4.4084466714387975, FN: 4.5776664280601285
Epoch 45: Current Learning Rate: 6e-05
[Epoch 45] ‚è±Ô∏è Total: 2008.24s | Current time: 2025-07-16 18:04:01 | üèãÔ∏è Train: 1714.79s | ‚úÖ Val: 293.46s
grad AddEdge W: 3.3645057226361352e-12
grad ChooseDest W: 7.1839423179626465
grad AddEdge W: 7.649708402247521e-18
grad ChooseDest W: 3.9319710731506348
grad AddEdge W: 8.109041796398217e-18
grad ChooseDest W: 1.9068059921264648
grad AddEdge W: 5.126507671062569e-14
grad ChooseDest W: 3.003777027130127
grad AddEdge W: 1.927259147085393e-15
grad ChooseDest W: 8.086892127990723
grad AddEdge W: 3.815735704834622e-14
grad ChooseDest W: 1.8943310976028442
grad AddEdge W: 2.3260447865166696e-17
grad ChooseDest W: 5.258808612823486
grad AddEdge W: 7.957474845782166e-16
grad ChooseDest W: 3.884397268295288
grad AddEdge W: 1.0301853759703829e-17
grad ChooseDest W: 5.036972999572754
grad AddEdge W: 1.2160714380876061e-15
grad ChooseDest W: 2.569780111312866
grad AddEdge W: 6.56653110654097e-18
grad ChooseDest W: 6.08283805847168
grad AddEdge W: 5.355969237272171e-18
grad ChooseDest W: 3.4920456409454346
grad AddEdge W: 8.847307309070042e-16
grad ChooseDest W: 1.6224972009658813
grad AddEdge W: 1.1585304782773718e-15
grad ChooseDest W: 4.322052955627441
grad AddEdge W: 4.046313182027439e-16
grad ChooseDest W: 3.7735538482666016
grad AddEdge W: 6.765904365089483e-16
grad ChooseDest W: 5.681114673614502
grad AddEdge W: 1.709453108263312e-17
grad ChooseDest W: 2.460279703140259
grad AddEdge W: 1.0488933062941884e-17
grad ChooseDest W: 5.559091567993164
grad AddEdge W: 5.459166636133062e-18
grad ChooseDest W: 4.399805545806885
grad AddEdge W: 9.234141432729569e-18
grad ChooseDest W: 3.627255439758301
grad AddEdge W: 1.5047993602948325e-17
grad ChooseDest W: 4.028510570526123
grad AddEdge W: 6.349411912177439e-18
grad ChooseDest W: 2.751936197280884
grad AddEdge W: 2.3249312690632753e-16
grad ChooseDest W: 3.3234894275665283
grad AddEdge W: 6.917260682434354e-16
grad ChooseDest W: 4.5521345138549805
grad AddEdge W: 5.567772026033539e-16
grad ChooseDest W: 4.4057393074035645
grad AddEdge W: 1.95587290107985e-18
grad ChooseDest W: 7.3816399574279785
grad AddEdge W: 1.2753766616478528e-15
grad ChooseDest W: 3.5661275386810303
grad AddEdge W: 1.0121554028766258e-17
grad ChooseDest W: 4.55585241317749
grad AddEdge W: 5.443343498195535e-18
grad ChooseDest W: 2.4346396923065186
grad AddEdge W: 1.4843909352287975e-15
grad ChooseDest W: 3.60941219329834
grad AddEdge W: 1.3987212222326647e-17
grad ChooseDest W: 6.789844989776611
grad AddEdge W: 8.023761129605225e-18
grad ChooseDest W: 4.15609073638916
grad AddEdge W: 1.4747978503497792e-17
grad ChooseDest W: 6.652015686035156
grad AddEdge W: 9.063690941345667e-18
grad ChooseDest W: 4.174065113067627
grad AddEdge W: 4.0361786975113374e-16
grad ChooseDest W: 5.149181842803955
grad AddEdge W: 3.742749599827677e-12
grad ChooseDest W: 3.0866506099700928
grad AddEdge W: 8.075895842072605e-18
grad ChooseDest W: 3.539733409881592
grad AddEdge W: 5.9739260944084864e-18
grad ChooseDest W: 8.139094352722168
grad AddEdge W: 7.070070686987725e-18
grad ChooseDest W: 3.4586596488952637
grad AddEdge W: 8.120729858453592e-18
grad ChooseDest W: 3.900221824645996
grad AddEdge W: 2.763769410653398e-17
grad ChooseDest W: 4.6807146072387695
grad AddEdge W: 2.046563287719908e-17
grad ChooseDest W: 4.693545818328857
grad AddEdge W: 9.780186516132974e-18
grad ChooseDest W: 7.507950782775879
grad AddEdge W: 5.222932538584349e-18
grad ChooseDest W: 3.6189515590667725
grad AddEdge W: 2.990766922311705e-12
grad ChooseDest W: 3.156961679458618
grad AddEdge W: 3.291976079449301e-16
grad ChooseDest W: 3.867053747177124
grad AddEdge W: 6.266381507522836e-16
grad ChooseDest W: 5.78570556640625
grad AddEdge W: 7.55232360155104e-18
grad ChooseDest W: 3.642880439758301
grad AddEdge W: 3.023251435861526e-16
grad ChooseDest W: 4.210307598114014
grad AddEdge W: 3.430240683513497e-12
grad ChooseDest W: 2.9046103954315186
grad AddEdge W: 8.419845779491055e-16
grad ChooseDest W: 5.602479934692383
grad AddEdge W: 1.771217857422034e-17
grad ChooseDest W: 3.5794124603271484
grad AddEdge W: 7.57577109480504e-16
grad ChooseDest W: 3.7346155643463135
grad AddEdge W: 9.014899692914226e-18
grad ChooseDest W: 4.640869140625
grad AddEdge W: 1.5036341936839903e-17
grad ChooseDest W: 3.672222852706909
grad AddEdge W: 1.1772964932237908e-15
grad ChooseDest W: 3.7609386444091797
grad AddEdge W: 9.912594971145562e-18
grad ChooseDest W: 3.8408114910125732
grad AddEdge W: 1.1733251759418666e-17
grad ChooseDest W: 3.695509433746338
grad AddEdge W: 3.9004406089226517e-16
grad ChooseDest W: 2.0390701293945312
grad AddEdge W: 6.82645292713718e-16
grad ChooseDest W: 2.911778211593628
grad AddEdge W: 5.833068039069506e-16
grad ChooseDest W: 7.12526273727417
grad AddEdge W: 1.050235820428362e-17
grad ChooseDest W: 4.253998756408691
grad AddEdge W: 7.399845263336078e-18
grad ChooseDest W: 2.9648382663726807
grad AddEdge W: 4.868965609384004e-16
grad ChooseDest W: 3.3427722454071045
grad AddEdge W: 6.386526778343754e-16
grad ChooseDest W: 3.9972400665283203
grad AddEdge W: 2.883028572514925e-16
grad ChooseDest W: 6.192940711975098
=== Epoch 46: Train Loss: 4.3061, Train Log Prob: 0.0328 ===
Total mismatches: 58859
Predicted valid destination but wrong order: 8479
Epoch 46: Validation Loss: 3.2246, Validation Log Prob: 0.0541
Epoch 46: Edge Precision: 0.3566, Recall: 0.3481, F1: 0.3520, Jaccard: 0.2306
Epoch 46: TP: 2.4387974230493916, FP: 4.413457408733, FN: 4.582677165354331
Epoch 46: Current Learning Rate: 6e-05
[Epoch 46] ‚è±Ô∏è Total: 2006.32s | Current time: 2025-07-16 18:37:28 | üèãÔ∏è Train: 1712.42s | ‚úÖ Val: 293.91s
grad AddEdge W: 2.9876810389833225e-14
grad ChooseDest W: 9.457125663757324
grad AddEdge W: 5.134656237202996e-18
grad ChooseDest W: 6.630767822265625
grad AddEdge W: 9.278526290037939e-18
grad ChooseDest W: 2.911588668823242
grad AddEdge W: 1.830329342047931e-17
grad ChooseDest W: 4.240118503570557
grad AddEdge W: 3.1132818399062467e-17
grad ChooseDest W: 3.564096212387085
grad AddEdge W: 8.28573253832322e-18
grad ChooseDest W: 3.9409873485565186
grad AddEdge W: 1.0913093910266213e-17
grad ChooseDest W: 3.8701517581939697
grad AddEdge W: 7.486032718603776e-16
grad ChooseDest W: 5.049086570739746
grad AddEdge W: 1.5550131939437415e-17
grad ChooseDest W: 4.68571138381958
grad AddEdge W: 7.489772369065903e-16
grad ChooseDest W: 3.8380849361419678
grad AddEdge W: 1.2267178646848169e-17
grad ChooseDest W: 2.9830057621002197
grad AddEdge W: 6.889033110943757e-18
grad ChooseDest W: 3.203636646270752
grad AddEdge W: 1.0376479513026525e-17
grad ChooseDest W: 3.7492775917053223
grad AddEdge W: 3.1928173733855884e-16
grad ChooseDest W: 3.638176918029785
grad AddEdge W: 1.28174867875181e-15
grad ChooseDest W: 3.0230154991149902
grad AddEdge W: 3.128446575163406e-16
grad ChooseDest W: 4.679499626159668
grad AddEdge W: 1.0621034633947215e-17
grad ChooseDest W: 5.575360298156738
grad AddEdge W: 1.3197076132500643e-15
grad ChooseDest W: 2.9051742553710938
grad AddEdge W: 2.7998406544604837e-12
grad ChooseDest W: 1.5620983839035034
grad AddEdge W: 8.48633376303468e-18
grad ChooseDest W: 4.728702545166016
grad AddEdge W: 1.3100472124127963e-17
grad ChooseDest W: 5.119116306304932
grad AddEdge W: 4.661190015609301e-16
grad ChooseDest W: 3.5595145225524902
grad AddEdge W: 6.939496885486555e-16
grad ChooseDest W: 4.121491432189941
grad AddEdge W: 1.6078527635546899e-15
grad ChooseDest W: 6.815229415893555
grad AddEdge W: 5.730686637339286e-16
grad ChooseDest W: 6.547389030456543
grad AddEdge W: 1.7706467719271275e-17
grad ChooseDest W: 4.455367088317871
grad AddEdge W: 5.842728449832941e-16
grad ChooseDest W: 2.291743278503418
grad AddEdge W: 4.18519032177909e-16
grad ChooseDest W: 4.679664134979248
grad AddEdge W: 5.876915592939186e-18
grad ChooseDest W: 3.1432158946990967
grad AddEdge W: 5.02488151170139e-18
grad ChooseDest W: 4.672966003417969
grad AddEdge W: 1.1446897547784446e-17
grad ChooseDest W: 4.117785930633545
grad AddEdge W: 9.504594751448682e-18
grad ChooseDest W: 5.641578197479248
grad AddEdge W: 5.177772646129241e-16
grad ChooseDest W: 3.382533073425293
grad AddEdge W: 8.090554839103249e-16
grad ChooseDest W: 6.872331619262695
grad AddEdge W: 9.691162294585363e-16
grad ChooseDest W: 3.3240249156951904
grad AddEdge W: 3.9607322652157025e-18
grad ChooseDest W: 4.541499614715576
grad AddEdge W: 9.53871224594735e-16
grad ChooseDest W: 4.431227207183838
grad AddEdge W: 7.419386578127031e-18
grad ChooseDest W: 3.9720520973205566
grad AddEdge W: 7.640871631763617e-18
grad ChooseDest W: 8.675213813781738
grad AddEdge W: 6.6624919158129405e-18
grad ChooseDest W: 5.965546131134033
grad AddEdge W: 8.516508650036124e-16
grad ChooseDest W: 6.253197193145752
grad AddEdge W: 5.165559291297671e-18
grad ChooseDest W: 6.174405097961426
grad AddEdge W: 1.0832179929926888e-17
grad ChooseDest W: 2.0216331481933594
grad AddEdge W: 1.7452189090250024e-17
grad ChooseDest W: 2.0576579570770264
grad AddEdge W: 4.174308034649021e-12
grad ChooseDest W: 1.2442816495895386
grad AddEdge W: 1.1028681647522535e-17
grad ChooseDest W: 3.0799129009246826
grad AddEdge W: 1.3708051173299188e-17
grad ChooseDest W: 3.2306952476501465
grad AddEdge W: 1.2020440430854829e-15
grad ChooseDest W: 4.3038225173950195
grad AddEdge W: 1.3682386240433505e-17
grad ChooseDest W: 5.706543445587158
grad AddEdge W: 4.883334993938582e-16
grad ChooseDest W: 2.861067771911621
grad AddEdge W: 6.013858751331464e-16
grad ChooseDest W: 4.060373306274414
grad AddEdge W: 7.824034234801234e-18
grad ChooseDest W: 3.0725650787353516
grad AddEdge W: 7.265553318068707e-18
grad ChooseDest W: 3.0538125038146973
grad AddEdge W: 4.0152027206659727e-16
grad ChooseDest W: 3.900529146194458
grad AddEdge W: 6.754658034024824e-18
grad ChooseDest W: 2.2131307125091553
grad AddEdge W: 1.853853366052204e-15
grad ChooseDest W: 3.500032663345337
grad AddEdge W: 1.726156200936473e-17
grad ChooseDest W: 4.840884208679199
grad AddEdge W: 7.800314003555664e-18
grad ChooseDest W: 4.181561470031738
grad AddEdge W: 1.441848387815595e-15
grad ChooseDest W: 5.141964435577393
grad AddEdge W: 1.8570758962825882e-17
grad ChooseDest W: 5.1734938621521
grad AddEdge W: 9.968985527864527e-18
grad ChooseDest W: 5.1959075927734375
grad AddEdge W: 5.377715832119803e-16
grad ChooseDest W: 4.213426113128662
grad AddEdge W: 2.2806952910302747e-15
grad ChooseDest W: 1.3849446773529053
grad AddEdge W: 1.2084030105880965e-17
grad ChooseDest W: 3.103872776031494
grad AddEdge W: 1.1412247779105212e-17
grad ChooseDest W: 3.1822757720947266
grad AddEdge W: 1.2012672543158466e-17
grad ChooseDest W: 3.204902172088623
=== Epoch 47: Train Loss: 4.2853, Train Log Prob: 0.0337 ===
Total mismatches: 58547
Predicted valid destination but wrong order: 8539
Epoch 47: Validation Loss: 3.1944, Validation Log Prob: 0.0553
Epoch 47: Edge Precision: 0.3574, Recall: 0.3478, F1: 0.3522, Jaccard: 0.2309
Epoch 47: TP: 2.4363636363636365, FP: 4.399570508231926, FN: 4.585110952040086
Epoch 47: Current Learning Rate: 6e-05
[Epoch 47] ‚è±Ô∏è Total: 2004.48s | Current time: 2025-07-16 19:10:52 | üèãÔ∏è Train: 1712.23s | ‚úÖ Val: 292.25s
grad AddEdge W: 9.932595653079634e-14
grad ChooseDest W: 7.596219062805176
grad AddEdge W: 1.0119691218026788e-17
grad ChooseDest W: 2.5554261207580566
grad AddEdge W: 1.0238523157645544e-17
grad ChooseDest W: 3.839332342147827
grad AddEdge W: 7.171535208639766e-16
grad ChooseDest W: 5.751351356506348
grad AddEdge W: 8.799868170067881e-16
grad ChooseDest W: 2.661562204360962
grad AddEdge W: 6.394729233646728e-16
grad ChooseDest W: 3.119680881500244
grad AddEdge W: 1.7840174847845572e-17
grad ChooseDest W: 4.1790595054626465
grad AddEdge W: 9.391270279609978e-16
grad ChooseDest W: 5.075862407684326
grad AddEdge W: 7.013491533089098e-18
grad ChooseDest W: 4.830014228820801
grad AddEdge W: 8.672430178505164e-14
grad ChooseDest W: 7.687877655029297
grad AddEdge W: 5.27093176157927e-18
grad ChooseDest W: 3.9496636390686035
grad AddEdge W: 1.0786734627073225e-17
grad ChooseDest W: 4.348404407501221
grad AddEdge W: 7.233856584176287e-18
grad ChooseDest W: 3.083592176437378
grad AddEdge W: 6.055916054745008e-16
grad ChooseDest W: 4.93992280960083
grad AddEdge W: 5.7728715747213475e-18
grad ChooseDest W: 4.391477584838867
grad AddEdge W: 5.951621169190994e-18
grad ChooseDest W: 3.890915632247925
grad AddEdge W: 3.4126568339115616e-16
grad ChooseDest W: 2.9378647804260254
grad AddEdge W: 1.1937102149576234e-17
grad ChooseDest W: 5.615185737609863
grad AddEdge W: 6.766514228811506e-16
grad ChooseDest W: 5.546820640563965
grad AddEdge W: 3.228674395625231e-16
grad ChooseDest W: 5.676060199737549
grad AddEdge W: 1.3611694558104111e-17
grad ChooseDest W: 6.528654098510742
grad AddEdge W: 7.33890733083044e-16
grad ChooseDest W: 3.8454699516296387
grad AddEdge W: 8.083421531285613e-18
grad ChooseDest W: 3.1855382919311523
grad AddEdge W: 3.3768964015520467e-18
grad ChooseDest W: 3.228062868118286
grad AddEdge W: 1.5261593143805443e-17
grad ChooseDest W: 5.557916164398193
grad AddEdge W: 3.913218355041867e-14
grad ChooseDest W: 2.6553866863250732
grad AddEdge W: 6.687946744803035e-18
grad ChooseDest W: 3.860673666000366
grad AddEdge W: 1.630668290820699e-17
grad ChooseDest W: 3.8250131607055664
grad AddEdge W: 2.084593081818156e-17
grad ChooseDest W: 2.6405303478240967
grad AddEdge W: 1.4155957067287464e-17
grad ChooseDest W: 3.898782730102539
grad AddEdge W: 7.462695372720143e-16
grad ChooseDest W: 6.149702548980713
grad AddEdge W: 8.058353822822193e-18
grad ChooseDest W: 4.3407392501831055
grad AddEdge W: 4.071811868133611e-18
grad ChooseDest W: 4.717155933380127
grad AddEdge W: 1.5664468152426945e-17
grad ChooseDest W: 4.7373504638671875
grad AddEdge W: 1.2039139841218325e-17
grad ChooseDest W: 5.481535911560059
grad AddEdge W: 9.011059093330143e-18
grad ChooseDest W: 4.668561935424805
grad AddEdge W: 3.509222758888839e-18
grad ChooseDest W: 4.711698055267334
grad AddEdge W: 1.385081298481796e-15
grad ChooseDest W: 4.080872535705566
grad AddEdge W: 3.7426545174147326e-16
grad ChooseDest W: 4.657515525817871
grad AddEdge W: 1.0206709956722877e-15
grad ChooseDest W: 2.8467626571655273
grad AddEdge W: 1.476855544841566e-17
grad ChooseDest W: 4.210448741912842
grad AddEdge W: 6.88753300235928e-16
grad ChooseDest W: 4.452149391174316
grad AddEdge W: 2.0678499536033477e-17
grad ChooseDest W: 3.0763299465179443
grad AddEdge W: 9.568207838994876e-15
grad ChooseDest W: 6.072225093841553
grad AddEdge W: 6.0161987460227026e-18
grad ChooseDest W: 4.659287452697754
grad AddEdge W: 5.0164744615457075e-18
grad ChooseDest W: 2.1601576805114746
grad AddEdge W: 1.1144102294295122e-17
grad ChooseDest W: 2.398263454437256
grad AddEdge W: 2.4893449592790738e-14
grad ChooseDest W: 2.165231943130493
grad AddEdge W: 4.0691534254501513e-16
grad ChooseDest W: 3.2900450229644775
grad AddEdge W: 4.4399767735219995e-16
grad ChooseDest W: 4.737297058105469
grad AddEdge W: 3.289852599283398e-18
grad ChooseDest W: 3.2037112712860107
grad AddEdge W: 1.2714054700973816e-17
grad ChooseDest W: 3.844453811645508
grad AddEdge W: 4.327516209115046e-16
grad ChooseDest W: 5.166094779968262
grad AddEdge W: 4.597923007196509e-16
grad ChooseDest W: 5.294133186340332
grad AddEdge W: 2.880063808968769e-14
grad ChooseDest W: 2.5863845348358154
grad AddEdge W: 4.111275556106797e-12
grad ChooseDest W: 3.303953170776367
grad AddEdge W: 8.58075063969332e-18
grad ChooseDest W: 5.2749738693237305
grad AddEdge W: 1.0128476934923285e-15
grad ChooseDest W: 1.739073395729065
grad AddEdge W: 5.900226634479339e-16
grad ChooseDest W: 2.9142372608184814
grad AddEdge W: 8.886212061733024e-16
grad ChooseDest W: 4.144923686981201
grad AddEdge W: 8.114802216009588e-16
grad ChooseDest W: 3.7110376358032227
grad AddEdge W: 3.1374180834429274e-15
grad ChooseDest W: 4.172869682312012
grad AddEdge W: 6.416648328739301e-16
grad ChooseDest W: 6.116274356842041
grad AddEdge W: 7.991811278445365e-18
grad ChooseDest W: 5.425313472747803
grad AddEdge W: 2.0226754696822342e-17
grad ChooseDest W: 3.7816269397735596
grad AddEdge W: 6.320447355848282e-18
grad ChooseDest W: 2.892533540725708
=== Epoch 48: Train Loss: 4.2514, Train Log Prob: 0.0344 ===
Total mismatches: 57986
Predicted valid destination but wrong order: 8550
Epoch 48: Validation Loss: 3.1555, Validation Log Prob: 0.0579
Epoch 48: Edge Precision: 0.3572, Recall: 0.3479, F1: 0.3522, Jaccard: 0.2312
Epoch 48: TP: 2.4357909806728704, FP: 4.400286327845383, FN: 4.585683607730852
Epoch 48: Current Learning Rate: 6e-05
[Epoch 48] ‚è±Ô∏è Total: 1987.45s | Current time: 2025-07-16 19:44:00 | üèãÔ∏è Train: 1695.60s | ‚úÖ Val: 291.85s
grad AddEdge W: 1.4021071901171994e-15
grad ChooseDest W: 10.154313087463379
grad AddEdge W: 4.4139838946454305e-14
grad ChooseDest W: 3.0351274013519287
grad AddEdge W: 7.277648849293825e-16
grad ChooseDest W: 2.2259275913238525
grad AddEdge W: 7.890499024200482e-18
grad ChooseDest W: 2.5792198181152344
grad AddEdge W: 4.160082792840982e-18
grad ChooseDest W: 2.2256767749786377
grad AddEdge W: 6.265826171546792e-16
grad ChooseDest W: 5.722265720367432
grad AddEdge W: 7.135556954809548e-16
grad ChooseDest W: 4.563658714294434
grad AddEdge W: 5.855649895091658e-14
grad ChooseDest W: 3.139403820037842
grad AddEdge W: 1.434691171169533e-17
grad ChooseDest W: 4.314496994018555
grad AddEdge W: 7.985041632312231e-18
grad ChooseDest W: 5.384103775024414
grad AddEdge W: 8.224694682399585e-16
grad ChooseDest W: 2.171236753463745
grad AddEdge W: 4.627114938932444e-16
grad ChooseDest W: 3.6481151580810547
grad AddEdge W: 1.4894235616325743e-17
grad ChooseDest W: 3.934215784072876
grad AddEdge W: 7.094791806774485e-18
grad ChooseDest W: 4.969156742095947
grad AddEdge W: 1.437421198063203e-17
grad ChooseDest W: 5.623133182525635
grad AddEdge W: 4.778085856286314e-16
grad ChooseDest W: 5.87661075592041
grad AddEdge W: 6.265260760510888e-18
grad ChooseDest W: 4.894659996032715
grad AddEdge W: 9.413368310412658e-16
grad ChooseDest W: 6.010157108306885
grad AddEdge W: 1.007096200814129e-17
grad ChooseDest W: 6.816014289855957
grad AddEdge W: 4.4779769876415425e-18
grad ChooseDest W: 4.758443832397461
grad AddEdge W: 7.942387600804791e-16
grad ChooseDest W: 5.4814276695251465
grad AddEdge W: 4.214873267926637e-16
grad ChooseDest W: 5.636510848999023
grad AddEdge W: 5.290460384772648e-14
grad ChooseDest W: 5.060178756713867
grad AddEdge W: 6.026354042403016e-18
grad ChooseDest W: 4.458242893218994
grad AddEdge W: 1.297538421753647e-17
grad ChooseDest W: 6.032145023345947
grad AddEdge W: 1.1706623988319971e-17
grad ChooseDest W: 2.080064535140991
grad AddEdge W: 8.292879378815678e-18
grad ChooseDest W: 3.813025951385498
grad AddEdge W: 4.077536309358802e-14
grad ChooseDest W: 5.342482566833496
grad AddEdge W: 4.649651838680921e-16
grad ChooseDest W: 5.456225872039795
grad AddEdge W: 7.622442610198752e-16
grad ChooseDest W: 4.47528600692749
grad AddEdge W: 9.216504089185366e-16
grad ChooseDest W: 3.7591795921325684
grad AddEdge W: 1.8119426059559798e-17
grad ChooseDest W: 4.118338108062744
grad AddEdge W: 6.589475855962272e-18
grad ChooseDest W: 5.547179222106934
grad AddEdge W: 1.1087662933920017e-17
grad ChooseDest W: 5.951685428619385
grad AddEdge W: 6.195180447372231e-16
grad ChooseDest W: 3.831911563873291
grad AddEdge W: 5.557408577923882e-16
grad ChooseDest W: 3.881432056427002
grad AddEdge W: 6.4248818033152456e-18
grad ChooseDest W: 4.289823532104492
grad AddEdge W: 1.4647138569382677e-17
grad ChooseDest W: 7.697546005249023
grad AddEdge W: 5.5460979418384036e-18
grad ChooseDest W: 6.4470696449279785
grad AddEdge W: 1.2268146448164856e-17
grad ChooseDest W: 3.052504062652588
grad AddEdge W: 7.348669914943138e-16
grad ChooseDest W: 1.342475175857544
grad AddEdge W: 4.8482671734717195e-14
grad ChooseDest W: 2.972262144088745
grad AddEdge W: 8.920125474230963e-18
grad ChooseDest W: 3.7196028232574463
grad AddEdge W: 1.2375794078721282e-17
grad ChooseDest W: 7.790885925292969
grad AddEdge W: 7.839394747165772e-16
grad ChooseDest W: 2.4468417167663574
grad AddEdge W: 5.742437118443684e-18
grad ChooseDest W: 2.9923861026763916
grad AddEdge W: 3.535288146432671e-16
grad ChooseDest W: 6.193273067474365
grad AddEdge W: 5.7224056991397935e-18
grad ChooseDest W: 3.635908842086792
grad AddEdge W: 6.970041389872913e-18
grad ChooseDest W: 5.245245456695557
grad AddEdge W: 4.442400346542331e-16
grad ChooseDest W: 4.891516208648682
grad AddEdge W: 2.0594800438571686e-17
grad ChooseDest W: 4.165461540222168
grad AddEdge W: 6.290059784167738e-16
grad ChooseDest W: 3.20276141166687
grad AddEdge W: 1.1758532549607879e-15
grad ChooseDest W: 5.293872356414795
grad AddEdge W: 1.6837279566490354e-17
grad ChooseDest W: 5.640727996826172
grad AddEdge W: 1.3816673321878875e-15
grad ChooseDest W: 3.780240297317505
grad AddEdge W: 5.861162533743155e-16
grad ChooseDest W: 8.616446495056152
grad AddEdge W: 1.5625694888394134e-17
grad ChooseDest W: 4.894887924194336
grad AddEdge W: 7.133145557887833e-16
grad ChooseDest W: 3.495955467224121
grad AddEdge W: 5.203447400606645e-14
grad ChooseDest W: 4.582192420959473
grad AddEdge W: 2.111876145398115e-16
grad ChooseDest W: 5.352235794067383
grad AddEdge W: 6.694336715035007e-18
grad ChooseDest W: 9.190443992614746
grad AddEdge W: 1.7731949845221584e-17
grad ChooseDest W: 4.072388172149658
grad AddEdge W: 6.456796102252379e-16
grad ChooseDest W: 4.366615295410156
grad AddEdge W: 8.787310873537611e-18
grad ChooseDest W: 2.200157403945923
grad AddEdge W: 6.987719893924396e-18
grad ChooseDest W: 2.1645500659942627
grad AddEdge W: 8.90948535075002e-16
grad ChooseDest W: 4.839626789093018
=== Epoch 49: Train Loss: 4.2342, Train Log Prob: 0.0349 ===
Total mismatches: 57777
Predicted valid destination but wrong order: 8596
Epoch 49: Validation Loss: 3.1359, Validation Log Prob: 0.0586
Epoch 49: Edge Precision: 0.3549, Recall: 0.3459, F1: 0.3500, Jaccard: 0.2286
Epoch 49: TP: 2.4230493915533287, FP: 4.421617752326414, FN: 4.5984251968503935
Epoch 49: Current Learning Rate: 6e-05
[Epoch 49] ‚è±Ô∏è Total: 1988.57s | Current time: 2025-07-16 20:17:08 | üèãÔ∏è Train: 1695.40s | ‚úÖ Val: 293.16s
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:3638: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
grad AddEdge W: 7.736053117546327e-12
grad ChooseDest W: 7.429901599884033
grad AddEdge W: 2.481012772939294e-17
grad ChooseDest W: 4.611361026763916
grad AddEdge W: 3.9694057069050714e-11
grad ChooseDest W: 6.675533294677734
grad AddEdge W: 5.170841666689435e-18
grad ChooseDest W: 4.10612154006958
grad AddEdge W: 7.264402709836646e-18
grad ChooseDest W: 5.292488098144531
grad AddEdge W: 4.1298698059005236e-16
grad ChooseDest W: 5.1702446937561035
grad AddEdge W: 6.646949605693375e-18
grad ChooseDest W: 5.037321090698242
grad AddEdge W: 1.2585723750072747e-15
grad ChooseDest W: 3.4922211170196533
grad AddEdge W: 1.1260894406523322e-17
grad ChooseDest W: 3.16422700881958
grad AddEdge W: 1.2991178404152557e-17
grad ChooseDest W: 5.423043727874756
grad AddEdge W: 1.478080930201002e-17
grad ChooseDest W: 5.209326267242432
grad AddEdge W: 5.447691159495114e-18
grad ChooseDest W: 2.659717321395874
grad AddEdge W: 5.381906527626343e-16
grad ChooseDest W: 1.9365448951721191
grad AddEdge W: 5.713244640768544e-16
grad ChooseDest W: 4.476264476776123
grad AddEdge W: 3.5573872360265357e-16
grad ChooseDest W: 4.588621616363525
grad AddEdge W: 5.816469369676874e-16
grad ChooseDest W: 3.225665807723999
grad AddEdge W: 8.919910208748352e-16
grad ChooseDest W: 2.5022971630096436
grad AddEdge W: 1.4798199947208336e-17
grad ChooseDest W: 2.2745583057403564
grad AddEdge W: 4.520049445003909e-16
grad ChooseDest W: 5.27108097076416
grad AddEdge W: 6.242100944130322e-18
grad ChooseDest W: 6.513239860534668
grad AddEdge W: 6.404490974035201e-18
grad ChooseDest W: 4.532474517822266
grad AddEdge W: 9.999902230439309e-18
grad ChooseDest W: 4.198277473449707
grad AddEdge W: 1.1567908843619482e-15
grad ChooseDest W: 2.8952558040618896
grad AddEdge W: 7.07471199740476e-18
grad ChooseDest W: 3.1903789043426514
grad AddEdge W: 8.503150344887882e-18
grad ChooseDest W: 4.120387077331543
grad AddEdge W: 3.4692185918710344e-14
grad ChooseDest W: 3.299867630004883
grad AddEdge W: 3.966157263723957e-12
grad ChooseDest W: 4.864526748657227
grad AddEdge W: 8.208607045189388e-18
grad ChooseDest W: 4.981786727905273
grad AddEdge W: 4.54577065923847e-16
grad ChooseDest W: 7.785045146942139
grad AddEdge W: 2.942506167279938e-16
grad ChooseDest W: 2.4569265842437744
grad AddEdge W: 7.807817358892132e-18
grad ChooseDest W: 2.3335704803466797
grad AddEdge W: 8.97900170869065e-18
grad ChooseDest W: 4.796496391296387
grad AddEdge W: 1.4084917009571287e-15
grad ChooseDest W: 2.5781712532043457
grad AddEdge W: 9.473632553940209e-18
grad ChooseDest W: 5.768604278564453
grad AddEdge W: 9.879927954394618e-18
grad ChooseDest W: 4.094686508178711
grad AddEdge W: 8.372119145595195e-18
grad ChooseDest W: 4.016880989074707
grad AddEdge W: 6.173347345976139e-18
grad ChooseDest W: 3.264223575592041
grad AddEdge W: 1.9816721267179806e-17
grad ChooseDest W: 3.1356399059295654
grad AddEdge W: 1.8461601469571196e-14
grad ChooseDest W: 2.56376314163208
grad AddEdge W: 1.6921160246640213e-15
grad ChooseDest W: 2.872558832168579
grad AddEdge W: 9.477375546212012e-18
grad ChooseDest W: 5.417656898498535
grad AddEdge W: 4.286889994335668e-18
grad ChooseDest W: 6.779962062835693
grad AddEdge W: 7.789083637076827e-16
grad ChooseDest W: 4.249943256378174
grad AddEdge W: 5.90790426022726e-18
grad ChooseDest W: 5.339944362640381
grad AddEdge W: 1.2500450197033023e-17
grad ChooseDest W: 3.901369333267212
grad AddEdge W: 8.30914174965847e-18
grad ChooseDest W: 5.35110330581665
grad AddEdge W: 3.4217250412590518e-18
grad ChooseDest W: 6.711476802825928
grad AddEdge W: 2.6428108227669934e-16
grad ChooseDest W: 4.849485874176025
grad AddEdge W: 5.759843099570556e-16
grad ChooseDest W: 2.8929011821746826
grad AddEdge W: 1.0876420791634226e-15
grad ChooseDest W: 2.849775791168213
grad AddEdge W: 6.369719611791155e-14
grad ChooseDest W: 2.7750473022460938
grad AddEdge W: 3.2503129110540263e-16
grad ChooseDest W: 6.600972652435303
grad AddEdge W: 8.509041525210485e-18
grad ChooseDest W: 6.013783931732178
grad AddEdge W: 4.395534791120526e-18
grad ChooseDest W: 3.777604341506958
grad AddEdge W: 5.614520254157259e-18
grad ChooseDest W: 4.538426399230957
grad AddEdge W: 4.990981234831578e-16
grad ChooseDest W: 2.29178786277771
grad AddEdge W: 4.341320993270719e-16
grad ChooseDest W: 3.215935230255127
grad AddEdge W: 1.028253743803949e-17
grad ChooseDest W: 5.613986492156982
grad AddEdge W: 1.3695978472258977e-17
grad ChooseDest W: 5.224581241607666
grad AddEdge W: 5.2596816916582424e-18
grad ChooseDest W: 2.772995710372925
grad AddEdge W: 1.7595650332607546e-17
grad ChooseDest W: 5.267346382141113
grad AddEdge W: 9.29273394423915e-18
grad ChooseDest W: 3.4276747703552246
grad AddEdge W: 1.6151580940759747e-12
grad ChooseDest W: 1.1682984828948975
grad AddEdge W: 3.8966412426377425e-14
grad ChooseDest W: 3.9012868404388428
grad AddEdge W: 1.499818409518283e-17
grad ChooseDest W: 9.6687593460083
grad AddEdge W: 8.322786921043145e-18
grad ChooseDest W: 2.9838454723358154
=== Epoch 50: Train Loss: 4.1963, Train Log Prob: 0.0362 ===
Total mismatches: 57216
Predicted valid destination but wrong order: 8601
Epoch 50: Validation Loss: 3.1204, Validation Log Prob: 0.0594
Epoch 50: Edge Precision: 0.3562, Recall: 0.3469, F1: 0.3511, Jaccard: 0.2300
Epoch 50: TP: 2.4304939155332854, FP: 4.406871868289191, FN: 4.590980672870437
Epoch 50: Current Learning Rate: 6e-05
üíæ Checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/checkpoint_epoch_50.pth
[Epoch 50] ‚è±Ô∏è Total: 1956.38s | Current time: 2025-07-16 20:49:45 | üèãÔ∏è Train: 1666.54s | ‚úÖ Val: 289.83s
Training finished at: 2025-07-16 20:49:45
Training time: 206316.32643032074
‚úÖ Model saved to: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/model.pth
üìà Metrics saved to: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM
Device for model: cuda:2

Epoch-wise Validation Metrics:

Epoch 1:
  Validation Loss: 6.5135, Validation Log Prob: 0.0026
  Edge Precision: 0.3572, Recall: 0.3531, F1: 0.3550, Jaccard: 0.2329
  TP: 2.4734430923407302, FP: 4.461417322834646, FN: 4.548031496062992

Epoch 2:
  Validation Loss: 6.1143, Validation Log Prob: 0.0039
  Edge Precision: 0.3622, Recall: 0.3606, F1: 0.3613, Jaccard: 0.2380
  TP: 2.525984251968504, FP: 4.46284896206156, FN: 4.495490336435219

Epoch 3:
  Validation Loss: 5.7479, Validation Log Prob: 0.0058
  Edge Precision: 0.3679, Recall: 0.3673, F1: 0.3676, Jaccard: 0.2430
  TP: 2.574087329992842, FP: 4.4386542591267, FN: 4.44738725841088

Epoch 4:
  Validation Loss: 5.5131, Validation Log Prob: 0.0072
  Edge Precision: 0.3684, Recall: 0.3679, F1: 0.3681, Jaccard: 0.2432
  TP: 2.576521116678597, FP: 4.437079455977094, FN: 4.444953471725126

Epoch 5:
  Validation Loss: 5.5107, Validation Log Prob: 0.0073
  Edge Precision: 0.3661, Recall: 0.3656, F1: 0.3658, Jaccard: 0.2419
  TP: 2.5606299212598427, FP: 4.452541159627774, FN: 4.4608446671438795

Epoch 6:
  Validation Loss: 5.5316, Validation Log Prob: 0.0072
  Edge Precision: 0.3670, Recall: 0.3664, F1: 0.3667, Jaccard: 0.2423
  TP: 2.5662133142448105, FP: 4.446098783106657, FN: 4.455261274158912

Epoch 7:
  Validation Loss: 5.4096, Validation Log Prob: 0.0080
  Edge Precision: 0.3676, Recall: 0.3670, F1: 0.3673, Jaccard: 0.2427
  TP: 2.569935576234789, FP: 4.442233357193987, FN: 4.451539012168934

Epoch 8:
  Validation Loss: 5.4319, Validation Log Prob: 0.0080
  Edge Precision: 0.3661, Recall: 0.3655, F1: 0.3658, Jaccard: 0.2410
  TP: 2.559914101646385, FP: 4.450536864710093, FN: 4.461560486757337

Epoch 9:
  Validation Loss: 5.3694, Validation Log Prob: 0.0083
  Edge Precision: 0.3671, Recall: 0.3663, F1: 0.3667, Jaccard: 0.2418
  TP: 2.565926986399427, FP: 4.441947029348604, FN: 4.455547602004295

Epoch 10:
  Validation Loss: 5.3244, Validation Log Prob: 0.0085
  Edge Precision: 0.3640, Recall: 0.3632, F1: 0.3636, Jaccard: 0.2398
  TP: 2.54201861130995, FP: 4.4642806012884755, FN: 4.479455977093773

Epoch 11:
  Validation Loss: 5.3092, Validation Log Prob: 0.0086
  Edge Precision: 0.3656, Recall: 0.3648, F1: 0.3652, Jaccard: 0.2408
  TP: 2.554187544738726, FP: 4.452541159627774, FN: 4.467287043664997

Epoch 12:
  Validation Loss: 5.1799, Validation Log Prob: 0.0095
  Edge Precision: 0.3664, Recall: 0.3654, F1: 0.3659, Jaccard: 0.2412
  TP: 2.558339298496779, FP: 4.445955619183966, FN: 4.463135289906943

Epoch 13:
  Validation Loss: 5.1316, Validation Log Prob: 0.0098
  Edge Precision: 0.3610, Recall: 0.3597, F1: 0.3603, Jaccard: 0.2371
  TP: 2.5183965640658554, FP: 4.4781675017895495, FN: 4.503078024337867

Epoch 14:
  Validation Loss: 5.0942, Validation Log Prob: 0.0103
  Edge Precision: 0.3639, Recall: 0.3624, F1: 0.3631, Jaccard: 0.2399
  TP: 2.537580529706514, FP: 4.453972798854688, FN: 4.483894058697208

Epoch 15:
  Validation Loss: 4.9851, Validation Log Prob: 0.0113
  Edge Precision: 0.3650, Recall: 0.3635, F1: 0.3642, Jaccard: 0.2405
  TP: 2.5458840372226197, FP: 4.447100930565497, FN: 4.475590551181102

Epoch 16:
  Validation Loss: 4.9158, Validation Log Prob: 0.0119
  Edge Precision: 0.3664, Recall: 0.3648, F1: 0.3656, Jaccard: 0.2416
  TP: 2.5549033643521835, FP: 4.4346456692913385, FN: 4.466571224051539

Epoch 17:
  Validation Loss: 4.8778, Validation Log Prob: 0.0125
  Edge Precision: 0.3628, Recall: 0.3611, F1: 0.3619, Jaccard: 0.2389
  TP: 2.5285612025769506, FP: 4.458267716535433, FN: 4.492913385826772

Epoch 18:
  Validation Loss: 4.7779, Validation Log Prob: 0.0136
  Edge Precision: 0.3646, Recall: 0.3622, F1: 0.3633, Jaccard: 0.2398
  TP: 2.536721546170365, FP: 4.44151753758053, FN: 4.484753042233357

Epoch 19:
  Validation Loss: 4.6332, Validation Log Prob: 0.0153
  Edge Precision: 0.3631, Recall: 0.3605, F1: 0.3617, Jaccard: 0.2382
  TP: 2.5258410880458126, FP: 4.444953471725126, FN: 4.4956335003579095

Epoch 20:
  Validation Loss: 4.5774, Validation Log Prob: 0.0162
  Edge Precision: 0.3636, Recall: 0.3614, F1: 0.3624, Jaccard: 0.2387
  TP: 2.5305654974946314, FP: 4.44824624194703, FN: 4.490909090909091

Epoch 21:
  Validation Loss: 4.4907, Validation Log Prob: 0.0174
  Edge Precision: 0.3605, Recall: 0.3576, F1: 0.3590, Jaccard: 0.2358
  TP: 2.5056549749463137, FP: 4.458410880458125, FN: 4.515819613457409

Epoch 22:
  Validation Loss: 4.4343, Validation Log Prob: 0.0184
  Edge Precision: 0.3599, Recall: 0.3559, F1: 0.3577, Jaccard: 0.2356
  TP: 2.4937723693629206, FP: 4.448818897637795, FN: 4.527702219040802

Epoch 23:
  Validation Loss: 4.3481, Validation Log Prob: 0.0200
  Edge Precision: 0.3601, Recall: 0.3565, F1: 0.3582, Jaccard: 0.2359
  TP: 2.49663564781675, FP: 4.455833929849678, FN: 4.524838940586972

Epoch 24:
  Validation Loss: 4.2345, Validation Log Prob: 0.0220
  Edge Precision: 0.3612, Recall: 0.3573, F1: 0.3591, Jaccard: 0.2361
  TP: 2.5035075161059415, FP: 4.442806012884753, FN: 4.517967072297781

Epoch 25:
  Validation Loss: 4.1874, Validation Log Prob: 0.0229
  Edge Precision: 0.3618, Recall: 0.3573, F1: 0.3594, Jaccard: 0.2367
  TP: 2.5035075161059415, FP: 4.432498210450967, FN: 4.517967072297781

Epoch 26:
  Validation Loss: 4.0899, Validation Log Prob: 0.0251
  Edge Precision: 0.3576, Recall: 0.3523, F1: 0.3547, Jaccard: 0.2329
  TP: 2.4670007158196134, FP: 4.448389405869721, FN: 4.554473872584109

Epoch 27:
  Validation Loss: 3.9991, Validation Log Prob: 0.0270
  Edge Precision: 0.3621, Recall: 0.3568, F1: 0.3592, Jaccard: 0.2363
  TP: 2.499212598425197, FP: 4.416607015032212, FN: 4.522261989978525

Epoch 28:
  Validation Loss: 3.9600, Validation Log Prob: 0.0283
  Edge Precision: 0.3619, Recall: 0.3567, F1: 0.3591, Jaccard: 0.2361
  TP: 2.4982104509663565, FP: 4.423478883321403, FN: 4.523264137437366

Epoch 29:
  Validation Loss: 3.8926, Validation Log Prob: 0.0299
  Edge Precision: 0.3609, Recall: 0.3555, F1: 0.3580, Jaccard: 0.2356
  TP: 2.490050107372942, FP: 4.4259126700071585, FN: 4.53142448103078

Epoch 30:
  Validation Loss: 3.8567, Validation Log Prob: 0.0308
  Edge Precision: 0.3625, Recall: 0.3563, F1: 0.3591, Jaccard: 0.2366
  TP: 2.495919828203293, FP: 4.40200429491768, FN: 4.525554760200429

Epoch 31:
  Validation Loss: 3.7822, Validation Log Prob: 0.0330
  Edge Precision: 0.3598, Recall: 0.3531, F1: 0.3562, Jaccard: 0.2336
  TP: 2.472870436649964, FP: 4.417752326413744, FN: 4.548604151753758

Epoch 32:
  Validation Loss: 3.7417, Validation Log Prob: 0.0344
  Edge Precision: 0.3593, Recall: 0.3528, F1: 0.3558, Jaccard: 0.2336
  TP: 2.471295633500358, FP: 4.423908375089478, FN: 4.550178954903364

Epoch 33:
  Validation Loss: 3.7005, Validation Log Prob: 0.0354
  Edge Precision: 0.3610, Recall: 0.3544, F1: 0.3575, Jaccard: 0.2350
  TP: 2.4833214030064426, FP: 4.407587687902648, FN: 4.53815318539728

Epoch 34:
  Validation Loss: 3.5974, Validation Log Prob: 0.0386
  Edge Precision: 0.3598, Recall: 0.3528, F1: 0.3560, Jaccard: 0.2338
  TP: 2.4705798138869004, FP: 4.413314244810308, FN: 4.550894774516822

Epoch 35:
  Validation Loss: 3.5602, Validation Log Prob: 0.0402
  Edge Precision: 0.3597, Recall: 0.3526, F1: 0.3559, Jaccard: 0.2336
  TP: 2.468861846814603, FP: 4.418468146027201, FN: 4.55261274158912

Epoch 36:
  Validation Loss: 3.5607, Validation Log Prob: 0.0402
  Edge Precision: 0.3568, Recall: 0.3493, F1: 0.3527, Jaccard: 0.2309
  TP: 2.4459556191839655, FP: 4.426485325697924, FN: 4.575518969219757

Epoch 37:
  Validation Loss: 3.4852, Validation Log Prob: 0.0430
  Edge Precision: 0.3590, Recall: 0.3518, F1: 0.3551, Jaccard: 0.2330
  TP: 2.4642806012884755, FP: 4.414889047959914, FN: 4.557193987115247

Epoch 38:
  Validation Loss: 3.4718, Validation Log Prob: 0.0437
  Edge Precision: 0.3576, Recall: 0.3497, F1: 0.3533, Jaccard: 0.2317
  TP: 2.449821045096636, FP: 4.416034359341446, FN: 4.571653543307087

Epoch 39:
  Validation Loss: 3.4363, Validation Log Prob: 0.0449
  Edge Precision: 0.3573, Recall: 0.3493, F1: 0.3530, Jaccard: 0.2317
  TP: 2.445812455261274, FP: 4.416893342877595, FN: 4.5756621331424485

Epoch 40:
  Validation Loss: 3.4000, Validation Log Prob: 0.0466
  Edge Precision: 0.3581, Recall: 0.3503, F1: 0.3539, Jaccard: 0.2321
  TP: 2.4541159627773803, FP: 4.413027916964925, FN: 4.567358625626342

Epoch 41:
  Validation Loss: 3.3477, Validation Log Prob: 0.0488
  Edge Precision: 0.3560, Recall: 0.3483, F1: 0.3519, Jaccard: 0.2307
  TP: 2.439799570508232, FP: 4.426198997852541, FN: 4.58167501789549

Epoch 42:
  Validation Loss: 3.3211, Validation Log Prob: 0.0497
  Edge Precision: 0.3585, Recall: 0.3498, F1: 0.3538, Jaccard: 0.2320
  TP: 2.45025053686471, FP: 4.398425196850393, FN: 4.571224051539012

Epoch 43:
  Validation Loss: 3.2934, Validation Log Prob: 0.0509
  Edge Precision: 0.3549, Recall: 0.3462, F1: 0.3502, Jaccard: 0.2293
  TP: 2.4260558339298495, FP: 4.4224767358625625, FN: 4.595418754473872

Epoch 44:
  Validation Loss: 3.2599, Validation Log Prob: 0.0525
  Edge Precision: 0.3577, Recall: 0.3491, F1: 0.3531, Jaccard: 0.2317
  TP: 2.446098783106657, FP: 4.405869720830351, FN: 4.575375805297065

Epoch 45:
  Validation Loss: 3.2555, Validation Log Prob: 0.0526
  Edge Precision: 0.3575, Recall: 0.3488, F1: 0.3528, Jaccard: 0.2314
  TP: 2.4438081603435933, FP: 4.4084466714387975, FN: 4.5776664280601285

Epoch 46:
  Validation Loss: 3.2246, Validation Log Prob: 0.0541
  Edge Precision: 0.3566, Recall: 0.3481, F1: 0.3520, Jaccard: 0.2306
  TP: 2.4387974230493916, FP: 4.413457408733, FN: 4.582677165354331

Epoch 47:
  Validation Loss: 3.1944, Validation Log Prob: 0.0553
  Edge Precision: 0.3574, Recall: 0.3478, F1: 0.3522, Jaccard: 0.2309
  TP: 2.4363636363636365, FP: 4.399570508231926, FN: 4.585110952040086

Epoch 48:
  Validation Loss: 3.1555, Validation Log Prob: 0.0579
  Edge Precision: 0.3572, Recall: 0.3479, F1: 0.3522, Jaccard: 0.2312
  TP: 2.4357909806728704, FP: 4.400286327845383, FN: 4.585683607730852

Epoch 49:
  Validation Loss: 3.1359, Validation Log Prob: 0.0586
  Edge Precision: 0.3549, Recall: 0.3459, F1: 0.3500, Jaccard: 0.2286
  TP: 2.4230493915533287, FP: 4.421617752326414, FN: 4.5984251968503935/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4434: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4449: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))


Epoch 50:
  Validation Loss: 3.1204, Validation Log Prob: 0.0594
  Edge Precision: 0.3562, Recall: 0.3469, F1: 0.3511, Jaccard: 0.2300
  TP: 2.4304939155332854, FP: 4.406871868289191, FN: 4.590980672870437
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 70.00%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 30.00%
  ‚ùå False Discovery rate (FP/TP+FP): 23.92%
  üéØ Precision (TP/TP+FP): 76.08%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 21.29%
  ‚ö†Ô∏è Std. False Negative rate: 21.29%
  ‚ùå Std. False Discovery rate: 19.10%
  üéØ Std. Precision: 19.10%
üìâ  Average detailed edge-metrics
  F1: 0.73
  Jaccard: 0.61
  TP: 4.05
  FP: 1.20
  FN: 1.70

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 54.26%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 45.74%
  ‚ùå False Discovery rate (FP/TP+FP): 43.37%
  üéØ Precision (TP/TP+FP): 56.63%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 22.90%
  ‚ö†Ô∏è Std. False Negative rate: 22.90%
  ‚ùå Std. False Discovery rate: 23.18%
  üéØ Std. Precision: 23.18%
üìâ  Average detailed edge-metrics
  F1: 0.55
  Jaccard: 0.42
  TP: 3.31
  FP: 2.46
  FN: 2.73

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 44.84%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 55.16%
  ‚ùå False Discovery rate (FP/TP+FP): 53.31%
  üéØ Precision (TP/TP+FP): 46.69%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.89%
  ‚ö†Ô∏è Std. False Negative rate: 17.89%
  ‚ùå Std. False Discovery rate: 18.43%
  üéØ Std. Precision: 18.43%
üìâ  Average detailed edge-metrics
  F1: 0.46
  Jaccard: 0.32
  TP: 2.94
  FP: 3.34
  FN: 3.59

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 38.00%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 62.00%
  ‚ùå False Discovery rate (FP/TP+FP): 61.19%
  üéØ Precision (TP/TP+FP): 38.81%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.88%
  ‚ö†Ô∏è Std. False Negative rate: 17.88%
  ‚ùå Std. False Discovery rate: 18.20%
  üéØ Std. Precision: 18.20%
üìâ  Average detailed edge-metrics
  F1: 0.38
  Jaccard: 0.25
  TP: 2.67
  FP: 4.20
  FN: 4.35

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 31.44%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 68.56%
  ‚ùå False Discovery rate (FP/TP+FP): 68.11%
  üéØ Precision (TP/TP+FP): 31.89%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.48%
  ‚ö†Ô∏è Std. False Negative rate: 17.48%
  ‚ùå Std. False Discovery rate: 17.66%
  üéØ Std. Precision: 17.66%
üìâ  Average detailed edge-metrics
  F1: 0.32
  Jaccard: 0.20
  TP: 2.24
  FP: 4.77
  FN: 4.88

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 35.13%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 64.87%
  ‚ùå False Discovery rate (FP/TP+FP): 64.12%
  üéØ Precision (TP/TP+FP): 35.88%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.29%
  ‚ö†Ô∏è Std. False Negative rate: 18.29%
  ‚ùå Std. False Discovery rate: 18.66%
  üéØ Std. Precision: 18.66%
üìâ  Average detailed edge-metrics
  F1: 0.35
  Jaccard: 0.23
  TP: 2.46
  FP: 4.42
  FN: 4.56
[6, 7, 8, 9, 10, 999]
[0.7, 0.5425770308123249, 0.4484204413472706, 0.3800009282465423, 0.3143598307315693, 0.3513355148788219]
[np.float64(0.19099701859220503), np.float64(0.2317766885418199), np.float64(0.18426199169538882), np.float64(0.18201786818113974), np.float64(0.17664999199794404), np.float64(0.18658499975137716)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.FIXED

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 67.83%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 32.17%
  ‚ùå False Discovery rate (FP/TP+FP): 25.58%
  üéØ Precision (TP/TP+FP): 74.42%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 21.14%
  ‚ö†Ô∏è Std. False Negative rate: 21.14%
  ‚ùå Std. False Discovery rate: 19.69%
  üéØ Std. Precision: 19.69%
üìâ  Average detailed edge-metrics
  F1: 0.71
  Jaccard: 0.59
  TP: 3.90
  FP: 1.30
  FN: 1.85

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 58.13%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 41.87%
  ‚ùå False Discovery rate (FP/TP+FP): 40.58%
  üéØ Precision (TP/TP+FP): 59.42%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 22.82%
  ‚ö†Ô∏è Std. False Negative rate: 22.82%
  ‚ùå Std. False Discovery rate: 22.36%
  üéØ Std. Precision: 22.36%
üìâ  Average detailed edge-metrics
  F1: 0.59
  Jaccard: 0.46
  TP: 3.55
  FP: 2.34
  FN: 2.49

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 46.34%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 53.66%
  ‚ùå False Discovery rate (FP/TP+FP): 52.92%
  üéØ Precision (TP/TP+FP): 47.08%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.22%
  ‚ö†Ô∏è Std. False Negative rate: 18.22%
  ‚ùå Std. False Discovery rate: 18.46%
  üéØ Std. Precision: 18.46%
üìâ  Average detailed edge-metrics
  F1: 0.47
  Jaccard: 0.32
  TP: 3.04
  FP: 3.39
  FN: 3.50

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 38.79%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 61.21%
  ‚ùå False Discovery rate (FP/TP+FP): 60.92%
  üéØ Precision (TP/TP+FP): 39.08%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.78%
  ‚ö†Ô∏è Std. False Negative rate: 17.78%
  ‚ùå Std. False Discovery rate: 17.85%
  üéØ Std. Precision: 17.85%
üìâ  Average detailed edge-metrics
  F1: 0.39
  Jaccard: 0.26
  TP: 2.72
  FP: 4.24
  FN: 4.29

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 32.91%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 67.09%
  ‚ùå False Discovery rate (FP/TP+FP): 66.91%
  üéØ Precision (TP/TP+FP): 33.09%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.33%
  ‚ö†Ô∏è Std. False Negative rate: 17.33%
  ‚ùå Std. False Discovery rate: 17.39%
  üéØ Std. Precision: 17.39%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.34
  FP: 4.73
  FN: 4.77

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.39%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.61%
  ‚ùå False Discovery rate (FP/TP+FP): 63.32%
  üéØ Precision (TP/TP+FP): 36.68%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.38%
  ‚ö†Ô∏è Std. False Negative rate: 18.38%
  ‚ùå Std. False Discovery rate: 18.51%
  üéØ Std. Precision: 18.51%
üìâ  Average detailed edge-metrics
  F1: 0.37
  Jaccard: 0.24
  TP: 2.55
  FP: 4.42
  FN: 4.47
[6, 7, 8, 9, 10, 999]
[0.6783333333333333, 0.581312525010004, 0.4633681765389082, 0.3879374361830502, 0.3290877833954558, 0.36387718580631967]
[np.float64(0.19694013472795907), np.float64(0.22357814059073486), np.float64(0.1845827218716215), np.float64(0.17850504657269006), np.float64(0.17386244909184856), np.float64(0.1851091580379323)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_ASC

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 70.83%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 29.17%
  ‚ùå False Discovery rate (FP/TP+FP): 21.17%
  üéØ Precision (TP/TP+FP): 78.83%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 21.47%
  ‚ö†Ô∏è Std. False Negative rate: 21.47%
  ‚ùå Std. False Discovery rate: 17.89%
  üéØ Std. Precision: 17.89%
üìâ  Average detailed edge-metrics
  F1: 0.74
  Jaccard: 0.63
  TP: 4.10
  FP: 1.00
  FN: 1.65

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4477: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4491: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))

  ‚úÖ Recall (TP/TP+FN): 54.59%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 45.41%
  ‚ùå False Discovery rate (FP/TP+FP): 41.79%
  üéØ Precision (TP/TP+FP): 58.21%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.80%
  ‚ö†Ô∏è Std. False Negative rate: 19.80%
  ‚ùå Std. False Discovery rate: 19.75%
  üéØ Std. Precision: 19.75%
üìâ  Average detailed edge-metrics
  F1: 0.56
  Jaccard: 0.42
  TP: 3.33
  FP: 2.32
  FN: 2.71

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 44.13%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 55.87%
  ‚ùå False Discovery rate (FP/TP+FP): 53.36%
  üéØ Precision (TP/TP+FP): 46.64%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.20%
  ‚ö†Ô∏è Std. False Negative rate: 19.20%
  ‚ùå Std. False Discovery rate: 19.75%
  üéØ Std. Precision: 19.75%
üìâ  Average detailed edge-metrics
  F1: 0.45
  Jaccard: 0.31
  TP: 2.90
  FP: 3.27
  FN: 3.63

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 37.52%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 62.48%
  ‚ùå False Discovery rate (FP/TP+FP): 61.13%
  üéØ Precision (TP/TP+FP): 38.87%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.26%
  ‚ö†Ô∏è Std. False Negative rate: 18.26%
  ‚ùå Std. False Discovery rate: 18.66%
  üéØ Std. Precision: 18.66%
üìâ  Average detailed edge-metrics
  F1: 0.38
  Jaccard: 0.25
  TP: 2.64
  FP: 4.13
  FN: 4.38

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 31.33%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 68.67%
  ‚ùå False Discovery rate (FP/TP+FP): 67.90%
  üéØ Precision (TP/TP+FP): 32.10%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.43%
  ‚ö†Ô∏è Std. False Negative rate: 17.43%
  ‚ùå Std. False Discovery rate: 17.71%
  üéØ Std. Precision: 17.71%
üìâ  Average detailed edge-metrics
  F1: 0.32
  Jaccard: 0.20
  TP: 2.23
  FP: 4.70
  FN: 4.88

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 34.86%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 65.14%
  ‚ùå False Discovery rate (FP/TP+FP): 63.98%
  üéØ Precision (TP/TP+FP): 36.02%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.58%
  ‚ö†Ô∏è Std. False Negative rate: 18.58%
  ‚ùå Std. False Discovery rate: 19.08%
  üéØ Std. Precision: 19.08%
üìâ  Average detailed edge-metrics
  F1: 0.35
  Jaccard: 0.23
  TP: 2.44
  FP: 4.36
  FN: 4.58
[6, 7, 8, 9, 10, 999]
[0.7083333333333333, 0.5459383753501401, 0.44130081300813007, 0.3751995730065905, 0.31331727989568336, 0.3486477826635307]
[np.float64(0.17889320215642007), np.float64(0.19748601555795428), np.float64(0.1975140024529259), np.float64(0.18663204606650005), np.float64(0.17706058255527105), np.float64(0.1907503739876321)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 83.67%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 16.33%
  ‚ùå False Discovery rate (FP/TP+FP): 13.75%
  üéØ Precision (TP/TP+FP): 86.25%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 23.12%
  ‚ö†Ô∏è Std. False Negative rate: 23.12%
  ‚ùå Std. False Discovery rate: 21.15%
  üéØ Std. Precision: 21.15%
üìâ  Average detailed edge-metrics
  F1: 0.85
  Jaccard: 0.79
  TP: 4.85
  FP: 0.70
  FN: 0.90

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 55.59%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 44.41%
  ‚ùå False Discovery rate (FP/TP+FP): 43.49%
  üéØ Precision (TP/TP+FP): 56.51%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 23.71%
  ‚ö†Ô∏è Std. False Negative rate: 23.71%
  ‚ùå Std. False Discovery rate: 23.40%
  üéØ Std. Precision: 23.40%
üìâ  Average detailed edge-metrics
  F1: 0.56
  Jaccard: 0.43
  TP: 3.40
  FP: 2.53
  FN: 2.64

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 45.43%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 54.57%
  ‚ùå False Discovery rate (FP/TP+FP): 54.00%
  üéØ Precision (TP/TP+FP): 46.00%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.72%
  ‚ö†Ô∏è Std. False Negative rate: 17.72%
  ‚ùå Std. False Discovery rate: 17.95%
  üéØ Std. Precision: 17.95%
üìâ  Average detailed edge-metrics
  F1: 0.46
  Jaccard: 0.31
  TP: 2.99
  FP: 3.48
  FN: 3.55

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 39.48%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 60.52%
  ‚ùå False Discovery rate (FP/TP+FP): 60.29%
  üéØ Precision (TP/TP+FP): 39.71%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.67%
  ‚ö†Ô∏è Std. False Negative rate: 17.67%
  ‚ùå Std. False Discovery rate: 17.78%
  üéØ Std. Precision: 17.78%
üìâ  Average detailed edge-metrics
  F1: 0.40
  Jaccard: 0.26
  TP: 2.77
  FP: 4.20
  FN: 4.24

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 32.58%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 67.42%
  ‚ùå False Discovery rate (FP/TP+FP): 67.33%
  üéØ Precision (TP/TP+FP): 32.67%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.03%
  ‚ö†Ô∏è Std. False Negative rate: 17.03%
  ‚ùå Std. False Discovery rate: 17.08%
  üéØ Std. Precision: 17.08%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.32
  FP: 4.77
  FN: 4.79

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.41%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.59%
  ‚ùå False Discovery rate (FP/TP+FP): 63.41%
  üéØ Precision (TP/TP+FP): 36.59%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.28%
  ‚ö†Ô∏è Std. False Negative rate: 18.28%
  ‚ùå Std. False Discovery rate: 18.40%
  üéØ Std. Precision: 18.40%
üìâ  Average detailed edge-metrics
  F1: 0.36
  Jaccard: 0.24
  TP: 2.55
  FP: 4.44
  FN: 4.47
[6, 7, 8, 9, 10, 999]
[0.8366666666666667, 0.5558623449379752, 0.45433217189314745, 0.39481806367771277, 0.32579867389993966, 0.3640704571019532]
[np.float64(0.21146808269807527), np.float64(0.23402355449082762), np.float64(0.1794894371615904), np.float64(0.1777521902407695), np.float64(0.1708313058700024), np.float64(0.1839562607284647)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 69.00%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 31.00%
  ‚ùå False Discovery rate (FP/TP+FP): 25.75%
  üéØ Precision (TP/TP+FP): 74.25%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 22.81%
  ‚ö†Ô∏è Std. False Negative rate: 22.81%
  ‚ùå Std. False Discovery rate: 20.73%
  üéØ Std. Precision: 20.73%
üìâ  Average detailed edge-metrics
  F1: 0.71
  Jaccard: 0.60
  TP: 4.00
  FP: 1.30
  FN: 1.75

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 52.42%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 47.58%
  ‚ùå False Discovery rate (FP/TP+FP): 44.87%
  üéØ Precision (TP/TP+FP): 55.13%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 20.74%
  ‚ö†Ô∏è Std. False Negative rate: 20.74%
  ‚ùå Std. False Discovery rate: 21.48%
  üéØ Std. Precision: 21.48%
üìâ  Average detailed edge-metrics
  F1: 0.54
  Jaccard: 0.40
  TP: 3.19
  FP: 2.56
  FN: 2.85

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 43.98%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 56.02%
  ‚ùå False Discovery rate (FP/TP+FP): 54.34%
  üéØ Precision (TP/TP+FP): 45.66%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.59%
  ‚ö†Ô∏è Std. False Negative rate: 19.59%
  ‚ùå Std. False Discovery rate: 19.90%
  üéØ Std. Precision: 19.90%/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4505: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4525: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5344: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5361: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5377: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5409: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5663: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))

üìâ  Average detailed edge-metrics
  F1: 0.45
  Jaccard: 0.31
  TP: 2.89
  FP: 3.40
  FN: 3.64

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 37.34%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 62.66%
  ‚ùå False Discovery rate (FP/TP+FP): 61.59%
  üéØ Precision (TP/TP+FP): 38.41%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.09%
  ‚ö†Ô∏è Std. False Negative rate: 18.09%
  ‚ùå Std. False Discovery rate: 18.37%
  üéØ Std. Precision: 18.37%
üìâ  Average detailed edge-metrics
  F1: 0.38
  Jaccard: 0.25
  TP: 2.62
  FP: 4.19
  FN: 4.39

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 31.22%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 68.78%
  ‚ùå False Discovery rate (FP/TP+FP): 68.09%
  üéØ Precision (TP/TP+FP): 31.91%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.38%
  ‚ö†Ô∏è Std. False Negative rate: 17.38%
  ‚ùå Std. False Discovery rate: 17.67%
  üéØ Std. Precision: 17.67%
üìâ  Average detailed edge-metrics
  F1: 0.32
  Jaccard: 0.20
  TP: 2.22
  FP: 4.73
  FN: 4.89

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 34.69%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 65.31%
  ‚ùå False Discovery rate (FP/TP+FP): 64.38%
  üéØ Precision (TP/TP+FP): 35.62%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.49%
  ‚ö†Ô∏è Std. False Negative rate: 18.49%
  ‚ùå Std. False Discovery rate: 18.88%
  üéØ Std. Precision: 18.88%
üìâ  Average detailed edge-metrics
  F1: 0.35
  Jaccard: 0.23
  TP: 2.43
  FP: 4.41
  FN: 4.59
[6, 7, 8, 9, 10, 999]
[0.69, 0.5241696678671468, 0.43981416957026714, 0.37342894272718835, 0.312184005609477, 0.3468589153628524]
[np.float64(0.20732924701225022), np.float64(0.2147959386353386), np.float64(0.1990187680816493), np.float64(0.1837304041197937), np.float64(0.17665555148105555), np.float64(0.1887908758520257)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 78.83%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 21.17%
  ‚ùå False Discovery rate (FP/TP+FP): 16.67%
  üéØ Precision (TP/TP+FP): 83.33%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 22.12%
  ‚ö†Ô∏è Std. False Negative rate: 22.12%
  ‚ùå Std. False Discovery rate: 19.11%
  üéØ Std. Precision: 19.11%
üìâ  Average detailed edge-metrics
  F1: 0.81
  Jaccard: 0.73
  TP: 4.55
  FP: 0.85
  FN: 1.20

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 58.30%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 41.70%
  ‚ùå False Discovery rate (FP/TP+FP): 40.78%
  üéØ Precision (TP/TP+FP): 59.22%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 23.70%
  ‚ö†Ô∏è Std. False Negative rate: 23.70%
  ‚ùå Std. False Discovery rate: 23.56%
  üéØ Std. Precision: 23.56%
üìâ  Average detailed edge-metrics
  F1: 0.59
  Jaccard: 0.46
  TP: 3.56
  FP: 2.38
  FN: 2.48

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 45.70%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 54.30%
  ‚ùå False Discovery rate (FP/TP+FP): 53.57%
  üéØ Precision (TP/TP+FP): 46.43%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.00%
  ‚ö†Ô∏è Std. False Negative rate: 18.00%
  ‚ùå Std. False Discovery rate: 18.38%
  üéØ Std. Precision: 18.38%
üìâ  Average detailed edge-metrics
  F1: 0.46
  Jaccard: 0.32
  TP: 3.00
  FP: 3.44
  FN: 3.53

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 39.36%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 60.64%
  ‚ùå False Discovery rate (FP/TP+FP): 60.30%
  üéØ Precision (TP/TP+FP): 39.70%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.11%
  ‚ö†Ô∏è Std. False Negative rate: 18.11%
  ‚ùå Std. False Discovery rate: 18.19%
  üéØ Std. Precision: 18.19%
üìâ  Average detailed edge-metrics
  F1: 0.40
  Jaccard: 0.26
  TP: 2.76
  FP: 4.19
  FN: 4.25

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 32.68%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 67.32%
  ‚ùå False Discovery rate (FP/TP+FP): 67.15%
  üéØ Precision (TP/TP+FP): 32.85%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.22%
  ‚ö†Ô∏è Std. False Negative rate: 17.22%
  ‚ùå Std. False Discovery rate: 17.29%
  üéØ Std. Precision: 17.29%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.33
  FP: 4.75
  FN: 4.79

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.47%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.53%
  ‚ùå False Discovery rate (FP/TP+FP): 63.24%
  üéØ Precision (TP/TP+FP): 36.76%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.55%
  ‚ö†Ô∏è Std. False Negative rate: 18.55%
  ‚ùå Std. False Discovery rate: 18.69%
  üéØ Std. Precision: 18.69%
üìâ  Average detailed edge-metrics
  F1: 0.37
  Jaccard: 0.24
  TP: 2.55
  FP: 4.42
  FN: 4.47
[6, 7, 8, 9, 10, 999]
[0.7883333333333333, 0.5830332132853141, 0.4570499419279907, 0.393632228719948, 0.3267827926830768, 0.36466441694788154]
[np.float64(0.1911224156863286), np.float64(0.23563169768517073), np.float64(0.1838454602874036), np.float64(0.18187524982674363), np.float64(0.17290736003013044), np.float64(0.18693183382205475)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM
Device for model: cuda:2
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/random
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.FIXED
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/fixed
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_ASC
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/deg_asc
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/deg_desc
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/neigh_min_rem
56682
5916
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_min_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/neigh_max_rem
56342
6848
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM
Device for model: cuda:2
‚úÖ Code finished successfully at: 2025-07-16 22:04:54
