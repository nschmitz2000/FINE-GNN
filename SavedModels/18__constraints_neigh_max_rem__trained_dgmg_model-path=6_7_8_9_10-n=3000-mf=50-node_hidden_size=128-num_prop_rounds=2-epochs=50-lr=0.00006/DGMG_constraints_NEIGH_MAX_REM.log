nohup: ignoring input
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:2859: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  metadata = torch.load(load_path_metadata)
/home/nschmitz/.pyenv/versions/3.12.9/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
‚úÖ Using device: cuda:3
/home/nschmitz/GNNs/kirigami-database/data/cleaned
Model without constraints:  False
üíø Folder that model data gets saved into:  18__constraints_neigh_max_rem__
Metrics---
Amount unfoldings:  3000
Max files per dir:  50
Batch size:  1
Num prop rounds:  2
Learning rate:  6e-05
Num epochs:  50
Accum steps:  4
Training node ordering strategy:  NodeOrder.NEIGH_MAX_REM
Add edge is deterministic:  True
Add edge is deterministic threshold:  0.5
Calc PR-Curve:  False
---------------
6_7_8_9_10
Found existing dataset files; skipping save.
Loading dataset from disk‚Ä¶
6_7_8_9_10
Max amount of features per node in a graph:  128
Loaded 46571 graphs with actions!
Train size: 32599, Validation size: 6985, Test size: 6987
Graph(num_nodes=9, num_edges=16,
      ndata_schemes={'a': Scheme(shape=(256,), dtype=torch.float32), 'hv': Scheme(shape=(128,), dtype=torch.float32), 'deg_rem': Scheme(shape=(1,), dtype=torch.int16), 'deg_target': Scheme(shape=(1,), dtype=torch.int16)}
      edata_schemes={'he': Scheme(shape=(1,), dtype=torch.float32)})
[[0. 1. 0. 1. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [1. 0. 1. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]]
--------
Graph(num_nodes=9, num_edges=30,
      ndata_schemes={'deg_target': Scheme(shape=(1,), dtype=torch.int16), 'hv': Scheme(shape=(128,), dtype=torch.float32)}
      edata_schemes={})
[[0. 1. 0. 1. 0. 1. 0. 0. 1.]
 [1. 0. 0. 0. 0. 1. 1. 0. 1.]
 [0. 0. 0. 1. 1. 1. 0. 0. 0.]
 [1. 0. 1. 0. 0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 0. 0. 1. 1. 0.]
 [1. 1. 1. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 1. 0. 0. 1. 0.]
 [0. 0. 0. 1. 1. 0. 1. 0. 0.]
 [1. 1. 0. 0. 0. 1. 0. 0. 0.]]
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM
Device for model: cuda:3
Dir_name to save to:  6_7_8_9_10
Training started at: 2025-07-14 11:31:27
grad AddEdge W: 11.536144256591797
grad ChooseDest W: 2.9995880126953125
grad AddEdge W: 0.44112226366996765
grad ChooseDest W: 2.3008384704589844
grad AddEdge W: 0.10368121415376663
grad ChooseDest W: 2.6654927730560303
grad AddEdge W: 0.042850445955991745
grad ChooseDest W: 3.2772650718688965
grad AddEdge W: 0.032166626304388046
grad ChooseDest W: 4.752161979675293
grad AddEdge W: 0.0901472344994545
grad ChooseDest W: 4.3291144371032715
grad AddEdge W: 0.02974790334701538
grad ChooseDest W: 3.554128646850586
grad AddEdge W: 0.00837863888591528
grad ChooseDest W: 2.47517728805542
grad AddEdge W: 0.009091926738619804
grad ChooseDest W: 4.501589298248291
grad AddEdge W: 0.014254659414291382
grad ChooseDest W: 3.9413299560546875
grad AddEdge W: 0.007300006225705147
grad ChooseDest W: 3.124720811843872
grad AddEdge W: 0.010535252280533314
grad ChooseDest W: 6.742387771606445
grad AddEdge W: 0.010479336604475975
grad ChooseDest W: 3.6215593814849854
grad AddEdge W: 0.004667280241847038
grad ChooseDest W: 3.196028709411621
grad AddEdge W: 0.003194011515006423
grad ChooseDest W: 3.5129098892211914
grad AddEdge W: 0.0017056608339771628
grad ChooseDest W: 4.6115312576293945
grad AddEdge W: 0.0019402113975957036
grad ChooseDest W: 2.996011734008789
grad AddEdge W: 0.006733983289450407
grad ChooseDest W: 2.1583125591278076
grad AddEdge W: 0.006569111254066229
grad ChooseDest W: 5.262243747711182
grad AddEdge W: 0.001442422391846776
grad ChooseDest W: 3.6556615829467773
grad AddEdge W: 0.004599412437528372
grad ChooseDest W: 3.680736780166626
grad AddEdge W: 0.0014416297199204564
grad ChooseDest W: 4.929131507873535
grad AddEdge W: 0.0015616192249581218
grad ChooseDest W: 4.46363639831543
grad AddEdge W: 0.0006601664936169982
grad ChooseDest W: 2.9410619735717773
grad AddEdge W: 0.001006965059787035
grad ChooseDest W: 2.2152974605560303
grad AddEdge W: 0.0024166719522327185
grad ChooseDest W: 11.139430046081543
grad AddEdge W: 0.001271827146410942
grad ChooseDest W: 2.824754476547241
grad AddEdge W: 0.0006475753034465015
grad ChooseDest W: 3.261998176574707
grad AddEdge W: 0.0018609531689435244
grad ChooseDest W: 2.755127429962158
grad AddEdge W: 0.011951004154980183
grad ChooseDest W: 3.044170379638672
grad AddEdge W: 0.0003924329939763993
grad ChooseDest W: 3.5600457191467285
grad AddEdge W: 0.0005191442323848605
grad ChooseDest W: 2.095123529434204
grad AddEdge W: 0.0016720299609005451
grad ChooseDest W: 3.2991855144500732
grad AddEdge W: 0.0004314956604503095
grad ChooseDest W: 2.609069347381592
grad AddEdge W: 0.0005922988639213145
grad ChooseDest W: 3.9735641479492188
grad AddEdge W: 0.00030082743614912033
grad ChooseDest W: 2.1592798233032227
grad AddEdge W: 0.00032650213688611984
grad ChooseDest W: 3.876014471054077
grad AddEdge W: 0.0003070793754886836
grad ChooseDest W: 2.068347692489624
grad AddEdge W: 0.0007904486847110093
grad ChooseDest W: 2.870290756225586
grad AddEdge W: 0.0002711904817260802
grad ChooseDest W: 4.110263347625732
grad AddEdge W: 0.00018497102428227663
grad ChooseDest W: 3.5953493118286133
grad AddEdge W: 9.89847321761772e-05
grad ChooseDest W: 2.0221734046936035
grad AddEdge W: 0.00016012067499104887
grad ChooseDest W: 4.316198348999023
grad AddEdge W: 0.0007350092637352645
grad ChooseDest W: 2.3560471534729004
grad AddEdge W: 0.0006347139133140445
grad ChooseDest W: 3.8342461585998535
grad AddEdge W: 0.0004847742966376245
grad ChooseDest W: 4.361187934875488
grad AddEdge W: 0.0004303170135244727
grad ChooseDest W: 2.8216493129730225
grad AddEdge W: 0.00046149612171575427
grad ChooseDest W: 2.9013562202453613
grad AddEdge W: 0.00014478960656560957
grad ChooseDest W: 3.6124088764190674
grad AddEdge W: 0.0002197190624428913
grad ChooseDest W: 3.2013959884643555
grad AddEdge W: 0.0001819415483623743
grad ChooseDest W: 3.789942502975464
grad AddEdge W: 7.399888272630051e-05
grad ChooseDest W: 2.7591028213500977
grad AddEdge W: 7.63779753469862e-05
grad ChooseDest W: 3.8922841548919678
grad AddEdge W: 0.0002445800928398967
grad ChooseDest W: 2.6289069652557373
grad AddEdge W: 0.00012058454740326852
grad ChooseDest W: 2.900146245956421
grad AddEdge W: 0.0002537619147915393
grad ChooseDest W: 3.514572858810425
grad AddEdge W: 0.0008546646567992866
grad ChooseDest W: 3.7282602787017822
grad AddEdge W: 0.00013398432929534465
grad ChooseDest W: 2.365623712539673
grad AddEdge W: 5.621722812065855e-05
grad ChooseDest W: 3.1162102222442627
grad AddEdge W: 3.7948375393170863e-05
grad ChooseDest W: 2.518714427947998
grad AddEdge W: 4.6654604375362396e-05
grad ChooseDest W: 2.0385055541992188
grad AddEdge W: 3.988014941569418e-05
grad ChooseDest W: 3.580395221710205
grad AddEdge W: 0.00019813820836134255
grad ChooseDest W: 3.5626111030578613
grad AddEdge W: 0.00019969508866779506
grad ChooseDest W: 3.5487940311431885
grad AddEdge W: 2.4930734070949256e-05
grad ChooseDest W: 5.64040994644165
grad AddEdge W: 1.8204314983449876e-05
grad ChooseDest W: 3.3346052169799805
=== Epoch 1: Train Loss: 5.7826, Train Log Prob: 0.0100 ===
Total mismatches: 93201
Predicted valid destination but wrong order: 36644
Epoch 1: Validation Loss: 6.9717, Validation Log Prob: 0.0016
Epoch 1: Edge Precision: 0.3696, Recall: 0.3676, F1: 0.3685, Jaccard: 0.2428
Epoch 1: TP: 2.573514674302076, FP: 4.412884753042233, FN: 4.447959914101647
Epoch 1: warmup, skipping learning rate scheduler
Epoch 1: Current Learning Rate: 6e-05
[Epoch 1] ‚è±Ô∏è Total: 4505.18s | Current time: 2025-07-14 12:46:33 | üèãÔ∏è Train: 3749.75s | ‚úÖ Val: 755.43s
grad AddEdge W: 0.0003218849014956504
grad ChooseDest W: 3.950697422027588
grad AddEdge W: 6.99344091117382e-05
grad ChooseDest W: 3.1897363662719727
grad AddEdge W: 9.47886219364591e-05
grad ChooseDest W: 1.959342122077942
grad AddEdge W: 3.1962175853550434e-05
grad ChooseDest W: 3.7201600074768066
grad AddEdge W: 9.626545215724036e-05
grad ChooseDest W: 4.4709792137146
grad AddEdge W: 8.012406033230945e-05
grad ChooseDest W: 2.001094341278076
grad AddEdge W: 1.829217217164114e-05
grad ChooseDest W: 4.670591831207275
grad AddEdge W: 2.4103484975057654e-05
grad ChooseDest W: 3.173396587371826
grad AddEdge W: 3.021836164407432e-05
grad ChooseDest W: 2.563253164291382
grad AddEdge W: 1.4433730939344969e-05
grad ChooseDest W: 3.0117440223693848
grad AddEdge W: 9.154887266049627e-06
grad ChooseDest W: 1.2544705867767334
grad AddEdge W: 5.56584564037621e-05
grad ChooseDest W: 4.089421272277832
grad AddEdge W: 1.0109510185429826e-05
grad ChooseDest W: 4.030692100524902
grad AddEdge W: 1.4564547200279776e-05
grad ChooseDest W: 2.876509428024292
grad AddEdge W: 1.2010358659608755e-05
grad ChooseDest W: 4.156647682189941
grad AddEdge W: 6.891530938446522e-05
grad ChooseDest W: 2.526073455810547
grad AddEdge W: 6.33282206763397e-06
grad ChooseDest W: 1.9584509134292603
grad AddEdge W: 3.718839070643298e-05
grad ChooseDest W: 5.13245153427124
grad AddEdge W: 1.1158426787005737e-05
grad ChooseDest W: 5.14998197555542
grad AddEdge W: 2.6353600333095528e-05
grad ChooseDest W: 3.7006850242614746
grad AddEdge W: 6.946424491616199e-06
grad ChooseDest W: 2.9817564487457275
grad AddEdge W: 2.4648665203130804e-05
grad ChooseDest W: 2.694568395614624
grad AddEdge W: 6.790452061977703e-06
grad ChooseDest W: 3.9456634521484375
grad AddEdge W: 2.5463084512011847e-06
grad ChooseDest W: 2.471320152282715
grad AddEdge W: 1.8955310224555433e-05
grad ChooseDest W: 3.0400938987731934
grad AddEdge W: 1.93013565876754e-05
grad ChooseDest W: 3.0117440223693848
grad AddEdge W: 5.575960130954627e-06
grad ChooseDest W: 3.7957143783569336
grad AddEdge W: 9.3695080067846e-06
grad ChooseDest W: 2.627854824066162
grad AddEdge W: 3.1642803151044063e-06
grad ChooseDest W: 4.487033367156982
grad AddEdge W: 3.858810487145092e-06
grad ChooseDest W: 3.443512201309204
grad AddEdge W: 2.448781060593319e-06
grad ChooseDest W: 3.7888481616973877
grad AddEdge W: 3.4762927043630043e-06
grad ChooseDest W: 2.6684014797210693
grad AddEdge W: 5.451805463962955e-06
grad ChooseDest W: 4.731887340545654
grad AddEdge W: 2.5362644464621553e-06
grad ChooseDest W: 4.091805458068848
grad AddEdge W: 1.0041198947874364e-05
grad ChooseDest W: 2.7515029907226562
grad AddEdge W: 8.497430826537311e-06
grad ChooseDest W: 7.776647567749023
grad AddEdge W: 2.2276608433458023e-06
grad ChooseDest W: 2.769843816757202
grad AddEdge W: 1.5226813729896094e-06
grad ChooseDest W: 12.298763275146484
grad AddEdge W: 2.7497469545778586e-06
grad ChooseDest W: 5.018137454986572
grad AddEdge W: 9.654652330937097e-07
grad ChooseDest W: 3.9608311653137207
grad AddEdge W: 7.365135843429016e-06
grad ChooseDest W: 3.3626766204833984
grad AddEdge W: 1.8059807871395606e-06
grad ChooseDest W: 1.7608895301818848
grad AddEdge W: 1.0118830687133595e-05
grad ChooseDest W: 1.271083116531372
grad AddEdge W: 6.687552058792789e-07
grad ChooseDest W: 2.9912312030792236
grad AddEdge W: 5.263203547656303e-06
grad ChooseDest W: 2.8407037258148193
grad AddEdge W: 4.3280970203340985e-06
grad ChooseDest W: 3.525463104248047
grad AddEdge W: 8.03723196440842e-07
grad ChooseDest W: 2.803321123123169
grad AddEdge W: 3.00780743600626e-06
grad ChooseDest W: 2.910883665084839
grad AddEdge W: 2.4956887045846088e-06
grad ChooseDest W: 2.227546453475952
grad AddEdge W: 1.1014064966730075e-06
grad ChooseDest W: 3.673213005065918
grad AddEdge W: 7.144207643250411e-07
grad ChooseDest W: 4.3177876472473145
grad AddEdge W: 4.905521677756042e-07
grad ChooseDest W: 2.749157190322876
grad AddEdge W: 0.00012210271961521357
grad ChooseDest W: 1.281672477722168
grad AddEdge W: 5.723819640479633e-07
grad ChooseDest W: 5.765660285949707
grad AddEdge W: 4.223077496590122e-07
grad ChooseDest W: 3.7759058475494385
grad AddEdge W: 2.2315750811685575e-06
grad ChooseDest W: 3.120020866394043
grad AddEdge W: 5.697524443348811e-07
grad ChooseDest W: 2.663942337036133
grad AddEdge W: 3.430762376410712e-07
grad ChooseDest W: 4.098880290985107
grad AddEdge W: 5.369694463297492e-06
grad ChooseDest W: 3.944162607192993
grad AddEdge W: 3.415015612517891e-07
grad ChooseDest W: 4.389301300048828
grad AddEdge W: 1.4585099279429414e-06
grad ChooseDest W: 3.9777843952178955
grad AddEdge W: 9.548664365865989e-07
grad ChooseDest W: 2.2922418117523193
grad AddEdge W: 1.1648069175862474e-06
grad ChooseDest W: 10.475168228149414
grad AddEdge W: 3.3810584909588215e-07
grad ChooseDest W: 5.396562099456787
grad AddEdge W: 1.4347283183724358e-07
grad ChooseDest W: 3.0077574253082275
grad AddEdge W: 8.910275028028991e-06
grad ChooseDest W: 3.0874037742614746
=== Epoch 2: Train Loss: 5.6549, Train Log Prob: 0.0110 ===
Total mismatches: 91278
Predicted valid destination but wrong order: 36667
Epoch 2: Validation Loss: 6.7411, Validation Log Prob: 0.0019
Epoch 2: Edge Precision: 0.3704, Recall: 0.3691, F1: 0.3697, Jaccard: 0.2432
Epoch 2: TP: 2.584251968503937, FP: 4.417179670722978, FN: 4.437222619899785
Epoch 2: warmup, skipping learning rate scheduler
Epoch 2: Current Learning Rate: 6e-05
[Epoch 2] ‚è±Ô∏è Total: 4674.87s | Current time: 2025-07-14 14:04:27 | üèãÔ∏è Train: 3916.90s | ‚úÖ Val: 757.97s
grad AddEdge W: 2.61721925198799e-06
grad ChooseDest W: 5.565343856811523
grad AddEdge W: 1.1973521907293616e-07
grad ChooseDest W: 2.87294864654541
grad AddEdge W: 1.3647402852257073e-07
grad ChooseDest W: 3.574589729309082
grad AddEdge W: 8.24456520831518e-08
grad ChooseDest W: 3.200298547744751
grad AddEdge W: 1.0204397682400668e-07
grad ChooseDest W: 3.163925886154175
grad AddEdge W: 1.2529784498838126e-06
grad ChooseDest W: 2.6494057178497314
grad AddEdge W: 1.991688947100556e-07
grad ChooseDest W: 6.265671730041504
grad AddEdge W: 7.137567337167638e-08
grad ChooseDest W: 3.847055673599243
grad AddEdge W: 4.345121453752654e-07
grad ChooseDest W: 2.2818572521209717
grad AddEdge W: 7.268776869295834e-08
grad ChooseDest W: 3.729570150375366
grad AddEdge W: 2.0259879818240734e-07
grad ChooseDest W: 2.3205084800720215
grad AddEdge W: 6.097106961533427e-07
grad ChooseDest W: 4.776119232177734
grad AddEdge W: 1.110873668608292e-07
grad ChooseDest W: 4.509994983673096
grad AddEdge W: 2.0211773517075926e-06
grad ChooseDest W: 0.6626935005187988
grad AddEdge W: 2.477020757396531e-07
grad ChooseDest W: 6.038304328918457
grad AddEdge W: 4.6575191703368546e-08
grad ChooseDest W: 6.002879619598389
grad AddEdge W: 5.20811980209146e-08
grad ChooseDest W: 3.0342319011688232
grad AddEdge W: 7.48417434692783e-08
grad ChooseDest W: 4.309779644012451
grad AddEdge W: 2.4234219608842977e-07
grad ChooseDest W: 2.4791369438171387
grad AddEdge W: 1.9534795114850567e-08
grad ChooseDest W: 3.490888833999634
grad AddEdge W: 1.4344972498747666e-07
grad ChooseDest W: 3.1589930057525635
grad AddEdge W: 4.38202323493897e-06
grad ChooseDest W: 2.577444076538086
grad AddEdge W: 3.425740828788548e-07
grad ChooseDest W: 4.101012706756592
grad AddEdge W: 3.3413442679375294e-08
grad ChooseDest W: 2.181786060333252
grad AddEdge W: 2.323449876939776e-08
grad ChooseDest W: 4.96242618560791
grad AddEdge W: 1.2329532239618857e-07
grad ChooseDest W: 2.7759313583374023
grad AddEdge W: 1.4731428166214755e-07
grad ChooseDest W: 2.1376962661743164
grad AddEdge W: 1.6901266164381923e-08
grad ChooseDest W: 3.1553707122802734
grad AddEdge W: 4.5421174377224816e-07
grad ChooseDest W: 1.9487215280532837
grad AddEdge W: 1.7446582845082048e-08
grad ChooseDest W: 4.033141136169434
grad AddEdge W: 1.0580889053812825e-08
grad ChooseDest W: 3.7830698490142822
grad AddEdge W: 5.393835600386865e-09
grad ChooseDest W: 1.9427162408828735
grad AddEdge W: 3.245245352445636e-07
grad ChooseDest W: 2.252347946166992
grad AddEdge W: 9.262209488269946e-08
grad ChooseDest W: 4.452816486358643
grad AddEdge W: 2.0576059966970206e-08
grad ChooseDest W: 3.8097238540649414
grad AddEdge W: 5.34105708993593e-07
grad ChooseDest W: 4.9794511795043945
grad AddEdge W: 3.9487321856768176e-08
grad ChooseDest W: 4.123212814331055
grad AddEdge W: 5.450556983532806e-08
grad ChooseDest W: 1.692393183708191
grad AddEdge W: 5.372130829073285e-08
grad ChooseDest W: 2.4862334728240967
grad AddEdge W: 2.699181322896038e-09
grad ChooseDest W: 2.8963534832000732
grad AddEdge W: 4.939290754180092e-09
grad ChooseDest W: 3.1715924739837646
grad AddEdge W: 6.666390550691403e-09
grad ChooseDest W: 4.260375022888184
grad AddEdge W: 8.861926303893597e-09
grad ChooseDest W: 2.259248971939087
grad AddEdge W: 4.813210718879191e-09
grad ChooseDest W: 3.2762651443481445
grad AddEdge W: 4.610575032870656e-09
grad ChooseDest W: 2.97141432762146
grad AddEdge W: 5.395813129638327e-09
grad ChooseDest W: 3.273303270339966
grad AddEdge W: 5.813172165858305e-09
grad ChooseDest W: 2.414416551589966
grad AddEdge W: 2.2901486929072234e-08
grad ChooseDest W: 2.663429021835327
grad AddEdge W: 2.2079305050937137e-09
grad ChooseDest W: 3.0295610427856445
grad AddEdge W: 1.7215042724672003e-09
grad ChooseDest W: 6.2020769119262695
grad AddEdge W: 2.90068435937485e-09
grad ChooseDest W: 3.21216082572937
grad AddEdge W: 1.504444071542821e-07
grad ChooseDest W: 3.4086265563964844
grad AddEdge W: 2.8230235926685054e-08
grad ChooseDest W: 2.7348475456237793
grad AddEdge W: 1.7982290101414833e-09
grad ChooseDest W: 3.3810505867004395
grad AddEdge W: 2.0368846165297327e-08
grad ChooseDest W: 2.290790557861328
grad AddEdge W: 1.2129246496073165e-09
grad ChooseDest W: 2.905696153640747
grad AddEdge W: 4.010382026464754e-10
grad ChooseDest W: 2.7748634815216064
grad AddEdge W: 1.0178432319207786e-07
grad ChooseDest W: 5.029793739318848
grad AddEdge W: 4.846643264500017e-08
grad ChooseDest W: 5.124536037445068
grad AddEdge W: 5.702475380786609e-09
grad ChooseDest W: 4.47326135635376
grad AddEdge W: 1.2049145015069485e-09
grad ChooseDest W: 4.8651509284973145
grad AddEdge W: 1.3386131314518934e-08
grad ChooseDest W: 1.7417221069335938
grad AddEdge W: 1.3120616815243125e-09
grad ChooseDest W: 3.141420364379883
grad AddEdge W: 9.533975386943894e-09
grad ChooseDest W: 3.86887264251709
grad AddEdge W: 6.90256185542637e-10
grad ChooseDest W: 4.309250354766846
grad AddEdge W: 2.40977099386086e-10
grad ChooseDest W: 2.541569471359253
=== Epoch 3: Train Loss: 5.5886, Train Log Prob: 0.0117 ===
Total mismatches: 90108
Predicted valid destination but wrong order: 36627
Epoch 3: Validation Loss: 6.5174, Validation Log Prob: 0.0025
Epoch 3: Edge Precision: 0.3724, Recall: 0.3708, F1: 0.3715, Jaccard: 0.2450
Epoch 3: TP: 2.596707229778096, FP: 4.401288475304224, FN: 4.424767358625626
Epoch 3: warmup, skipping learning rate scheduler
Epoch 3: Current Learning Rate: 6e-05
[Epoch 3] ‚è±Ô∏è Total: 4683.89s | Current time: 2025-07-14 15:22:31 | üèãÔ∏è Train: 3911.94s | ‚úÖ Val: 771.95s
grad AddEdge W: 9.763565067544278e-09
grad ChooseDest W: 7.173048496246338
grad AddEdge W: 4.284771704110568e-10
grad ChooseDest W: 5.031194686889648
grad AddEdge W: 3.2837565999699336e-09
grad ChooseDest W: 2.6682446002960205
grad AddEdge W: 3.3881346617192776e-09
grad ChooseDest W: 2.4632885456085205
grad AddEdge W: 1.9742775470010798e-10
grad ChooseDest W: 3.6914350986480713
grad AddEdge W: 2.797450604496987e-10
grad ChooseDest W: 4.501652240753174
grad AddEdge W: 2.498712070586606e-10
grad ChooseDest W: 4.545039653778076
grad AddEdge W: 5.034991090724361e-09
grad ChooseDest W: 2.7803633213043213
grad AddEdge W: 2.1553593632095414e-10
grad ChooseDest W: 6.245120048522949
grad AddEdge W: 3.2612942901799613e-10
grad ChooseDest W: 4.361495494842529
grad AddEdge W: 2.2840188074191303e-10
grad ChooseDest W: 1.9838286638259888
grad AddEdge W: 1.1334082561376135e-09
grad ChooseDest W: 2.346911668777466
grad AddEdge W: 1.4659413960504963e-10
grad ChooseDest W: 2.8827381134033203
grad AddEdge W: 1.816630318396406e-10
grad ChooseDest W: 4.030941009521484
grad AddEdge W: 1.985549641370099e-10
grad ChooseDest W: 6.974577903747559
grad AddEdge W: 1.285502593262322e-09
grad ChooseDest W: 2.9484663009643555
grad AddEdge W: 2.01992489401448e-09
grad ChooseDest W: 4.90067195892334
grad AddEdge W: 7.923189188074886e-11
grad ChooseDest W: 3.6793744564056396
grad AddEdge W: 1.1654828213636392e-09
grad ChooseDest W: 2.927124500274658
grad AddEdge W: 8.511544802303206e-10
grad ChooseDest W: 3.7879507541656494
grad AddEdge W: 1.0093231805896608e-10
grad ChooseDest W: 4.680558681488037
grad AddEdge W: 6.039938688084945e-11
grad ChooseDest W: 1.8904720544815063
grad AddEdge W: 8.588077471394229e-10
grad ChooseDest W: 3.0985453128814697
grad AddEdge W: 1.7368002869222465e-11
grad ChooseDest W: 2.756291151046753
grad AddEdge W: 5.018936932721374e-10
grad ChooseDest W: 1.9781502485275269
grad AddEdge W: 3.5455968716568975e-10
grad ChooseDest W: 3.8413891792297363
grad AddEdge W: 5.961155458145129e-10
grad ChooseDest W: 3.616868257522583
grad AddEdge W: 3.6398696556805277e-11
grad ChooseDest W: 4.01217794418335
grad AddEdge W: 9.171927106699229e-11
grad ChooseDest W: 5.176730632781982
grad AddEdge W: 5.2858696586444154e-11
grad ChooseDest W: 2.7745907306671143
grad AddEdge W: 4.013571419658746e-11
grad ChooseDest W: 3.379812002182007
grad AddEdge W: 2.921270447764357e-10
grad ChooseDest W: 3.8674890995025635
grad AddEdge W: 2.7970768062823836e-11
grad ChooseDest W: 4.12101936340332
grad AddEdge W: 1.7906033600301186e-10
grad ChooseDest W: 3.9656484127044678
grad AddEdge W: 2.157859828322284e-11
grad ChooseDest W: 7.315649032592773
grad AddEdge W: 2.213582095400568e-11
grad ChooseDest W: 2.529309034347534
grad AddEdge W: 9.607256656884999e-11
grad ChooseDest W: 4.006199359893799
grad AddEdge W: 1.924673059816584e-10
grad ChooseDest W: 4.172719955444336
grad AddEdge W: 7.836718080023175e-11
grad ChooseDest W: 2.5764882564544678
grad AddEdge W: 5.83404921594699e-11
grad ChooseDest W: 2.951476573944092
grad AddEdge W: 1.5843494571843308e-10
grad ChooseDest W: 4.908806800842285
grad AddEdge W: 1.2236495670892023e-11
grad ChooseDest W: 2.575138807296753
grad AddEdge W: 6.318720286585533e-12
grad ChooseDest W: 4.022243976593018
grad AddEdge W: 1.8881179952856542e-10
grad ChooseDest W: 2.9619593620300293
grad AddEdge W: 7.116890410330257e-12
grad ChooseDest W: 3.690443754196167
grad AddEdge W: 5.120101825156764e-12
grad ChooseDest W: 1.8285051584243774
grad AddEdge W: 9.540837564436799e-11
grad ChooseDest W: 4.061422348022461
grad AddEdge W: 4.620855781345412e-12
grad ChooseDest W: 1.9704452753067017
grad AddEdge W: 5.468472463049379e-12
grad ChooseDest W: 2.4114739894866943
grad AddEdge W: 3.7754183460081414e-12
grad ChooseDest W: 4.331151008605957
grad AddEdge W: 3.921455174471511e-12
grad ChooseDest W: 2.9536807537078857
grad AddEdge W: 1.9199351830589961e-10
grad ChooseDest W: 5.746557235717773
grad AddEdge W: 8.468344775414138e-11
grad ChooseDest W: 3.5269088745117188
grad AddEdge W: 7.794079270873056e-11
grad ChooseDest W: 3.8314619064331055
grad AddEdge W: 3.5942318999532263e-12
grad ChooseDest W: 4.030464172363281
grad AddEdge W: 7.888403472100514e-12
grad ChooseDest W: 3.7864158153533936
grad AddEdge W: 5.222673422206059e-12
grad ChooseDest W: 2.9250240325927734
grad AddEdge W: 3.696316256546206e-12
grad ChooseDest W: 3.527892589569092
grad AddEdge W: 9.164174974429784e-11
grad ChooseDest W: 2.812605619430542
grad AddEdge W: 7.892674847020942e-10
grad ChooseDest W: 2.39742112159729
grad AddEdge W: 4.230790370818305e-11
grad ChooseDest W: 3.8196308612823486
grad AddEdge W: 2.8478493434985763e-12
grad ChooseDest W: 3.3597514629364014
grad AddEdge W: 1.5764100094739497e-12
grad ChooseDest W: 2.351637840270996
grad AddEdge W: 3.326577902029726e-11
grad ChooseDest W: 2.502309560775757
grad AddEdge W: 3.83238232815053e-12
grad ChooseDest W: 3.3799946308135986
grad AddEdge W: 1.4329848072036633e-12
grad ChooseDest W: 2.423487663269043
=== Epoch 4: Train Loss: 5.5512, Train Log Prob: 0.0121 ===
Total mismatches: 89554
Predicted valid destination but wrong order: 36182
Epoch 4: Validation Loss: 6.3046, Validation Log Prob: 0.0030
Epoch 4: Edge Precision: 0.3700, Recall: 0.3680, F1: 0.3689, Jaccard: 0.2425
Epoch 4: TP: 2.577236936292054, FP: 4.409305654974946, FN: 4.444237652111668
Epoch 4: warmup, skipping learning rate scheduler
Epoch 4: Current Learning Rate: 6e-05
[Epoch 4] ‚è±Ô∏è Total: 4641.70s | Current time: 2025-07-14 16:39:53 | üèãÔ∏è Train: 3895.44s | ‚úÖ Val: 746.26s
grad AddEdge W: 1.4256817948421485e-09
grad ChooseDest W: 5.877817153930664
grad AddEdge W: 4.632656758207787e-11
grad ChooseDest W: 4.042433738708496
grad AddEdge W: 2.3064690782281794e-11
grad ChooseDest W: 4.873490333557129
grad AddEdge W: 2.3078259789310884e-11
grad ChooseDest W: 7.917003631591797
grad AddEdge W: 3.7371036712245675e-13
grad ChooseDest W: 6.726359844207764
grad AddEdge W: 1.1725213855395822e-10
grad ChooseDest W: 3.664076328277588
grad AddEdge W: 1.823976213122247e-11
grad ChooseDest W: 7.587340354919434
grad AddEdge W: 3.676491836662743e-12
grad ChooseDest W: 7.124490737915039
grad AddEdge W: 1.2044060410457136e-12
grad ChooseDest W: 3.637357234954834
grad AddEdge W: 7.688273628847497e-12
grad ChooseDest W: 5.403156280517578
grad AddEdge W: 3.613987364578519e-13
grad ChooseDest W: 3.8873560428619385
grad AddEdge W: 9.361846367572646e-12
grad ChooseDest W: 2.4964334964752197
grad AddEdge W: 2.494984741811118e-13
grad ChooseDest W: 4.522099494934082
grad AddEdge W: 1.2343760562305572e-11
grad ChooseDest W: 1.8003785610198975
grad AddEdge W: 3.024434001852594e-13
grad ChooseDest W: 4.141000270843506
grad AddEdge W: 1.248497941315485e-12
grad ChooseDest W: 5.628811359405518
grad AddEdge W: 3.022384642906162e-12
grad ChooseDest W: 3.3943216800689697
grad AddEdge W: 4.845889856920638e-13
grad ChooseDest W: 2.512763023376465
grad AddEdge W: 6.590028544563309e-13
grad ChooseDest W: 2.5593764781951904
grad AddEdge W: 1.3379468366547798e-10
grad ChooseDest W: 4.765890121459961
grad AddEdge W: 6.180123145009542e-13
grad ChooseDest W: 3.2380902767181396
grad AddEdge W: 2.3724127046556576e-13
grad ChooseDest W: 4.362435817718506
grad AddEdge W: 1.5246736165264174e-11
grad ChooseDest W: 2.403278350830078
grad AddEdge W: 4.1820550928522993e-13
grad ChooseDest W: 4.859742641448975
grad AddEdge W: 3.2123162094742364e-13
grad ChooseDest W: 3.532787322998047
grad AddEdge W: 1.7666972756697624e-13
grad ChooseDest W: 6.716564178466797
grad AddEdge W: 8.023980378789666e-14
grad ChooseDest W: 4.903651237487793
grad AddEdge W: 6.577085664288829e-12
grad ChooseDest W: 4.662313461303711
grad AddEdge W: 2.487518925651383e-13
grad ChooseDest W: 2.293578863143921
grad AddEdge W: 5.19574888755564e-13
grad ChooseDest W: 2.674940824508667
grad AddEdge W: 3.2532203927265746e-12
grad ChooseDest W: 2.8699944019317627
grad AddEdge W: 5.275694377887552e-12
grad ChooseDest W: 3.4327471256256104
grad AddEdge W: 1.088918122338356e-10
grad ChooseDest W: 5.750864505767822
grad AddEdge W: 2.1281178316963356e-13
grad ChooseDest W: 2.997727155685425
grad AddEdge W: 2.939141714296961e-13
grad ChooseDest W: 2.883000612258911
grad AddEdge W: 2.555594624809032e-10
grad ChooseDest W: 1.5698298215866089
grad AddEdge W: 2.0481321716739331e-13
grad ChooseDest W: 4.0666279792785645
grad AddEdge W: 2.7871932192780058e-12
grad ChooseDest W: 3.5011374950408936
grad AddEdge W: 1.4321363376885304e-13
grad ChooseDest W: 3.631723165512085
grad AddEdge W: 1.4652019289691787e-13
grad ChooseDest W: 4.008387565612793
grad AddEdge W: 6.525857870580409e-13
grad ChooseDest W: 2.442357063293457
grad AddEdge W: 2.8025545426024756e-11
grad ChooseDest W: 3.3887150287628174
grad AddEdge W: 1.1627927509749725e-09
grad ChooseDest W: 2.5724592208862305
grad AddEdge W: 4.2761689236217246e-14
grad ChooseDest W: 3.772467613220215
grad AddEdge W: 1.4937568959538794e-09
grad ChooseDest W: 3.2043488025665283
grad AddEdge W: 4.999499772506688e-11
grad ChooseDest W: 2.11538028717041
grad AddEdge W: 5.619331822775964e-12
grad ChooseDest W: 5.303359508514404
grad AddEdge W: 3.870708874947892e-12
grad ChooseDest W: 4.157311916351318
grad AddEdge W: 2.70675138119153e-14
grad ChooseDest W: 4.024421215057373
grad AddEdge W: 8.101853199828438e-14
grad ChooseDest W: 2.874089241027832
grad AddEdge W: 2.910052502246539e-14
grad ChooseDest W: 2.298671007156372
grad AddEdge W: 4.782180727802565e-12
grad ChooseDest W: 4.968963623046875
grad AddEdge W: 4.0427950836205775e-14
grad ChooseDest W: 4.531410217285156
grad AddEdge W: 5.4690540290947e-12
grad ChooseDest W: 8.722453117370605
grad AddEdge W: 2.901009727579451e-12
grad ChooseDest W: 5.64934778213501
grad AddEdge W: 4.7330950871940156e-14
grad ChooseDest W: 4.184806823730469
grad AddEdge W: 7.017043612833618e-10
grad ChooseDest W: 3.0844528675079346
grad AddEdge W: 8.787738332155515e-13
grad ChooseDest W: 1.7572875022888184
grad AddEdge W: 1.7950548816467182e-12
grad ChooseDest W: 3.77665114402771
grad AddEdge W: 4.277008163865864e-14
grad ChooseDest W: 3.5791263580322266
grad AddEdge W: 3.2378407003067974e-14
grad ChooseDest W: 3.39194917678833
grad AddEdge W: 3.0453126999285818e-12
grad ChooseDest W: 3.2744152545928955
grad AddEdge W: 2.0896177447887396e-14
grad ChooseDest W: 3.2573001384735107
grad AddEdge W: 5.690933293188427e-13
grad ChooseDest W: 2.8769376277923584
grad AddEdge W: 4.5627404984685885e-14
grad ChooseDest W: 2.9645676612854004
grad AddEdge W: 3.498899981595541e-14
grad ChooseDest W: 6.036599636077881
=== Epoch 5: Train Loss: 5.5299, Train Log Prob: 0.0124 ===
Total mismatches: 88948
Predicted valid destination but wrong order: 36167
Epoch 5: Validation Loss: 6.2361, Validation Log Prob: 0.0033
Epoch 5: Edge Precision: 0.3706, Recall: 0.3691, F1: 0.3698, Jaccard: 0.2436
Epoch 5: TP: 2.5858267716535432, FP: 4.413314244810308, FN: 4.435647816750179
Epoch 5: warmup, skipping learning rate scheduler
Epoch 5: Current Learning Rate: 6e-05
[Epoch 5] ‚è±Ô∏è Total: 4635.04s | Current time: 2025-07-14 17:57:08 | üèãÔ∏è Train: 3888.51s | ‚úÖ Val: 746.53s
grad AddEdge W: 1.5683548210132514e-12
grad ChooseDest W: 9.207786560058594
grad AddEdge W: 1.386181100460071e-12
grad ChooseDest W: 5.080792427062988
grad AddEdge W: 1.7058550684750755e-14
grad ChooseDest W: 2.313948154449463
grad AddEdge W: 1.6234343581359063e-14
grad ChooseDest W: 4.113306045532227
grad AddEdge W: 7.381020030198734e-14
grad ChooseDest W: 2.336458683013916
grad AddEdge W: 2.758661456550831e-14
grad ChooseDest W: 3.0711617469787598
grad AddEdge W: 5.667943328221958e-14
grad ChooseDest W: 2.8658344745635986
grad AddEdge W: 8.608656729093209e-14
grad ChooseDest W: 4.385375022888184
grad AddEdge W: 9.092101529127594e-13
grad ChooseDest W: 8.47824764251709
grad AddEdge W: 4.7572826212226305e-14
grad ChooseDest W: 3.0623888969421387
grad AddEdge W: 3.1263748913930994e-14
grad ChooseDest W: 3.5030739307403564
grad AddEdge W: 5.532986178599653e-13
grad ChooseDest W: 2.461149215698242
grad AddEdge W: 2.0103274947502747e-11
grad ChooseDest W: 3.181105852127075
grad AddEdge W: 2.5036943750319195e-14
grad ChooseDest W: 2.3480639457702637
grad AddEdge W: 1.2077066777194112e-11
grad ChooseDest W: 2.8007094860076904
grad AddEdge W: 1.5338178318592688e-13
grad ChooseDest W: 4.904714584350586
grad AddEdge W: 6.022053922151091e-14
grad ChooseDest W: 3.456700086593628
grad AddEdge W: 2.7443274567976253e-14
grad ChooseDest W: 4.215897560119629
grad AddEdge W: 1.686750782450841e-12
grad ChooseDest W: 3.268089771270752
grad AddEdge W: 2.057339521454541e-14
grad ChooseDest W: 4.301909923553467
grad AddEdge W: 3.410707249940602e-14
grad ChooseDest W: 7.4728827476501465
grad AddEdge W: 4.0277026505664004e-13
grad ChooseDest W: 4.083477020263672
grad AddEdge W: 1.207509895025105e-12
grad ChooseDest W: 4.467693328857422
grad AddEdge W: 1.639962003815911e-14
grad ChooseDest W: 3.9257736206054688
grad AddEdge W: 4.214919246958715e-10
grad ChooseDest W: 2.588585376739502
grad AddEdge W: 2.868136229818713e-14
grad ChooseDest W: 4.226101875305176
grad AddEdge W: 3.504853267962023e-14
grad ChooseDest W: 4.526566028594971
grad AddEdge W: 1.3016974481541276e-14
grad ChooseDest W: 7.150707244873047
grad AddEdge W: 3.4201907798958597e-12
grad ChooseDest W: 4.566639423370361
grad AddEdge W: 4.838668596113434e-14
grad ChooseDest W: 4.333870887756348
grad AddEdge W: 1.7454359355420501e-12
grad ChooseDest W: 3.498126983642578
grad AddEdge W: 8.805325357741367e-10
grad ChooseDest W: 1.9250249862670898
grad AddEdge W: 4.448138973584155e-14
grad ChooseDest W: 3.3706555366516113
grad AddEdge W: 8.002691933607642e-13
grad ChooseDest W: 7.4596052169799805
grad AddEdge W: 3.5384354138152246e-14
grad ChooseDest W: 3.479198694229126
grad AddEdge W: 3.6651359573182385e-14
grad ChooseDest W: 4.482206344604492
grad AddEdge W: 6.833255766373935e-15
grad ChooseDest W: 4.554595470428467
grad AddEdge W: 4.786829569877749e-13
grad ChooseDest W: 1.2071763277053833
grad AddEdge W: 4.4229607498204315e-13
grad ChooseDest W: 2.9126625061035156
grad AddEdge W: 9.14953639382602e-15
grad ChooseDest W: 6.496749401092529
grad AddEdge W: 5.138003737747976e-13
grad ChooseDest W: 7.080049991607666
grad AddEdge W: 1.1287506302901096e-12
grad ChooseDest W: 5.392460823059082
grad AddEdge W: 3.006157199255191e-14
grad ChooseDest W: 7.259802341461182
grad AddEdge W: 1.1513927831996451e-11
grad ChooseDest W: 2.74399733543396
grad AddEdge W: 3.7691242271362113e-13
grad ChooseDest W: 4.276574611663818
grad AddEdge W: 3.897473164517218e-13
grad ChooseDest W: 6.378547191619873
grad AddEdge W: 6.193376432366005e-13
grad ChooseDest W: 3.357700824737549
grad AddEdge W: 1.0721167910973748e-14
grad ChooseDest W: 2.510871648788452
grad AddEdge W: 5.725034704119614e-15
grad ChooseDest W: 3.1880905628204346
grad AddEdge W: 1.6346612377506758e-13
grad ChooseDest W: 3.702260732650757
grad AddEdge W: 9.223694128358134e-14
grad ChooseDest W: 4.344050884246826
grad AddEdge W: 3.8166532109230877e-13
grad ChooseDest W: 2.8843934535980225
grad AddEdge W: 1.1297333274223279e-14
grad ChooseDest W: 3.366764783859253
grad AddEdge W: 1.7057347288101932e-12
grad ChooseDest W: 5.627424240112305
grad AddEdge W: 3.2634074580834265e-15
grad ChooseDest W: 3.8158533573150635
grad AddEdge W: 2.1941969725533528e-13
grad ChooseDest W: 7.371192932128906
grad AddEdge W: 5.629089317067681e-13
grad ChooseDest W: 2.433015823364258
grad AddEdge W: 1.7863967209240557e-14
grad ChooseDest W: 5.967691898345947
grad AddEdge W: 6.554476047608863e-15
grad ChooseDest W: 3.0117456912994385
grad AddEdge W: 2.5013919700746928e-14
grad ChooseDest W: 2.5696933269500732
grad AddEdge W: 7.310643958962932e-15
grad ChooseDest W: 3.9945576190948486
grad AddEdge W: 1.739627322402959e-13
grad ChooseDest W: 4.268648624420166
grad AddEdge W: 2.1140906950392613e-13
grad ChooseDest W: 4.19582986831665
grad AddEdge W: 5.208051329606833e-13
grad ChooseDest W: 3.15208101272583
grad AddEdge W: 1.5246071278281897e-13
grad ChooseDest W: 3.027953624725342
grad AddEdge W: 1.6403614577775727e-11
grad ChooseDest W: 2.2827162742614746
=== Epoch 6: Train Loss: 5.5074, Train Log Prob: 0.0127 ===
Total mismatches: 88444
Predicted valid destination but wrong order: 36055
Epoch 6: Validation Loss: 6.1808, Validation Log Prob: 0.0034
Epoch 6: Edge Precision: 0.3706, Recall: 0.3695, F1: 0.3700, Jaccard: 0.2436
Epoch 6: TP: 2.588690050107373, FP: 4.416320687186829, FN: 4.4327845382963496
Epoch 6: Current Learning Rate: 6e-05
[Epoch 6] ‚è±Ô∏è Total: 4647.31s | Current time: 2025-07-14 19:14:35 | üèãÔ∏è Train: 3901.20s | ‚úÖ Val: 746.11s
grad AddEdge W: 5.465025654238787e-10
grad ChooseDest W: 8.217084884643555
grad AddEdge W: 2.6298230018889472e-15
grad ChooseDest W: 4.6406354904174805
grad AddEdge W: 1.0492966223555125e-14
grad ChooseDest W: 5.848673343658447
grad AddEdge W: 3.464904737773963e-15
grad ChooseDest W: 3.67751407623291
grad AddEdge W: 4.363105154947533e-10
grad ChooseDest W: 0.44861385226249695
grad AddEdge W: 6.4830178070796485e-15
grad ChooseDest W: 4.397112846374512
grad AddEdge W: 2.6986135121507843e-15
grad ChooseDest W: 3.1460468769073486
grad AddEdge W: 3.3061606837664348e-12
grad ChooseDest W: 4.8012213706970215
grad AddEdge W: 4.3775380998041513e-13
grad ChooseDest W: 3.15665602684021
grad AddEdge W: 4.6702620937200123e-11
grad ChooseDest W: 1.6219605207443237
grad AddEdge W: 1.4855600524542453e-15
grad ChooseDest W: 4.638002872467041
grad AddEdge W: 8.013902041970056e-12
grad ChooseDest W: 4.131870269775391
grad AddEdge W: 3.258346647981819e-15
grad ChooseDest W: 4.125140190124512
grad AddEdge W: 4.205832408824559e-15
grad ChooseDest W: 3.7279458045959473
grad AddEdge W: 2.0257054497360705e-13
grad ChooseDest W: 4.766744136810303
grad AddEdge W: 2.7497004239512846e-13
grad ChooseDest W: 4.924234867095947
grad AddEdge W: 2.1757360504042053e-15
grad ChooseDest W: 3.238304615020752
grad AddEdge W: 6.926254139455121e-14
grad ChooseDest W: 3.3909454345703125
grad AddEdge W: 3.830484581325393e-15
grad ChooseDest W: 4.874377250671387
grad AddEdge W: 1.1490383941363796e-14
grad ChooseDest W: 4.453181743621826
grad AddEdge W: 1.68233157439579e-13
grad ChooseDest W: 2.611905813217163
grad AddEdge W: 1.174924008995673e-13
grad ChooseDest W: 5.667283058166504
grad AddEdge W: 1.2337282210379483e-14
grad ChooseDest W: 3.364516496658325
grad AddEdge W: 4.0399689581920636e-15
grad ChooseDest W: 3.987065315246582
grad AddEdge W: 4.51509625043825e-14
grad ChooseDest W: 3.4849483966827393
grad AddEdge W: 6.248260931682225e-15
grad ChooseDest W: 4.212889671325684
grad AddEdge W: 5.424328859888507e-14
grad ChooseDest W: 6.946753025054932
grad AddEdge W: 4.0817721517214314e-15
grad ChooseDest W: 8.041159629821777
grad AddEdge W: 1.557345120646158e-14
grad ChooseDest W: 5.395167827606201
grad AddEdge W: 3.657025362738394e-15
grad ChooseDest W: 3.349212408065796
grad AddEdge W: 8.431230431697745e-15
grad ChooseDest W: 4.729593276977539
grad AddEdge W: 1.1219307140221055e-14
grad ChooseDest W: 6.536621570587158
grad AddEdge W: 1.4881123744825593e-14
grad ChooseDest W: 3.042847156524658
grad AddEdge W: 1.85903657241469e-15
grad ChooseDest W: 2.914541721343994
grad AddEdge W: 2.2595198908653908e-15
grad ChooseDest W: 3.319725751876831
grad AddEdge W: 2.946030734900934e-13
grad ChooseDest W: 4.5746941566467285
grad AddEdge W: 1.2624714370700757e-13
grad ChooseDest W: 2.6851422786712646
grad AddEdge W: 1.4564704422983382e-14
grad ChooseDest W: 2.1861228942871094
grad AddEdge W: 1.4595166438272078e-13
grad ChooseDest W: 3.533026695251465
grad AddEdge W: 1.391815943095967e-13
grad ChooseDest W: 3.5388944149017334
grad AddEdge W: 7.297483797796822e-12
grad ChooseDest W: 2.975926160812378
grad AddEdge W: 7.82596271044645e-15
grad ChooseDest W: 5.177839756011963
grad AddEdge W: 1.6901935309242971e-13
grad ChooseDest W: 3.185771942138672
grad AddEdge W: 7.7110948784034e-15
grad ChooseDest W: 2.6528611183166504
grad AddEdge W: 3.9824283160191844e-15
grad ChooseDest W: 4.196272373199463
grad AddEdge W: 4.021316445660577e-15
grad ChooseDest W: 2.947352170944214
grad AddEdge W: 2.2639716842779226e-15
grad ChooseDest W: 4.521324157714844
grad AddEdge W: 9.313128610313748e-14
grad ChooseDest W: 6.31704044342041
grad AddEdge W: 1.2042898450660092e-14
grad ChooseDest W: 3.6699182987213135
grad AddEdge W: 9.453508288681522e-12
grad ChooseDest W: 3.1753315925598145
grad AddEdge W: 8.919395847991132e-15
grad ChooseDest W: 3.8380820751190186
grad AddEdge W: 6.26122688852232e-14
grad ChooseDest W: 2.4416415691375732
grad AddEdge W: 1.8407910592143587e-15
grad ChooseDest W: 4.065843105316162
grad AddEdge W: 9.915183366189517e-14
grad ChooseDest W: 4.303711414337158
grad AddEdge W: 3.7508998728189025e-13
grad ChooseDest W: 5.49945592880249
grad AddEdge W: 1.557067734296591e-13
grad ChooseDest W: 3.1666743755340576
grad AddEdge W: 3.709891230075377e-15
grad ChooseDest W: 4.5292229652404785
grad AddEdge W: 1.3176830774511844e-14
grad ChooseDest W: 3.427947759628296
grad AddEdge W: 1.011294976220893e-15
grad ChooseDest W: 3.743374824523926
grad AddEdge W: 1.0910119108056143e-13
grad ChooseDest W: 6.484146595001221
grad AddEdge W: 2.1485033804141684e-12
grad ChooseDest W: 4.120213508605957
grad AddEdge W: 6.428025463528584e-15
grad ChooseDest W: 4.234349727630615
grad AddEdge W: 8.930580918059625e-15
grad ChooseDest W: 4.441084384918213
grad AddEdge W: 6.507363651566105e-15
grad ChooseDest W: 2.820817232131958
grad AddEdge W: 1.078131264820037e-10
grad ChooseDest W: 4.874412536621094
grad AddEdge W: 6.494413703648713e-14
grad ChooseDest W: 3.9102869033813477
=== Epoch 7: Train Loss: 5.4889, Train Log Prob: 0.0130 ===
Total mismatches: 88001
Predicted valid destination but wrong order: 36028
Epoch 7: Validation Loss: 6.1699, Validation Log Prob: 0.0035
Epoch 7: Edge Precision: 0.3705, Recall: 0.3692, F1: 0.3698, Jaccard: 0.2436
Epoch 7: TP: 2.586256263421618, FP: 4.41445955619184, FN: 4.435218324982104
Epoch 7: Current Learning Rate: 6e-05
[Epoch 7] ‚è±Ô∏è Total: 4635.20s | Current time: 2025-07-14 20:31:51 | üèãÔ∏è Train: 3888.38s | ‚úÖ Val: 746.81s
grad AddEdge W: 1.6622471357263108e-13
grad ChooseDest W: 7.791360378265381
grad AddEdge W: 2.544404176081324e-15
grad ChooseDest W: 3.601268768310547
grad AddEdge W: 1.451878846097862e-13
grad ChooseDest W: 3.7129223346710205
grad AddEdge W: 8.532776807172737e-14
grad ChooseDest W: 3.1403629779815674
grad AddEdge W: 2.265172234896043e-13
grad ChooseDest W: 4.8671183586120605
grad AddEdge W: 1.4977317888513475e-14
grad ChooseDest W: 4.174286842346191
grad AddEdge W: 1.221589696802708e-13
grad ChooseDest W: 2.7898435592651367
grad AddEdge W: 1.2277638808915045e-15
grad ChooseDest W: 1.9518417119979858
grad AddEdge W: 3.97990924003405e-15
grad ChooseDest W: 5.578474998474121
grad AddEdge W: 3.879781898855593e-15
grad ChooseDest W: 2.491973638534546
grad AddEdge W: 2.5422434882065026e-13
grad ChooseDest W: 4.8621416091918945
grad AddEdge W: 4.802003843255289e-15
grad ChooseDest W: 5.130319595336914
grad AddEdge W: 1.9342989395539772e-13
grad ChooseDest W: 3.7927188873291016
grad AddEdge W: 6.790851301424761e-12
grad ChooseDest W: 5.688258647918701
grad AddEdge W: 1.0258147006532783e-13
grad ChooseDest W: 3.755925416946411
grad AddEdge W: 5.1577610692338845e-15
grad ChooseDest W: 3.3063313961029053
grad AddEdge W: 6.221596588612543e-14
grad ChooseDest W: 4.210156440734863
grad AddEdge W: 3.808419779625233e-13
grad ChooseDest W: 2.9486851692199707
grad AddEdge W: 3.9242352043670353e-14
grad ChooseDest W: 2.1423020362854004
grad AddEdge W: 1.0892193180386811e-13
grad ChooseDest W: 2.6463372707366943
grad AddEdge W: 1.917503233357155e-15
grad ChooseDest W: 6.131063461303711
grad AddEdge W: 1.689469825974163e-13
grad ChooseDest W: 3.684161424636841
grad AddEdge W: 2.6064506100171225e-15
grad ChooseDest W: 3.7904930114746094
grad AddEdge W: 8.340355993731799e-14
grad ChooseDest W: 4.395594120025635
grad AddEdge W: 1.8005859119245987e-13
grad ChooseDest W: 3.719142436981201
grad AddEdge W: 6.945359136987031e-14
grad ChooseDest W: 3.905305862426758
grad AddEdge W: 1.984831764752031e-15
grad ChooseDest W: 4.576161861419678
grad AddEdge W: 2.473119636598603e-13
grad ChooseDest W: 3.3839304447174072
grad AddEdge W: 7.87756768453815e-14
grad ChooseDest W: 3.686160087585449
grad AddEdge W: 5.412643532161365e-14
grad ChooseDest W: 4.037825584411621
grad AddEdge W: 5.135406599206423e-15
grad ChooseDest W: 3.8756935596466064
grad AddEdge W: 2.677342939363947e-13
grad ChooseDest W: 2.4065935611724854
grad AddEdge W: 1.2983055385981657e-13
grad ChooseDest W: 6.238259792327881
grad AddEdge W: 8.84377729926236e-16
grad ChooseDest W: 4.6519999504089355
grad AddEdge W: 6.957080124801252e-15
grad ChooseDest W: 3.667220115661621
grad AddEdge W: 7.789979078465237e-11
grad ChooseDest W: 1.673144817352295
grad AddEdge W: 2.7500958866936986e-13
grad ChooseDest W: 8.594452857971191
grad AddEdge W: 5.746682325152592e-15
grad ChooseDest W: 3.5405044555664062
grad AddEdge W: 2.907473413977092e-15
grad ChooseDest W: 5.288670063018799
grad AddEdge W: 2.2820393205593304e-15
grad ChooseDest W: 5.446882724761963
grad AddEdge W: 3.2151002780228533e-12
grad ChooseDest W: 1.8196302652359009
grad AddEdge W: 1.6867274128118262e-15
grad ChooseDest W: 3.1516940593719482
grad AddEdge W: 4.7783395214781935e-14
grad ChooseDest W: 2.8333046436309814
grad AddEdge W: 7.171178120725027e-14
grad ChooseDest W: 6.1476945877075195
grad AddEdge W: 2.450453822670719e-15
grad ChooseDest W: 4.876891136169434
grad AddEdge W: 5.630679574839339e-16
grad ChooseDest W: 5.484297752380371
grad AddEdge W: 1.1473806659553082e-13
grad ChooseDest W: 4.901496887207031
grad AddEdge W: 1.0771724766496515e-14
grad ChooseDest W: 4.830524921417236
grad AddEdge W: 6.109095536030711e-15
grad ChooseDest W: 5.755302906036377
grad AddEdge W: 4.821027779734147e-15
grad ChooseDest W: 3.5156092643737793
grad AddEdge W: 2.555682419774015e-15
grad ChooseDest W: 3.483426570892334
grad AddEdge W: 9.256716893652969e-14
grad ChooseDest W: 3.148613691329956
grad AddEdge W: 4.487807694908419e-12
grad ChooseDest W: 3.3964738845825195
grad AddEdge W: 2.272319363379703e-14
grad ChooseDest W: 4.806694030761719
grad AddEdge W: 3.956029094261994e-14
grad ChooseDest W: 7.044903755187988
grad AddEdge W: 5.651056964088791e-15
grad ChooseDest W: 2.962808847427368
grad AddEdge W: 1.3609988812419353e-15
grad ChooseDest W: 5.085249423980713
grad AddEdge W: 6.935630709477931e-15
grad ChooseDest W: 3.615593433380127
grad AddEdge W: 2.039180659724714e-14
grad ChooseDest W: 4.683041095733643
grad AddEdge W: 7.410527778169054e-15
grad ChooseDest W: 3.0113277435302734
grad AddEdge W: 7.156161243009745e-15
grad ChooseDest W: 4.388718605041504
grad AddEdge W: 2.8477357191109e-12
grad ChooseDest W: 3.3951351642608643
grad AddEdge W: 8.23491529957851e-14
grad ChooseDest W: 3.626418113708496
grad AddEdge W: 7.430940086351756e-14
grad ChooseDest W: 3.4755160808563232
grad AddEdge W: 1.2012325812868485e-13
grad ChooseDest W: 3.9569880962371826
grad AddEdge W: 9.35309636814971e-14
grad ChooseDest W: 6.883321762084961
=== Epoch 8: Train Loss: 5.4700, Train Log Prob: 0.0132 ===
Total mismatches: 87356
Predicted valid destination but wrong order: 36131
Epoch 8: Validation Loss: 6.2737, Validation Log Prob: 0.0033
Epoch 8: Edge Precision: 0.3706, Recall: 0.3693, F1: 0.3699, Jaccard: 0.2436
Epoch 8: TP: 2.5876879026485327, FP: 4.413314244810308, FN: 4.43378668575519
Epoch 8: Current Learning Rate: 6e-05
[Epoch 8] ‚è±Ô∏è Total: 4656.93s | Current time: 2025-07-14 21:49:28 | üèãÔ∏è Train: 3890.86s | ‚úÖ Val: 766.06s
grad AddEdge W: 4.1543111724100246e-13
grad ChooseDest W: 12.836917877197266
grad AddEdge W: 2.7012657840668744e-15
grad ChooseDest W: 5.959286689758301
grad AddEdge W: 2.8179121735409218e-15
grad ChooseDest W: 4.1252546310424805
grad AddEdge W: 1.4248286795207066e-13
grad ChooseDest W: 5.692403316497803
grad AddEdge W: 2.2110086199102425e-15
grad ChooseDest W: 3.7451982498168945
grad AddEdge W: 6.633008283797072e-15
grad ChooseDest W: 3.530164957046509
grad AddEdge W: 8.937664844332893e-12
grad ChooseDest W: 4.127928733825684
grad AddEdge W: 5.441961798861384e-15
grad ChooseDest W: 3.9306013584136963
grad AddEdge W: 8.60275799164853e-14
grad ChooseDest W: 15.117483139038086
grad AddEdge W: 7.7748483378378e-14
grad ChooseDest W: 3.8711633682250977
grad AddEdge W: 1.2868550168743326e-15
grad ChooseDest W: 3.6443872451782227
grad AddEdge W: 8.305786207462099e-14
grad ChooseDest W: 3.292182683944702
grad AddEdge W: 4.406521619083415e-15
grad ChooseDest W: 5.825841426849365
grad AddEdge W: 2.410944436714163e-14
grad ChooseDest W: 5.277873516082764
grad AddEdge W: 1.841823380618825e-15
grad ChooseDest W: 6.338103294372559
grad AddEdge W: 8.691551439069661e-14
grad ChooseDest W: 5.7457194328308105
grad AddEdge W: 6.787360980192855e-15
grad ChooseDest W: 6.79286003112793
grad AddEdge W: 9.862941423321482e-15
grad ChooseDest W: 7.718907356262207
grad AddEdge W: 2.4071018293972965e-15
grad ChooseDest W: 3.1384570598602295
grad AddEdge W: 5.10804870555953e-15
grad ChooseDest W: 5.346889019012451
grad AddEdge W: 7.139824942072525e-16
grad ChooseDest W: 4.5488505363464355
grad AddEdge W: 8.349331154840905e-14
grad ChooseDest W: 4.9287519454956055
grad AddEdge W: 7.963065792629636e-16
grad ChooseDest W: 1.820399284362793
grad AddEdge W: 3.4473866141170864e-15
grad ChooseDest W: 4.532477378845215
grad AddEdge W: 6.582733913762214e-15
grad ChooseDest W: 3.9547221660614014
grad AddEdge W: 2.977762537280859e-15
grad ChooseDest W: 4.0371994972229
grad AddEdge W: 2.3011752771454628e-15
grad ChooseDest W: 4.753693103790283
grad AddEdge W: 4.105892939553803e-14
grad ChooseDest W: 3.631497621536255
grad AddEdge W: 1.5014929539503355e-14
grad ChooseDest W: 6.20981502532959
grad AddEdge W: 1.684900726968766e-13
grad ChooseDest W: 4.264327049255371
grad AddEdge W: 5.880443121205031e-16
grad ChooseDest W: 3.7006895542144775
grad AddEdge W: 1.823063930177747e-15
grad ChooseDest W: 5.213975429534912
grad AddEdge W: 3.96879775595285e-16
grad ChooseDest W: 3.0129950046539307
grad AddEdge W: 2.8415789093619174e-15
grad ChooseDest W: 6.314782619476318
grad AddEdge W: 2.959555140563154e-15
grad ChooseDest W: 4.309477806091309
grad AddEdge W: 7.709735221116468e-14
grad ChooseDest W: 4.537484169006348
grad AddEdge W: 5.307370889344812e-14
grad ChooseDest W: 5.789742946624756
grad AddEdge W: 1.947614619357335e-15
grad ChooseDest W: 2.6946873664855957
grad AddEdge W: 4.736152198507246e-14
grad ChooseDest W: 1.5411173105239868
grad AddEdge W: 2.616943442409472e-15
grad ChooseDest W: 4.050267696380615
grad AddEdge W: 4.652242065433625e-15
grad ChooseDest W: 4.8943915367126465
grad AddEdge W: 1.0061305104623651e-15
grad ChooseDest W: 3.0266060829162598
grad AddEdge W: 2.0308735111494594e-15
grad ChooseDest W: 5.176984786987305
grad AddEdge W: 1.5261211780455831e-15
grad ChooseDest W: 5.312090873718262
grad AddEdge W: 9.858248521980514e-14
grad ChooseDest W: 5.5521697998046875
grad AddEdge W: 6.016400485447937e-15
grad ChooseDest W: 3.3964667320251465
grad AddEdge W: 1.7674292585034834e-10
grad ChooseDest W: 4.260968208312988
grad AddEdge W: 3.661168200883415e-15
grad ChooseDest W: 7.295840263366699
grad AddEdge W: 1.5062170954927012e-10
grad ChooseDest W: 3.5085763931274414
grad AddEdge W: 2.2787092105272e-15
grad ChooseDest W: 4.139557361602783
grad AddEdge W: 1.5293898146615664e-13
grad ChooseDest W: 5.197516441345215
grad AddEdge W: 4.959259321561313e-15
grad ChooseDest W: 2.416424036026001
grad AddEdge W: 2.2743289913987113e-15
grad ChooseDest W: 3.328725814819336
grad AddEdge W: 1.3092906961856125e-15
grad ChooseDest W: 2.696450710296631
grad AddEdge W: 6.3760669378847e-14
grad ChooseDest W: 3.5965280532836914
grad AddEdge W: 2.7149938588012616e-15
grad ChooseDest W: 4.95208740234375
grad AddEdge W: 6.578002281839733e-16
grad ChooseDest W: 2.805248260498047
grad AddEdge W: 1.7817347532263217e-13
grad ChooseDest W: 3.574673891067505
grad AddEdge W: 3.480831616865798e-13
grad ChooseDest W: 3.370206832885742
grad AddEdge W: 8.615145001469177e-14
grad ChooseDest W: 5.4712138175964355
grad AddEdge W: 8.022944830189671e-13
grad ChooseDest W: 5.436953544616699
grad AddEdge W: 3.0654914336938812e-15
grad ChooseDest W: 4.360278606414795
grad AddEdge W: 4.396679943269267e-15
grad ChooseDest W: 5.860313415527344
grad AddEdge W: 9.227027414763817e-16
grad ChooseDest W: 7.202089309692383
grad AddEdge W: 7.847795831694877e-15
grad ChooseDest W: 3.974515914916992
grad AddEdge W: 1.8144330710210405e-13
grad ChooseDest W: 4.19536828994751
=== Epoch 9: Train Loss: 5.4497, Train Log Prob: 0.0134 ===
Total mismatches: 87045
Predicted valid destination but wrong order: 36192
Epoch 9: Validation Loss: 6.0833, Validation Log Prob: 0.0038
Epoch 9: Edge Precision: 0.3696, Recall: 0.3682, F1: 0.3689, Jaccard: 0.2428
Epoch 9: TP: 2.5802433786685754, FP: 4.417895490336435, FN: 4.441231209735147
Epoch 9: Current Learning Rate: 6e-05
[Epoch 9] ‚è±Ô∏è Total: 4663.34s | Current time: 2025-07-14 23:07:11 | üèãÔ∏è Train: 3914.80s | ‚úÖ Val: 748.54s
grad AddEdge W: 1.8452737168134226e-12
grad ChooseDest W: 7.649014472961426
grad AddEdge W: 1.2046526928047893e-15
grad ChooseDest W: 8.28923511505127
grad AddEdge W: 1.7041398691085329e-15
grad ChooseDest W: 6.82395076751709
grad AddEdge W: 7.310554342877112e-14
grad ChooseDest W: 2.647233247756958
grad AddEdge W: 2.500837512895332e-16
grad ChooseDest W: 3.8536293506622314
grad AddEdge W: 9.568782974366061e-16
grad ChooseDest W: 5.361836910247803
grad AddEdge W: 1.5766618425335638e-13
grad ChooseDest W: 7.59220027923584
grad AddEdge W: 3.862535461016548e-15
grad ChooseDest W: 7.422078609466553
grad AddEdge W: 9.24464146490197e-16
grad ChooseDest W: 4.697296619415283
grad AddEdge W: 5.1139017879283516e-14
grad ChooseDest W: 5.623072624206543
grad AddEdge W: 1.3129096444527565e-15
grad ChooseDest W: 7.904205322265625
grad AddEdge W: 3.702631734200934e-14
grad ChooseDest W: 2.7816264629364014
grad AddEdge W: 2.184059207902163e-15
grad ChooseDest W: 3.9190006256103516
grad AddEdge W: 1.038376419489626e-15
grad ChooseDest W: 3.127095937728882
grad AddEdge W: 6.306392040522635e-14
grad ChooseDest W: 1.58426833152771
grad AddEdge W: 9.911948377957364e-14
grad ChooseDest W: 3.9000446796417236
grad AddEdge W: 5.765455116362589e-14
grad ChooseDest W: 2.7848620414733887
grad AddEdge W: 2.1472761668929344e-15
grad ChooseDest W: 4.396553039550781
grad AddEdge W: 2.447730908206828e-14
grad ChooseDest W: 2.3724536895751953
grad AddEdge W: 1.118490172069477e-15
grad ChooseDest W: 6.966740608215332
grad AddEdge W: 5.599140197169207e-14
grad ChooseDest W: 4.3490376472473145
grad AddEdge W: 9.776097464580104e-16
grad ChooseDest W: 3.8511545658111572
grad AddEdge W: 5.2851708334236344e-14
grad ChooseDest W: 3.0973875522613525
grad AddEdge W: 4.47682120324408e-14
grad ChooseDest W: 6.344771385192871
grad AddEdge W: 1.0268108198695789e-15
grad ChooseDest W: 5.156242847442627
grad AddEdge W: 1.8228088309590884e-12
grad ChooseDest W: 2.732335329055786
grad AddEdge W: 1.1520105480450002e-15
grad ChooseDest W: 3.9030942916870117
grad AddEdge W: 7.777236970749057e-14
grad ChooseDest W: 4.337465763092041
grad AddEdge W: 1.4153727848623858e-15
grad ChooseDest W: 5.70175313949585
grad AddEdge W: 8.567513289526457e-14
grad ChooseDest W: 8.08555793762207
grad AddEdge W: 7.582334011959485e-14
grad ChooseDest W: 5.611834526062012
grad AddEdge W: 7.977032836534268e-16
grad ChooseDest W: 5.119105815887451
grad AddEdge W: 1.3399169723386046e-15
grad ChooseDest W: 4.589075565338135
grad AddEdge W: 1.7160162239411038e-15
grad ChooseDest W: 8.86081314086914
grad AddEdge W: 8.381622888350613e-16
grad ChooseDest W: 3.458951473236084
grad AddEdge W: 2.0992642218601925e-15
grad ChooseDest W: 4.278146266937256
grad AddEdge W: 1.6962858619842358e-15
grad ChooseDest W: 3.5066819190979004
grad AddEdge W: 2.4579657343634436e-15
grad ChooseDest W: 4.474431037902832
grad AddEdge W: 1.0571313171286122e-15
grad ChooseDest W: 4.621557712554932
grad AddEdge W: 1.1063151322438804e-15
grad ChooseDest W: 4.54874849319458
grad AddEdge W: 4.001764384139106e-15
grad ChooseDest W: 4.727895736694336
grad AddEdge W: 9.063070254130765e-14
grad ChooseDest W: 5.485631465911865
grad AddEdge W: 1.6938737239086924e-15
grad ChooseDest W: 5.581380844116211
grad AddEdge W: 1.6161356869876528e-15
grad ChooseDest W: 3.4302141666412354
grad AddEdge W: 1.6283403729664032e-15
grad ChooseDest W: 5.495718479156494
grad AddEdge W: 1.9545564778765574e-15
grad ChooseDest W: 9.4452486038208
grad AddEdge W: 9.979986977071923e-15
grad ChooseDest W: 5.058688163757324
grad AddEdge W: 2.2093215420375487e-15
grad ChooseDest W: 6.634100437164307
grad AddEdge W: 1.3145668644140595e-15
grad ChooseDest W: 7.721557140350342
grad AddEdge W: 3.223448467038468e-16
grad ChooseDest W: 4.858140468597412
grad AddEdge W: 8.593421888624774e-16
grad ChooseDest W: 4.727950096130371
grad AddEdge W: 3.0715362843219615e-15
grad ChooseDest W: 5.210692405700684
grad AddEdge W: 7.621631258514401e-15
grad ChooseDest W: 7.139917850494385
grad AddEdge W: 9.762451806151484e-14
grad ChooseDest W: 4.960425853729248
grad AddEdge W: 1.0564802664295289e-15
grad ChooseDest W: 4.838533401489258
grad AddEdge W: 9.094786555807755e-14
grad ChooseDest W: 3.63217830657959
grad AddEdge W: 3.634843294408885e-12
grad ChooseDest W: 2.377211570739746
grad AddEdge W: 1.6901788941949686e-13
grad ChooseDest W: 5.335994720458984
grad AddEdge W: 3.2206295413900058e-15
grad ChooseDest W: 4.8711934089660645
grad AddEdge W: 1.852200593013874e-15
grad ChooseDest W: 3.6890969276428223
grad AddEdge W: 1.2586609958293812e-15
grad ChooseDest W: 5.912820816040039
grad AddEdge W: 1.5145129345518066e-13
grad ChooseDest W: 6.409040451049805
grad AddEdge W: 2.8378753001516953e-14
grad ChooseDest W: 2.6766982078552246
grad AddEdge W: 2.972555063424434e-14
grad ChooseDest W: 4.571416854858398
grad AddEdge W: 2.487192076812861e-16
grad ChooseDest W: 6.2645039558410645
grad AddEdge W: 7.582581090792573e-11
grad ChooseDest W: 2.859184741973877
=== Epoch 10: Train Loss: 5.4301, Train Log Prob: 0.0137 ===
Total mismatches: 86545
Predicted valid destination but wrong order: 36132
Epoch 10: Validation Loss: 6.1530, Validation Log Prob: 0.0036
Epoch 10: Edge Precision: 0.3664, Recall: 0.3650, F1: 0.3657, Jaccard: 0.2401
Epoch 10: TP: 2.5579098067287043, FP: 4.4380816034359345, FN: 4.463564781675018
Epoch 10: Current Learning Rate: 6e-05
[Epoch 10] ‚è±Ô∏è Total: 4653.49s | Current time: 2025-07-15 00:24:44 | üèãÔ∏è Train: 3898.18s | ‚úÖ Val: 755.32s
grad AddEdge W: 2.391736548396084e-12
grad ChooseDest W: 9.831989288330078
grad AddEdge W: 3.4766238621157e-15
grad ChooseDest W: 4.608865261077881
grad AddEdge W: 1.4917420165406624e-15
grad ChooseDest W: 5.260249137878418
grad AddEdge W: 1.6623326521726656e-14
grad ChooseDest W: 4.7503342628479
grad AddEdge W: 1.064502409593856e-15
grad ChooseDest W: 3.6750457286834717
grad AddEdge W: 8.080682670103e-16
grad ChooseDest W: 4.8788676261901855
grad AddEdge W: 3.57454468246656e-15
grad ChooseDest W: 5.426410675048828
grad AddEdge W: 7.583651995225413e-14
grad ChooseDest W: 3.7198901176452637
grad AddEdge W: 3.2112099437875408e-12
grad ChooseDest W: 3.615536689758301
grad AddEdge W: 8.535512892998958e-16
grad ChooseDest W: 4.0105366706848145
grad AddEdge W: 1.319284308534674e-15
grad ChooseDest W: 6.587529182434082
grad AddEdge W: 1.1869311414801353e-13
grad ChooseDest W: 6.9163103103637695
grad AddEdge W: 8.218270996285845e-16
grad ChooseDest W: 4.585740566253662
grad AddEdge W: 3.289063493132696e-14
grad ChooseDest W: 3.447601556777954
grad AddEdge W: 4.8513873040362254e-14
grad ChooseDest W: 8.081161499023438
grad AddEdge W: 1.0647262380501679e-15
grad ChooseDest W: 3.1315808296203613
grad AddEdge W: 2.4851051993890635e-16
grad ChooseDest W: 5.278419017791748
grad AddEdge W: 1.8961751555035284e-15
grad ChooseDest W: 5.718679428100586
grad AddEdge W: 1.781710934659845e-15
grad ChooseDest W: 5.073448181152344
grad AddEdge W: 4.674405529531481e-15
grad ChooseDest W: 3.3529608249664307
grad AddEdge W: 6.860221483766249e-16
grad ChooseDest W: 5.048697471618652
grad AddEdge W: 1.6996866357399908e-14
grad ChooseDest W: 4.1238861083984375
grad AddEdge W: 6.695562937501223e-15
grad ChooseDest W: 5.1450114250183105
grad AddEdge W: 3.252569248006834e-15
grad ChooseDest W: 4.865891456604004
grad AddEdge W: 3.111574896463962e-15
grad ChooseDest W: 3.7405593395233154
grad AddEdge W: 5.362670959965614e-14
grad ChooseDest W: 5.474512100219727
grad AddEdge W: 2.244405170256789e-16
grad ChooseDest W: 5.035214424133301
grad AddEdge W: 7.021789630323218e-15
grad ChooseDest W: 4.6797003746032715
grad AddEdge W: 2.2345561273735826e-13
grad ChooseDest W: 4.289867877960205
grad AddEdge W: 4.220823113221771e-14
grad ChooseDest W: 5.224151611328125
grad AddEdge W: 1.166616065903514e-12
grad ChooseDest W: 2.0738866329193115
grad AddEdge W: 2.1013919686236953e-15
grad ChooseDest W: 4.465135097503662
grad AddEdge W: 1.1027812642011234e-13
grad ChooseDest W: 6.002044200897217
grad AddEdge W: 4.811269430786186e-16
grad ChooseDest W: 5.131723880767822
grad AddEdge W: 8.963458290149064e-16
grad ChooseDest W: 3.5325427055358887
grad AddEdge W: 7.005832546036483e-14
grad ChooseDest W: 11.413596153259277
grad AddEdge W: 2.0905568555113163e-16
grad ChooseDest W: 3.5539567470550537
grad AddEdge W: 7.7906967054457545e-16
grad ChooseDest W: 5.0699462890625
grad AddEdge W: 1.0585868797209977e-13
grad ChooseDest W: 4.5762553215026855
grad AddEdge W: 1.6766457090362503e-15
grad ChooseDest W: 3.292654037475586
grad AddEdge W: 1.217144302012585e-10
grad ChooseDest W: 1.270928144454956
grad AddEdge W: 6.32792632854727e-14
grad ChooseDest W: 2.1010258197784424
grad AddEdge W: 1.0861682206593764e-14
grad ChooseDest W: 1.613817811012268
grad AddEdge W: 3.442792180356237e-14
grad ChooseDest W: 2.6436455249786377
grad AddEdge W: 1.2885509716523136e-13
grad ChooseDest W: 12.13996410369873
grad AddEdge W: 6.48895159051252e-16
grad ChooseDest W: 6.201385498046875
grad AddEdge W: 2.427148558159964e-16
grad ChooseDest W: 4.720398902893066
grad AddEdge W: 6.406802036595591e-14
grad ChooseDest W: 4.373912334442139
grad AddEdge W: 1.4722223540294239e-15
grad ChooseDest W: 4.2265400886535645
grad AddEdge W: 1.4011395608540798e-15
grad ChooseDest W: 5.032167911529541
grad AddEdge W: 1.2004200691673596e-15
grad ChooseDest W: 13.311369895935059
grad AddEdge W: 1.868533929335779e-15
grad ChooseDest W: 4.611590385437012
grad AddEdge W: 2.3671784137361228e-15
grad ChooseDest W: 6.044632434844971
grad AddEdge W: 1.6111627565543228e-15
grad ChooseDest W: 6.052786350250244
grad AddEdge W: 1.0560992455988655e-13
grad ChooseDest W: 4.038919448852539
grad AddEdge W: 5.556566309536956e-16
grad ChooseDest W: 3.96260404586792
grad AddEdge W: 9.547300101241324e-16
grad ChooseDest W: 3.9664106369018555
grad AddEdge W: 1.3370759179543953e-15
grad ChooseDest W: 4.265894412994385
grad AddEdge W: 9.770746333935825e-16
grad ChooseDest W: 5.521669387817383
grad AddEdge W: 2.972341611121726e-14
grad ChooseDest W: 3.9702768325805664
grad AddEdge W: 1.2846007445643337e-15
grad ChooseDest W: 4.520345211029053
grad AddEdge W: 1.3466025610396355e-14
grad ChooseDest W: 3.982478380203247
grad AddEdge W: 3.955665886534211e-14
grad ChooseDest W: 3.710045337677002
grad AddEdge W: 4.481471414124506e-14
grad ChooseDest W: 4.930318355560303
grad AddEdge W: 1.120438136089925e-15
grad ChooseDest W: 2.9736976623535156
grad AddEdge W: 1.0251897258634762e-14
grad ChooseDest W: 3.6748552322387695
=== Epoch 11: Train Loss: 5.4061, Train Log Prob: 0.0139 ===
Total mismatches: 86042
Predicted valid destination but wrong order: 36288
Epoch 11: Validation Loss: 5.9639, Validation Log Prob: 0.0043
Epoch 11: Edge Precision: 0.3716, Recall: 0.3701, F1: 0.3708, Jaccard: 0.2442
Epoch 11: TP: 2.593128131710809, FP: 4.405010737294202, FN: 4.428346456692913
Epoch 11: Current Learning Rate: 6e-05
[Epoch 11] ‚è±Ô∏è Total: 4644.73s | Current time: 2025-07-15 01:42:09 | üèãÔ∏è Train: 3888.94s | ‚úÖ Val: 755.80s
grad AddEdge W: 1.8010491101977388e-12
grad ChooseDest W: 9.208602905273438
grad AddEdge W: 2.480560549989689e-14
grad ChooseDest W: 5.292344093322754
grad AddEdge W: 4.24045742164242e-15
grad ChooseDest W: 6.102294445037842
grad AddEdge W: 2.603762339200774e-16
grad ChooseDest W: 5.044991970062256
grad AddEdge W: 8.425693483200662e-16
grad ChooseDest W: 5.991306781768799
grad AddEdge W: 9.361267313827046e-16
grad ChooseDest W: 6.146933555603027
grad AddEdge W: 5.045566473172369e-15
grad ChooseDest W: 4.734599590301514
grad AddEdge W: 1.9064388654836456e-15
grad ChooseDest W: 8.595619201660156
grad AddEdge W: 9.127628098403525e-16
grad ChooseDest W: 4.511880874633789
grad AddEdge W: 2.038318507239351e-15
grad ChooseDest W: 5.158249378204346
grad AddEdge W: 5.980882170508078e-16
grad ChooseDest W: 8.650723457336426
grad AddEdge W: 2.6956563083736828e-15
grad ChooseDest W: 3.9225549697875977
grad AddEdge W: 1.0889417453418659e-15
grad ChooseDest W: 8.0828275680542
grad AddEdge W: 2.7088056478468585e-15
grad ChooseDest W: 4.502674579620361
grad AddEdge W: 3.767005560565903e-14
grad ChooseDest W: 5.016236782073975
grad AddEdge W: 7.66829568000718e-16
grad ChooseDest W: 5.373697757720947
grad AddEdge W: 1.751016789991954e-15
grad ChooseDest W: 5.057497978210449
grad AddEdge W: 1.6540670135057825e-16
grad ChooseDest W: 4.068286418914795
grad AddEdge W: 9.489473585448747e-15
grad ChooseDest W: 4.8507537841796875
grad AddEdge W: 6.44546265318735e-14
grad ChooseDest W: 4.257871627807617
grad AddEdge W: 1.246341854523633e-15
grad ChooseDest W: 3.6764256954193115
grad AddEdge W: 6.175965822601123e-15
grad ChooseDest W: 5.63472843170166
grad AddEdge W: 3.30784661814465e-14
grad ChooseDest W: 4.06370210647583
grad AddEdge W: 1.7500512757701037e-12
grad ChooseDest W: 6.543566703796387
grad AddEdge W: 5.17451207750208e-14
grad ChooseDest W: 2.352735996246338
grad AddEdge W: 2.7042026590532418e-14
grad ChooseDest W: 3.868889570236206
grad AddEdge W: 3.909114988632188e-14
grad ChooseDest W: 4.07301139831543
grad AddEdge W: 1.5777855503227092e-14
grad ChooseDest W: 3.70576810836792
grad AddEdge W: 1.211849902111456e-13
grad ChooseDest W: 3.7311017513275146
grad AddEdge W: 1.6154951183212918e-16
grad ChooseDest W: 5.871775150299072
grad AddEdge W: 6.303715188726855e-11
grad ChooseDest W: 2.756775379180908
grad AddEdge W: 6.371141271889827e-15
grad ChooseDest W: 4.788545608520508
grad AddEdge W: 6.709876841397755e-16
grad ChooseDest W: 6.502960205078125
grad AddEdge W: 8.755403493139122e-15
grad ChooseDest W: 5.9045867919921875
grad AddEdge W: 4.0280993266835115e-16
grad ChooseDest W: 7.168678283691406
grad AddEdge W: 8.680125239896909e-16
grad ChooseDest W: 2.742093086242676
grad AddEdge W: 5.285904872175725e-16
grad ChooseDest W: 5.7439351081848145
grad AddEdge W: 7.20881365804402e-16
grad ChooseDest W: 6.023269176483154
grad AddEdge W: 2.97653273014474e-14
grad ChooseDest W: 8.789690017700195
grad AddEdge W: 4.320973239586509e-14
grad ChooseDest W: 1.326863169670105
grad AddEdge W: 1.0207134955504162e-14
grad ChooseDest W: 3.954529047012329
grad AddEdge W: 1.3369008150683741e-14
grad ChooseDest W: 3.280229091644287
grad AddEdge W: 2.3394597369932586e-16
grad ChooseDest W: 3.104936122894287
grad AddEdge W: 2.541365995954465e-14
grad ChooseDest W: 4.0558977127075195
grad AddEdge W: 1.6125073281844648e-12
grad ChooseDest W: 3.945425033569336
grad AddEdge W: 1.7617401000492514e-14
grad ChooseDest W: 3.7896745204925537
grad AddEdge W: 6.440269430660263e-16
grad ChooseDest W: 2.9544034004211426
grad AddEdge W: 6.584676901464097e-16
grad ChooseDest W: 4.991766452789307
grad AddEdge W: 2.7106335449470333e-16
grad ChooseDest W: 6.805169582366943
grad AddEdge W: 1.364395385951636e-13
grad ChooseDest W: 4.515869140625
grad AddEdge W: 4.355673172468555e-16
grad ChooseDest W: 6.190051555633545
grad AddEdge W: 3.8700369542040217e-14
grad ChooseDest W: 5.9825944900512695
grad AddEdge W: 6.603108867791942e-16
grad ChooseDest W: 5.819218635559082
grad AddEdge W: 1.7119597302171438e-16
grad ChooseDest W: 5.067326068878174
grad AddEdge W: 1.0205353645216086e-15
grad ChooseDest W: 3.984262466430664
grad AddEdge W: 4.2910964393610713e-16
grad ChooseDest W: 5.07149076461792
grad AddEdge W: 7.475481864454539e-16
grad ChooseDest W: 3.288651943206787
grad AddEdge W: 3.500408917851515e-16
grad ChooseDest W: 5.057334899902344
grad AddEdge W: 6.372857254761846e-16
grad ChooseDest W: 4.945673942565918
grad AddEdge W: 8.421477376705704e-16
grad ChooseDest W: 4.744827747344971
grad AddEdge W: 2.125244575237054e-16
grad ChooseDest W: 5.06458854675293
grad AddEdge W: 6.78360362674213e-14
grad ChooseDest W: 7.783443927764893
grad AddEdge W: 4.2373601397916664e-16
grad ChooseDest W: 4.050942897796631
grad AddEdge W: 6.771169098373142e-15
grad ChooseDest W: 4.1087260246276855
grad AddEdge W: 1.4569705305503972e-14
grad ChooseDest W: 4.435391426086426
grad AddEdge W: 1.1298734478476275e-15
grad ChooseDest W: 3.35990047454834
=== Epoch 12: Train Loss: 5.3837, Train Log Prob: 0.0142 ===
Total mismatches: 85447
Predicted valid destination but wrong order: 36337
Epoch 12: Validation Loss: 6.0777, Validation Log Prob: 0.0039
Epoch 12: Edge Precision: 0.3713, Recall: 0.3700, F1: 0.3706, Jaccard: 0.2442
Epoch 12: TP: 2.5924123120973515, FP: 4.408876163206872, FN: 4.429062276306371
Epoch 12: Current Learning Rate: 6e-05
[Epoch 12] ‚è±Ô∏è Total: 4633.35s | Current time: 2025-07-15 02:59:22 | üèãÔ∏è Train: 3879.95s | ‚úÖ Val: 753.41s
grad AddEdge W: 4.628980575585584e-13
grad ChooseDest W: 7.698090553283691
grad AddEdge W: 9.339990110889814e-17
grad ChooseDest W: 4.566054344177246
grad AddEdge W: 2.018436367566247e-16
grad ChooseDest W: 4.955073356628418
grad AddEdge W: 6.928753331341818e-16
grad ChooseDest W: 3.9154698848724365
grad AddEdge W: 7.336424994899393e-16
grad ChooseDest W: 5.611793041229248
grad AddEdge W: 3.155918384153793e-14
grad ChooseDest W: 7.0677666664123535
grad AddEdge W: 3.361308637857822e-16
grad ChooseDest W: 2.825571298599243
grad AddEdge W: 1.017710403763397e-15
grad ChooseDest W: 4.662915229797363
grad AddEdge W: 8.614824823015114e-16
grad ChooseDest W: 4.907466888427734
grad AddEdge W: 1.3085232767121036e-12
grad ChooseDest W: 6.552063465118408
grad AddEdge W: 8.546733432572117e-16
grad ChooseDest W: 5.983393669128418
grad AddEdge W: 5.132341186970309e-15
grad ChooseDest W: 5.431419372558594
grad AddEdge W: 4.280356061589887e-16
grad ChooseDest W: 3.2060868740081787
grad AddEdge W: 4.754437606959393e-14
grad ChooseDest W: 3.4890341758728027
grad AddEdge W: 1.4822596150450485e-16
grad ChooseDest W: 5.577909469604492
grad AddEdge W: 3.7448530973084495e-16
grad ChooseDest W: 5.822623252868652
grad AddEdge W: 2.802783233366718e-14
grad ChooseDest W: 6.914931297302246
grad AddEdge W: 1.225474528711995e-13
grad ChooseDest W: 4.901754856109619
grad AddEdge W: 1.421307055339202e-14
grad ChooseDest W: 3.062584638595581
grad AddEdge W: 3.2411562201964113e-16
grad ChooseDest W: 5.716002941131592
grad AddEdge W: 1.8020380313280536e-15
grad ChooseDest W: 4.085184097290039
grad AddEdge W: 4.899784903169852e-16
grad ChooseDest W: 10.575509071350098
grad AddEdge W: 3.001067113577344e-16
grad ChooseDest W: 3.260460138320923
grad AddEdge W: 1.401701567214583e-15
grad ChooseDest W: 4.939908981323242
grad AddEdge W: 3.625625269048526e-16
grad ChooseDest W: 4.124014377593994
grad AddEdge W: 8.71527191913116e-15
grad ChooseDest W: 9.441433906555176
grad AddEdge W: 7.879691280840212e-16
grad ChooseDest W: 5.140122413635254
grad AddEdge W: 4.383258282703549e-14
grad ChooseDest W: 4.307744026184082
grad AddEdge W: 1.296353568111877e-13
grad ChooseDest W: 1.8760693073272705
grad AddEdge W: 4.1898812961201027e-16
grad ChooseDest W: 11.19279956817627
grad AddEdge W: 6.2365287630331e-14
grad ChooseDest W: 4.859053134918213
grad AddEdge W: 1.9951533914569497e-12
grad ChooseDest W: 1.5884943008422852
grad AddEdge W: 2.636954849898239e-14
grad ChooseDest W: 6.281053066253662
grad AddEdge W: 5.804068277933479e-16
grad ChooseDest W: 4.170110702514648
grad AddEdge W: 2.178622738688448e-15
grad ChooseDest W: 5.058357238769531
grad AddEdge W: 8.536448335010082e-16
grad ChooseDest W: 3.1492769718170166
grad AddEdge W: 1.7732085873418957e-14
grad ChooseDest W: 5.365902900695801
grad AddEdge W: 8.186967305533286e-16
grad ChooseDest W: 5.632388114929199
grad AddEdge W: 2.6990349110420433e-14
grad ChooseDest W: 3.6895692348480225
grad AddEdge W: 2.4308566566448066e-14
grad ChooseDest W: 4.68257474899292
grad AddEdge W: 2.992486087486507e-14
grad ChooseDest W: 4.966246604919434
grad AddEdge W: 3.551155191117112e-16
grad ChooseDest W: 4.8467278480529785
grad AddEdge W: 2.498791398932121e-16
grad ChooseDest W: 7.4151692390441895
grad AddEdge W: 2.046217579057541e-12
grad ChooseDest W: 5.452332019805908
grad AddEdge W: 1.002338386309155e-14
grad ChooseDest W: 3.874811887741089
grad AddEdge W: 4.354328242966993e-16
grad ChooseDest W: 3.3361077308654785
grad AddEdge W: 1.879760927535161e-16
grad ChooseDest W: 6.1189494132995605
grad AddEdge W: 2.536046676479553e-12
grad ChooseDest W: 4.551506996154785
grad AddEdge W: 5.792188111452645e-16
grad ChooseDest W: 5.0069146156311035
grad AddEdge W: 1.8475861692754695e-15
grad ChooseDest W: 4.840066909790039
grad AddEdge W: 4.8616364026980025e-14
grad ChooseDest W: 3.6937010288238525
grad AddEdge W: 3.108035611645507e-14
grad ChooseDest W: 5.7857537269592285
grad AddEdge W: 5.650010454882459e-16
grad ChooseDest W: 5.5501179695129395
grad AddEdge W: 3.528364192072371e-14
grad ChooseDest W: 6.316180229187012
grad AddEdge W: 2.7141200398261844e-09
grad ChooseDest W: 2.5705068111419678
grad AddEdge W: 2.946650763018324e-14
grad ChooseDest W: 4.328238487243652
grad AddEdge W: 1.5399884838206753e-15
grad ChooseDest W: 5.24462890625
grad AddEdge W: 2.3705953446453466e-15
grad ChooseDest W: 3.7953786849975586
grad AddEdge W: 1.1524362879801139e-15
grad ChooseDest W: 2.927769660949707
grad AddEdge W: 6.403297353073031e-14
grad ChooseDest W: 5.0301008224487305
grad AddEdge W: 3.81078174793931e-15
grad ChooseDest W: 4.311096668243408
grad AddEdge W: 8.529994473347596e-15
grad ChooseDest W: 3.583749771118164
grad AddEdge W: 5.76567238031356e-15
grad ChooseDest W: 5.9250898361206055
grad AddEdge W: 5.727630436586474e-16
grad ChooseDest W: 3.284675121307373
grad AddEdge W: 6.576175337651624e-16
grad ChooseDest W: 3.6050844192504883
grad AddEdge W: 3.5964625917130066e-14
grad ChooseDest W: 6.0940775871276855
=== Epoch 13: Train Loss: 5.3552, Train Log Prob: 0.0146 ===
Total mismatches: 84984
Predicted valid destination but wrong order: 36354
Epoch 13: Validation Loss: 5.8391, Validation Log Prob: 0.0049
Epoch 13: Edge Precision: 0.3696, Recall: 0.3679, F1: 0.3687, Jaccard: 0.2426
Epoch 13: TP: 2.5778095919828203, FP: 4.416607015032212, FN: 4.443664996420902
Epoch 13: Current Learning Rate: 6e-05
[Epoch 13] ‚è±Ô∏è Total: 4644.27s | Current time: 2025-07-15 04:16:47 | üèãÔ∏è Train: 3876.61s | ‚úÖ Val: 767.66s
grad AddEdge W: 1.9835077252005305e-14
grad ChooseDest W: 17.20494270324707
grad AddEdge W: 1.464702077886345e-15
grad ChooseDest W: 4.438413619995117
grad AddEdge W: 5.326221099366188e-14
grad ChooseDest W: 2.9502017498016357
grad AddEdge W: 3.0207312481456486e-16
grad ChooseDest W: 3.655242681503296
grad AddEdge W: 3.5716211482491114e-15
grad ChooseDest W: 4.207856178283691
grad AddEdge W: 1.6641009181533536e-14
grad ChooseDest W: 3.597386360168457
grad AddEdge W: 4.749472977448946e-14
grad ChooseDest W: 8.83987808227539
grad AddEdge W: 2.583971583794767e-14
grad ChooseDest W: 4.536301136016846
grad AddEdge W: 3.1437111676641124e-16
grad ChooseDest W: 5.17631196975708
grad AddEdge W: 8.902097105867595e-16
grad ChooseDest W: 4.334392070770264
grad AddEdge W: 1.3388975258349367e-14
grad ChooseDest W: 6.609189510345459
grad AddEdge W: 5.607890153393462e-17
grad ChooseDest W: 4.252093315124512
grad AddEdge W: 7.224013134886906e-16
grad ChooseDest W: 4.393945693969727
grad AddEdge W: 1.1029974100686037e-15
grad ChooseDest W: 6.473632335662842
grad AddEdge W: 2.811288058256038e-16
grad ChooseDest W: 7.723892688751221
grad AddEdge W: 2.0138872394801638e-14
grad ChooseDest W: 4.81346321105957
grad AddEdge W: 2.1487735093854432e-14
grad ChooseDest W: 6.721452713012695
grad AddEdge W: 1.4797994237316162e-14
grad ChooseDest W: 4.5959320068359375
grad AddEdge W: 1.9339315305427762e-12
grad ChooseDest W: 4.414753437042236
grad AddEdge W: 2.4822652567355974e-16
grad ChooseDest W: 3.4709384441375732
grad AddEdge W: 1.10175532095475e-14
grad ChooseDest W: 4.350996494293213
grad AddEdge W: 4.473876090862301e-16
grad ChooseDest W: 5.928832530975342
grad AddEdge W: 8.278149555272264e-15
grad ChooseDest W: 4.650214195251465
grad AddEdge W: 3.00472605238751e-14
grad ChooseDest W: 5.058264255523682
grad AddEdge W: 1.8050209902007076e-16
grad ChooseDest W: 3.9108245372772217
grad AddEdge W: 6.169117772980809e-16
grad ChooseDest W: 6.185575008392334
grad AddEdge W: 5.5573865550672535e-14
grad ChooseDest W: 2.342379093170166
grad AddEdge W: 6.174407493736412e-16
grad ChooseDest W: 5.262339115142822
grad AddEdge W: 1.5329856389292504e-16
grad ChooseDest W: 4.710574626922607
grad AddEdge W: 3.2825112803571224e-16
grad ChooseDest W: 3.9891324043273926
grad AddEdge W: 6.121327538822011e-15
grad ChooseDest W: 2.9532687664031982
grad AddEdge W: 9.623801999454965e-16
grad ChooseDest W: 7.421742916107178
grad AddEdge W: 2.954793798960048e-14
grad ChooseDest W: 4.879759311676025
grad AddEdge W: 7.913977056962698e-16
grad ChooseDest W: 6.41422700881958
grad AddEdge W: 2.601804337839901e-14
grad ChooseDest W: 4.518128395080566
grad AddEdge W: 1.914173229204143e-16
grad ChooseDest W: 3.6797831058502197
grad AddEdge W: 2.7676418015015747e-13
grad ChooseDest W: 4.56081485748291
grad AddEdge W: 1.929785168740695e-14
grad ChooseDest W: 5.434431076049805
grad AddEdge W: 8.878708620369772e-15
grad ChooseDest W: 4.334426403045654
grad AddEdge W: 8.92539898224656e-16
grad ChooseDest W: 4.459477424621582
grad AddEdge W: 1.972998418017357e-14
grad ChooseDest W: 8.256681442260742
grad AddEdge W: 7.510604009379473e-13
grad ChooseDest W: 3.185964822769165
grad AddEdge W: 3.292497364700865e-14
grad ChooseDest W: 5.826956748962402
grad AddEdge W: 3.649564008324709e-16
grad ChooseDest W: 4.03515100479126
grad AddEdge W: 3.2602138897623936e-14
grad ChooseDest W: 3.4140658378601074
grad AddEdge W: 3.801511311144791e-14
grad ChooseDest W: 5.854956150054932
grad AddEdge W: 3.866072162384514e-14
grad ChooseDest W: 5.8453545570373535
grad AddEdge W: 5.231190778947833e-16
grad ChooseDest W: 5.441330909729004
grad AddEdge W: 3.7213044061165657e-14
grad ChooseDest W: 3.675654172897339
grad AddEdge W: 3.144861216060335e-14
grad ChooseDest W: 4.417078018188477
grad AddEdge W: 3.521183724371907e-14
grad ChooseDest W: 2.8309121131896973
grad AddEdge W: 1.7016693704629236e-14
grad ChooseDest W: 5.48218297958374
grad AddEdge W: 2.2803238670829036e-16
grad ChooseDest W: 4.703125476837158
grad AddEdge W: 2.326290778119442e-14
grad ChooseDest W: 7.435093402862549
grad AddEdge W: 3.225910071838131e-14
grad ChooseDest W: 3.652017116546631
grad AddEdge W: 2.531240966443637e-16
grad ChooseDest W: 3.9294400215148926
grad AddEdge W: 3.2697406704904877e-16
grad ChooseDest W: 4.95142126083374
grad AddEdge W: 4.579574685372205e-16
grad ChooseDest W: 5.861362457275391
grad AddEdge W: 4.4207099503455164e-16
grad ChooseDest W: 5.5306901931762695
grad AddEdge W: 5.49079716156281e-13
grad ChooseDest W: 3.143664598464966
grad AddEdge W: 3.2842127577899195e-16
grad ChooseDest W: 3.6066341400146484
grad AddEdge W: 4.691037339088174e-16
grad ChooseDest W: 5.623945713043213
grad AddEdge W: 4.0397576234717236e-14
grad ChooseDest W: 3.8707950115203857
grad AddEdge W: 3.2674280058466875e-16
grad ChooseDest W: 5.506287097930908
grad AddEdge W: 1.187743225847986e-14
grad ChooseDest W: 2.700788974761963
grad AddEdge W: 6.497310746910736e-16
grad ChooseDest W: 4.938721656799316
=== Epoch 14: Train Loss: 5.3265, Train Log Prob: 0.0150 ===
Total mismatches: 84278
Predicted valid destination but wrong order: 36491
Epoch 14: Validation Loss: 5.8328, Validation Log Prob: 0.0049
Epoch 14: Edge Precision: 0.3679, Recall: 0.3663, F1: 0.3670, Jaccard: 0.2412
Epoch 14: TP: 2.5675017895490337, FP: 4.429205440229063, FN: 4.453972798854688
Epoch 14: Current Learning Rate: 6e-05
[Epoch 14] ‚è±Ô∏è Total: 4630.07s | Current time: 2025-07-15 05:33:57 | üèãÔ∏è Train: 3877.04s | ‚úÖ Val: 753.02s
grad AddEdge W: 3.761010600178416e-14
grad ChooseDest W: 8.71578311920166
grad AddEdge W: 2.6058019945377625e-14
grad ChooseDest W: 4.4665069580078125
grad AddEdge W: 1.4296265494196628e-15
grad ChooseDest W: 5.2541117668151855
grad AddEdge W: 5.343954581149904e-14
grad ChooseDest W: 3.9211103916168213
grad AddEdge W: 7.281740653703773e-15
grad ChooseDest W: 4.346322059631348
grad AddEdge W: 2.8485294499688493e-14
grad ChooseDest W: 3.0613338947296143
grad AddEdge W: 6.561552372608462e-16
grad ChooseDest W: 5.261059284210205
grad AddEdge W: 3.8756331314799414e-16
grad ChooseDest W: 3.1816976070404053
grad AddEdge W: 6.42268966418065e-14
grad ChooseDest W: 4.266040802001953
grad AddEdge W: 7.873885928777967e-16
grad ChooseDest W: 3.7864041328430176
grad AddEdge W: 1.0738949673635458e-14
grad ChooseDest W: 2.7317047119140625
grad AddEdge W: 6.497652842342308e-15
grad ChooseDest W: 6.9256510734558105
grad AddEdge W: 3.2753668221448284e-16
grad ChooseDest W: 5.389616966247559
grad AddEdge W: 3.813949809980719e-16
grad ChooseDest W: 5.807586193084717
grad AddEdge W: 1.4747419594101502e-15
grad ChooseDest W: 4.464076995849609
grad AddEdge W: 7.937030117413407e-16
grad ChooseDest W: 3.123814582824707
grad AddEdge W: 2.5022833197857475e-13
grad ChooseDest W: 4.18630838394165
grad AddEdge W: 6.355109267538908e-16
grad ChooseDest W: 5.388169765472412
grad AddEdge W: 1.4568539999926786e-15
grad ChooseDest W: 3.7921690940856934
grad AddEdge W: 2.078180301253612e-15
grad ChooseDest W: 4.729612827301025
grad AddEdge W: 7.682325192591671e-16
grad ChooseDest W: 4.4897613525390625
grad AddEdge W: 6.8184564067195075e-15
grad ChooseDest W: 8.112357139587402
grad AddEdge W: 9.43466547156371e-15
grad ChooseDest W: 6.2059783935546875
grad AddEdge W: 1.3218428088213052e-12
grad ChooseDest W: 4.387890815734863
grad AddEdge W: 4.582890819360705e-16
grad ChooseDest W: 4.482390403747559
grad AddEdge W: 8.516308803200131e-17
grad ChooseDest W: 6.067525863647461
grad AddEdge W: 3.303735014339559e-16
grad ChooseDest W: 3.874462842941284
grad AddEdge W: 3.1177530489021163e-16
grad ChooseDest W: 4.747403621673584
grad AddEdge W: 4.2140288819573427e-16
grad ChooseDest W: 3.47609281539917
grad AddEdge W: 1.3994074843560632e-16
grad ChooseDest W: 4.611164093017578
grad AddEdge W: 6.057306247569689e-15
grad ChooseDest W: 4.16757869720459
grad AddEdge W: 2.806679888500696e-11
grad ChooseDest W: 4.0732598304748535
grad AddEdge W: 6.92256946143127e-15
grad ChooseDest W: 4.486093997955322
grad AddEdge W: 1.5088470634049868e-14
grad ChooseDest W: 4.060561656951904
grad AddEdge W: 7.658888320336737e-16
grad ChooseDest W: 4.600646018981934
grad AddEdge W: 1.9628446952654408e-14
grad ChooseDest W: 5.736379623413086
grad AddEdge W: 2.6255261731541156e-14
grad ChooseDest W: 2.0452945232391357
grad AddEdge W: 2.1790495638845253e-16
grad ChooseDest W: 3.4309725761413574
grad AddEdge W: 5.91488083386243e-16
grad ChooseDest W: 5.075873851776123
grad AddEdge W: 7.912645627048732e-16
grad ChooseDest W: 4.095607757568359
grad AddEdge W: 2.5829427987528555e-16
grad ChooseDest W: 5.877627372741699
grad AddEdge W: 2.384189200232978e-11
grad ChooseDest W: 1.9012550115585327
grad AddEdge W: 1.6410507799663118e-14
grad ChooseDest W: 4.1117262840271
grad AddEdge W: 2.130309885201194e-14
grad ChooseDest W: 4.505537509918213
grad AddEdge W: 8.344795666102856e-13
grad ChooseDest W: 3.4554636478424072
grad AddEdge W: 3.850418284129161e-16
grad ChooseDest W: 5.256691932678223
grad AddEdge W: 8.284588911495528e-16
grad ChooseDest W: 5.444141387939453
grad AddEdge W: 1.3908292905316432e-16
grad ChooseDest W: 6.630334854125977
grad AddEdge W: 1.149635202963103e-16
grad ChooseDest W: 4.465242862701416
grad AddEdge W: 8.033047866490024e-15
grad ChooseDest W: 3.919509172439575
grad AddEdge W: 6.541863621145128e-16
grad ChooseDest W: 6.126675128936768
grad AddEdge W: 5.853055369646747e-16
grad ChooseDest W: 5.07112979888916
grad AddEdge W: 4.8563305883164015e-14
grad ChooseDest W: 5.3441290855407715
grad AddEdge W: 1.2509622074926684e-14
grad ChooseDest W: 6.381686210632324
grad AddEdge W: 1.2163020428074961e-14
grad ChooseDest W: 5.1639838218688965
grad AddEdge W: 2.682670446259327e-15
grad ChooseDest W: 4.420382499694824
grad AddEdge W: 8.431028202581588e-16
grad ChooseDest W: 7.327728748321533
grad AddEdge W: 7.150766659575272e-14
grad ChooseDest W: 4.777671813964844
grad AddEdge W: 7.17291057838787e-16
grad ChooseDest W: 2.045616388320923
grad AddEdge W: 2.153562972482398e-14
grad ChooseDest W: 2.629852056503296
grad AddEdge W: 5.484881911210825e-16
grad ChooseDest W: 6.000304698944092
grad AddEdge W: 1.0770278034222605e-14
grad ChooseDest W: 3.606689929962158
grad AddEdge W: 2.249814269718396e-16
grad ChooseDest W: 3.867804765701294
grad AddEdge W: 2.392531380396865e-16
grad ChooseDest W: 4.51332950592041
grad AddEdge W: 2.1001583710151379e-16
grad ChooseDest W: 4.374888896942139
grad AddEdge W: 7.373016818220778e-16
grad ChooseDest W: 4.019899845123291
=== Epoch 15: Train Loss: 5.2955, Train Log Prob: 0.0154 ===
Total mismatches: 83522
Predicted valid destination but wrong order: 36541
Epoch 15: Validation Loss: 5.7054, Validation Log Prob: 0.0054
Epoch 15: Edge Precision: 0.3726, Recall: 0.3706, F1: 0.3715, Jaccard: 0.2449
Epoch 15: TP: 2.597566213314245, FP: 4.390264853256979, FN: 4.423908375089478
Epoch 15: Current Learning Rate: 6e-05
[Epoch 15] ‚è±Ô∏è Total: 4623.75s | Current time: 2025-07-15 06:51:01 | üèãÔ∏è Train: 3869.94s | ‚úÖ Val: 753.81s
grad AddEdge W: 3.4451865730915354e-14
grad ChooseDest W: 12.96544075012207
grad AddEdge W: 4.635194045062474e-16
grad ChooseDest W: 5.044872283935547
grad AddEdge W: 1.9627951438380264e-15
grad ChooseDest W: 5.022538661956787
grad AddEdge W: 7.941093228582268e-16
grad ChooseDest W: 5.3221282958984375
grad AddEdge W: 3.185290691555846e-16
grad ChooseDest W: 3.30812668800354
grad AddEdge W: 3.0761150056216394e-14
grad ChooseDest W: 2.9866135120391846
grad AddEdge W: 2.9767371509586483e-16
grad ChooseDest W: 4.85530948638916
grad AddEdge W: 1.6459069098501714e-16
grad ChooseDest W: 3.9751267433166504
grad AddEdge W: 2.841252642858547e-16
grad ChooseDest W: 4.431653022766113
grad AddEdge W: 1.4190187958321946e-14
grad ChooseDest W: 7.9776082038879395
grad AddEdge W: 4.4034926292640334e-15
grad ChooseDest W: 6.231873989105225
grad AddEdge W: 1.188645173000002e-16
grad ChooseDest W: 3.7238194942474365
grad AddEdge W: 7.668571283352393e-15
grad ChooseDest W: 3.669684410095215
grad AddEdge W: 5.745353542216587e-16
grad ChooseDest W: 4.749989032745361
grad AddEdge W: 2.8969346723990014e-14
grad ChooseDest W: 4.992147922515869
grad AddEdge W: 4.4139299809983377e-16
grad ChooseDest W: 5.811647891998291
grad AddEdge W: 6.381738792260501e-17
grad ChooseDest W: 3.4551796913146973
grad AddEdge W: 7.541871857932869e-13
grad ChooseDest W: 4.271812438964844
grad AddEdge W: 1.3584505296805615e-16
grad ChooseDest W: 4.1093597412109375
grad AddEdge W: 1.6987185406087501e-15
grad ChooseDest W: 5.11077356338501
grad AddEdge W: 5.748272544807767e-14
grad ChooseDest W: 6.4761810302734375
grad AddEdge W: 2.7398390292101248e-14
grad ChooseDest W: 5.82430362701416
grad AddEdge W: 4.1379293243936483e-16
grad ChooseDest W: 6.824594497680664
grad AddEdge W: 3.1760215041349237e-16
grad ChooseDest W: 6.449396133422852
grad AddEdge W: 3.601773488292291e-14
grad ChooseDest W: 6.115218639373779
grad AddEdge W: 4.768346036184074e-16
grad ChooseDest W: 3.7161238193511963
grad AddEdge W: 1.82944420585294e-15
grad ChooseDest W: 7.047786712646484
grad AddEdge W: 8.119437074417845e-16
grad ChooseDest W: 5.829154968261719
grad AddEdge W: 3.9811488198127976e-16
grad ChooseDest W: 5.736416816711426
grad AddEdge W: 3.5964574248120284e-16
grad ChooseDest W: 4.192223072052002
grad AddEdge W: 1.4169208169828319e-16
grad ChooseDest W: 5.494322299957275
grad AddEdge W: 7.109564673091207e-13
grad ChooseDest W: 2.0126657485961914
grad AddEdge W: 3.870214767595474e-16
grad ChooseDest W: 6.015854358673096
grad AddEdge W: 2.014907405961837e-14
grad ChooseDest W: 5.393712043762207
grad AddEdge W: 4.269525135996641e-14
grad ChooseDest W: 3.425614595413208
grad AddEdge W: 1.2055797915413828e-14
grad ChooseDest W: 6.295614242553711
grad AddEdge W: 7.519696984301214e-16
grad ChooseDest W: 5.123034954071045
grad AddEdge W: 2.8588624578319637e-16
grad ChooseDest W: 5.548544883728027
grad AddEdge W: 2.498860220359085e-16
grad ChooseDest W: 4.859246730804443
grad AddEdge W: 2.3256051896519345e-16
grad ChooseDest W: 6.4288177490234375
grad AddEdge W: 4.705239434635669e-16
grad ChooseDest W: 5.660842418670654
grad AddEdge W: 2.094032480509829e-14
grad ChooseDest W: 3.1821603775024414
grad AddEdge W: 1.0547415195470526e-14
grad ChooseDest W: 5.0256805419921875
grad AddEdge W: 1.080387732402266e-12
grad ChooseDest W: 5.045600891113281
grad AddEdge W: 1.1554594120963355e-14
grad ChooseDest W: 4.900046348571777
grad AddEdge W: 2.35071071681296e-16
grad ChooseDest W: 3.9971466064453125
grad AddEdge W: 7.924805843797752e-16
grad ChooseDest W: 5.934033393859863
grad AddEdge W: 3.866206448870389e-16
grad ChooseDest W: 3.7806100845336914
grad AddEdge W: 8.067909201500169e-15
grad ChooseDest W: 4.89462947845459
grad AddEdge W: 3.0538179432521777e-16
grad ChooseDest W: 9.822510719299316
grad AddEdge W: 3.0114060930227365e-14
grad ChooseDest W: 4.17974853515625
grad AddEdge W: 7.455484898877523e-15
grad ChooseDest W: 4.725151538848877
grad AddEdge W: 1.7897126855059668e-14
grad ChooseDest W: 6.535321235656738
grad AddEdge W: 2.966175444199775e-16
grad ChooseDest W: 5.170726299285889
grad AddEdge W: 2.189682077303833e-16
grad ChooseDest W: 5.704693794250488
grad AddEdge W: 7.168126165724864e-17
grad ChooseDest W: 5.442380428314209
grad AddEdge W: 1.2513072675395562e-15
grad ChooseDest W: 6.272740840911865
grad AddEdge W: 7.445708444600314e-15
grad ChooseDest W: 6.3083295822143555
grad AddEdge W: 1.6243157753268632e-16
grad ChooseDest W: 4.489646911621094
grad AddEdge W: 1.6981364172157496e-16
grad ChooseDest W: 4.895824432373047
grad AddEdge W: 1.681577055445826e-16
grad ChooseDest W: 6.1841044425964355
grad AddEdge W: 4.333940689322174e-16
grad ChooseDest W: 4.624499797821045
grad AddEdge W: 4.494008687589996e-15
grad ChooseDest W: 4.210118770599365
grad AddEdge W: 1.043498004205199e-14
grad ChooseDest W: 5.978753089904785
grad AddEdge W: 7.781162290833223e-15
grad ChooseDest W: 4.884657859802246
grad AddEdge W: 2.662625538617496e-14
grad ChooseDest W: 11.80521011352539
=== Epoch 16: Train Loss: 5.2614, Train Log Prob: 0.0159 ===
Total mismatches: 82818
Predicted valid destination but wrong order: 36717
Epoch 16: Validation Loss: 5.6697, Validation Log Prob: 0.0056
Epoch 16: Edge Precision: 0.3708, Recall: 0.3690, F1: 0.3698, Jaccard: 0.2439
Epoch 16: TP: 2.586113099498926, FP: 4.408017179670723, FN: 4.435361488904796
Epoch 16: Current Learning Rate: 6e-05
[Epoch 16] ‚è±Ô∏è Total: 4637.21s | Current time: 2025-07-15 08:08:18 | üèãÔ∏è Train: 3874.85s | ‚úÖ Val: 762.36s
grad AddEdge W: 9.517598890945852e-16
grad ChooseDest W: 10.954937934875488
grad AddEdge W: 7.544116414774963e-16
grad ChooseDest W: 4.13502836227417
grad AddEdge W: 1.2884528174743858e-14
grad ChooseDest W: 4.092745304107666
grad AddEdge W: 1.7387970945893783e-13
grad ChooseDest W: 5.614013195037842
grad AddEdge W: 5.991017449117567e-16
grad ChooseDest W: 5.410689830780029
grad AddEdge W: 1.7193367254432387e-16
grad ChooseDest W: 5.090389251708984
grad AddEdge W: 5.920236729067038e-16
grad ChooseDest W: 4.785030364990234
grad AddEdge W: 3.2367106207123063e-16
grad ChooseDest W: 4.1859822273254395
grad AddEdge W: 2.939259892333762e-14
grad ChooseDest W: 6.1497416496276855
grad AddEdge W: 2.6688917613063414e-16
grad ChooseDest W: 2.3590734004974365
grad AddEdge W: 1.3818703024578733e-14
grad ChooseDest W: 10.29028606414795
grad AddEdge W: 1.325503224433415e-14
grad ChooseDest W: 6.387534141540527
grad AddEdge W: 6.285319046641074e-16
grad ChooseDest W: 5.283779144287109
grad AddEdge W: 4.521618573538697e-16
grad ChooseDest W: 7.284722805023193
grad AddEdge W: 3.936368305473831e-16
grad ChooseDest W: 3.9191806316375732
grad AddEdge W: 7.83783048907043e-15
grad ChooseDest W: 3.750176429748535
grad AddEdge W: 1.7108492086134047e-13
grad ChooseDest W: 4.2886481285095215
grad AddEdge W: 5.503549458577125e-16
grad ChooseDest W: 5.04935359954834
grad AddEdge W: 1.6856867057811822e-14
grad ChooseDest W: 6.96012020111084
grad AddEdge W: 2.32802982146345e-16
grad ChooseDest W: 6.052527904510498
grad AddEdge W: 5.15123404510058e-16
grad ChooseDest W: 5.281739234924316
grad AddEdge W: 7.787646395807091e-13
grad ChooseDest W: 1.2707772254943848
grad AddEdge W: 5.85445075054823e-15
grad ChooseDest W: 6.406623840332031
grad AddEdge W: 5.051583795229663e-16
grad ChooseDest W: 5.323606014251709
grad AddEdge W: 1.817546526965922e-14
grad ChooseDest W: 5.399398326873779
grad AddEdge W: 4.997577330266567e-13
grad ChooseDest W: 4.503860950469971
grad AddEdge W: 4.479966257753059e-16
grad ChooseDest W: 5.640461444854736
grad AddEdge W: 1.3146557816976975e-14
grad ChooseDest W: 4.648460388183594
grad AddEdge W: 1.5248107121970623e-15
grad ChooseDest W: 3.6387546062469482
grad AddEdge W: 6.036417991573924e-15
grad ChooseDest W: 3.655022621154785
grad AddEdge W: 1.5586351518248262e-14
grad ChooseDest W: 3.4693679809570312
grad AddEdge W: 7.584396999327225e-10
grad ChooseDest W: 3.210608720779419
grad AddEdge W: 2.3307236509335146e-16
grad ChooseDest W: 5.983249664306641
grad AddEdge W: 9.488751066344739e-14
grad ChooseDest W: 3.7865188121795654
grad AddEdge W: 1.1956679813992024e-14
grad ChooseDest W: 3.3523566722869873
grad AddEdge W: 6.27078660824415e-16
grad ChooseDest W: 5.493441104888916
grad AddEdge W: 1.916531305401828e-14
grad ChooseDest W: 6.818070888519287
grad AddEdge W: 9.011132904310562e-15
grad ChooseDest W: 6.305624008178711
grad AddEdge W: 1.9687184452381757e-16
grad ChooseDest W: 7.1132378578186035
grad AddEdge W: 1.653792521891313e-16
grad ChooseDest W: 3.9459333419799805
grad AddEdge W: 1.6061269339246592e-13
grad ChooseDest W: 6.606987476348877
grad AddEdge W: 2.353739050181807e-14
grad ChooseDest W: 5.5167460441589355
grad AddEdge W: 3.964254619015506e-11
grad ChooseDest W: 4.166895389556885
grad AddEdge W: 5.37500708004679e-13
grad ChooseDest W: 3.5395560264587402
grad AddEdge W: 9.457976936355639e-13
grad ChooseDest W: 4.8364176750183105
grad AddEdge W: 7.540020311672807e-14
grad ChooseDest W: 4.313512325286865
grad AddEdge W: 4.17966846105638e-16
grad ChooseDest W: 5.267556190490723
grad AddEdge W: 2.368712019826686e-16
grad ChooseDest W: 5.628224849700928
grad AddEdge W: 1.8426232973583882e-16
grad ChooseDest W: 8.035988807678223
grad AddEdge W: 1.2857217662002452e-16
grad ChooseDest W: 3.740818977355957
grad AddEdge W: 5.487038774731889e-15
grad ChooseDest W: 4.865342140197754
grad AddEdge W: 9.301029998270982e-13
grad ChooseDest W: 6.233491897583008
grad AddEdge W: 1.929995232911614e-14
grad ChooseDest W: 3.9419355392456055
grad AddEdge W: 1.96825363590837e-15
grad ChooseDest W: 5.987829685211182
grad AddEdge W: 2.789599673762473e-14
grad ChooseDest W: 3.7033731937408447
grad AddEdge W: 2.2963231251400817e-13
grad ChooseDest W: 2.0254786014556885
grad AddEdge W: 2.3241139373263164e-13
grad ChooseDest W: 2.800467014312744
grad AddEdge W: 3.616185245670055e-15
grad ChooseDest W: 4.710062503814697
grad AddEdge W: 5.038742775749288e-16
grad ChooseDest W: 3.4361062049865723
grad AddEdge W: 2.9997198017956176e-16
grad ChooseDest W: 6.152212619781494
grad AddEdge W: 2.5273014691455165e-16
grad ChooseDest W: 2.8159780502319336
grad AddEdge W: 2.7846891061298847e-16
grad ChooseDest W: 5.4407548904418945
grad AddEdge W: 7.176630708975729e-13
grad ChooseDest W: 1.74806809425354
grad AddEdge W: 5.96454237555995e-16
grad ChooseDest W: 4.290992736816406
grad AddEdge W: 4.0310949116410355e-16
grad ChooseDest W: 4.730804443359375
grad AddEdge W: 4.999661204958567e-16
grad ChooseDest W: 5.015927791595459
=== Epoch 17: Train Loss: 5.2238, Train Log Prob: 0.0164 ===
Total mismatches: 82140
Predicted valid destination but wrong order: 36760
Epoch 17: Validation Loss: 5.6217, Validation Log Prob: 0.0060
Epoch 17: Edge Precision: 0.3714, Recall: 0.3696, F1: 0.3704, Jaccard: 0.2437
Epoch 17: TP: 2.5904080171796706, FP: 4.402433786685755, FN: 4.431066571224052
Epoch 17: Current Learning Rate: 6e-05
[Epoch 17] ‚è±Ô∏è Total: 4630.53s | Current time: 2025-07-15 09:25:28 | üèãÔ∏è Train: 3878.49s | ‚úÖ Val: 752.04s
grad AddEdge W: 8.5996944428849e-14
grad ChooseDest W: 12.703819274902344
grad AddEdge W: 2.2713246756490835e-13
grad ChooseDest W: 3.7574825286865234
grad AddEdge W: 3.759473743598918e-13
grad ChooseDest W: 3.1854963302612305
grad AddEdge W: 2.1247870450966387e-16
grad ChooseDest W: 4.754883766174316
grad AddEdge W: 8.5417073804675e-13
grad ChooseDest W: 4.151589393615723
grad AddEdge W: 1.755187924331369e-16
grad ChooseDest W: 5.793793678283691
grad AddEdge W: 5.829952016614794e-16
grad ChooseDest W: 5.713399887084961
grad AddEdge W: 1.3488481300861013e-14
grad ChooseDest W: 6.821866989135742
grad AddEdge W: 6.741736397521949e-15
grad ChooseDest W: 4.945616245269775
grad AddEdge W: 3.9779933573864793e-16
grad ChooseDest W: 9.211817741394043
grad AddEdge W: 2.492304714742929e-16
grad ChooseDest W: 5.254537582397461
grad AddEdge W: 9.53007894821667e-10
grad ChooseDest W: 6.250380992889404
grad AddEdge W: 3.965335244083152e-16
grad ChooseDest W: 4.648923873901367
grad AddEdge W: 9.982954292304833e-17
grad ChooseDest W: 5.49099588394165
grad AddEdge W: 4.656974438509934e-16
grad ChooseDest W: 4.39216423034668
grad AddEdge W: 5.346926311542051e-14
grad ChooseDest W: 5.836745262145996
grad AddEdge W: 2.997745156237331e-16
grad ChooseDest W: 1.6688364744186401
grad AddEdge W: 2.6447524658040726e-14
grad ChooseDest W: 5.030350685119629
grad AddEdge W: 6.605790256465594e-16
grad ChooseDest W: 4.805134296417236
grad AddEdge W: 1.0359090125142741e-14
grad ChooseDest W: 4.642632484436035
grad AddEdge W: 5.9219118425993516e-15
grad ChooseDest W: 5.590550899505615
grad AddEdge W: 9.343909109990611e-13
grad ChooseDest W: 5.266221046447754
grad AddEdge W: 1.258768913756092e-12
grad ChooseDest W: 4.717483043670654
grad AddEdge W: 4.930861483213428e-16
grad ChooseDest W: 8.115442276000977
grad AddEdge W: 2.3246278830373925e-14
grad ChooseDest W: 8.885332107543945
grad AddEdge W: 1.1035674632421059e-16
grad ChooseDest W: 5.7492289543151855
grad AddEdge W: 6.222631459586181e-13
grad ChooseDest W: 5.633792400360107
grad AddEdge W: 1.3076379496170621e-16
grad ChooseDest W: 7.453823566436768
grad AddEdge W: 8.369964009227249e-16
grad ChooseDest W: 6.833883762359619
grad AddEdge W: 1.7657672334936772e-14
grad ChooseDest W: 4.5053935050964355
grad AddEdge W: 7.62141272401401e-15
grad ChooseDest W: 7.190540313720703
grad AddEdge W: 5.757102693876304e-13
grad ChooseDest W: 4.27075719833374
grad AddEdge W: 1.2790428438602675e-10
grad ChooseDest W: 0.0
grad AddEdge W: 1.3020925890240218e-14
grad ChooseDest W: 3.650031805038452
grad AddEdge W: 1.1408741823709743e-15
grad ChooseDest W: 4.616392135620117
grad AddEdge W: 1.1054237888855729e-16
grad ChooseDest W: 2.1850574016571045
grad AddEdge W: 3.2121545955879e-14
grad ChooseDest W: 15.927661895751953
grad AddEdge W: 1.7676759693007556e-16
grad ChooseDest W: 4.9731764793396
grad AddEdge W: 1.1654224412620452e-16
grad ChooseDest W: 3.851851463317871
grad AddEdge W: 5.544338330152156e-16
grad ChooseDest W: 4.8195366859436035
grad AddEdge W: 2.476954360156313e-16
grad ChooseDest W: 4.279405117034912
grad AddEdge W: 1.690390110330696e-14
grad ChooseDest W: 3.9550750255584717
grad AddEdge W: 1.0494734828348992e-13
grad ChooseDest W: 2.853829860687256
grad AddEdge W: 4.071487001219837e-16
grad ChooseDest W: 4.2691216468811035
grad AddEdge W: 1.3361158590482417e-16
grad ChooseDest W: 7.208356857299805
grad AddEdge W: 4.340523194113524e-16
grad ChooseDest W: 4.899661540985107
grad AddEdge W: 8.292888775587436e-16
grad ChooseDest W: 5.523411273956299
grad AddEdge W: 5.647858461800841e-16
grad ChooseDest W: 9.070584297180176
grad AddEdge W: 2.6573095598405282e-14
grad ChooseDest W: 3.3303463459014893
grad AddEdge W: 1.3467668854314028e-14
grad ChooseDest W: 6.930415153503418
grad AddEdge W: 3.289039437396994e-14
grad ChooseDest W: 3.68631649017334
grad AddEdge W: 2.5961218373187444e-16
grad ChooseDest W: 6.364253520965576
grad AddEdge W: 3.7568817592528486e-16
grad ChooseDest W: 6.320295810699463
grad AddEdge W: 1.223510504946867e-16
grad ChooseDest W: 8.465149879455566
grad AddEdge W: 2.9322351516406506e-15
grad ChooseDest W: 5.609230041503906
grad AddEdge W: 2.0134254371173207e-14
grad ChooseDest W: 8.794036865234375
grad AddEdge W: 1.2844002624536304e-16
grad ChooseDest W: 5.844603061676025
grad AddEdge W: 1.2452618451642364e-14
grad ChooseDest W: 5.608386039733887
grad AddEdge W: 3.0962119422622554e-16
grad ChooseDest W: 4.914632320404053
grad AddEdge W: 6.537674513825364e-16
grad ChooseDest W: 5.0530686378479
grad AddEdge W: 8.702126920701839e-16
grad ChooseDest W: 5.181308746337891
grad AddEdge W: 1.1593334019838978e-14
grad ChooseDest W: 6.20143461227417
grad AddEdge W: 2.534656097407848e-16
grad ChooseDest W: 5.908874988555908
grad AddEdge W: 1.7288486407431085e-16
grad ChooseDest W: 5.859992980957031
grad AddEdge W: 1.043784830736963e-15
grad ChooseDest W: 6.40697717666626
grad AddEdge W: 3.496880231532813e-16
grad ChooseDest W: 4.6174468994140625
=== Epoch 18: Train Loss: 5.1813, Train Log Prob: 0.0170 ===
Total mismatches: 81138
Predicted valid destination but wrong order: 36920
Epoch 18: Validation Loss: 5.4722, Validation Log Prob: 0.0067
Epoch 18: Edge Precision: 0.3673, Recall: 0.3653, F1: 0.3662, Jaccard: 0.2405
Epoch 18: TP: 2.561059413027917, FP: 4.425196850393701, FN: 4.460415175375806
Epoch 18: Current Learning Rate: 6e-05
[Epoch 18] ‚è±Ô∏è Total: 4636.87s | Current time: 2025-07-15 10:42:45 | üèãÔ∏è Train: 3877.94s | ‚úÖ Val: 758.93s
grad AddEdge W: 1.578780509103872e-12
grad ChooseDest W: 10.413135528564453
grad AddEdge W: 1.1229052254279216e-15
grad ChooseDest W: 4.577032089233398
grad AddEdge W: 2.6309087287207378e-14
grad ChooseDest W: 5.48652458190918
grad AddEdge W: 4.8304300148552595e-14
grad ChooseDest W: 5.767426490783691
grad AddEdge W: 3.1496216047513753e-16
grad ChooseDest W: 6.397132396697998
grad AddEdge W: 3.505865662916405e-16
grad ChooseDest W: 7.123618125915527
grad AddEdge W: 7.181929340517937e-14
grad ChooseDest W: 5.630459308624268
grad AddEdge W: 4.256802430078338e-13
grad ChooseDest W: 5.555667877197266
grad AddEdge W: 3.095955295514402e-13
grad ChooseDest W: 3.987405776977539
grad AddEdge W: 1.2440962431255198e-14
grad ChooseDest W: 7.255805015563965
grad AddEdge W: 1.912131085707872e-16
grad ChooseDest W: 7.541833400726318
grad AddEdge W: 4.4115733765203986e-16
grad ChooseDest W: 5.862249851226807
grad AddEdge W: 4.021100187561231e-16
grad ChooseDest W: 8.347028732299805
grad AddEdge W: 1.401151975180713e-16
grad ChooseDest W: 5.490865707397461
grad AddEdge W: 5.470497174184079e-16
grad ChooseDest W: 5.488126277923584
grad AddEdge W: 2.3937447550938067e-16
grad ChooseDest W: 4.552767753601074
grad AddEdge W: 1.3738744808423822e-14
grad ChooseDest W: 3.535921573638916
grad AddEdge W: 1.3961366242785933e-14
grad ChooseDest W: 5.210859775543213
grad AddEdge W: 3.602001520149604e-16
grad ChooseDest W: 3.9251387119293213
grad AddEdge W: 7.244915896321893e-15
grad ChooseDest W: 4.44858980178833
grad AddEdge W: 2.0707300112078e-15
grad ChooseDest W: 4.50132417678833
grad AddEdge W: 2.6599965177558083e-14
grad ChooseDest W: 3.806485891342163
grad AddEdge W: 3.0942481493136055e-16
grad ChooseDest W: 4.1164164543151855
grad AddEdge W: 2.2823480111290454e-16
grad ChooseDest W: 6.304595470428467
grad AddEdge W: 2.0862641719439703e-14
grad ChooseDest W: 6.503669738769531
grad AddEdge W: 3.086004930550045e-16
grad ChooseDest W: 5.6906843185424805
grad AddEdge W: 7.047551025683214e-17
grad ChooseDest W: 4.388454437255859
grad AddEdge W: 2.040556103588201e-16
grad ChooseDest W: 3.5921003818511963
grad AddEdge W: 1.5268683669841798e-14
grad ChooseDest W: 4.8166704177856445
grad AddEdge W: 2.4378484054046225e-14
grad ChooseDest W: 5.980924606323242
grad AddEdge W: 4.559028313049775e-16
grad ChooseDest W: 5.256507873535156
grad AddEdge W: 1.0946992268077004e-16
grad ChooseDest W: 6.539605140686035
grad AddEdge W: 3.9080951397878704e-16
grad ChooseDest W: 5.353888034820557
grad AddEdge W: 1.2644230009805496e-14
grad ChooseDest W: 7.00930643081665
grad AddEdge W: 1.460083763090339e-16
grad ChooseDest W: 3.5272130966186523
grad AddEdge W: 8.329218548090765e-16
grad ChooseDest W: 5.75615930557251
grad AddEdge W: 1.3402008077852736e-16
grad ChooseDest W: 4.2916154861450195
grad AddEdge W: 3.692307673123325e-16
grad ChooseDest W: 6.255935192108154
grad AddEdge W: 3.275749575157869e-16
grad ChooseDest W: 4.749416828155518
grad AddEdge W: 5.011663284891499e-13
grad ChooseDest W: 3.352849006652832
grad AddEdge W: 1.641299977059394e-16
grad ChooseDest W: 5.767614364624023
grad AddEdge W: 9.511602956470476e-17
grad ChooseDest W: 3.3507280349731445
grad AddEdge W: 1.1351643068037534e-16
grad ChooseDest W: 2.365788459777832
grad AddEdge W: 1.260574549136347e-16
grad ChooseDest W: 7.462122917175293
grad AddEdge W: 1.3252460652306286e-14
grad ChooseDest W: 10.18747329711914
grad AddEdge W: 3.2501369081954987e-14
grad ChooseDest W: 16.136674880981445
grad AddEdge W: 7.878312734718555e-16
grad ChooseDest W: 4.456014156341553
grad AddEdge W: 5.993443669115859e-16
grad ChooseDest W: 9.687464714050293
grad AddEdge W: 7.180518657966262e-13
grad ChooseDest W: 5.647190093994141
grad AddEdge W: 1.7326901998567027e-16
grad ChooseDest W: 7.530338287353516
grad AddEdge W: 2.410702015884659e-14
grad ChooseDest W: 4.419842720031738
grad AddEdge W: 3.206757682812822e-16
grad ChooseDest W: 5.297811508178711
grad AddEdge W: 6.301731580543547e-15
grad ChooseDest W: 6.284078121185303
grad AddEdge W: 3.482813925954675e-16
grad ChooseDest W: 3.6859641075134277
grad AddEdge W: 1.8958777622357474e-14
grad ChooseDest W: 4.190496921539307
grad AddEdge W: 2.6813629441790887e-12
grad ChooseDest W: 5.352108001708984
grad AddEdge W: 1.6344354526482557e-14
grad ChooseDest W: 6.905474662780762
grad AddEdge W: 4.360566640623521e-16
grad ChooseDest W: 5.180575370788574
grad AddEdge W: 1.9080255699520897e-14
grad ChooseDest W: 4.957581996917725
grad AddEdge W: 2.0568683699655427e-16
grad ChooseDest W: 4.837161540985107
grad AddEdge W: 2.4010163635393954e-14
grad ChooseDest W: 6.11944580078125
grad AddEdge W: 3.5301469211406335e-16
grad ChooseDest W: 6.81218147277832
grad AddEdge W: 3.642616400569357e-14
grad ChooseDest W: 5.241903305053711
grad AddEdge W: 1.4742016847387e-16
grad ChooseDest W: 6.612763404846191
grad AddEdge W: 3.363697800164671e-16
grad ChooseDest W: 5.351085662841797
grad AddEdge W: 9.729311540446206e-13
grad ChooseDest W: 5.762407302856445
=== Epoch 19: Train Loss: 5.1404, Train Log Prob: 0.0176 ===
Total mismatches: 80393
Predicted valid destination but wrong order: 37008
Epoch 19: Validation Loss: 5.4861, Validation Log Prob: 0.0066
Epoch 19: Edge Precision: 0.3715, Recall: 0.3696, F1: 0.3705, Jaccard: 0.2444
Epoch 19: TP: 2.5908375089477453, FP: 4.401145311381532, FN: 4.430637079455977
Epoch 19: Current Learning Rate: 6e-05
[Epoch 19] ‚è±Ô∏è Total: 4638.64s | Current time: 2025-07-15 12:00:04 | üèãÔ∏è Train: 3884.61s | ‚úÖ Val: 754.02s
grad AddEdge W: 1.1542423951577205e-14
grad ChooseDest W: 9.981342315673828
grad AddEdge W: 4.3531381616761006e-16
grad ChooseDest W: 8.038972854614258
grad AddEdge W: 4.1793686431567875e-14
grad ChooseDest W: 5.0624566078186035
grad AddEdge W: 2.9925849785830986e-15
grad ChooseDest W: 7.507894992828369
grad AddEdge W: 8.294511902472612e-17
grad ChooseDest W: 5.575418949127197
grad AddEdge W: 7.698191178484928e-16
grad ChooseDest W: 5.467367172241211
grad AddEdge W: 8.301771271292113e-13
grad ChooseDest W: 3.0036978721618652
grad AddEdge W: 1.6006607920880108e-16
grad ChooseDest W: 6.3287224769592285
grad AddEdge W: 2.245960714500686e-14
grad ChooseDest W: 3.8607397079467773
grad AddEdge W: 9.883871607448136e-15
grad ChooseDest W: 5.251137733459473
grad AddEdge W: 1.1834489382119961e-14
grad ChooseDest W: 5.560489654541016
grad AddEdge W: 2.8923692283407717e-16
grad ChooseDest W: 4.001155853271484
grad AddEdge W: 1.0374418245114492e-15
grad ChooseDest W: 5.722588539123535
grad AddEdge W: 1.8200537074321033e-16
grad ChooseDest W: 4.17409610748291
grad AddEdge W: 2.0229559500843387e-16
grad ChooseDest W: 6.060354232788086
grad AddEdge W: 2.0385649144176633e-16
grad ChooseDest W: 5.40061092376709
grad AddEdge W: 1.8589806343588533e-12
grad ChooseDest W: 6.551515579223633
grad AddEdge W: 3.9367345148746204e-17
grad ChooseDest W: 5.403161525726318
grad AddEdge W: 6.1664003854068985e-16
grad ChooseDest W: 4.450685501098633
grad AddEdge W: 5.623090159631941e-16
grad ChooseDest W: 5.909731388092041
grad AddEdge W: 1.280616195701331e-15
grad ChooseDest W: 3.9677693843841553
grad AddEdge W: 1.861627822383194e-12
grad ChooseDest W: 8.470170021057129
grad AddEdge W: 2.693455716776716e-16
grad ChooseDest W: 3.889347791671753
grad AddEdge W: 2.0697350016046605e-14
grad ChooseDest W: 4.635838508605957
grad AddEdge W: 8.062666745183023e-13
grad ChooseDest W: 3.3565874099731445
grad AddEdge W: 4.1484899723613373e-16
grad ChooseDest W: 6.254778861999512
grad AddEdge W: 2.4679752815198253e-16
grad ChooseDest W: 5.071080684661865
grad AddEdge W: 4.700507696834069e-15
grad ChooseDest W: 1.8831270933151245
grad AddEdge W: 4.902781811616356e-16
grad ChooseDest W: 6.150791645050049
grad AddEdge W: 4.116833174748892e-16
grad ChooseDest W: 5.693474292755127
grad AddEdge W: 4.0206097025452113e-16
grad ChooseDest W: 7.176194667816162
grad AddEdge W: 2.237521217027423e-14
grad ChooseDest W: 12.789632797241211
grad AddEdge W: 7.32316310592335e-15
grad ChooseDest W: 4.397272109985352
grad AddEdge W: 2.1465102638201593e-16
grad ChooseDest W: 2.43641996383667
grad AddEdge W: 2.2317859569415642e-14
grad ChooseDest W: 9.683979988098145
grad AddEdge W: 6.800305761209269e-15
grad ChooseDest W: 4.886312961578369
grad AddEdge W: 3.753320218643705e-14
grad ChooseDest W: 8.256778717041016
grad AddEdge W: 3.780221752114441e-16
grad ChooseDest W: 6.936376571655273
grad AddEdge W: 4.1556402539251437e-16
grad ChooseDest W: 5.583837032318115
grad AddEdge W: 6.588976546583478e-15
grad ChooseDest W: 9.3347749710083
grad AddEdge W: 1.3489018743266046e-15
grad ChooseDest W: 5.363077640533447
grad AddEdge W: 3.750424137916042e-10
grad ChooseDest W: 5.377472400665283
grad AddEdge W: 1.3895097720184986e-16
grad ChooseDest W: 7.6462721824646
grad AddEdge W: 6.522954139993268e-15
grad ChooseDest W: 5.380894660949707
grad AddEdge W: 5.1165362668585934e-17
grad ChooseDest W: 4.884951591491699
grad AddEdge W: 3.3611302315433064e-16
grad ChooseDest W: 5.334412097930908
grad AddEdge W: 1.2608178064108866e-16
grad ChooseDest W: 3.7850289344787598
grad AddEdge W: 3.2172200559003883e-13
grad ChooseDest W: 4.089634418487549
grad AddEdge W: 6.022230877921684e-17
grad ChooseDest W: 6.30806827545166
grad AddEdge W: 4.3195035421318164e-16
grad ChooseDest W: 6.478902816772461
grad AddEdge W: 6.040042598021156e-13
grad ChooseDest W: 3.4359629154205322
grad AddEdge W: 5.481348195633998e-16
grad ChooseDest W: 5.3010406494140625
grad AddEdge W: 4.368435841401309e-16
grad ChooseDest W: 7.538914680480957
grad AddEdge W: 1.7534755942889352e-16
grad ChooseDest W: 4.666079044342041
grad AddEdge W: 5.017424544653674e-16
grad ChooseDest W: 6.35092830657959
grad AddEdge W: 8.09878609352643e-15
grad ChooseDest W: 3.030782699584961
grad AddEdge W: 2.640265488760494e-16
grad ChooseDest W: 5.491600036621094
grad AddEdge W: 3.017588755911335e-15
grad ChooseDest W: 6.182257175445557
grad AddEdge W: 1.138074129675368e-16
grad ChooseDest W: 5.9795026779174805
grad AddEdge W: 5.56073556452202e-17
grad ChooseDest W: 4.11236572265625
grad AddEdge W: 6.755416509015698e-16
grad ChooseDest W: 7.410630226135254
grad AddEdge W: 1.1797612955728298e-14
grad ChooseDest W: 5.477710723876953
grad AddEdge W: 6.216330860065171e-16
grad ChooseDest W: 6.173842430114746
grad AddEdge W: 7.885513531209754e-13
grad ChooseDest W: 7.3155999183654785
grad AddEdge W: 2.712482194354416e-16
grad ChooseDest W: 7.235755443572998
grad AddEdge W: 1.6057027398246743e-14
grad ChooseDest W: 4.887283802032471
=== Epoch 20: Train Loss: 5.0888, Train Log Prob: 0.0185 ===
Total mismatches: 79358
Predicted valid destination but wrong order: 36972
Epoch 20: Validation Loss: 5.2805, Validation Log Prob: 0.0080
Epoch 20: Edge Precision: 0.3686, Recall: 0.3665, F1: 0.3675, Jaccard: 0.2419
Epoch 20: TP: 2.5695060844667146, FP: 4.41746599856836, FN: 4.451968503937008
Epoch 20: Current Learning Rate: 6e-05
[Epoch 20] ‚è±Ô∏è Total: 4637.87s | Current time: 2025-07-15 13:17:22 | üèãÔ∏è Train: 3883.26s | ‚úÖ Val: 754.61s
grad AddEdge W: 2.8505870624043195e-14
grad ChooseDest W: 12.788582801818848
grad AddEdge W: 2.95432995313022e-16
grad ChooseDest W: 5.371034622192383
grad AddEdge W: 2.6213170970326195e-14
grad ChooseDest W: 7.9356231689453125
grad AddEdge W: 2.9986791688803565e-15
grad ChooseDest W: 4.504756927490234
grad AddEdge W: 4.3917319325452453e-13
grad ChooseDest W: 3.7421655654907227
grad AddEdge W: 8.468199184680659e-15
grad ChooseDest W: 8.459222793579102
grad AddEdge W: 2.194065472805874e-16
grad ChooseDest W: 6.103122711181641
grad AddEdge W: 1.3346024493995147e-16
grad ChooseDest W: 5.239793300628662
grad AddEdge W: 7.870826657530722e-15
grad ChooseDest W: 5.198321342468262
grad AddEdge W: 7.643814470270627e-10
grad ChooseDest W: 2.247227430343628
grad AddEdge W: 7.346273738144694e-17
grad ChooseDest W: 4.674310684204102
grad AddEdge W: 1.8648170640599322e-14
grad ChooseDest W: 8.956485748291016
grad AddEdge W: 1.8057093368192498e-16
grad ChooseDest W: 5.630982398986816
grad AddEdge W: 2.3372272757816515e-16
grad ChooseDest W: 7.363994121551514
grad AddEdge W: 2.558144585733006e-16
grad ChooseDest W: 4.907430648803711
grad AddEdge W: 9.308783670107512e-15
grad ChooseDest W: 4.839812755584717
grad AddEdge W: 2.54934814857577e-16
grad ChooseDest W: 3.7838637828826904
grad AddEdge W: 2.8272421885153687e-16
grad ChooseDest W: 8.515950202941895
grad AddEdge W: 2.0800054454967083e-14
grad ChooseDest W: 3.9228179454803467
grad AddEdge W: 9.534406142201746e-15
grad ChooseDest W: 4.524331092834473
grad AddEdge W: 2.2058086846470483e-15
grad ChooseDest W: 5.195700168609619
grad AddEdge W: 3.693587486967067e-16
grad ChooseDest W: 7.468047618865967
grad AddEdge W: 1.845986335796343e-14
grad ChooseDest W: 4.765340805053711
grad AddEdge W: 1.965986645398668e-13
grad ChooseDest W: 2.6993391513824463
grad AddEdge W: 2.176389324564775e-14
grad ChooseDest W: 4.663573265075684
grad AddEdge W: 1.2031477058399315e-14
grad ChooseDest W: 7.488559722900391
grad AddEdge W: 4.748287700423051e-17
grad ChooseDest W: 5.699228286743164
grad AddEdge W: 3.3034642284942336e-16
grad ChooseDest W: 7.220159530639648
grad AddEdge W: 4.3954255382963603e-13
grad ChooseDest W: 3.3757166862487793
grad AddEdge W: 9.550857639619792e-17
grad ChooseDest W: 5.865005970001221
grad AddEdge W: 3.211116810820924e-14
grad ChooseDest W: 8.51209831237793
grad AddEdge W: 1.48231467218662e-16
grad ChooseDest W: 6.354894638061523
grad AddEdge W: 7.413579108482419e-17
grad ChooseDest W: 4.321110725402832
grad AddEdge W: 1.9100906627472753e-16
grad ChooseDest W: 6.99363374710083
grad AddEdge W: 1.5299061659462126e-14
grad ChooseDest W: 4.990838050842285
grad AddEdge W: 1.1902172132105467e-16
grad ChooseDest W: 5.765131950378418
grad AddEdge W: 2.7639860327122135e-16
grad ChooseDest W: 5.950405597686768
grad AddEdge W: 3.265550769077335e-16
grad ChooseDest W: 5.9488654136657715
grad AddEdge W: 1.7726351460366045e-14
grad ChooseDest W: 8.977566719055176
grad AddEdge W: 2.1408966853111294e-16
grad ChooseDest W: 8.247159004211426
grad AddEdge W: 5.3055551895190775e-15
grad ChooseDest W: 7.5346221923828125
grad AddEdge W: 1.9729390515956663e-16
grad ChooseDest W: 6.391732215881348
grad AddEdge W: 1.2790694711879973e-14
grad ChooseDest W: 4.510369300842285
grad AddEdge W: 4.6252886300190456e-15
grad ChooseDest W: 4.5657758712768555
grad AddEdge W: 1.351377878686371e-14
grad ChooseDest W: 2.553950786590576
grad AddEdge W: 1.9668570903365844e-16
grad ChooseDest W: 6.692098140716553
grad AddEdge W: 1.2750601254354638e-14
grad ChooseDest W: 8.335921287536621
grad AddEdge W: 3.149048133799931e-13
grad ChooseDest W: 3.101841926574707
grad AddEdge W: 2.263120416165932e-14
grad ChooseDest W: 3.3106634616851807
grad AddEdge W: 1.1767986284332184e-14
grad ChooseDest W: 5.575664520263672
grad AddEdge W: 1.3459566613596626e-16
grad ChooseDest W: 5.499466896057129
grad AddEdge W: 3.2872877520862486e-16
grad ChooseDest W: 4.381706714630127
grad AddEdge W: 4.775451054424762e-16
grad ChooseDest W: 4.039931774139404
grad AddEdge W: 5.658190675570567e-17
grad ChooseDest W: 5.07801628112793
grad AddEdge W: 5.019948173440899e-16
grad ChooseDest W: 4.810938835144043
grad AddEdge W: 3.6342786419174544e-11
grad ChooseDest W: 3.8265087604522705
grad AddEdge W: 1.6875238884800413e-16
grad ChooseDest W: 5.179076194763184
grad AddEdge W: 6.56267363247239e-15
grad ChooseDest W: 3.884384870529175
grad AddEdge W: 3.3586973941001144e-16
grad ChooseDest W: 4.178955554962158
grad AddEdge W: 8.289156536663597e-16
grad ChooseDest W: 5.311868667602539
grad AddEdge W: 6.015746576012657e-16
grad ChooseDest W: 7.783406734466553
grad AddEdge W: 2.2981491460622784e-16
grad ChooseDest W: 5.129403591156006
grad AddEdge W: 8.37228794999716e-15
grad ChooseDest W: 1.909129023551941
grad AddEdge W: 3.916260537399402e-16
grad ChooseDest W: 5.172750473022461
grad AddEdge W: 7.34749412733323e-15
grad ChooseDest W: 8.640253067016602
grad AddEdge W: 9.535869815134601e-15
grad ChooseDest W: 4.746539115905762
=== Epoch 21: Train Loss: 5.0397, Train Log Prob: 0.0193 ===
Total mismatches: 78436
Predicted valid destination but wrong order: 37274
Epoch 21: Validation Loss: 5.2971, Validation Log Prob: 0.0078
Epoch 21: Edge Precision: 0.3695, Recall: 0.3673, F1: 0.3683, Jaccard: 0.2424
Epoch 21: TP: 2.5739441660701505, FP: 4.41259842519685, FN: 4.447530422333572
Epoch 21: Current Learning Rate: 6e-05
[Epoch 21] ‚è±Ô∏è Total: 4635.29s | Current time: 2025-07-15 14:34:37 | üèãÔ∏è Train: 3867.24s | ‚úÖ Val: 768.06s
grad AddEdge W: 1.8878580990144833e-11
grad ChooseDest W: 13.891703605651855
grad AddEdge W: 1.7112562587665373e-14
grad ChooseDest W: 7.0006513595581055
grad AddEdge W: 1.0902632014428773e-14
grad ChooseDest W: 5.144212245941162
grad AddEdge W: 2.7869933004442125e-16
grad ChooseDest W: 4.887532711029053
grad AddEdge W: 6.30668502921909e-15
grad ChooseDest W: 5.844608306884766
grad AddEdge W: 2.3790404325360615e-13
grad ChooseDest W: 2.7582502365112305
grad AddEdge W: 5.540196868435675e-16
grad ChooseDest W: 6.856400489807129
grad AddEdge W: 5.830975338294196e-16
grad ChooseDest W: 8.893147468566895
grad AddEdge W: 6.032893487480399e-15
grad ChooseDest W: 5.955333709716797
grad AddEdge W: 3.00237392659628e-16
grad ChooseDest W: 4.258594036102295
grad AddEdge W: 1.646400835937539e-16
grad ChooseDest W: 6.190267086029053
grad AddEdge W: 1.2949417674767115e-14
grad ChooseDest W: 7.117987632751465
grad AddEdge W: 9.16320581152881e-15
grad ChooseDest W: 3.9064269065856934
grad AddEdge W: 7.229612816761645e-17
grad ChooseDest W: 3.5978446006774902
grad AddEdge W: 9.409193496773879e-15
grad ChooseDest W: 5.814313888549805
grad AddEdge W: 1.0407625510067107e-16
grad ChooseDest W: 5.915409564971924
grad AddEdge W: 1.7127135895402e-16
grad ChooseDest W: 4.619878768920898
grad AddEdge W: 9.386078629387799e-17
grad ChooseDest W: 4.45289421081543
grad AddEdge W: 1.2896075774913775e-14
grad ChooseDest W: 5.41021728515625
grad AddEdge W: 7.88469089281138e-15
grad ChooseDest W: 4.208250999450684
grad AddEdge W: 4.1471841016665553e-14
grad ChooseDest W: 5.198002338409424
grad AddEdge W: 6.361169153001802e-15
grad ChooseDest W: 4.672188758850098
grad AddEdge W: 2.3555895207100263e-15
grad ChooseDest W: 5.122618198394775
grad AddEdge W: 5.4161665618881666e-17
grad ChooseDest W: 5.281039237976074
grad AddEdge W: 4.930494612068149e-16
grad ChooseDest W: 4.7632904052734375
grad AddEdge W: 1.0591711630480137e-14
grad ChooseDest W: 7.573794364929199
grad AddEdge W: 9.180980189122011e-17
grad ChooseDest W: 4.978207588195801
grad AddEdge W: 6.58637268789428e-17
grad ChooseDest W: 3.9288241863250732
grad AddEdge W: 5.3075326011693325e-17
grad ChooseDest W: 5.293726921081543
grad AddEdge W: 4.6705244245671615e-15
grad ChooseDest W: 6.599209308624268
grad AddEdge W: 6.206472984745907e-16
grad ChooseDest W: 6.708279132843018
grad AddEdge W: 1.3581168781086822e-16
grad ChooseDest W: 4.678396224975586
grad AddEdge W: 2.7507262613185395e-16
grad ChooseDest W: 5.6601996421813965
grad AddEdge W: 1.1064982480615826e-12
grad ChooseDest W: 2.4674150943756104
grad AddEdge W: 1.8763204491100034e-14
grad ChooseDest W: 5.501147747039795
grad AddEdge W: 1.749059376260188e-16
grad ChooseDest W: 4.7278337478637695
grad AddEdge W: 3.9556412166996224e-16
grad ChooseDest W: 4.424280166625977
grad AddEdge W: 1.2222248677516126e-16
grad ChooseDest W: 4.068660736083984
grad AddEdge W: 2.276275314292824e-16
grad ChooseDest W: 3.934276819229126
grad AddEdge W: 1.31605623209137e-13
grad ChooseDest W: 3.4332761764526367
grad AddEdge W: 7.328975869523883e-16
grad ChooseDest W: 3.3531439304351807
grad AddEdge W: 1.5477420116616044e-16
grad ChooseDest W: 3.9931583404541016
grad AddEdge W: 4.090578329757561e-16
grad ChooseDest W: 4.988556385040283
grad AddEdge W: 7.320281076320318e-16
grad ChooseDest W: 4.251588344573975
grad AddEdge W: 4.939531858045311e-17
grad ChooseDest W: 4.943019866943359
grad AddEdge W: 2.537835901443488e-14
grad ChooseDest W: 3.6148290634155273
grad AddEdge W: 2.103531812076476e-16
grad ChooseDest W: 6.561225891113281
grad AddEdge W: 1.5078550184171625e-14
grad ChooseDest W: 4.217807292938232
grad AddEdge W: 3.068531170243781e-16
grad ChooseDest W: 9.201997756958008
grad AddEdge W: 5.694721618398869e-16
grad ChooseDest W: 5.328729152679443
grad AddEdge W: 9.106941648007443e-15
grad ChooseDest W: 3.7719457149505615
grad AddEdge W: 2.063629281071408e-16
grad ChooseDest W: 8.54669189453125
grad AddEdge W: 3.090780872883579e-16
grad ChooseDest W: 4.066840648651123
grad AddEdge W: 1.3375476941301923e-14
grad ChooseDest W: 6.002266883850098
grad AddEdge W: 1.0909704951296583e-16
grad ChooseDest W: 7.342161178588867
grad AddEdge W: 1.4613541801623223e-16
grad ChooseDest W: 5.816978931427002
grad AddEdge W: 5.9295098869549e-17
grad ChooseDest W: 3.4396610260009766
grad AddEdge W: 2.0822086840716352e-16
grad ChooseDest W: 8.497678756713867
grad AddEdge W: 6.665411317436048e-16
grad ChooseDest W: 7.414793968200684
grad AddEdge W: 1.9458743636974212e-16
grad ChooseDest W: 5.079147815704346
grad AddEdge W: 4.022854075157639e-16
grad ChooseDest W: 6.842721939086914
grad AddEdge W: 4.015669700517706e-15
grad ChooseDest W: 6.103067874908447
grad AddEdge W: 1.7652443229976457e-16
grad ChooseDest W: 5.909626483917236
grad AddEdge W: 2.5883581302593717e-13
grad ChooseDest W: 2.9330666065216064
grad AddEdge W: 2.7200648756498837e-14
grad ChooseDest W: 4.2120795249938965
grad AddEdge W: 2.2647704892867426e-16
grad ChooseDest W: 7.375417709350586
=== Epoch 22: Train Loss: 4.9858, Train Log Prob: 0.0203 ===
Total mismatches: 77129
Predicted valid destination but wrong order: 37440
Epoch 22: Validation Loss: 5.2251, Validation Log Prob: 0.0084
Epoch 22: Edge Precision: 0.3709, Recall: 0.3692, F1: 0.3700, Jaccard: 0.2442
Epoch 22: TP: 2.5875447387258412, FP: 4.404724409448819, FN: 4.433929849677881
Epoch 22: Current Learning Rate: 6e-05
[Epoch 22] ‚è±Ô∏è Total: 4575.68s | Current time: 2025-07-15 15:50:53 | üèãÔ∏è Train: 3818.20s | ‚úÖ Val: 757.48s
grad AddEdge W: 1.445712107428672e-14
grad ChooseDest W: 19.88508415222168
grad AddEdge W: 1.1321764245520935e-14
grad ChooseDest W: 5.955569744110107
grad AddEdge W: 4.426334990269114e-15
grad ChooseDest W: 5.088634490966797
grad AddEdge W: 9.0929860390476e-17
grad ChooseDest W: 5.661441802978516
grad AddEdge W: 1.9869995338222916e-16
grad ChooseDest W: 4.323890686035156
grad AddEdge W: 6.707451891948884e-15
grad ChooseDest W: 5.0201826095581055
grad AddEdge W: 9.139281128764387e-13
grad ChooseDest W: 2.7090742588043213
grad AddEdge W: 6.609207538893491e-13
grad ChooseDest W: 4.961002826690674
grad AddEdge W: 1.7008380552707968e-16
grad ChooseDest W: 6.922809600830078
grad AddEdge W: 9.807671411782396e-17
grad ChooseDest W: 4.062740802764893
grad AddEdge W: 6.300736634467878e-16
grad ChooseDest W: 4.221169948577881
grad AddEdge W: 8.24659366045966e-17
grad ChooseDest W: 6.061070442199707
grad AddEdge W: 5.392739020230542e-15
grad ChooseDest W: 4.984456539154053
grad AddEdge W: 7.753432769951052e-13
grad ChooseDest W: 4.981051921844482
grad AddEdge W: 5.979432685377089e-16
grad ChooseDest W: 4.670922756195068
grad AddEdge W: 1.6768404207350004e-16
grad ChooseDest W: 4.719298839569092
grad AddEdge W: 5.5509313169762285e-15
grad ChooseDest W: 5.918673992156982
grad AddEdge W: 4.7937605186126105e-15
grad ChooseDest W: 6.527010440826416
grad AddEdge W: 6.265373961832077e-15
grad ChooseDest W: 5.2659831047058105
grad AddEdge W: 6.871152443950566e-15
grad ChooseDest W: 7.933659076690674
grad AddEdge W: 7.0619300981771254e-15
grad ChooseDest W: 7.332838535308838
grad AddEdge W: 1.1754236504378523e-16
grad ChooseDest W: 4.923948764801025
grad AddEdge W: 1.4611678329139264e-15
grad ChooseDest W: 3.879119873046875
grad AddEdge W: 2.6371113815868916e-14
grad ChooseDest W: 3.6107048988342285
grad AddEdge W: 2.423481434893945e-16
grad ChooseDest W: 5.295769691467285
grad AddEdge W: 2.9741634942879753e-16
grad ChooseDest W: 5.11477518081665
grad AddEdge W: 1.4556417370264801e-16
grad ChooseDest W: 4.7950544357299805
grad AddEdge W: 2.4227850149926243e-16
grad ChooseDest W: 4.8070149421691895
grad AddEdge W: 1.9478655952196063e-14
grad ChooseDest W: 5.206274509429932
grad AddEdge W: 9.902840195617847e-17
grad ChooseDest W: 6.264251232147217
grad AddEdge W: 1.5959965522243958e-16
grad ChooseDest W: 6.677276611328125
grad AddEdge W: 3.315281396899615e-16
grad ChooseDest W: 4.502285957336426
grad AddEdge W: 3.062357094151685e-16
grad ChooseDest W: 4.071704864501953
grad AddEdge W: 2.509875513261213e-15
grad ChooseDest W: 4.182283401489258
grad AddEdge W: 7.752834994447977e-17
grad ChooseDest W: 5.938883304595947
grad AddEdge W: 4.7833727315406104e-17
grad ChooseDest W: 9.011330604553223
grad AddEdge W: 1.9243969211126671e-13
grad ChooseDest W: 4.400864124298096
grad AddEdge W: 8.341990084399686e-17
grad ChooseDest W: 4.987346172332764
grad AddEdge W: 1.0205826289600654e-14
grad ChooseDest W: 6.040460109710693
grad AddEdge W: 2.479822360776157e-16
grad ChooseDest W: 4.071402072906494
grad AddEdge W: 6.047818552118155e-17
grad ChooseDest W: 6.272210121154785
grad AddEdge W: 1.2034024086471709e-14
grad ChooseDest W: 5.2930755615234375
grad AddEdge W: 1.4548550551767177e-16
grad ChooseDest W: 8.01344108581543
grad AddEdge W: 1.7710592041805465e-16
grad ChooseDest W: 6.028146743774414
grad AddEdge W: 1.269797763844057e-14
grad ChooseDest W: 6.171803951263428
grad AddEdge W: 3.427870869133229e-16
grad ChooseDest W: 5.509578704833984
grad AddEdge W: 3.0855303274017866e-16
grad ChooseDest W: 3.9826161861419678
grad AddEdge W: 7.274238879936636e-17
grad ChooseDest W: 3.772429943084717
grad AddEdge W: 7.137873192873594e-17
grad ChooseDest W: 6.556937217712402
grad AddEdge W: 8.055422241791746e-15
grad ChooseDest W: 5.291645050048828
grad AddEdge W: 7.165710136591719e-17
grad ChooseDest W: 4.927933692932129
grad AddEdge W: 2.4424378668173737e-17
grad ChooseDest W: 7.670769691467285
grad AddEdge W: 1.7524336114149144e-16
grad ChooseDest W: 4.023636341094971
grad AddEdge W: 6.132796364927834e-14
grad ChooseDest W: 2.424975872039795
grad AddEdge W: 1.905158416365193e-16
grad ChooseDest W: 5.497073173522949
grad AddEdge W: 5.1219172421793733e-17
grad ChooseDest W: 2.5113630294799805
grad AddEdge W: 2.008017994663917e-16
grad ChooseDest W: 4.3173346519470215
grad AddEdge W: 4.459725876082826e-16
grad ChooseDest W: 4.219594478607178
grad AddEdge W: 1.4753956041476345e-16
grad ChooseDest W: 5.564467430114746
grad AddEdge W: 6.024723007671184e-15
grad ChooseDest W: 6.713111877441406
grad AddEdge W: 4.886906296602443e-16
grad ChooseDest W: 6.383539199829102
grad AddEdge W: 3.4023097970652583e-16
grad ChooseDest W: 3.8820252418518066
grad AddEdge W: 4.95078899208877e-15
grad ChooseDest W: 7.01511812210083
grad AddEdge W: 2.7243203352955714e-13
grad ChooseDest W: 7.325882434844971
grad AddEdge W: 2.377971519308274e-15
grad ChooseDest W: 4.271860122680664
grad AddEdge W: 1.1482231725702506e-16
grad ChooseDest W: 4.335703372955322
=== Epoch 23: Train Loss: 4.9368, Train Log Prob: 0.0212 ===
Total mismatches: 76277
Predicted valid destination but wrong order: 37553
Epoch 23: Validation Loss: 5.0754, Validation Log Prob: 0.0095
Epoch 23: Edge Precision: 0.3697, Recall: 0.3676, F1: 0.3686, Jaccard: 0.2428
Epoch 23: TP: 2.5769506084466713, FP: 4.409878310665713, FN: 4.444523979957051
Epoch 23: Current Learning Rate: 6e-05
[Epoch 23] ‚è±Ô∏è Total: 4598.72s | Current time: 2025-07-15 17:07:31 | üèãÔ∏è Train: 3832.59s | ‚úÖ Val: 766.13s
grad AddEdge W: 1.3215277599987717e-14
grad ChooseDest W: 10.572029113769531
grad AddEdge W: 7.915384587493018e-17
grad ChooseDest W: 4.880730628967285
grad AddEdge W: 1.0037955370883166e-14
grad ChooseDest W: 6.114138126373291
grad AddEdge W: 1.9279915934155428e-13
grad ChooseDest W: 3.429262638092041
grad AddEdge W: 9.372481394773983e-15
grad ChooseDest W: 2.189864158630371
grad AddEdge W: 1.988359529922403e-14
grad ChooseDest W: 5.780168533325195
grad AddEdge W: 1.9240708091928095e-16
grad ChooseDest W: 5.762185573577881
grad AddEdge W: 1.5444989342648045e-16
grad ChooseDest W: 6.866769790649414
grad AddEdge W: 8.194687084405223e-17
grad ChooseDest W: 4.684170246124268
grad AddEdge W: 1.8630762313588233e-16
grad ChooseDest W: 5.154429912567139
grad AddEdge W: 8.418995040774657e-15
grad ChooseDest W: 2.783402681350708
grad AddEdge W: 1.4237941134789302e-14
grad ChooseDest W: 4.21497106552124
grad AddEdge W: 6.354684692274097e-16
grad ChooseDest W: 5.070462226867676
grad AddEdge W: 8.954255938921636e-17
grad ChooseDest W: 7.7966742515563965
grad AddEdge W: 9.459151102956243e-17
grad ChooseDest W: 4.260387420654297
grad AddEdge W: 3.254187927969037e-15
grad ChooseDest W: 3.3042068481445312
grad AddEdge W: 1.1832252368106262e-14
grad ChooseDest W: 5.577390670776367
grad AddEdge W: 6.295354004785873e-17
grad ChooseDest W: 5.793693542480469
grad AddEdge W: 7.90956083893395e-16
grad ChooseDest W: 5.965461730957031
grad AddEdge W: 7.049696824836405e-15
grad ChooseDest W: 5.283265590667725
grad AddEdge W: 3.285668330970217e-16
grad ChooseDest W: 5.16806173324585
grad AddEdge W: 1.3791336184146335e-16
grad ChooseDest W: 4.231846809387207
grad AddEdge W: 4.881448558920818e-17
grad ChooseDest W: 5.6768975257873535
grad AddEdge W: 1.086076741101073e-14
grad ChooseDest W: 7.010799407958984
grad AddEdge W: 3.457744662391704e-16
grad ChooseDest W: 5.119416236877441
grad AddEdge W: 8.994032156138445e-15
grad ChooseDest W: 4.112471580505371
grad AddEdge W: 9.246720143046838e-13
grad ChooseDest W: 5.2460832595825195
grad AddEdge W: 9.094014389985126e-17
grad ChooseDest W: 6.984817981719971
grad AddEdge W: 6.637412113968965e-16
grad ChooseDest W: 7.345712184906006
grad AddEdge W: 1.8916519466256312e-16
grad ChooseDest W: 5.090701103210449
grad AddEdge W: 3.3198578841498946e-14
grad ChooseDest W: 4.399202823638916
grad AddEdge W: 6.489653886704912e-15
grad ChooseDest W: 5.452208995819092
grad AddEdge W: 4.3175074560520524e-16
grad ChooseDest W: 4.894535541534424
grad AddEdge W: 6.1773930731172464e-15
grad ChooseDest W: 4.599878311157227
grad AddEdge W: 1.061948160565587e-14
grad ChooseDest W: 4.939600944519043
grad AddEdge W: 2.457579010883463e-16
grad ChooseDest W: 4.92089319229126
grad AddEdge W: 4.552378575018237e-16
grad ChooseDest W: 7.3491530418396
grad AddEdge W: 1.869401375777062e-14
grad ChooseDest W: 3.054184675216675
grad AddEdge W: 5.566893758746354e-16
grad ChooseDest W: 5.866532325744629
grad AddEdge W: 2.4904527235258718e-17
grad ChooseDest W: 4.755198955535889
grad AddEdge W: 1.5599390743438295e-14
grad ChooseDest W: 7.63398551940918
grad AddEdge W: 1.6092738307302984e-14
grad ChooseDest W: 6.766140460968018
grad AddEdge W: 9.41588889517523e-17
grad ChooseDest W: 5.557487964630127
grad AddEdge W: 1.5775357338367843e-16
grad ChooseDest W: 3.7530927658081055
grad AddEdge W: 8.778531829407854e-13
grad ChooseDest W: 5.481815814971924
grad AddEdge W: 1.1395000567025114e-16
grad ChooseDest W: 6.833455562591553
grad AddEdge W: 7.794656055078576e-16
grad ChooseDest W: 4.870408058166504
grad AddEdge W: 1.2002308631827666e-14
grad ChooseDest W: 6.284595012664795
grad AddEdge W: 1.2781676352090557e-15
grad ChooseDest W: 3.193065881729126
grad AddEdge W: 4.225402152158804e-16
grad ChooseDest W: 5.76512336730957
grad AddEdge W: 4.594446995739214e-16
grad ChooseDest W: 5.428520202636719
grad AddEdge W: 7.032469868755147e-15
grad ChooseDest W: 6.090641498565674
grad AddEdge W: 1.0711053490550584e-14
grad ChooseDest W: 5.859978675842285
grad AddEdge W: 7.297521724544068e-15
grad ChooseDest W: 3.8613877296447754
grad AddEdge W: 2.7579286077106246e-11
grad ChooseDest W: 5.149449348449707
grad AddEdge W: 1.7681566604983224e-16
grad ChooseDest W: 8.531429290771484
grad AddEdge W: 6.0409157365238445e-15
grad ChooseDest W: 6.744749069213867
grad AddEdge W: 2.1632520023715386e-16
grad ChooseDest W: 5.043689727783203
grad AddEdge W: 9.528755373652375e-17
grad ChooseDest W: 6.6305060386657715
grad AddEdge W: 1.1067135288966656e-16
grad ChooseDest W: 6.215992450714111
grad AddEdge W: 1.4072869659962489e-14
grad ChooseDest W: 4.633962154388428
grad AddEdge W: 1.1241174989820319e-14
grad ChooseDest W: 2.673574447631836
grad AddEdge W: 3.512104060572933e-16
grad ChooseDest W: 4.127764701843262
grad AddEdge W: 3.2957871983163535e-15
grad ChooseDest W: 5.627535820007324
grad AddEdge W: 6.421059252812128e-16
grad ChooseDest W: 5.874084949493408
grad AddEdge W: 1.4428683478329873e-16
grad ChooseDest W: 4.988317012786865
=== Epoch 24: Train Loss: 4.8775, Train Log Prob: 0.0223 ===
Total mismatches: 75171
Predicted valid destination but wrong order: 37882
Epoch 24: Validation Loss: 5.1174, Validation Log Prob: 0.0092
Epoch 24: Edge Precision: 0.3694, Recall: 0.3675, F1: 0.3684, Jaccard: 0.2425
Epoch 24: TP: 2.5760916249105223, FP: 4.415748031496063, FN: 4.445382963493199
Epoch 24: Current Learning Rate: 6e-05
[Epoch 24] ‚è±Ô∏è Total: 4615.05s | Current time: 2025-07-15 18:24:26 | üèãÔ∏è Train: 3856.00s | ‚úÖ Val: 759.05s
grad AddEdge W: 2.945206199148759e-13
grad ChooseDest W: 9.670434951782227
grad AddEdge W: 1.2124368620625853e-14
grad ChooseDest W: 8.405234336853027
grad AddEdge W: 8.936294208228415e-17
grad ChooseDest W: 6.8594255447387695
grad AddEdge W: 5.6481918751447036e-15
grad ChooseDest W: 6.715847969055176
grad AddEdge W: 1.2970933158660321e-16
grad ChooseDest W: 6.85575532913208
grad AddEdge W: 8.502045421854287e-12
grad ChooseDest W: 3.826787233352661
grad AddEdge W: 2.182819659887151e-14
grad ChooseDest W: 3.564335584640503
grad AddEdge W: 1.4837582016171986e-17
grad ChooseDest W: 7.642472267150879
grad AddEdge W: 1.1492340534532394e-16
grad ChooseDest W: 9.984936714172363
grad AddEdge W: 8.507418673611017e-14
grad ChooseDest W: 4.014074802398682
grad AddEdge W: 5.456144095013736e-15
grad ChooseDest W: 4.643648147583008
grad AddEdge W: 2.874789324218305e-16
grad ChooseDest W: 4.696267604827881
grad AddEdge W: 7.221057519296581e-15
grad ChooseDest W: 4.358021259307861
grad AddEdge W: 3.408215469592193e-16
grad ChooseDest W: 4.624113082885742
grad AddEdge W: 2.6616702760354064e-16
grad ChooseDest W: 5.641621112823486
grad AddEdge W: 3.0589844546291545e-10
grad ChooseDest W: 1.6611913442611694
grad AddEdge W: 8.476302378310126e-17
grad ChooseDest W: 6.0128984451293945
grad AddEdge W: 1.017025511451093e-16
grad ChooseDest W: 4.7850189208984375
grad AddEdge W: 1.3969121782330112e-16
grad ChooseDest W: 6.096687316894531
grad AddEdge W: 7.463288613420577e-15
grad ChooseDest W: 6.1510701179504395
grad AddEdge W: 3.100338369143923e-15
grad ChooseDest W: 4.639720916748047
grad AddEdge W: 3.561500459782138e-14
grad ChooseDest W: 6.259899139404297
grad AddEdge W: 6.034968797610511e-17
grad ChooseDest W: 4.993157863616943
grad AddEdge W: 3.8062375749959687e-16
grad ChooseDest W: 5.28727388381958
grad AddEdge W: 5.52587585740332e-17
grad ChooseDest W: 4.120336532592773
grad AddEdge W: 2.1192753752390754e-16
grad ChooseDest W: 6.303365230560303
grad AddEdge W: 5.094706666090854e-13
grad ChooseDest W: 4.82084321975708
grad AddEdge W: 9.506512500215714e-15
grad ChooseDest W: 5.679733753204346
grad AddEdge W: 1.6194061606063405e-16
grad ChooseDest W: 3.5379629135131836
grad AddEdge W: 1.455010538662098e-15
grad ChooseDest W: 6.244345664978027
grad AddEdge W: 1.513285718957036e-09
grad ChooseDest W: 5.412279051597579e-07
grad AddEdge W: 7.238380190100879e-15
grad ChooseDest W: 4.9683451652526855
grad AddEdge W: 2.043286329005218e-16
grad ChooseDest W: 5.66438102722168
grad AddEdge W: 6.197213326445642e-17
grad ChooseDest W: 4.3624587059021
grad AddEdge W: 1.3745482584942315e-16
grad ChooseDest W: 4.412743091583252
grad AddEdge W: 4.2609950059980216e-16
grad ChooseDest W: 4.62760066986084
grad AddEdge W: 4.225056245078969e-15
grad ChooseDest W: 5.891179084777832
grad AddEdge W: 8.992290232882416e-16
grad ChooseDest W: 4.91872501373291
grad AddEdge W: 7.075989574552126e-15
grad ChooseDest W: 3.748533010482788
grad AddEdge W: 2.0930528233788583e-16
grad ChooseDest W: 3.76242733001709
grad AddEdge W: 1.053518827487691e-14
grad ChooseDest W: 6.9257659912109375
grad AddEdge W: 2.435993699711667e-16
grad ChooseDest W: 3.610743999481201
grad AddEdge W: 8.188145210725562e-16
grad ChooseDest W: 5.244510173797607
grad AddEdge W: 1.5173962347042602e-13
grad ChooseDest W: 4.617247104644775
grad AddEdge W: 4.17342953400426e-16
grad ChooseDest W: 7.783320903778076
grad AddEdge W: 5.911125301532542e-15
grad ChooseDest W: 3.268306016921997
grad AddEdge W: 1.9084077141601994e-16
grad ChooseDest W: 3.739971399307251
grad AddEdge W: 1.2396924076596704e-16
grad ChooseDest W: 4.19868803024292
grad AddEdge W: 5.5065909686022584e-17
grad ChooseDest W: 7.95228910446167
grad AddEdge W: 1.7643258216454668e-16
grad ChooseDest W: 6.140052318572998
grad AddEdge W: 7.122297712811466e-17
grad ChooseDest W: 4.015310287475586
grad AddEdge W: 6.069557079158017e-10
grad ChooseDest W: 2.4497780799865723
grad AddEdge W: 2.252769620610925e-16
grad ChooseDest W: 7.365197658538818
grad AddEdge W: 6.217129850362449e-17
grad ChooseDest W: 5.627133369445801
grad AddEdge W: 2.084420631204051e-16
grad ChooseDest W: 8.375718116760254
grad AddEdge W: 1.7848077400545658e-16
grad ChooseDest W: 4.005178928375244
grad AddEdge W: 2.141950778109318e-17
grad ChooseDest W: 6.786813259124756
grad AddEdge W: 1.752707441284894e-16
grad ChooseDest W: 8.463968276977539
grad AddEdge W: 3.807970869103847e-15
grad ChooseDest W: 4.454629421234131
grad AddEdge W: 6.741315405206505e-13
grad ChooseDest W: 3.563324213027954
grad AddEdge W: 2.0018837555901218e-16
grad ChooseDest W: 6.779477596282959
grad AddEdge W: 4.977642054099099e-15
grad ChooseDest W: 4.282423973083496
grad AddEdge W: 6.809670425125214e-17
grad ChooseDest W: 5.000304222106934
grad AddEdge W: 3.767673276638224e-15
grad ChooseDest W: 5.37604284286499
grad AddEdge W: 3.5414951509305913e-15
grad ChooseDest W: 4.129775047302246
grad AddEdge W: 1.1148213547375633e-16
grad ChooseDest W: 5.577639579772949
=== Epoch 25: Train Loss: 4.8241, Train Log Prob: 0.0235 ===
Total mismatches: 74263
Predicted valid destination but wrong order: 37876
Epoch 25: Validation Loss: 5.0194, Validation Log Prob: 0.0102
Epoch 25: Edge Precision: 0.3717, Recall: 0.3697, F1: 0.3706, Jaccard: 0.2445
Epoch 25: TP: 2.5906943450250535, FP: 4.398568360773085, FN: 4.430780243378669
Epoch 25: Current Learning Rate: 6e-05
üíæ Checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/checkpoint_epoch_25.pth
[Epoch 25] ‚è±Ô∏è Total: 4620.14s | Current time: 2025-07-15 19:41:27 | üèãÔ∏è Train: 3860.64s | ‚úÖ Val: 759.50s
grad AddEdge W: 4.45992445448698e-13
grad ChooseDest W: 12.95478630065918
grad AddEdge W: 8.098925853962727e-15
grad ChooseDest W: 5.57085657119751
grad AddEdge W: 2.5630066713172757e-14
grad ChooseDest W: 4.362801551818848
grad AddEdge W: 7.6166337641256e-15
grad ChooseDest W: 7.407588005065918
grad AddEdge W: 2.021587859525625e-16
grad ChooseDest W: 4.233092784881592
grad AddEdge W: 1.294749681580098e-16
grad ChooseDest W: 4.459042072296143
grad AddEdge W: 2.972814594319473e-14
grad ChooseDest W: 5.449549674987793
grad AddEdge W: 1.690333020310051e-14
grad ChooseDest W: 4.041159152984619
grad AddEdge W: 3.121007243606349e-16
grad ChooseDest W: 4.4570794105529785
grad AddEdge W: 3.6940660605822657e-16
grad ChooseDest W: 4.897826194763184
grad AddEdge W: 8.078112560382794e-15
grad ChooseDest W: 4.903499126434326
grad AddEdge W: 1.153562524162607e-16
grad ChooseDest W: 4.0546979904174805
grad AddEdge W: 4.1415064504090216e-16
grad ChooseDest W: 7.367247104644775
grad AddEdge W: 5.498813803248145e-15
grad ChooseDest W: 4.512220859527588
grad AddEdge W: 9.804915245981369e-17
grad ChooseDest W: 4.7435688972473145
grad AddEdge W: 5.227357954861508e-16
grad ChooseDest W: 2.387585401535034
grad AddEdge W: 1.465825720030437e-16
grad ChooseDest W: 4.301172256469727
grad AddEdge W: 1.2722076202245988e-16
grad ChooseDest W: 10.082174301147461
grad AddEdge W: 9.436988882938029e-15
grad ChooseDest W: 4.562960624694824
grad AddEdge W: 4.9576155482480475e-16
grad ChooseDest W: 8.20340347290039
grad AddEdge W: 7.570315143833539e-16
grad ChooseDest W: 5.6103315353393555
grad AddEdge W: 1.4370239528458306e-16
grad ChooseDest W: 3.7734367847442627
grad AddEdge W: 1.3150380307261345e-13
grad ChooseDest W: 6.365429878234863
grad AddEdge W: 4.113234608212041e-16
grad ChooseDest W: 4.780599117279053
grad AddEdge W: 7.80433118303947e-15
grad ChooseDest W: 2.541257619857788
grad AddEdge W: 2.4004021693614294e-16
grad ChooseDest W: 6.276782035827637
grad AddEdge W: 9.600982084822986e-15
grad ChooseDest W: 3.7269420623779297
grad AddEdge W: 7.405748024187257e-17
grad ChooseDest W: 4.840602397918701
grad AddEdge W: 3.1607491030492346e-17
grad ChooseDest W: 4.154901504516602
grad AddEdge W: 1.4243310053125473e-15
grad ChooseDest W: 3.17044734954834
grad AddEdge W: 7.615314298552015e-16
grad ChooseDest W: 5.527393341064453
grad AddEdge W: 1.9203798843009726e-14
grad ChooseDest W: 3.8642311096191406
grad AddEdge W: 9.6518974205709e-17
grad ChooseDest W: 5.562497615814209
grad AddEdge W: 6.359911812000942e-17
grad ChooseDest W: 6.697509765625
grad AddEdge W: 5.816497745280607e-15
grad ChooseDest W: 5.131344795227051
grad AddEdge W: 1.1191811921357588e-16
grad ChooseDest W: 4.9039764404296875
grad AddEdge W: 5.41842783545487e-15
grad ChooseDest W: 4.692997455596924
grad AddEdge W: 1.5911536413484693e-16
grad ChooseDest W: 4.159426212310791
grad AddEdge W: 1.344647778403962e-14
grad ChooseDest W: 5.935218334197998
grad AddEdge W: 3.2231885338027794e-16
grad ChooseDest W: 8.587066650390625
grad AddEdge W: 8.306596018605277e-17
grad ChooseDest W: 4.313825607299805
grad AddEdge W: 8.535251159818256e-15
grad ChooseDest W: 8.393714904785156
grad AddEdge W: 9.619314418900411e-15
grad ChooseDest W: 6.455780506134033
grad AddEdge W: 9.185277796068363e-15
grad ChooseDest W: 6.816481590270996
grad AddEdge W: 7.930726930206744e-13
grad ChooseDest W: 3.9443440437316895
grad AddEdge W: 1.7651158122176794e-16
grad ChooseDest W: 3.9972598552703857
grad AddEdge W: 1.6651958945849282e-13
grad ChooseDest W: 2.9992268085479736
grad AddEdge W: 1.257395676836949e-14
grad ChooseDest W: 4.8417840003967285
grad AddEdge W: 2.1176335871592801e-16
grad ChooseDest W: 5.112907409667969
grad AddEdge W: 2.778250596939568e-16
grad ChooseDest W: 5.122405529022217
grad AddEdge W: 1.0393448296112438e-16
grad ChooseDest W: 6.258383750915527
grad AddEdge W: 1.1414645113956513e-16
grad ChooseDest W: 7.906578540802002
grad AddEdge W: 5.879354948565605e-17
grad ChooseDest W: 4.757668972015381
grad AddEdge W: 2.6655094528688365e-16
grad ChooseDest W: 6.735097885131836
grad AddEdge W: 1.2983290575267374e-16
grad ChooseDest W: 4.726004600524902
grad AddEdge W: 1.311331145615989e-16
grad ChooseDest W: 6.43273401260376
grad AddEdge W: 4.056422149676005e-14
grad ChooseDest W: 1.9096380472183228
grad AddEdge W: 2.116445491101858e-16
grad ChooseDest W: 6.962643146514893
grad AddEdge W: 6.108995687786889e-13
grad ChooseDest W: 4.8072943687438965
grad AddEdge W: 3.618495104517217e-16
grad ChooseDest W: 4.9353742599487305
grad AddEdge W: 9.76905663555495e-17
grad ChooseDest W: 3.5380916595458984
grad AddEdge W: 5.035014308769042e-17
grad ChooseDest W: 4.957862377166748
grad AddEdge W: 8.374822537273141e-17
grad ChooseDest W: 5.658882141113281
grad AddEdge W: 2.061411907634174e-16
grad ChooseDest W: 8.814794540405273
grad AddEdge W: 2.4576252694702948e-14
grad ChooseDest W: 6.355322360992432
grad AddEdge W: 3.595611715353754e-16
grad ChooseDest W: 8.88941764831543
=== Epoch 26: Train Loss: 4.7730, Train Log Prob: 0.0246 ===
Total mismatches: 73246
Predicted valid destination but wrong order: 38183
Epoch 26: Validation Loss: 4.8927, Validation Log Prob: 0.0113
Epoch 26: Edge Precision: 0.3680, Recall: 0.3657, F1: 0.3668, Jaccard: 0.2414
Epoch 26: TP: 2.5633500357909806, FP: 4.419183965640658, FN: 4.458124552612742
Epoch 26: Current Learning Rate: 6e-05
[Epoch 26] ‚è±Ô∏è Total: 4624.18s | Current time: 2025-07-15 20:58:31 | üèãÔ∏è Train: 3854.40s | ‚úÖ Val: 769.77s
grad AddEdge W: 5.990719530689359e-13
grad ChooseDest W: 11.528624534606934
grad AddEdge W: 7.762902801589262e-15
grad ChooseDest W: 8.477788925170898
grad AddEdge W: 5.897387485919261e-16
grad ChooseDest W: 3.645958662033081
grad AddEdge W: 4.283282645125945e-14
grad ChooseDest W: 4.943843841552734
grad AddEdge W: 1.2313882925211583e-14
grad ChooseDest W: 4.482938766479492
grad AddEdge W: 5.047349226063433e-17
grad ChooseDest W: 5.677321910858154
grad AddEdge W: 2.4542954474293666e-13
grad ChooseDest W: 3.6848714351654053
grad AddEdge W: 4.43757299989681e-15
grad ChooseDest W: 3.97822904586792
grad AddEdge W: 1.2525367358624958e-16
grad ChooseDest W: 5.494062900543213
grad AddEdge W: 8.429598437436402e-17
grad ChooseDest W: 4.460666656494141
grad AddEdge W: 8.999955457538594e-15
grad ChooseDest W: 5.044956684112549
grad AddEdge W: 9.580977707707682e-15
grad ChooseDest W: 4.057902812957764
grad AddEdge W: 1.3284247998871727e-16
grad ChooseDest W: 4.976788520812988
grad AddEdge W: 6.792131893268265e-15
grad ChooseDest W: 4.911103248596191
grad AddEdge W: 1.354666409988703e-16
grad ChooseDest W: 5.758426189422607
grad AddEdge W: 5.275098319955536e-16
grad ChooseDest W: 5.101351261138916
grad AddEdge W: 8.785964388936498e-17
grad ChooseDest W: 4.061949729919434
grad AddEdge W: 1.9806322283391035e-16
grad ChooseDest W: 6.068888187408447
grad AddEdge W: 4.97055111778116e-15
grad ChooseDest W: 3.1220107078552246
grad AddEdge W: 3.835502497149156e-17
grad ChooseDest W: 5.796048641204834
grad AddEdge W: 1.2635934169120087e-14
grad ChooseDest W: 5.13576602935791
grad AddEdge W: 8.799451403388052e-17
grad ChooseDest W: 4.287417411804199
grad AddEdge W: 1.4412677202604727e-16
grad ChooseDest W: 5.379878044128418
grad AddEdge W: 7.895997935624278e-15
grad ChooseDest W: 2.9108808040618896
grad AddEdge W: 1.0348468331984172e-14
grad ChooseDest W: 4.285282135009766
grad AddEdge W: 5.78635692488551e-15
grad ChooseDest W: 1.7090420722961426
grad AddEdge W: 8.987674697413269e-17
grad ChooseDest W: 5.670930862426758
grad AddEdge W: 2.227109233929889e-15
grad ChooseDest W: 3.2613186836242676
grad AddEdge W: 6.393877965534737e-16
grad ChooseDest W: 5.2366485595703125
grad AddEdge W: 3.6762301936910503e-16
grad ChooseDest W: 5.987893581390381
grad AddEdge W: 1.339581150244798e-16
grad ChooseDest W: 4.718716621398926
grad AddEdge W: 2.475288352228182e-16
grad ChooseDest W: 7.254603385925293
grad AddEdge W: 1.9427727672725924e-16
grad ChooseDest W: 3.8694238662719727
grad AddEdge W: 1.6881226348946317e-16
grad ChooseDest W: 4.3411054611206055
grad AddEdge W: 1.5795260336227274e-14
grad ChooseDest W: 5.499847888946533
grad AddEdge W: 3.2481295756357775e-13
grad ChooseDest W: 6.6984124183654785
grad AddEdge W: 2.1979855148159872e-16
grad ChooseDest W: 5.478450298309326
grad AddEdge W: 2.834326825025763e-15
grad ChooseDest W: 2.7077038288116455
grad AddEdge W: 1.4812713975873547e-14
grad ChooseDest W: 5.196972846984863
grad AddEdge W: 6.11167348080568e-15
grad ChooseDest W: 3.4859888553619385
grad AddEdge W: 1.4190237933265834e-14
grad ChooseDest W: 5.976888179779053
grad AddEdge W: 3.782675765381314e-17
grad ChooseDest W: 5.661221504211426
grad AddEdge W: 2.412314893671173e-16
grad ChooseDest W: 5.595409870147705
grad AddEdge W: 4.2875585352063206e-13
grad ChooseDest W: 5.306515216827393
grad AddEdge W: 7.611971053478328e-10
grad ChooseDest W: 0.157712921500206
grad AddEdge W: 1.282053652964469e-14
grad ChooseDest W: 3.690429449081421
grad AddEdge W: 1.5039140123416047e-16
grad ChooseDest W: 4.480971336364746
grad AddEdge W: 1.9052935498840157e-14
grad ChooseDest W: 3.1266636848449707
grad AddEdge W: 1.2024965439677739e-16
grad ChooseDest W: 3.3820910453796387
grad AddEdge W: 1.1100362141556176e-16
grad ChooseDest W: 4.551693916320801
grad AddEdge W: 1.0103205177546361e-16
grad ChooseDest W: 8.959061622619629
grad AddEdge W: 4.713213326568155e-13
grad ChooseDest W: 4.475834369659424
grad AddEdge W: 8.183032440446596e-17
grad ChooseDest W: 3.360879898071289
grad AddEdge W: 8.727053909199451e-17
grad ChooseDest W: 5.995143890380859
grad AddEdge W: 3.86879023023176e-17
grad ChooseDest W: 5.943723201751709
grad AddEdge W: 8.71877283865106e-17
grad ChooseDest W: 3.387026786804199
grad AddEdge W: 9.737946041844586e-16
grad ChooseDest W: 7.174066543579102
grad AddEdge W: 1.436917100063051e-12
grad ChooseDest W: 4.463348388671875
grad AddEdge W: 3.155637920957045e-16
grad ChooseDest W: 5.185283660888672
grad AddEdge W: 5.526129345247675e-15
grad ChooseDest W: 3.5839383602142334
grad AddEdge W: 7.743587909293023e-15
grad ChooseDest W: 6.226159572601318
grad AddEdge W: 3.4583387501250845e-15
grad ChooseDest W: 8.62031364440918
grad AddEdge W: 3.071855245166162e-16
grad ChooseDest W: 5.868316173553467
grad AddEdge W: 2.1825468835143995e-16
grad ChooseDest W: 3.4284119606018066
grad AddEdge W: 1.425501260738518e-16
grad ChooseDest W: 4.0641937255859375
grad AddEdge W: 1.7506532858026398e-14
grad ChooseDest W: 6.771536827087402
=== Epoch 27: Train Loss: 4.7187, Train Log Prob: 0.0259 ===
Total mismatches: 72320
Predicted valid destination but wrong order: 38274
Epoch 27: Validation Loss: 4.8115, Validation Log Prob: 0.0122
Epoch 27: Edge Precision: 0.3671, Recall: 0.3646, F1: 0.3657, Jaccard: 0.2407
Epoch 27: TP: 2.5549033643521835, FP: 4.426485325697924, FN: 4.466571224051539
Epoch 27: Current Learning Rate: 6e-05
[Epoch 27] ‚è±Ô∏è Total: 4616.06s | Current time: 2025-07-15 22:15:27 | üèãÔ∏è Train: 3856.02s | ‚úÖ Val: 760.04s
grad AddEdge W: 1.5117622119962572e-14
grad ChooseDest W: 12.171579360961914
grad AddEdge W: 1.976661761398849e-16
grad ChooseDest W: 4.666767120361328
grad AddEdge W: 4.15888687827241e-15
grad ChooseDest W: 4.152205467224121
grad AddEdge W: 5.469508130869261e-17
grad ChooseDest W: 5.5316972732543945
grad AddEdge W: 5.568323444482201e-15
grad ChooseDest W: 10.825098037719727
grad AddEdge W: 8.149659342324776e-17
grad ChooseDest W: 4.8763604164123535
grad AddEdge W: 1.449926096341262e-14
grad ChooseDest W: 4.119065761566162
grad AddEdge W: 7.678484956967057e-15
grad ChooseDest W: 3.173048973083496
grad AddEdge W: 2.13295111921919e-16
grad ChooseDest W: 3.6464407444000244
grad AddEdge W: 9.915878547305153e-17
grad ChooseDest W: 3.895855665206909
grad AddEdge W: 7.814931427679861e-13
grad ChooseDest W: 8.42764949798584
grad AddEdge W: 2.3306423887101374e-16
grad ChooseDest W: 5.129354953765869
grad AddEdge W: 6.380516550187393e-16
grad ChooseDest W: 3.053246259689331
grad AddEdge W: 3.3495596953093636e-17
grad ChooseDest W: 3.7106194496154785
grad AddEdge W: 1.4685120056133152e-16
grad ChooseDest W: 5.849000453948975
grad AddEdge W: 5.8765422168455695e-15
grad ChooseDest W: 4.197972774505615
grad AddEdge W: 2.158860665935617e-16
grad ChooseDest W: 3.534026622772217
grad AddEdge W: 6.030756406413817e-13
grad ChooseDest W: 6.103506088256836
grad AddEdge W: 1.2659909277873443e-16
grad ChooseDest W: 7.306704998016357
grad AddEdge W: 1.113324774574707e-14
grad ChooseDest W: 3.6774632930755615
grad AddEdge W: 8.967650298556673e-14
grad ChooseDest W: 4.898244380950928
grad AddEdge W: 5.476096591361022e-16
grad ChooseDest W: 6.027579307556152
grad AddEdge W: 9.41994803587715e-17
grad ChooseDest W: 6.021013259887695
grad AddEdge W: 3.884032521943152e-16
grad ChooseDest W: 6.210139751434326
grad AddEdge W: 2.12368312293835e-16
grad ChooseDest W: 6.264252662658691
grad AddEdge W: 6.952125484985627e-17
grad ChooseDest W: 5.0181498527526855
grad AddEdge W: 1.431456542926382e-14
grad ChooseDest W: 2.4846179485321045
grad AddEdge W: 3.1049038237900646e-17
grad ChooseDest W: 6.119863986968994
grad AddEdge W: 2.9923720556759826e-16
grad ChooseDest W: 6.836325645446777
grad AddEdge W: 2.0180732021901117e-17
grad ChooseDest W: 6.076153755187988
grad AddEdge W: 4.608595622331913e-16
grad ChooseDest W: 4.735166072845459
grad AddEdge W: 1.0296341924410143e-16
grad ChooseDest W: 3.9500534534454346
grad AddEdge W: 1.7456858028499519e-16
grad ChooseDest W: 4.509657859802246
grad AddEdge W: 1.0441320930695136e-14
grad ChooseDest W: 7.62449312210083
grad AddEdge W: 1.2733470118875539e-16
grad ChooseDest W: 4.246696472167969
grad AddEdge W: 6.21044159880159e-17
grad ChooseDest W: 4.665324687957764
grad AddEdge W: 2.069682993781699e-13
grad ChooseDest W: 4.513878345489502
grad AddEdge W: 9.234335985609641e-17
grad ChooseDest W: 4.145801544189453
grad AddEdge W: 3.3688499310059045e-15
grad ChooseDest W: 7.526711463928223
grad AddEdge W: 2.0373903708873972e-14
grad ChooseDest W: 6.748669624328613
grad AddEdge W: 3.625900306999247e-13
grad ChooseDest W: 4.9020304679870605
grad AddEdge W: 2.9305854618022487e-13
grad ChooseDest W: 4.611719131469727
grad AddEdge W: 6.1543012609092e-15
grad ChooseDest W: 4.005060195922852
grad AddEdge W: 1.3091581090595876e-16
grad ChooseDest W: 3.615131139755249
grad AddEdge W: 2.6858157442902783e-16
grad ChooseDest W: 6.056884765625
grad AddEdge W: 7.966462606506363e-15
grad ChooseDest W: 6.1545915603637695
grad AddEdge W: 3.834364415740291e-15
grad ChooseDest W: 6.46094274520874
grad AddEdge W: 1.6730169934204333e-16
grad ChooseDest W: 6.597568511962891
grad AddEdge W: 1.928959777485243e-16
grad ChooseDest W: 6.1326751708984375
grad AddEdge W: 1.5969898307039495e-16
grad ChooseDest W: 9.703826904296875
grad AddEdge W: 5.984758272684052e-17
grad ChooseDest W: 7.038302898406982
grad AddEdge W: 1.0272802878805946e-14
grad ChooseDest W: 7.614187240600586
grad AddEdge W: 9.437021705464735e-16
grad ChooseDest W: 3.585886240005493
grad AddEdge W: 7.589554967834828e-15
grad ChooseDest W: 4.088591575622559
grad AddEdge W: 8.159518011737428e-17
grad ChooseDest W: 7.982934474945068
grad AddEdge W: 3.1390355267370273e-13
grad ChooseDest W: 5.145733833312988
grad AddEdge W: 1.7925107106164556e-16
grad ChooseDest W: 3.8757638931274414
grad AddEdge W: 5.523338795202946e-15
grad ChooseDest W: 6.700817108154297
grad AddEdge W: 1.7332552060267403e-11
grad ChooseDest W: 3.3907387256622314
grad AddEdge W: 1.9430808755071562e-14
grad ChooseDest W: 4.132414817810059
grad AddEdge W: 3.129117848774105e-16
grad ChooseDest W: 4.466134071350098
grad AddEdge W: 1.0212160746087356e-16
grad ChooseDest W: 6.134488582611084
grad AddEdge W: 2.6968867677251707e-13
grad ChooseDest W: 4.629556179046631
grad AddEdge W: 6.441576376028097e-17
grad ChooseDest W: 4.537600517272949
grad AddEdge W: 6.458381853808757e-15
grad ChooseDest W: 4.166473388671875
grad AddEdge W: 4.7392155143368146e-17
grad ChooseDest W: 3.946762800216675
=== Epoch 28: Train Loss: 4.6609, Train Log Prob: 0.0269 ===
Total mismatches: 71315
Predicted valid destination but wrong order: 38379
Epoch 28: Validation Loss: 4.7522, Validation Log Prob: 0.0129
Epoch 28: Edge Precision: 0.3681, Recall: 0.3658, F1: 0.3669, Jaccard: 0.2415
Epoch 28: TP: 2.562634216177523, FP: 4.419183965640658, FN: 4.458840372226199
Epoch 28: Current Learning Rate: 6e-05
[Epoch 28] ‚è±Ô∏è Total: 4617.43s | Current time: 2025-07-15 23:32:24 | üèãÔ∏è Train: 3853.93s | ‚úÖ Val: 763.51s
grad AddEdge W: 1.2157545207103909e-14
grad ChooseDest W: 10.165255546569824
grad AddEdge W: 1.6271914574767475e-14
grad ChooseDest W: 4.453962802886963
grad AddEdge W: 6.683062160567848e-17
grad ChooseDest W: 8.067727088928223
grad AddEdge W: 4.27039468943238e-17
grad ChooseDest W: 4.473270416259766
grad AddEdge W: 1.3655493328169985e-14
grad ChooseDest W: 4.794098377227783
grad AddEdge W: 8.100904655276411e-17
grad ChooseDest W: 4.705443382263184
grad AddEdge W: 1.3367639345440979e-14
grad ChooseDest W: 6.075922966003418
grad AddEdge W: 1.9394652359624624e-16
grad ChooseDest W: 5.344088554382324
grad AddEdge W: 1.902007056754713e-16
grad ChooseDest W: 5.417053699493408
grad AddEdge W: 1.265657466797878e-14
grad ChooseDest W: 5.000055313110352
grad AddEdge W: 1.648927905796113e-16
grad ChooseDest W: 4.619771480560303
grad AddEdge W: 2.6109822786365804e-14
grad ChooseDest W: 5.958752155303955
grad AddEdge W: 1.7986059067653384e-16
grad ChooseDest W: 4.335109710693359
grad AddEdge W: 7.139301237483106e-17
grad ChooseDest W: 4.708558559417725
grad AddEdge W: 7.453488442220844e-15
grad ChooseDest W: 4.3591227531433105
grad AddEdge W: 3.650264928088562e-16
grad ChooseDest W: 3.4024319648742676
grad AddEdge W: 1.9471279989291372e-15
grad ChooseDest W: 7.274059772491455
grad AddEdge W: 2.1525121434080342e-14
grad ChooseDest W: 6.980854511260986
grad AddEdge W: 7.52701005500957e-17
grad ChooseDest W: 3.3235912322998047
grad AddEdge W: 3.035554854118191e-16
grad ChooseDest W: 5.655154228210449
grad AddEdge W: 8.264301413617604e-15
grad ChooseDest W: 5.49057674407959
grad AddEdge W: 5.387998084180531e-17
grad ChooseDest W: 6.210803508758545
grad AddEdge W: 7.51950449606395e-15
grad ChooseDest W: 5.9909186363220215
grad AddEdge W: 5.873440276313606e-16
grad ChooseDest W: 4.078223705291748
grad AddEdge W: 2.321399671068817e-16
grad ChooseDest W: 4.27765417098999
grad AddEdge W: 5.200199960990167e-15
grad ChooseDest W: 6.170929431915283
grad AddEdge W: 6.9685722443131246e-15
grad ChooseDest W: 5.15244722366333
grad AddEdge W: 6.6503572126227345e-15
grad ChooseDest W: 2.3000504970550537
grad AddEdge W: 2.1307452601360827e-16
grad ChooseDest W: 5.603434085845947
grad AddEdge W: 2.4710359821351695e-16
grad ChooseDest W: 1.8921334743499756
grad AddEdge W: 4.902018105535288e-15
grad ChooseDest W: 8.311718940734863
grad AddEdge W: 1.6595864919483283e-16
grad ChooseDest W: 3.5737314224243164
grad AddEdge W: 9.151561676072685e-17
grad ChooseDest W: 6.877664566040039
grad AddEdge W: 1.3619478493104357e-16
grad ChooseDest W: 5.633244037628174
grad AddEdge W: 2.2713201715513864e-16
grad ChooseDest W: 5.1585283279418945
grad AddEdge W: 6.284719347314418e-15
grad ChooseDest W: 3.4547688961029053
grad AddEdge W: 3.103183764571987e-15
grad ChooseDest W: 7.527441501617432
grad AddEdge W: 2.8890864462745693e-16
grad ChooseDest W: 4.705760478973389
grad AddEdge W: 4.38620288686556e-16
grad ChooseDest W: 7.123427867889404
grad AddEdge W: 9.778604285057282e-17
grad ChooseDest W: 4.582801342010498
grad AddEdge W: 5.909431235638033e-17
grad ChooseDest W: 4.228506088256836
grad AddEdge W: 6.732471551144882e-15
grad ChooseDest W: 7.523223400115967
grad AddEdge W: 1.0492296220493847e-14
grad ChooseDest W: 6.697604179382324
grad AddEdge W: 1.4554106558505573e-16
grad ChooseDest W: 4.262845039367676
grad AddEdge W: 3.663575984680214e-17
grad ChooseDest W: 3.8833112716674805
grad AddEdge W: 7.32686344876277e-17
grad ChooseDest W: 8.67721939086914
grad AddEdge W: 5.972049840329702e-15
grad ChooseDest W: 2.656787157058716
grad AddEdge W: 9.913858241377053e-17
grad ChooseDest W: 9.013236999511719
grad AddEdge W: 2.2375971429432326e-16
grad ChooseDest W: 3.0252606868743896
grad AddEdge W: 4.6395361477083365e-15
grad ChooseDest W: 5.663723945617676
grad AddEdge W: 2.956555627490337e-14
grad ChooseDest W: 6.237861156463623
grad AddEdge W: 8.926474502331336e-15
grad ChooseDest W: 7.698566913604736
grad AddEdge W: 1.2556982863381225e-16
grad ChooseDest W: 6.501394271850586
grad AddEdge W: 1.2200511694507212e-16
grad ChooseDest W: 4.02672004699707
grad AddEdge W: 4.2503096853684086e-15
grad ChooseDest W: 6.045422077178955
grad AddEdge W: 7.828013800728227e-17
grad ChooseDest W: 6.084970951080322
grad AddEdge W: 5.901673604981266e-17
grad ChooseDest W: 3.79843807220459
grad AddEdge W: 2.8720399082110765e-16
grad ChooseDest W: 5.565704345703125
grad AddEdge W: 9.938700897156854e-15
grad ChooseDest W: 5.199442386627197
grad AddEdge W: 3.286144617596458e-14
grad ChooseDest W: 4.870980739593506
grad AddEdge W: 9.230313902599163e-17
grad ChooseDest W: 8.508455276489258
grad AddEdge W: 4.17023403656569e-13
grad ChooseDest W: 5.639742851257324
grad AddEdge W: 2.49796008903395e-15
grad ChooseDest W: 6.033674240112305
grad AddEdge W: 3.5038665592817663e-15
grad ChooseDest W: 5.8209991455078125
grad AddEdge W: 2.4491473272891383e-16
grad ChooseDest W: 4.468653678894043
grad AddEdge W: 4.930974773870124e-15
grad ChooseDest W: 2.0583038330078125
=== Epoch 29: Train Loss: 4.6033, Train Log Prob: 0.0285 ===
Total mismatches: 70087
Predicted valid destination but wrong order: 38741
Epoch 29: Validation Loss: 4.7469, Validation Log Prob: 0.0130
Epoch 29: Edge Precision: 0.3672, Recall: 0.3648, F1: 0.3659, Jaccard: 0.2411
Epoch 29: TP: 2.5567644953471724, FP: 4.424767358625626, FN: 4.464710093056549
Epoch 29: Current Learning Rate: 6e-05
[Epoch 29] ‚è±Ô∏è Total: 4613.59s | Current time: 2025-07-16 00:49:18 | üèãÔ∏è Train: 3854.37s | ‚úÖ Val: 759.22s
grad AddEdge W: 4.1428124693345693e-16
grad ChooseDest W: 12.75675106048584
grad AddEdge W: 2.252094641231082e-16
grad ChooseDest W: 4.354663372039795
grad AddEdge W: 2.8744096946392572e-15
grad ChooseDest W: 7.55643892288208
grad AddEdge W: 1.8554498908088804e-16
grad ChooseDest W: 7.328827857971191
grad AddEdge W: 6.191800785912687e-16
grad ChooseDest W: 4.578444004058838
grad AddEdge W: 1.1436403272789108e-16
grad ChooseDest W: 4.229491710662842
grad AddEdge W: 8.963814838080299e-17
grad ChooseDest W: 4.769924640655518
grad AddEdge W: 2.1265343544234205e-13
grad ChooseDest W: 3.2141425609588623
grad AddEdge W: 1.0547235730364827e-16
grad ChooseDest W: 7.517177104949951
grad AddEdge W: 1.7233926145974326e-13
grad ChooseDest W: 4.021684646606445
grad AddEdge W: 3.439758447152351e-15
grad ChooseDest W: 7.00244140625
grad AddEdge W: 6.231651462619515e-15
grad ChooseDest W: 5.539812088012695
grad AddEdge W: 5.6919283949064e-17
grad ChooseDest W: 4.125967979431152
grad AddEdge W: 1.7535833262919141e-16
grad ChooseDest W: 4.815952301025391
grad AddEdge W: 2.443190777728227e-13
grad ChooseDest W: 3.6685686111450195
grad AddEdge W: 5.3193420933386295e-17
grad ChooseDest W: 4.945766448974609
grad AddEdge W: 1.0383932754452763e-14
grad ChooseDest W: 3.505347728729248
grad AddEdge W: 8.437583046453254e-17
grad ChooseDest W: 6.8518524169921875
grad AddEdge W: 1.0300018047401226e-15
grad ChooseDest W: 4.157342433929443
grad AddEdge W: 5.523758076511837e-15
grad ChooseDest W: 7.81732702255249
grad AddEdge W: 5.7976432153911996e-15
grad ChooseDest W: 3.931762456893921
grad AddEdge W: 9.505959387701157e-15
grad ChooseDest W: 4.379456043243408
grad AddEdge W: 1.7579691174995915e-11
grad ChooseDest W: 3.3679616451263428
grad AddEdge W: 6.504412588777871e-15
grad ChooseDest W: 2.8950612545013428
grad AddEdge W: 3.2824860281873824e-15
grad ChooseDest W: 8.386631965637207
grad AddEdge W: 3.2578346165652037e-15
grad ChooseDest W: 4.856593132019043
grad AddEdge W: 1.9669146885769977e-15
grad ChooseDest W: 4.317470550537109
grad AddEdge W: 9.10113693121991e-15
grad ChooseDest W: 6.736359596252441
grad AddEdge W: 1.2415905502609521e-14
grad ChooseDest W: 5.162995338439941
grad AddEdge W: 1.3540050625453546e-16
grad ChooseDest W: 5.909307956695557
grad AddEdge W: 3.630189717843043e-16
grad ChooseDest W: 3.208338737487793
grad AddEdge W: 2.4695568508510267e-16
grad ChooseDest W: 5.411269664764404
grad AddEdge W: 1.4475569451877918e-14
grad ChooseDest W: 4.694913387298584
grad AddEdge W: 2.948138966025332e-13
grad ChooseDest W: 2.8630871772766113
grad AddEdge W: 1.3439873097572964e-16
grad ChooseDest W: 6.645517826080322
grad AddEdge W: 1.322844837998237e-16
grad ChooseDest W: 5.161500930786133
grad AddEdge W: 2.426708365725188e-16
grad ChooseDest W: 4.39382266998291
grad AddEdge W: 4.2549737267086766e-17
grad ChooseDest W: 5.904382705688477
grad AddEdge W: 1.7739270406877568e-14
grad ChooseDest W: 5.0084004402160645
grad AddEdge W: 2.8692118769584312e-15
grad ChooseDest W: 4.8836283683776855
grad AddEdge W: 2.511340668501726e-15
grad ChooseDest W: 5.862871170043945
grad AddEdge W: 6.357408432595111e-17
grad ChooseDest W: 5.3341522216796875
grad AddEdge W: 8.756291766002995e-17
grad ChooseDest W: 6.957857608795166
grad AddEdge W: 6.764312472544237e-17
grad ChooseDest W: 6.912238597869873
grad AddEdge W: 1.790533682778005e-16
grad ChooseDest W: 3.996394634246826
grad AddEdge W: 8.306753513793907e-17
grad ChooseDest W: 5.597637176513672
grad AddEdge W: 5.6097056626367835e-15
grad ChooseDest W: 6.737452030181885
grad AddEdge W: 2.557631866102121e-17
grad ChooseDest W: 6.437079429626465
grad AddEdge W: 4.057647382834208e-15
grad ChooseDest W: 6.594570159912109
grad AddEdge W: 8.480504032305422e-15
grad ChooseDest W: 4.210293769836426
grad AddEdge W: 8.9491611680928e-17
grad ChooseDest W: 6.495095252990723
grad AddEdge W: 3.094446937358414e-17
grad ChooseDest W: 3.6234941482543945
grad AddEdge W: 3.015277150058719e-16
grad ChooseDest W: 4.512046813964844
grad AddEdge W: 8.638572053435879e-17
grad ChooseDest W: 5.816614151000977
grad AddEdge W: 8.033510346479224e-15
grad ChooseDest W: 7.205740928649902
grad AddEdge W: 1.885976455302113e-15
grad ChooseDest W: 3.6911861896514893
grad AddEdge W: 2.3580147030929574e-14
grad ChooseDest W: 5.099452495574951
grad AddEdge W: 6.213533930803558e-17
grad ChooseDest W: 5.883624076843262
grad AddEdge W: 5.793082366486709e-15
grad ChooseDest W: 7.401615619659424
grad AddEdge W: 7.312614157598245e-15
grad ChooseDest W: 6.813076496124268
grad AddEdge W: 7.035181221219308e-15
grad ChooseDest W: 5.217552185058594
grad AddEdge W: 4.453575490443473e-17
grad ChooseDest W: 4.229856967926025
grad AddEdge W: 2.0348275138868017e-16
grad ChooseDest W: 5.821529865264893
grad AddEdge W: 4.1035579847086e-17
grad ChooseDest W: 6.966370105743408
grad AddEdge W: 5.639901963689926e-15
grad ChooseDest W: 4.876927375793457
grad AddEdge W: 5.55131332883544e-15
grad ChooseDest W: 8.32734489440918
=== Epoch 30: Train Loss: 4.5576, Train Log Prob: 0.0296 ===
Total mismatches: 69524
Predicted valid destination but wrong order: 38887
Epoch 30: Validation Loss: 4.5039, Validation Log Prob: 0.0162
Epoch 30: Edge Precision: 0.3664, Recall: 0.3640, F1: 0.3651, Jaccard: 0.2398
Epoch 30: TP: 2.5504652827487475, FP: 4.430207587687903, FN: 4.471009305654975
Epoch 30: Current Learning Rate: 6e-05
[Epoch 30] ‚è±Ô∏è Total: 4609.23s | Current time: 2025-07-16 02:06:07 | üèãÔ∏è Train: 3850.83s | ‚úÖ Val: 758.40s
grad AddEdge W: 1.2584913139542225e-14
grad ChooseDest W: 10.70238208770752
grad AddEdge W: 1.5698760413645433e-16
grad ChooseDest W: 3.9244155883789062
grad AddEdge W: 1.2273279765610237e-16
grad ChooseDest W: 4.349475860595703
grad AddEdge W: 1.2426515437306829e-15
grad ChooseDest W: 6.966665267944336
grad AddEdge W: 1.9447300486554603e-15
grad ChooseDest W: 4.9950480461120605
grad AddEdge W: 2.6322332341403593e-16
grad ChooseDest W: 3.228285551071167
grad AddEdge W: 1.3669187945102956e-15
grad ChooseDest W: 5.850418567657471
grad AddEdge W: 2.776674850959879e-16
grad ChooseDest W: 7.2701849937438965
grad AddEdge W: 1.4893378988083383e-16
grad ChooseDest W: 4.167764186859131
grad AddEdge W: 6.057240178999803e-15
grad ChooseDest W: 5.190437316894531
grad AddEdge W: 2.3814475307655687e-15
grad ChooseDest W: 4.208376884460449
grad AddEdge W: 8.058158158211377e-15
grad ChooseDest W: 5.6291046142578125
grad AddEdge W: 4.0575339862983944e-16
grad ChooseDest W: 3.0695083141326904
grad AddEdge W: 3.835896565992976e-17
grad ChooseDest W: 4.726967811584473
grad AddEdge W: 1.3568289909821616e-16
grad ChooseDest W: 3.132889747619629
grad AddEdge W: 3.787435031753699e-17
grad ChooseDest W: 6.159451007843018
grad AddEdge W: 6.945688696330984e-16
grad ChooseDest W: 5.685615539550781
grad AddEdge W: 4.552302342052984e-15
grad ChooseDest W: 6.989253520965576
grad AddEdge W: 8.75102348576887e-15
grad ChooseDest W: 5.925424575805664
grad AddEdge W: 4.897164924384876e-16
grad ChooseDest W: 6.78513765335083
grad AddEdge W: 6.160409744887442e-16
grad ChooseDest W: 3.807060956954956
grad AddEdge W: 1.2267891841972312e-16
grad ChooseDest W: 4.550576210021973
grad AddEdge W: 1.2139891928752386e-16
grad ChooseDest W: 8.331626892089844
grad AddEdge W: 4.095135896409381e-16
grad ChooseDest W: 2.7308201789855957
grad AddEdge W: 1.8420827844589215e-15
grad ChooseDest W: 4.85150671005249
grad AddEdge W: 7.273627772134972e-15
grad ChooseDest W: 8.106755256652832
grad AddEdge W: 2.3774173480025328e-15
grad ChooseDest W: 5.751867294311523
grad AddEdge W: 1.591271564216595e-16
grad ChooseDest W: 7.067214012145996
grad AddEdge W: 1.4255195195924872e-14
grad ChooseDest W: 4.394903182983398
grad AddEdge W: 3.801969852430787e-16
grad ChooseDest W: 4.158098220825195
grad AddEdge W: 1.6034202401417081e-15
grad ChooseDest W: 5.104363918304443
grad AddEdge W: 7.458557087377214e-15
grad ChooseDest W: 6.972157955169678
grad AddEdge W: 1.1212198945606815e-16
grad ChooseDest W: 3.5315921306610107
grad AddEdge W: 4.88518459625803e-15
grad ChooseDest W: 2.360558032989502
grad AddEdge W: 1.126093378032048e-16
grad ChooseDest W: 6.391509056091309
grad AddEdge W: 4.9472114028522535e-17
grad ChooseDest W: 5.8269453048706055
grad AddEdge W: 1.0573447059038493e-14
grad ChooseDest W: 5.14598274230957
grad AddEdge W: 1.907026240480919e-13
grad ChooseDest W: 5.047553539276123
grad AddEdge W: 2.9838413751059477e-16
grad ChooseDest W: 4.1220383644104
grad AddEdge W: 1.0408937087646371e-16
grad ChooseDest W: 5.030485153198242
grad AddEdge W: 7.837163874140941e-15
grad ChooseDest W: 5.493041515350342
grad AddEdge W: 4.94751732733e-15
grad ChooseDest W: 6.267773628234863
grad AddEdge W: 1.9399729594224474e-17
grad ChooseDest W: 3.71415114402771
grad AddEdge W: 3.27150560921988e-17
grad ChooseDest W: 4.461599826812744
grad AddEdge W: 6.905735872744673e-17
grad ChooseDest W: 9.47597599029541
grad AddEdge W: 1.84626964184742e-16
grad ChooseDest W: 6.6309051513671875
grad AddEdge W: 1.8840418849901434e-16
grad ChooseDest W: 5.091378211975098
grad AddEdge W: 4.942538044149566e-15
grad ChooseDest W: 3.1153509616851807
grad AddEdge W: 9.021415725471552e-17
grad ChooseDest W: 5.7799391746521
grad AddEdge W: 8.137753166638506e-14
grad ChooseDest W: 6.621788024902344
grad AddEdge W: 1.1473411095166714e-14
grad ChooseDest W: 8.976573944091797
grad AddEdge W: 1.432832330897004e-16
grad ChooseDest W: 9.253335952758789
grad AddEdge W: 3.203794589509759e-17
grad ChooseDest W: 5.475134372711182
grad AddEdge W: 2.084498584704978e-16
grad ChooseDest W: 5.838043689727783
grad AddEdge W: 1.997347894157575e-16
grad ChooseDest W: 5.773465633392334
grad AddEdge W: 1.4747687997666663e-16
grad ChooseDest W: 6.798711776733398
grad AddEdge W: 5.266743716359412e-15
grad ChooseDest W: 3.971039056777954
grad AddEdge W: 9.380543666124186e-16
grad ChooseDest W: 7.5304388999938965
grad AddEdge W: 4.2486037610126384e-15
grad ChooseDest W: 5.007613658905029
grad AddEdge W: 4.2945968030156e-15
grad ChooseDest W: 7.476728439331055
grad AddEdge W: 1.3574516925472915e-16
grad ChooseDest W: 5.452356815338135
grad AddEdge W: 5.546698905097036e-16
grad ChooseDest W: 8.604406356811523
grad AddEdge W: 4.649902983949782e-15
grad ChooseDest W: 10.695311546325684
grad AddEdge W: 6.028620682717534e-17
grad ChooseDest W: 5.50177526473999
grad AddEdge W: 3.3434870646475913e-16
grad ChooseDest W: 8.101456642150879
grad AddEdge W: 4.3799955117907255e-15
grad ChooseDest W: 2.073301076889038
=== Epoch 31: Train Loss: 4.5049, Train Log Prob: 0.0310 ===
Total mismatches: 68672
Predicted valid destination but wrong order: 38955
Epoch 31: Validation Loss: 4.5586, Validation Log Prob: 0.0154
Epoch 31: Edge Precision: 0.3683, Recall: 0.3661, F1: 0.3671, Jaccard: 0.2412
Epoch 31: TP: 2.5650680028632786, FP: 4.419899785254116, FN: 4.456406585540444
Epoch 31: Current Learning Rate: 6e-05
[Epoch 31] ‚è±Ô∏è Total: 4619.68s | Current time: 2025-07-16 03:23:07 | üèãÔ∏è Train: 3849.42s | ‚úÖ Val: 770.27s
grad AddEdge W: 2.7911902322308273e-14
grad ChooseDest W: 10.822604179382324
grad AddEdge W: 2.888601784610062e-16
grad ChooseDest W: 4.735457420349121
grad AddEdge W: 1.2832574297193272e-16
grad ChooseDest W: 5.925841331481934
grad AddEdge W: 8.747704969500205e-17
grad ChooseDest W: 2.705474376678467
grad AddEdge W: 4.896240864378981e-17
grad ChooseDest W: 3.3240609169006348
grad AddEdge W: 4.909378398330454e-15
grad ChooseDest W: 3.0035557746887207
grad AddEdge W: 1.2409619671107947e-14
grad ChooseDest W: 5.195224761962891
grad AddEdge W: 1.4434852578103403e-14
grad ChooseDest W: 4.241123199462891
grad AddEdge W: 2.0459792320329966e-16
grad ChooseDest W: 8.267199516296387
grad AddEdge W: 6.712009511540264e-17
grad ChooseDest W: 5.234090328216553
grad AddEdge W: 1.792665272953506e-14
grad ChooseDest W: 4.828974723815918
grad AddEdge W: 2.0250728707079844e-16
grad ChooseDest W: 5.5263214111328125
grad AddEdge W: 1.819805970412064e-13
grad ChooseDest W: 5.403676986694336
grad AddEdge W: 1.5366407184455507e-16
grad ChooseDest W: 8.113079071044922
grad AddEdge W: 8.285106236868064e-15
grad ChooseDest W: 3.005068063735962
grad AddEdge W: 1.2918740047241696e-16
grad ChooseDest W: 5.002854824066162
grad AddEdge W: 3.9771819924672383e-17
grad ChooseDest W: 7.761272430419922
grad AddEdge W: 6.043975219529327e-15
grad ChooseDest W: 4.33383321762085
grad AddEdge W: 2.6589608293953767e-17
grad ChooseDest W: 3.620401620864868
grad AddEdge W: 1.6613037188999884e-15
grad ChooseDest W: 7.0096755027771
grad AddEdge W: 8.94922496026164e-15
grad ChooseDest W: 7.1162919998168945
grad AddEdge W: 7.676297070864299e-15
grad ChooseDest W: 5.32959508895874
grad AddEdge W: 1.8002933552149466e-16
grad ChooseDest W: 6.473836421966553
grad AddEdge W: 6.715965420101737e-15
grad ChooseDest W: 6.704519271850586
grad AddEdge W: 2.6461019248143203e-13
grad ChooseDest W: 4.273732662200928
grad AddEdge W: 1.3611049191790197e-16
grad ChooseDest W: 6.28101110458374
grad AddEdge W: 8.200396615865309e-17
grad ChooseDest W: 3.842005968093872
grad AddEdge W: 7.551687830532232e-17
grad ChooseDest W: 5.548710823059082
grad AddEdge W: 4.8612296177720615e-17
grad ChooseDest W: 7.561417579650879
grad AddEdge W: 7.10100881023165e-15
grad ChooseDest W: 4.147311687469482
grad AddEdge W: 9.682552072327625e-17
grad ChooseDest W: 7.305687427520752
grad AddEdge W: 1.1072697913149952e-16
grad ChooseDest W: 3.9202280044555664
grad AddEdge W: 2.342857768479847e-15
grad ChooseDest W: 5.425361156463623
grad AddEdge W: 3.2492994297798894e-13
grad ChooseDest W: 4.669421195983887
grad AddEdge W: 1.4290639607240083e-16
grad ChooseDest W: 6.261579990386963
grad AddEdge W: 3.5237359362859377e-13
grad ChooseDest W: 5.041760444641113
grad AddEdge W: 3.745691924624027e-15
grad ChooseDest W: 1.2224760055541992
grad AddEdge W: 1.161804705310807e-14
grad ChooseDest W: 5.523749828338623
grad AddEdge W: 4.04880745900546e-13
grad ChooseDest W: 3.47420597076416
grad AddEdge W: 3.5258911099762726e-15
grad ChooseDest W: 4.161531925201416
grad AddEdge W: 3.177193321277891e-16
grad ChooseDest W: 5.739098072052002
grad AddEdge W: 3.692856550473146e-15
grad ChooseDest W: 3.5676534175872803
grad AddEdge W: 1.6500274657347234e-14
grad ChooseDest W: 2.2432687282562256
grad AddEdge W: 1.8491802588124225e-16
grad ChooseDest W: 7.041471004486084
grad AddEdge W: 2.90201613951661e-16
grad ChooseDest W: 5.860707759857178
grad AddEdge W: 2.2784688649284167e-16
grad ChooseDest W: 7.4989094734191895
grad AddEdge W: 8.273545851794598e-17
grad ChooseDest W: 5.393911361694336
grad AddEdge W: 1.0768939986863739e-16
grad ChooseDest W: 5.573550224304199
grad AddEdge W: 7.894459723792064e-15
grad ChooseDest W: 5.012203693389893
grad AddEdge W: 2.458361457568489e-16
grad ChooseDest W: 8.235018730163574
grad AddEdge W: 5.250256220041107e-15
grad ChooseDest W: 4.154872894287109
grad AddEdge W: 4.8613077036218865e-17
grad ChooseDest W: 4.226941108703613
grad AddEdge W: 9.465254372387905e-15
grad ChooseDest W: 8.945876121520996
grad AddEdge W: 1.9791798315323583e-17
grad ChooseDest W: 6.50370454788208
grad AddEdge W: 4.1501058232334964e-13
grad ChooseDest W: 4.079358100891113
grad AddEdge W: 1.716948997798444e-14
grad ChooseDest W: 1.925533413887024
grad AddEdge W: 1.300864465365886e-16
grad ChooseDest W: 6.398957252502441
grad AddEdge W: 1.0791117585823903e-14
grad ChooseDest W: 9.350707054138184
grad AddEdge W: 3.1218633821577862e-15
grad ChooseDest W: 5.206950664520264
grad AddEdge W: 5.118847952120548e-15
grad ChooseDest W: 5.939667701721191
grad AddEdge W: 8.632089604411423e-17
grad ChooseDest W: 5.279866695404053
grad AddEdge W: 1.7930792814823e-16
grad ChooseDest W: 6.845353603363037
grad AddEdge W: 2.9509300158195003e-15
grad ChooseDest W: 6.282742500305176
grad AddEdge W: 6.5270846961605535e-15
grad ChooseDest W: 4.803741931915283
grad AddEdge W: 1.9210120938109354e-16
grad ChooseDest W: 8.505422592163086
grad AddEdge W: 2.6471599399044502e-17
grad ChooseDest W: 5.346855640411377
=== Epoch 32: Train Loss: 4.4587, Train Log Prob: 0.0322 ===
Total mismatches: 67759
Predicted valid destination but wrong order: 39068
Epoch 32: Validation Loss: 4.5025, Validation Log Prob: 0.0161
Epoch 32: Edge Precision: 0.3673, Recall: 0.3649, F1: 0.3660, Jaccard: 0.2409
Epoch 32: TP: 2.5559055118110234, FP: 4.421474588403722, FN: 4.465569076592699
Epoch 32: Current Learning Rate: 6e-05
[Epoch 32] ‚è±Ô∏è Total: 4607.21s | Current time: 2025-07-16 04:39:54 | üèãÔ∏è Train: 3846.01s | ‚úÖ Val: 761.20s
grad AddEdge W: 1.9350777694083186e-14
grad ChooseDest W: 8.943315505981445
grad AddEdge W: 2.1284347660142098e-15
grad ChooseDest W: 3.9754276275634766
grad AddEdge W: 4.759501712019165e-13
grad ChooseDest W: 3.1680045127868652
grad AddEdge W: 1.0978160036511308e-15
grad ChooseDest W: 5.195265769958496
grad AddEdge W: 5.3958993133916374e-17
grad ChooseDest W: 11.574397087097168
grad AddEdge W: 1.8268066776040882e-16
grad ChooseDest W: 5.021081924438477
grad AddEdge W: 1.582523699104928e-16
grad ChooseDest W: 5.785003185272217
grad AddEdge W: 5.3298376918229474e-17
grad ChooseDest W: 5.468411922454834
grad AddEdge W: 6.361494704821123e-17
grad ChooseDest W: 4.3063201904296875
grad AddEdge W: 4.771854870168075e-16
grad ChooseDest W: 4.572387218475342
grad AddEdge W: 4.96003195113448e-13
grad ChooseDest W: 3.797092914581299
grad AddEdge W: 5.714249010085751e-15
grad ChooseDest W: 8.851468086242676
grad AddEdge W: 7.570000285805176e-17
grad ChooseDest W: 7.46476936340332
grad AddEdge W: 1.0780899695032276e-16
grad ChooseDest W: 8.925826072692871
grad AddEdge W: 2.9679500841033914e-15
grad ChooseDest W: 3.7576119899749756
grad AddEdge W: 1.1501855096810224e-16
grad ChooseDest W: 4.232022762298584
grad AddEdge W: 1.1707129508170804e-14
grad ChooseDest W: 4.380479335784912
grad AddEdge W: 1.8252940620487493e-16
grad ChooseDest W: 3.938230037689209
grad AddEdge W: 4.648753560240358e-15
grad ChooseDest W: 3.6171536445617676
grad AddEdge W: 2.574553255047657e-15
grad ChooseDest W: 4.619911193847656
grad AddEdge W: 6.5782497213394495e-15
grad ChooseDest W: 6.586021900177002
grad AddEdge W: 1.5044054237999102e-16
grad ChooseDest W: 3.9315028190612793
grad AddEdge W: 1.1824008196430636e-16
grad ChooseDest W: 5.191955089569092
grad AddEdge W: 4.185517594134085e-15
grad ChooseDest W: 5.703876972198486
grad AddEdge W: 1.3286091619020985e-16
grad ChooseDest W: 5.03721809387207
grad AddEdge W: 7.783089290788227e-17
grad ChooseDest W: 4.753715515136719
grad AddEdge W: 7.955122529408522e-15
grad ChooseDest W: 4.87177038192749
grad AddEdge W: 4.476377220336865e-17
grad ChooseDest W: 5.747488975524902
grad AddEdge W: 1.5165325533244276e-16
grad ChooseDest W: 5.976414203643799
grad AddEdge W: 1.1865587349745455e-14
grad ChooseDest W: 2.602180004119873
grad AddEdge W: 1.2164204918948401e-13
grad ChooseDest W: 1.6475000381469727
grad AddEdge W: 1.504107638779391e-16
grad ChooseDest W: 4.498091220855713
grad AddEdge W: 1.7134884923380398e-16
grad ChooseDest W: 7.0960822105407715
grad AddEdge W: 6.84625496925721e-17
grad ChooseDest W: 5.235605716705322
grad AddEdge W: 4.643241387456982e-16
grad ChooseDest W: 6.639975070953369
grad AddEdge W: 2.7622290746213713e-15
grad ChooseDest W: 4.0343427658081055
grad AddEdge W: 2.3539503001988524e-15
grad ChooseDest W: 4.9611406326293945
grad AddEdge W: 6.87746680987455e-17
grad ChooseDest W: 3.4460527896881104
grad AddEdge W: 1.0727857565432926e-16
grad ChooseDest W: 8.580792427062988
grad AddEdge W: 8.325661539107889e-17
grad ChooseDest W: 3.185758590698242
grad AddEdge W: 6.789124502789038e-15
grad ChooseDest W: 7.129861831665039
grad AddEdge W: 7.55930657425543e-15
grad ChooseDest W: 5.921478271484375
grad AddEdge W: 4.594392467993235e-16
grad ChooseDest W: 4.8969526290893555
grad AddEdge W: 1.133192969967917e-16
grad ChooseDest W: 5.050399303436279
grad AddEdge W: 5.216741210019304e-13
grad ChooseDest W: 4.562098503112793
grad AddEdge W: 2.2302972541851174e-15
grad ChooseDest W: 5.269959449768066
grad AddEdge W: 1.1751025720112837e-16
grad ChooseDest W: 5.032421112060547
grad AddEdge W: 4.136143937759514e-17
grad ChooseDest W: 4.698352336883545
grad AddEdge W: 7.720097144566819e-15
grad ChooseDest W: 5.35515022277832
grad AddEdge W: 1.265339405926184e-14
grad ChooseDest W: 7.987155437469482
grad AddEdge W: 8.080215213795235e-17
grad ChooseDest W: 5.290130615234375
grad AddEdge W: 2.0157357618326042e-15
grad ChooseDest W: 5.6741533279418945
grad AddEdge W: 1.2764730928585143e-16
grad ChooseDest W: 5.895236492156982
grad AddEdge W: 3.1115787081122245e-15
grad ChooseDest W: 4.567271709442139
grad AddEdge W: 5.030009442546623e-15
grad ChooseDest W: 5.058692455291748
grad AddEdge W: 1.1675621258961484e-16
grad ChooseDest W: 6.080382823944092
grad AddEdge W: 1.792906830868195e-16
grad ChooseDest W: 10.223796844482422
grad AddEdge W: 1.4490524824413317e-16
grad ChooseDest W: 3.1738882064819336
grad AddEdge W: 3.172747854142684e-17
grad ChooseDest W: 5.104417324066162
grad AddEdge W: 4.6575195042114926e-15
grad ChooseDest W: 7.175993919372559
grad AddEdge W: 9.729666956529665e-17
grad ChooseDest W: 7.787412643432617
grad AddEdge W: 4.724016249252165e-15
grad ChooseDest W: 4.483635902404785
grad AddEdge W: 1.0670911143342325e-16
grad ChooseDest W: 6.4171295166015625
grad AddEdge W: 5.3381951636843894e-11
grad ChooseDest W: 2.3254926204681396
grad AddEdge W: 6.711995614905973e-17
grad ChooseDest W: 8.228346824645996
grad AddEdge W: 3.571199762592742e-17
grad ChooseDest W: 3.753541946411133
=== Epoch 33: Train Loss: 4.4067, Train Log Prob: 0.0337 ===
Total mismatches: 66995
Predicted valid destination but wrong order: 39239
Epoch 33: Validation Loss: 4.3564, Validation Log Prob: 0.0184
Epoch 33: Edge Precision: 0.3649, Recall: 0.3626, F1: 0.3636, Jaccard: 0.2389
Epoch 33: TP: 2.5401574803149605, FP: 4.44151753758053, FN: 4.481317108088762
Epoch 33: Current Learning Rate: 6e-05
[Epoch 33] ‚è±Ô∏è Total: 4612.62s | Current time: 2025-07-16 05:56:47 | üèãÔ∏è Train: 3847.79s | ‚úÖ Val: 764.83s
grad AddEdge W: 1.600202954898152e-14
grad ChooseDest W: 17.28435516357422
grad AddEdge W: 6.385698009544425e-17
grad ChooseDest W: 3.203178644180298
grad AddEdge W: 2.1149426693649716e-16
grad ChooseDest W: 7.642486095428467
grad AddEdge W: 1.422040204706698e-15
grad ChooseDest W: 5.346972465515137
grad AddEdge W: 1.3901967951480607e-16
grad ChooseDest W: 3.3377387523651123
grad AddEdge W: 3.5194007200179366e-17
grad ChooseDest W: 5.307015419006348
grad AddEdge W: 2.4506820451104447e-16
grad ChooseDest W: 4.898982524871826
grad AddEdge W: 7.959425668538811e-17
grad ChooseDest W: 4.074616432189941
grad AddEdge W: 5.103248146330966e-15
grad ChooseDest W: 5.667934417724609
grad AddEdge W: 3.1486075474948343e-16
grad ChooseDest W: 3.3829421997070312
grad AddEdge W: 2.5131967823869563e-16
grad ChooseDest W: 9.913732528686523
grad AddEdge W: 7.129945493882886e-17
grad ChooseDest W: 9.097264289855957
grad AddEdge W: 4.27796074505928e-17
grad ChooseDest W: 5.969086170196533
grad AddEdge W: 8.580664778345737e-17
grad ChooseDest W: 3.6180033683776855
grad AddEdge W: 2.794125065867792e-13
grad ChooseDest W: 4.098495006561279
grad AddEdge W: 1.0587432737667964e-16
grad ChooseDest W: 6.884077548980713
grad AddEdge W: 6.610149829165994e-17
grad ChooseDest W: 4.669516086578369
grad AddEdge W: 2.01925391979647e-17
grad ChooseDest W: 6.388876914978027
grad AddEdge W: 6.658363870965995e-17
grad ChooseDest W: 8.200143814086914
grad AddEdge W: 5.8271967980083605e-12
grad ChooseDest W: 0.4117521345615387
grad AddEdge W: 1.913642774820925e-16
grad ChooseDest W: 5.041042804718018
grad AddEdge W: 4.946490101358107e-17
grad ChooseDest W: 2.70510196685791
grad AddEdge W: 7.584361808835212e-15
grad ChooseDest W: 5.578530788421631
grad AddEdge W: 2.817995500407108e-16
grad ChooseDest W: 6.453907489776611
grad AddEdge W: 1.3124038334343476e-16
grad ChooseDest W: 3.877918004989624
grad AddEdge W: 3.586077036043427e-15
grad ChooseDest W: 5.37066650390625
grad AddEdge W: 1.530358248605986e-16
grad ChooseDest W: 6.608424186706543
grad AddEdge W: 2.3739023730296642e-15
grad ChooseDest W: 7.227926731109619
grad AddEdge W: 1.464557129373246e-15
grad ChooseDest W: 5.416740417480469
grad AddEdge W: 4.503644110881487e-15
grad ChooseDest W: 5.643131256103516
grad AddEdge W: 2.3468314117936536e-15
grad ChooseDest W: 5.2791595458984375
grad AddEdge W: 7.949547146791458e-17
grad ChooseDest W: 4.413488388061523
grad AddEdge W: 1.989280762955837e-14
grad ChooseDest W: 10.519501686096191
grad AddEdge W: 7.163572040144392e-17
grad ChooseDest W: 5.589519500732422
grad AddEdge W: 3.638818138074257e-17
grad ChooseDest W: 4.086012840270996
grad AddEdge W: 4.0341383407251896e-17
grad ChooseDest W: 4.075282096862793
grad AddEdge W: 1.4160760339668437e-16
grad ChooseDest W: 7.704127311706543
grad AddEdge W: 2.046837964622835e-15
grad ChooseDest W: 3.4574992656707764
grad AddEdge W: 1.3868496915174261e-15
grad ChooseDest W: 3.8186657428741455
grad AddEdge W: 2.1473312494454944e-13
grad ChooseDest W: 3.577362298965454
grad AddEdge W: 1.5696595185674014e-16
grad ChooseDest W: 5.264132022857666
grad AddEdge W: 3.5056860654618074e-17
grad ChooseDest W: 5.73391056060791
grad AddEdge W: 7.263090470512891e-17
grad ChooseDest W: 3.482478380203247
grad AddEdge W: 2.4521655108209973e-17
grad ChooseDest W: 6.458683013916016
grad AddEdge W: 2.050540591274838e-13
grad ChooseDest W: 2.3849852085113525
grad AddEdge W: 4.962244689183911e-15
grad ChooseDest W: 7.411931037902832
grad AddEdge W: 1.0109074189428548e-16
grad ChooseDest W: 7.050229072570801
grad AddEdge W: 3.034321362388752e-16
grad ChooseDest W: 2.8171818256378174
grad AddEdge W: 5.542352059066669e-13
grad ChooseDest W: 5.756272792816162
grad AddEdge W: 1.2933615428103139e-14
grad ChooseDest W: 9.316635131835938
grad AddEdge W: 7.190587097205884e-17
grad ChooseDest W: 3.9889345169067383
grad AddEdge W: 1.488900432760861e-15
grad ChooseDest W: 3.758988618850708
grad AddEdge W: 4.6072004664235566e-17
grad ChooseDest W: 5.445626258850098
grad AddEdge W: 2.056981925320034e-16
grad ChooseDest W: 4.62923002243042
grad AddEdge W: 5.3108029821437915e-15
grad ChooseDest W: 3.664612054824829
grad AddEdge W: 1.7658868345458295e-13
grad ChooseDest W: 4.592193126678467
grad AddEdge W: 8.81813644727867e-15
grad ChooseDest W: 3.3662548065185547
grad AddEdge W: 1.4352484923790468e-16
grad ChooseDest W: 4.255220413208008
grad AddEdge W: 5.924100152218583e-15
grad ChooseDest W: 5.630581855773926
grad AddEdge W: 1.2343408799686973e-16
grad ChooseDest W: 4.898983478546143
grad AddEdge W: 3.9907547028302534e-17
grad ChooseDest W: 6.398421764373779
grad AddEdge W: 3.575547093020076e-16
grad ChooseDest W: 4.825690269470215
grad AddEdge W: 4.960227255226898e-17
grad ChooseDest W: 6.305896759033203
grad AddEdge W: 9.201177292702596e-17
grad ChooseDest W: 4.901710033416748
grad AddEdge W: 4.408786161667899e-15
grad ChooseDest W: 5.072622299194336
grad AddEdge W: 8.715140523145217e-17
grad ChooseDest W: 4.229814052581787
=== Epoch 34: Train Loss: 4.3689, Train Log Prob: 0.0349 ===
Total mismatches: 66274
Predicted valid destination but wrong order: 39320
Epoch 34: Validation Loss: 4.4019, Validation Log Prob: 0.0178
Epoch 34: Edge Precision: 0.3669, Recall: 0.3648, F1: 0.3658, Jaccard: 0.2406
Epoch 34: TP: 2.5559055118110234, FP: 4.429921259842519, FN: 4.465569076592699
Epoch 34: Current Learning Rate: 6e-05
[Epoch 34] ‚è±Ô∏è Total: 4616.14s | Current time: 2025-07-16 07:13:43 | üèãÔ∏è Train: 3856.89s | ‚úÖ Val: 759.25s
grad AddEdge W: 1.9645263944789194e-14
grad ChooseDest W: 9.567911148071289
grad AddEdge W: 2.8739143921233503e-15
grad ChooseDest W: 4.936255931854248
grad AddEdge W: 2.0629123206211165e-15
grad ChooseDest W: 4.402146816253662
grad AddEdge W: 2.3078368206987034e-16
grad ChooseDest W: 6.3936004638671875
grad AddEdge W: 9.625806926441115e-15
grad ChooseDest W: 9.125256538391113
grad AddEdge W: 3.981985741304244e-15
grad ChooseDest W: 2.5808773040771484
grad AddEdge W: 2.355331281540232e-16
grad ChooseDest W: 4.167398929595947
grad AddEdge W: 9.284663639310838e-17
grad ChooseDest W: 4.75009298324585
grad AddEdge W: 2.1693216816528853e-15
grad ChooseDest W: 2.574281930923462
grad AddEdge W: 2.4939044952824964e-15
grad ChooseDest W: 3.9806902408599854
grad AddEdge W: 3.810599397627634e-17
grad ChooseDest W: 11.189565658569336
grad AddEdge W: 1.2689422817431539e-15
grad ChooseDest W: 3.4459030628204346
grad AddEdge W: 2.438456448005809e-15
grad ChooseDest W: 8.186806678771973
grad AddEdge W: 9.438257738293016e-15
grad ChooseDest W: 3.090322732925415
grad AddEdge W: 3.5652362138927085e-15
grad ChooseDest W: 8.200222969055176
grad AddEdge W: 1.638704276440972e-11
grad ChooseDest W: 3.6590325832366943
grad AddEdge W: 4.850038658177607e-16
grad ChooseDest W: 6.671671390533447
grad AddEdge W: 7.309114799795342e-17
grad ChooseDest W: 5.3975043296813965
grad AddEdge W: 9.552414062660372e-17
grad ChooseDest W: 6.7100934982299805
grad AddEdge W: 1.8088574258237622e-13
grad ChooseDest W: 2.6692142486572266
grad AddEdge W: 6.663711195255967e-14
grad ChooseDest W: 4.921077251434326
grad AddEdge W: 8.171568775947795e-16
grad ChooseDest W: 6.843599796295166
grad AddEdge W: 1.6071173271982363e-15
grad ChooseDest W: 5.290199279785156
grad AddEdge W: 1.2322468027648479e-15
grad ChooseDest W: 5.061581611633301
grad AddEdge W: 8.80011412725994e-15
grad ChooseDest W: 7.9240570068359375
grad AddEdge W: 1.0812511891066093e-16
grad ChooseDest W: 6.344976902008057
grad AddEdge W: 2.4731106834603505e-16
grad ChooseDest W: 6.710753917694092
grad AddEdge W: 1.0695738473119737e-16
grad ChooseDest W: 9.258328437805176
grad AddEdge W: 5.4940525648771796e-17
grad ChooseDest W: 5.2052106857299805
grad AddEdge W: 8.190015168305523e-17
grad ChooseDest W: 4.345735549926758
grad AddEdge W: 2.236669377168193e-16
grad ChooseDest W: 5.469620704650879
grad AddEdge W: 3.718167165486525e-15
grad ChooseDest W: 5.706025123596191
grad AddEdge W: 1.499266104332004e-15
grad ChooseDest W: 2.5067272186279297
grad AddEdge W: 6.778223665213908e-17
grad ChooseDest W: 6.069645404815674
grad AddEdge W: 3.815886074358583e-16
grad ChooseDest W: 3.814162492752075
grad AddEdge W: 6.986287382539577e-17
grad ChooseDest W: 4.330224514007568
grad AddEdge W: 5.701028705133463e-17
grad ChooseDest W: 7.21141242980957
grad AddEdge W: 2.75848434196156e-15
grad ChooseDest W: 6.088974475860596
grad AddEdge W: 1.2679270915800459e-14
grad ChooseDest W: 6.084388732910156
grad AddEdge W: 1.1317948362093554e-16
grad ChooseDest W: 5.318119049072266
grad AddEdge W: 1.4055410616853684e-16
grad ChooseDest W: 6.382472515106201
grad AddEdge W: 3.544951045355389e-15
grad ChooseDest W: 6.636844635009766
grad AddEdge W: 2.5151505168193576e-16
grad ChooseDest W: 6.09999942779541
grad AddEdge W: 8.625104891319026e-17
grad ChooseDest W: 6.018609523773193
grad AddEdge W: 4.38195188684834e-13
grad ChooseDest W: 3.921907901763916
grad AddEdge W: 8.573106994524962e-17
grad ChooseDest W: 7.390872955322266
grad AddEdge W: 6.893790722954917e-17
grad ChooseDest W: 6.687906742095947
grad AddEdge W: 7.08679048870926e-17
grad ChooseDest W: 5.345682621002197
grad AddEdge W: 1.030753731769268e-16
grad ChooseDest W: 5.477153301239014
grad AddEdge W: 1.922286521583924e-13
grad ChooseDest W: 7.802412033081055
grad AddEdge W: 9.814838104609555e-17
grad ChooseDest W: 5.44141960144043
grad AddEdge W: 3.4663930011320684e-17
grad ChooseDest W: 8.167412757873535
grad AddEdge W: 9.445922830600295e-17
grad ChooseDest W: 7.252647399902344
grad AddEdge W: 1.855441949875e-16
grad ChooseDest W: 6.833797931671143
grad AddEdge W: 1.149303854262049e-14
grad ChooseDest W: 3.869779586791992
grad AddEdge W: 1.5161100956419846e-15
grad ChooseDest W: 4.362284183502197
grad AddEdge W: 1.5592095142784475e-16
grad ChooseDest W: 10.004716873168945
grad AddEdge W: 1.7708871506131354e-16
grad ChooseDest W: 3.961099147796631
grad AddEdge W: 1.7505028527512075e-15
grad ChooseDest W: 6.563564777374268
grad AddEdge W: 1.7578950795016368e-10
grad ChooseDest W: 4.036057472229004
grad AddEdge W: 9.236731500663595e-17
grad ChooseDest W: 10.657624244689941
grad AddEdge W: 1.1395528639128168e-16
grad ChooseDest W: 5.444968223571777
grad AddEdge W: 4.67361175801595e-13
grad ChooseDest W: 6.031893253326416
grad AddEdge W: 8.554464408160826e-15
grad ChooseDest W: 5.006062030792236
grad AddEdge W: 1.2903022292114208e-13
grad ChooseDest W: 2.147597074508667
grad AddEdge W: 9.351126000107807e-15
grad ChooseDest W: 3.6006641387939453
=== Epoch 35: Train Loss: 4.3150, Train Log Prob: 0.0362 ===
Total mismatches: 65310
Predicted valid destination but wrong order: 39477
Epoch 35: Validation Loss: 4.2821, Validation Log Prob: 0.0199
Epoch 35: Edge Precision: 0.3652, Recall: 0.3627, F1: 0.3639, Jaccard: 0.2393
Epoch 35: TP: 2.541159627773801, FP: 4.4386542591267, FN: 4.480314960629921
Epoch 35: Current Learning Rate: 6e-05
[Epoch 35] ‚è±Ô∏è Total: 4610.27s | Current time: 2025-07-16 08:30:33 | üèãÔ∏è Train: 3850.32s | ‚úÖ Val: 759.95s
grad AddEdge W: 9.482142515290261e-15
grad ChooseDest W: 14.968955039978027
grad AddEdge W: 3.634844362411552e-17
grad ChooseDest W: 5.584287166595459
grad AddEdge W: 3.241665234058152e-17
grad ChooseDest W: 6.277825355529785
grad AddEdge W: 9.557985479469178e-12
grad ChooseDest W: 3.0370705127716064
grad AddEdge W: 1.12437734222047e-16
grad ChooseDest W: 3.238365888595581
grad AddEdge W: 1.456559179587475e-16
grad ChooseDest W: 4.1830973625183105
grad AddEdge W: 1.5331472758006726e-09
grad ChooseDest W: 3.549365520477295
grad AddEdge W: 9.1511685998456e-17
grad ChooseDest W: 5.167322635650635
grad AddEdge W: 2.057173248886995e-15
grad ChooseDest W: 4.075887203216553
grad AddEdge W: 1.0402588311008904e-16
grad ChooseDest W: 3.448662042617798
grad AddEdge W: 3.558528083527369e-16
grad ChooseDest W: 6.352818489074707
grad AddEdge W: 5.321474895830036e-17
grad ChooseDest W: 6.067278861999512
grad AddEdge W: 5.1801639921929073e-17
grad ChooseDest W: 3.768005132675171
grad AddEdge W: 1.3460019246827815e-16
grad ChooseDest W: 7.685163497924805
grad AddEdge W: 1.5790562903260038e-16
grad ChooseDest W: 3.0485410690307617
grad AddEdge W: 5.5130366908075166e-17
grad ChooseDest W: 8.413840293884277
grad AddEdge W: 6.6702205587523215e-15
grad ChooseDest W: 8.920893669128418
grad AddEdge W: 1.644827022251761e-14
grad ChooseDest W: 6.880795001983643
grad AddEdge W: 8.165608708023779e-17
grad ChooseDest W: 5.226998805999756
grad AddEdge W: 4.970720524370611e-15
grad ChooseDest W: 1.6028087139129639
grad AddEdge W: 2.5911314897704364e-16
grad ChooseDest W: 7.792214393615723
grad AddEdge W: 6.81599074674961e-17
grad ChooseDest W: 3.7964770793914795
grad AddEdge W: 7.80356432705463e-17
grad ChooseDest W: 5.007937908172607
grad AddEdge W: 1.1115831742499897e-16
grad ChooseDest W: 4.888664722442627
grad AddEdge W: 2.015690445569926e-15
grad ChooseDest W: 5.048760890960693
grad AddEdge W: 3.5019458856040876e-17
grad ChooseDest W: 4.369132995605469
grad AddEdge W: 2.1472388974432552e-15
grad ChooseDest W: 3.0943808555603027
grad AddEdge W: 2.626968924373174e-16
grad ChooseDest W: 4.463595867156982
grad AddEdge W: 1.3651387812414203e-16
grad ChooseDest W: 2.731998920440674
grad AddEdge W: 4.1090026346723985e-13
grad ChooseDest W: 3.028111457824707
grad AddEdge W: 1.3425405510136064e-15
grad ChooseDest W: 3.8822379112243652
grad AddEdge W: 5.267532727549779e-15
grad ChooseDest W: 6.097456455230713
grad AddEdge W: 3.02552863100066e-15
grad ChooseDest W: 1.7523788213729858
grad AddEdge W: 2.144665584879717e-16
grad ChooseDest W: 2.8769848346710205
grad AddEdge W: 4.852513053174774e-16
grad ChooseDest W: 4.576827049255371
grad AddEdge W: 2.885743842506467e-16
grad ChooseDest W: 6.064566612243652
grad AddEdge W: 1.3555765733603073e-16
grad ChooseDest W: 5.720368385314941
grad AddEdge W: 2.9040926937263633e-16
grad ChooseDest W: 6.900821208953857
grad AddEdge W: 2.0875247753451665e-17
grad ChooseDest W: 3.021209478378296
grad AddEdge W: 7.132725535425113e-15
grad ChooseDest W: 3.907275438308716
grad AddEdge W: 3.19711765377968e-16
grad ChooseDest W: 7.767767906188965
grad AddEdge W: 8.378569334575761e-17
grad ChooseDest W: 4.169762134552002
grad AddEdge W: 1.684215240145212e-13
grad ChooseDest W: 7.906233310699463
grad AddEdge W: 6.5801775683274e-15
grad ChooseDest W: 4.437767505645752
grad AddEdge W: 2.8450502621380023e-15
grad ChooseDest W: 6.805085182189941
grad AddEdge W: 3.0875280630782154e-13
grad ChooseDest W: 4.939192295074463
grad AddEdge W: 7.17323488612755e-15
grad ChooseDest W: 3.6848554611206055
grad AddEdge W: 3.411481167003849e-13
grad ChooseDest W: 4.882584571838379
grad AddEdge W: 1.8971433671017993e-16
grad ChooseDest W: 5.749929428100586
grad AddEdge W: 2.767028833139102e-17
grad ChooseDest W: 7.667393684387207
grad AddEdge W: 9.239972725175823e-17
grad ChooseDest W: 2.908677339553833
grad AddEdge W: 2.03748256512975e-16
grad ChooseDest W: 4.288082599639893
grad AddEdge W: 2.258156591336785e-15
grad ChooseDest W: 6.918099403381348
grad AddEdge W: 8.106380590931512e-17
grad ChooseDest W: 5.141158103942871
grad AddEdge W: 6.349352355173335e-17
grad ChooseDest W: 6.989099979400635
grad AddEdge W: 2.3882545246989295e-13
grad ChooseDest W: 3.4154934883117676
grad AddEdge W: 1.4638796618341202e-16
grad ChooseDest W: 7.040833950042725
grad AddEdge W: 1.5476036276538467e-15
grad ChooseDest W: 6.9317498207092285
grad AddEdge W: 1.7552884830240758e-15
grad ChooseDest W: 3.486504316329956
grad AddEdge W: 1.2127236230816447e-17
grad ChooseDest W: 5.116397380828857
grad AddEdge W: 8.056044996296435e-17
grad ChooseDest W: 3.714407444000244
grad AddEdge W: 1.5047836769504185e-16
grad ChooseDest W: 4.412227630615234
grad AddEdge W: 1.401584740195333e-14
grad ChooseDest W: 7.174518585205078
grad AddEdge W: 1.233898834649349e-16
grad ChooseDest W: 6.642081260681152
grad AddEdge W: 5.777989589325397e-17
grad ChooseDest W: 10.362997055053711
grad AddEdge W: 3.441427366756238e-17
grad ChooseDest W: 7.509708404541016
=== Epoch 36: Train Loss: 4.2743, Train Log Prob: 0.0379 ===
Total mismatches: 64663
Predicted valid destination but wrong order: 39674
Epoch 36: Validation Loss: 4.2529, Validation Log Prob: 0.0205
Epoch 36: Edge Precision: 0.3670, Recall: 0.3645, F1: 0.3657, Jaccard: 0.2403
Epoch 36: TP: 2.5547602004294916, FP: 4.423335719398712, FN: 4.46671438797423
Epoch 36: Current Learning Rate: 6e-05
[Epoch 36] ‚è±Ô∏è Total: 4618.47s | Current time: 2025-07-16 09:47:31 | üèãÔ∏è Train: 3851.32s | ‚úÖ Val: 767.15s
grad AddEdge W: 5.924520704076895e-15
grad ChooseDest W: 10.522562026977539
grad AddEdge W: 4.8642686793425814e-17
grad ChooseDest W: 6.229045391082764
grad AddEdge W: 5.273373866754045e-15
grad ChooseDest W: 4.778412818908691
grad AddEdge W: 1.183607576895105e-16
grad ChooseDest W: 5.5495524406433105
grad AddEdge W: 8.400488217910097e-15
grad ChooseDest W: 3.321654796600342
grad AddEdge W: 6.673071925762664e-14
grad ChooseDest W: 5.7795329093933105
grad AddEdge W: 5.084696430720202e-17
grad ChooseDest W: 3.7617619037628174
grad AddEdge W: 1.49285798244867e-16
grad ChooseDest W: 6.764166355133057
grad AddEdge W: 2.8953722777761435e-15
grad ChooseDest W: 3.9941492080688477
grad AddEdge W: 3.4053060437672723e-17
grad ChooseDest W: 7.096928119659424
grad AddEdge W: 3.468893839439057e-15
grad ChooseDest W: 6.701948642730713
grad AddEdge W: 1.0604456088204525e-14
grad ChooseDest W: 4.2254133224487305
grad AddEdge W: 3.710282474593714e-14
grad ChooseDest W: 5.391006946563721
grad AddEdge W: 6.46221684841701e-17
grad ChooseDest W: 4.488244533538818
grad AddEdge W: 2.030543115360471e-17
grad ChooseDest W: 4.055684566497803
grad AddEdge W: 4.469615409258493e-15
grad ChooseDest W: 6.2338762283325195
grad AddEdge W: 1.673011302417819e-15
grad ChooseDest W: 7.5809102058410645
grad AddEdge W: 1.1561463981682061e-15
grad ChooseDest W: 3.380260944366455
grad AddEdge W: 5.287258007308964e-16
grad ChooseDest W: 5.360161781311035
grad AddEdge W: 5.6248459530524805e-15
grad ChooseDest W: 3.975081205368042
grad AddEdge W: 4.3532474818658556e-15
grad ChooseDest W: 4.6674275398254395
grad AddEdge W: 7.556826938241902e-17
grad ChooseDest W: 7.157885551452637
grad AddEdge W: 1.6482077954420489e-16
grad ChooseDest W: 8.502728462219238
grad AddEdge W: 2.8001767726981846e-17
grad ChooseDest W: 3.522437334060669
grad AddEdge W: 5.7623841984123184e-15
grad ChooseDest W: 5.823086261749268
grad AddEdge W: 6.039261798815212e-16
grad ChooseDest W: 4.44886589050293
grad AddEdge W: 1.4301069362147642e-17
grad ChooseDest W: 3.41127347946167
grad AddEdge W: 2.2793990129836204e-16
grad ChooseDest W: 5.627017498016357
grad AddEdge W: 3.107528620074928e-15
grad ChooseDest W: 5.855505466461182
grad AddEdge W: 5.718762001628722e-15
grad ChooseDest W: 2.108367681503296
grad AddEdge W: 2.1204402771521216e-17
grad ChooseDest W: 6.24550199508667
grad AddEdge W: 6.439800915561313e-17
grad ChooseDest W: 3.594961166381836
grad AddEdge W: 1.0130067662798229e-14
grad ChooseDest W: 4.743446350097656
grad AddEdge W: 2.1101366838315895e-16
grad ChooseDest W: 5.517263889312744
grad AddEdge W: 2.415277518459137e-14
grad ChooseDest W: 5.979307174682617
grad AddEdge W: 1.4076336565815601e-14
grad ChooseDest W: 4.08001184463501
grad AddEdge W: 3.8207100244019854e-13
grad ChooseDest W: 4.803042411804199
grad AddEdge W: 6.2255922124313315e-15
grad ChooseDest W: 2.6795592308044434
grad AddEdge W: 7.435018041773017e-15
grad ChooseDest W: 4.307226657867432
grad AddEdge W: 4.4210563735860536e-17
grad ChooseDest W: 6.3048930168151855
grad AddEdge W: 9.443928994451797e-17
grad ChooseDest W: 2.5869338512420654
grad AddEdge W: 1.0960359639124759e-15
grad ChooseDest W: 9.305891036987305
grad AddEdge W: 7.78577523226397e-15
grad ChooseDest W: 3.4193379878997803
grad AddEdge W: 2.658462403445477e-16
grad ChooseDest W: 2.6506879329681396
grad AddEdge W: 7.628592852901295e-14
grad ChooseDest W: 5.354671001434326
grad AddEdge W: 1.4417190300026816e-16
grad ChooseDest W: 4.250572681427002
grad AddEdge W: 4.223211830836323e-15
grad ChooseDest W: 4.7180495262146
grad AddEdge W: 5.2737776764767586e-17
grad ChooseDest W: 5.281907558441162
grad AddEdge W: 3.8175457295396096e-17
grad ChooseDest W: 4.976343631744385
grad AddEdge W: 5.1934445423635687e-17
grad ChooseDest W: 6.320805072784424
grad AddEdge W: 5.720865157966975e-17
grad ChooseDest W: 3.7473926544189453
grad AddEdge W: 5.820014996654521e-17
grad ChooseDest W: 4.140430450439453
grad AddEdge W: 3.035680850269095e-16
grad ChooseDest W: 8.648114204406738
grad AddEdge W: 3.184201658648583e-17
grad ChooseDest W: 5.126340389251709
grad AddEdge W: 1.5314315028776282e-14
grad ChooseDest W: 7.21788215637207
grad AddEdge W: 1.5535928917447634e-16
grad ChooseDest W: 5.829396724700928
grad AddEdge W: 1.5212252640829822e-14
grad ChooseDest W: 3.7959628105163574
grad AddEdge W: 9.200195925623863e-17
grad ChooseDest W: 7.365907192230225
grad AddEdge W: 2.063675470836813e-16
grad ChooseDest W: 5.435169696807861
grad AddEdge W: 2.1803175584422796e-13
grad ChooseDest W: 8.647088050842285
grad AddEdge W: 1.7649984452149255e-15
grad ChooseDest W: 4.523138523101807
grad AddEdge W: 1.8189125952334742e-16
grad ChooseDest W: 4.739047527313232
grad AddEdge W: 6.803283082018868e-15
grad ChooseDest W: 9.606648445129395
grad AddEdge W: 7.595765969269468e-17
grad ChooseDest W: 4.491501331329346
grad AddEdge W: 5.999175699888606e-17
grad ChooseDest W: 6.190927982330322
grad AddEdge W: 1.448261565426833e-16
grad ChooseDest W: 4.468979358673096
=== Epoch 37: Train Loss: 4.2384, Train Log Prob: 0.0388 ===
Total mismatches: 64173
Predicted valid destination but wrong order: 39705
Epoch 37: Validation Loss: 4.1874, Validation Log Prob: 0.0216
Epoch 37: Edge Precision: 0.3639, Recall: 0.3612, F1: 0.3624, Jaccard: 0.2380
Epoch 37: TP: 2.5312813171080886, FP: 4.444953471725126, FN: 4.490193271295634
Epoch 37: Current Learning Rate: 6e-05
[Epoch 37] ‚è±Ô∏è Total: 4616.22s | Current time: 2025-07-16 11:04:28 | üèãÔ∏è Train: 3849.22s | ‚úÖ Val: 767.00s
grad AddEdge W: 1.5514619686077084e-14
grad ChooseDest W: 9.384594917297363
grad AddEdge W: 6.66367324817993e-14
grad ChooseDest W: 3.4737045764923096
grad AddEdge W: 2.7349866686228805e-17
grad ChooseDest W: 5.308548450469971
grad AddEdge W: 4.742654362223549e-15
grad ChooseDest W: 5.287614345550537
grad AddEdge W: 3.367854455534644e-15
grad ChooseDest W: 2.252962112426758
grad AddEdge W: 1.400638196758644e-16
grad ChooseDest W: 3.62966251373291
grad AddEdge W: 1.1334822105435807e-14
grad ChooseDest W: 4.1031012535095215
grad AddEdge W: 3.2007720715514904e-16
grad ChooseDest W: 3.2035629749298096
grad AddEdge W: 4.0981068644718754e-16
grad ChooseDest W: 6.021088600158691
grad AddEdge W: 3.909577553324684e-15
grad ChooseDest W: 3.507936716079712
grad AddEdge W: 1.1418881655121324e-14
grad ChooseDest W: 4.101682186126709
grad AddEdge W: 2.9906526846721748e-15
grad ChooseDest W: 6.0585198402404785
grad AddEdge W: 3.554503287364482e-17
grad ChooseDest W: 8.358213424682617
grad AddEdge W: 3.266189484859124e-16
grad ChooseDest W: 5.084788799285889
grad AddEdge W: 4.27685093337503e-16
grad ChooseDest W: 7.5336012840271
grad AddEdge W: 1.0289696024496647e-16
grad ChooseDest W: 7.881605625152588
grad AddEdge W: 1.0664610012308141e-16
grad ChooseDest W: 5.883862495422363
grad AddEdge W: 3.1670939092197614e-17
grad ChooseDest W: 9.726980209350586
grad AddEdge W: 1.3612411061950704e-16
grad ChooseDest W: 4.487552165985107
grad AddEdge W: 6.441751208922366e-15
grad ChooseDest W: 4.808806896209717
grad AddEdge W: 2.433295105681274e-16
grad ChooseDest W: 3.5579819679260254
grad AddEdge W: 3.0435382719949607e-17
grad ChooseDest W: 5.442046642303467
grad AddEdge W: 3.973944434019485e-15
grad ChooseDest W: 3.4882054328918457
grad AddEdge W: 1.4169585364187643e-16
grad ChooseDest W: 5.871063709259033
grad AddEdge W: 7.862635001897825e-15
grad ChooseDest W: 3.023068904876709
grad AddEdge W: 1.2134527827916102e-16
grad ChooseDest W: 8.338390350341797
grad AddEdge W: 5.882512105057818e-15
grad ChooseDest W: 2.8903849124908447
grad AddEdge W: 3.229541598544542e-15
grad ChooseDest W: 5.767073154449463
grad AddEdge W: 9.60090272842374e-17
grad ChooseDest W: 7.5073137283325195
grad AddEdge W: 3.4496859379919674e-17
grad ChooseDest W: 4.793245792388916
grad AddEdge W: 3.5005110382612184e-15
grad ChooseDest W: 4.142740726470947
grad AddEdge W: 9.204017500053858e-17
grad ChooseDest W: 5.890430927276611
grad AddEdge W: 1.4645280126156842e-15
grad ChooseDest W: 4.0642900466918945
grad AddEdge W: 2.2654016876511246e-15
grad ChooseDest W: 4.048037528991699
grad AddEdge W: 3.0303020851749117e-15
grad ChooseDest W: 4.3175368309021
grad AddEdge W: 1.2885949284271113e-15
grad ChooseDest W: 7.245972633361816
grad AddEdge W: 6.732057775550148e-15
grad ChooseDest W: 7.442640781402588
grad AddEdge W: 2.3442256165753794e-13
grad ChooseDest W: 5.43755578994751
grad AddEdge W: 5.627169364426799e-15
grad ChooseDest W: 2.297365188598633
grad AddEdge W: 1.1495115890923632e-16
grad ChooseDest W: 3.86887526512146
grad AddEdge W: 6.420299305439763e-17
grad ChooseDest W: 7.7621307373046875
grad AddEdge W: 8.98359416912943e-14
grad ChooseDest W: 7.912546634674072
grad AddEdge W: 8.099663222613091e-17
grad ChooseDest W: 8.17844009399414
grad AddEdge W: 4.105643657757426e-15
grad ChooseDest W: 5.390810012817383
grad AddEdge W: 3.6125607916887535e-15
grad ChooseDest W: 7.716529369354248
grad AddEdge W: 5.17747962566905e-17
grad ChooseDest W: 2.9815380573272705
grad AddEdge W: 5.718034215038573e-17
grad ChooseDest W: 4.3304877281188965
grad AddEdge W: 3.01525196141645e-14
grad ChooseDest W: 5.702086925506592
grad AddEdge W: 1.2177086468955303e-15
grad ChooseDest W: 8.403767585754395
grad AddEdge W: 7.418866446957858e-17
grad ChooseDest W: 4.530701637268066
grad AddEdge W: 9.155344207577767e-17
grad ChooseDest W: 2.3789451122283936
grad AddEdge W: 1.6383092096017575e-15
grad ChooseDest W: 4.4648613929748535
grad AddEdge W: 1.4314604816295868e-15
grad ChooseDest W: 8.50759506225586
grad AddEdge W: 6.682152288363819e-15
grad ChooseDest W: 6.924705982208252
grad AddEdge W: 1.4948020554115166e-16
grad ChooseDest W: 7.089271068572998
grad AddEdge W: 2.8891171512189073e-15
grad ChooseDest W: 4.785647392272949
grad AddEdge W: 5.523148742185406e-16
grad ChooseDest W: 5.269294261932373
grad AddEdge W: 4.040807997265878e-16
grad ChooseDest W: 9.611311912536621
grad AddEdge W: 4.924226568258467e-16
grad ChooseDest W: 6.527119159698486
grad AddEdge W: 7.618822497261306e-15
grad ChooseDest W: 5.577699661254883
grad AddEdge W: 2.0668268833868523e-15
grad ChooseDest W: 3.9018445014953613
grad AddEdge W: 2.9995450483106872e-15
grad ChooseDest W: 4.776407241821289
grad AddEdge W: 1.0563771401682007e-16
grad ChooseDest W: 6.761066436767578
grad AddEdge W: 1.1506952508899548e-11
grad ChooseDest W: 4.205585479736328
grad AddEdge W: 6.234643580035911e-17
grad ChooseDest W: 4.498895645141602
grad AddEdge W: 3.354944826385541e-15
grad ChooseDest W: 6.736058235168457
=== Epoch 38: Train Loss: 4.1983, Train Log Prob: 0.0404 ===
Total mismatches: 63378
Predicted valid destination but wrong order: 39955
Epoch 38: Validation Loss: 4.1238, Validation Log Prob: 0.0229
Epoch 38: Edge Precision: 0.3683, Recall: 0.3660, F1: 0.3671, Jaccard: 0.2416
Epoch 38: TP: 2.564638511095204, FP: 4.41646385110952, FN: 4.456836077308518
Epoch 38: Current Learning Rate: 6e-05
[Epoch 38] ‚è±Ô∏è Total: 4612.56s | Current time: 2025-07-16 12:21:20 | üèãÔ∏è Train: 3851.57s | ‚úÖ Val: 761.00s
grad AddEdge W: 1.4602346567159363e-14
grad ChooseDest W: 8.557609558105469
grad AddEdge W: 1.563787727358357e-16
grad ChooseDest W: 6.908953666687012
grad AddEdge W: 8.827149380763268e-17
grad ChooseDest W: 2.6074767112731934
grad AddEdge W: 7.57270814425843e-17
grad ChooseDest W: 6.350581645965576
grad AddEdge W: 3.3840005181658665e-17
grad ChooseDest W: 6.6033501625061035
grad AddEdge W: 6.994858958919096e-17
grad ChooseDest W: 5.429046154022217
grad AddEdge W: 2.2710099457344546e-16
grad ChooseDest W: 9.46523666381836
grad AddEdge W: 3.0531444885316394e-13
grad ChooseDest W: 3.179539680480957
grad AddEdge W: 1.7278054190834023e-14
grad ChooseDest W: 2.516150712966919
grad AddEdge W: 2.34656052006921e-16
grad ChooseDest W: 6.137343883514404
grad AddEdge W: 3.3082239713226515e-15
grad ChooseDest W: 6.778111934661865
grad AddEdge W: 9.375509246392841e-17
grad ChooseDest W: 6.307535648345947
grad AddEdge W: 5.96054371830442e-17
grad ChooseDest W: 4.408941745758057
grad AddEdge W: 9.389967701755778e-17
grad ChooseDest W: 9.97961711883545
grad AddEdge W: 4.0873312289542614e-15
grad ChooseDest W: 6.2860188484191895
grad AddEdge W: 4.840116659817258e-17
grad ChooseDest W: 4.123558044433594
grad AddEdge W: 1.271460378346443e-16
grad ChooseDest W: 5.328456401824951
grad AddEdge W: 6.670650851489527e-15
grad ChooseDest W: 3.4521071910858154
grad AddEdge W: 5.86953301920704e-15
grad ChooseDest W: 4.912604331970215
grad AddEdge W: 1.678126984372541e-16
grad ChooseDest W: 6.301915168762207
grad AddEdge W: 4.115350271521156e-17
grad ChooseDest W: 6.586170196533203
grad AddEdge W: 3.011923164285388e-15
grad ChooseDest W: 8.189984321594238
grad AddEdge W: 4.786474658837684e-16
grad ChooseDest W: 12.32708740234375
grad AddEdge W: 1.3799984104664181e-13
grad ChooseDest W: 6.948274612426758
grad AddEdge W: 1.7191737128937484e-13
grad ChooseDest W: 6.559719562530518
grad AddEdge W: 2.3251403803221287e-16
grad ChooseDest W: 5.273379325866699
grad AddEdge W: 3.4376153876213486e-17
grad ChooseDest W: 5.267505168914795
grad AddEdge W: 5.261266377805992e-15
grad ChooseDest W: 7.500869274139404
grad AddEdge W: 8.240869570620794e-17
grad ChooseDest W: 6.942496299743652
grad AddEdge W: 4.26787799572741e-15
grad ChooseDest W: 5.8517632484436035
grad AddEdge W: 6.870265706333909e-17
grad ChooseDest W: 4.225266933441162
grad AddEdge W: 5.539715250795822e-17
grad ChooseDest W: 6.458188533782959
grad AddEdge W: 3.91504515099921e-15
grad ChooseDest W: 6.434436321258545
grad AddEdge W: 1.3332125212726296e-16
grad ChooseDest W: 4.1721086502075195
grad AddEdge W: 9.942549497231822e-17
grad ChooseDest W: 4.511910438537598
grad AddEdge W: 9.385075927666707e-15
grad ChooseDest W: 5.906307697296143
grad AddEdge W: 5.719957244526637e-17
grad ChooseDest W: 6.316558837890625
grad AddEdge W: 1.0481971735899764e-14
grad ChooseDest W: 3.813770294189453
grad AddEdge W: 2.078071245761653e-15
grad ChooseDest W: 5.450843811035156
grad AddEdge W: 8.287572849750027e-17
grad ChooseDest W: 4.660523891448975
grad AddEdge W: 5.065867311819213e-15
grad ChooseDest W: 6.067849636077881
grad AddEdge W: 1.775872336554421e-15
grad ChooseDest W: 3.973757266998291
grad AddEdge W: 2.2207300699854393e-16
grad ChooseDest W: 5.11883020401001
grad AddEdge W: 7.07151544064561e-17
grad ChooseDest W: 7.255564212799072
grad AddEdge W: 1.5126722006673162e-16
grad ChooseDest W: 6.596619606018066
grad AddEdge W: 2.1646288279575208e-16
grad ChooseDest W: 4.485389709472656
grad AddEdge W: 9.393038196189575e-17
grad ChooseDest W: 6.457171440124512
grad AddEdge W: 5.9703019878430425e-15
grad ChooseDest W: 1.606753945350647
grad AddEdge W: 1.091730243978676e-16
grad ChooseDest W: 5.279962539672852
grad AddEdge W: 2.943181146659781e-15
grad ChooseDest W: 4.856277942657471
grad AddEdge W: 4.2310890519573304e-17
grad ChooseDest W: 6.775236129760742
grad AddEdge W: 1.2608029515705742e-14
grad ChooseDest W: 8.269633293151855
grad AddEdge W: 1.0986730686448541e-16
grad ChooseDest W: 4.506026744842529
grad AddEdge W: 5.807279062199164e-17
grad ChooseDest W: 5.537621021270752
grad AddEdge W: 5.733786380879543e-17
grad ChooseDest W: 5.503916263580322
grad AddEdge W: 7.723037963550347e-17
grad ChooseDest W: 6.029150009155273
grad AddEdge W: 1.2097078066098266e-14
grad ChooseDest W: 6.0852484703063965
grad AddEdge W: 1.2557438270645316e-13
grad ChooseDest W: 6.239920616149902
grad AddEdge W: 8.346448918773592e-17
grad ChooseDest W: 5.7061848640441895
grad AddEdge W: 1.3404972322591212e-14
grad ChooseDest W: 8.566457748413086
grad AddEdge W: 5.154125824052835e-12
grad ChooseDest W: 3.3902668952941895
grad AddEdge W: 6.015259002672393e-16
grad ChooseDest W: 5.943872928619385
grad AddEdge W: 3.344904944861736e-15
grad ChooseDest W: 5.792489528656006
grad AddEdge W: 1.0852236412803339e-16
grad ChooseDest W: 7.565085411071777
grad AddEdge W: 1.3024281675958561e-16
grad ChooseDest W: 7.432226657867432
grad AddEdge W: 5.29101968062772e-15
grad ChooseDest W: 5.29901647567749
=== Epoch 39: Train Loss: 4.1542, Train Log Prob: 0.0417 ===
Total mismatches: 62755
Predicted valid destination but wrong order: 40074
Epoch 39: Validation Loss: 4.1272, Validation Log Prob: 0.0227
Epoch 39: Edge Precision: 0.3709, Recall: 0.3681, F1: 0.3694, Jaccard: 0.2433
Epoch 39: TP: 2.579813886900501, FP: 4.395418754473873, FN: 4.4416607015032215
Epoch 39: Current Learning Rate: 6e-05
[Epoch 39] ‚è±Ô∏è Total: 2855.84s | Current time: 2025-07-16 13:08:56 | üèãÔ∏è Train: 2556.98s | ‚úÖ Val: 298.86s
grad AddEdge W: 9.622475884672743e-14
grad ChooseDest W: 10.671737670898438
grad AddEdge W: 4.596946907603917e-15
grad ChooseDest W: 2.3659772872924805
grad AddEdge W: 9.987999432296916e-17
grad ChooseDest W: 6.726190090179443
grad AddEdge W: 1.7734151469139955e-16
grad ChooseDest W: 3.6047239303588867
grad AddEdge W: 1.6644859158037044e-15
grad ChooseDest W: 4.578842639923096
grad AddEdge W: 6.587894461993361e-15
grad ChooseDest W: 3.806360960006714
grad AddEdge W: 5.9082199785234595e-15
grad ChooseDest W: 4.821476936340332
grad AddEdge W: 6.6334745754345354e-15
grad ChooseDest W: 5.565996170043945
grad AddEdge W: 5.256733785391226e-17
grad ChooseDest W: 5.96386194229126
grad AddEdge W: 1.599900447668952e-16
grad ChooseDest W: 5.689504623413086
grad AddEdge W: 3.799611924463868e-15
grad ChooseDest W: 2.320194959640503
grad AddEdge W: 1.1967885212851252e-15
grad ChooseDest W: 6.831665515899658
grad AddEdge W: 9.408133249751933e-17
grad ChooseDest W: 5.630605697631836
grad AddEdge W: 1.6479003489519752e-16
grad ChooseDest W: 5.978655815124512
grad AddEdge W: 6.893170006623257e-17
grad ChooseDest W: 7.981393337249756
grad AddEdge W: 2.9457947726976527e-16
grad ChooseDest W: 9.281814575195312
grad AddEdge W: 1.6672730181596406e-17
grad ChooseDest W: 5.008062839508057
grad AddEdge W: 4.861005617262182e-17
grad ChooseDest W: 4.5450544357299805
grad AddEdge W: 2.6007098442171065e-16
grad ChooseDest W: 8.121256828308105
grad AddEdge W: 7.379396696849277e-17
grad ChooseDest W: 4.200797080993652
grad AddEdge W: 6.6467296076788696e-12
grad ChooseDest W: 2.897675037384033
grad AddEdge W: 6.187682088206663e-16
grad ChooseDest W: 4.854632377624512
grad AddEdge W: 1.8157374128213526e-16
grad ChooseDest W: 4.213773250579834
grad AddEdge W: 1.0797756312427127e-16
grad ChooseDest W: 5.01793909072876
grad AddEdge W: 1.3867485176670413e-13
grad ChooseDest W: 2.9886279106140137
grad AddEdge W: 1.8904200430829683e-16
grad ChooseDest W: 8.698098182678223
grad AddEdge W: 2.5745947596620723e-16
grad ChooseDest W: 7.311142444610596
grad AddEdge W: 9.952690731541722e-17
grad ChooseDest W: 7.085383892059326
grad AddEdge W: 1.3488908628982903e-15
grad ChooseDest W: 8.313742637634277
grad AddEdge W: 2.4619857527311126e-15
grad ChooseDest W: 5.7018351554870605
grad AddEdge W: 1.1800105985450304e-16
grad ChooseDest W: 5.14600133895874
grad AddEdge W: 1.6410767330558157e-13
grad ChooseDest W: 5.577855587005615
grad AddEdge W: 4.8558294518610704e-17
grad ChooseDest W: 5.278429985046387
grad AddEdge W: 1.1640994816775524e-16
grad ChooseDest W: 7.715969562530518
grad AddEdge W: 1.1828613938081331e-16
grad ChooseDest W: 4.527146339416504
grad AddEdge W: 3.8862440826167854e-14
grad ChooseDest W: 6.371682167053223
grad AddEdge W: 1.0944552416142217e-16
grad ChooseDest W: 3.699875831604004
grad AddEdge W: 7.284485331620453e-17
grad ChooseDest W: 6.8291473388671875
grad AddEdge W: 1.8726724529067245e-16
grad ChooseDest W: 5.4510931968688965
grad AddEdge W: 7.452962170062804e-17
grad ChooseDest W: 6.057882785797119
grad AddEdge W: 2.1974016708873126e-15
grad ChooseDest W: 3.1040401458740234
grad AddEdge W: 1.524682730812688e-16
grad ChooseDest W: 5.615818977355957
grad AddEdge W: 7.49206597546839e-17
grad ChooseDest W: 6.214020252227783
grad AddEdge W: 6.690671560458846e-17
grad ChooseDest W: 6.921772003173828
grad AddEdge W: 3.0275485398820656e-16
grad ChooseDest W: 5.229341983795166
grad AddEdge W: 4.48005593736635e-15
grad ChooseDest W: 3.1248912811279297
grad AddEdge W: 4.491418341723284e-17
grad ChooseDest W: 4.048760414123535
grad AddEdge W: 1.9557033869569193e-16
grad ChooseDest W: 4.927048206329346
grad AddEdge W: 4.47980538766753e-17
grad ChooseDest W: 4.65818452835083
grad AddEdge W: 8.42410132595762e-17
grad ChooseDest W: 5.0809550285339355
grad AddEdge W: 6.611619007813006e-15
grad ChooseDest W: 5.23892879486084
grad AddEdge W: 4.683446335693999e-16
grad ChooseDest W: 4.2969865798950195
grad AddEdge W: 1.752395097885594e-16
grad ChooseDest W: 3.8311350345611572
grad AddEdge W: 1.674508565500989e-16
grad ChooseDest W: 4.236590385437012
grad AddEdge W: 1.2206004809049275e-14
grad ChooseDest W: 4.910702228546143
grad AddEdge W: 5.976679695949614e-17
grad ChooseDest W: 4.817726135253906
grad AddEdge W: 8.500139076585944e-17
grad ChooseDest W: 6.60302734375
grad AddEdge W: 7.89171134010624e-17
grad ChooseDest W: 5.590813636779785
grad AddEdge W: 2.386542298418032e-13
grad ChooseDest W: 4.723945617675781
grad AddEdge W: 6.76473291852343e-15
grad ChooseDest W: 5.483608245849609
grad AddEdge W: 1.7024823579796542e-16
grad ChooseDest W: 6.267025947570801
grad AddEdge W: 4.0576292775049605e-17
grad ChooseDest W: 5.080783367156982
grad AddEdge W: 2.1925960029912857e-16
grad ChooseDest W: 8.999088287353516
grad AddEdge W: 2.1998460756241905e-16
grad ChooseDest W: 5.817463397979736
grad AddEdge W: 8.688545673834902e-17
grad ChooseDest W: 5.560846328735352
grad AddEdge W: 1.4105610553868302e-16
grad ChooseDest W: 6.133866310119629
=== Epoch 40: Train Loss: 4.1191, Train Log Prob: 0.0433 ===
Total mismatches: 62219
Predicted valid destination but wrong order: 40222
Epoch 40: Validation Loss: 4.0411, Validation Log Prob: 0.0246
Epoch 40: Edge Precision: 0.3664, Recall: 0.3638, F1: 0.3650, Jaccard: 0.2400
Epoch 40: TP: 2.5496062992125985, FP: 4.429062276306371, FN: 4.471868289191124
Epoch 40: Current Learning Rate: 6e-05
[Epoch 40] ‚è±Ô∏è Total: 2035.57s | Current time: 2025-07-16 13:42:52 | üèãÔ∏è Train: 1745.88s | ‚úÖ Val: 289.69s
grad AddEdge W: 7.221459859946527e-15
grad ChooseDest W: 10.483892440795898
grad AddEdge W: 1.4963402143041712e-16
grad ChooseDest W: 4.937976360321045
grad AddEdge W: 2.933022309946446e-16
grad ChooseDest W: 6.728801727294922
grad AddEdge W: 4.309113690417007e-17
grad ChooseDest W: 4.17841911315918
grad AddEdge W: 5.870002513687835e-17
grad ChooseDest W: 5.490127086639404
grad AddEdge W: 5.0539317705594524e-15
grad ChooseDest W: 5.576683521270752
grad AddEdge W: 9.09642975737378e-17
grad ChooseDest W: 6.496026992797852
grad AddEdge W: 8.839556428207073e-17
grad ChooseDest W: 9.60793685913086
grad AddEdge W: 4.762664761213713e-17
grad ChooseDest W: 3.8954224586486816
grad AddEdge W: 6.7763647984715996e-15
grad ChooseDest W: 1.8853579759597778
grad AddEdge W: 1.3079440196785965e-15
grad ChooseDest W: 2.5784685611724854
grad AddEdge W: 5.508710536203864e-15
grad ChooseDest W: 6.571511745452881
grad AddEdge W: 8.481762723265042e-15
grad ChooseDest W: 3.9816055297851562
grad AddEdge W: 8.378718624132715e-15
grad ChooseDest W: 7.750034809112549
grad AddEdge W: 1.7209831457344642e-16
grad ChooseDest W: 4.539394378662109
grad AddEdge W: 1.1742577889952956e-16
grad ChooseDest W: 6.629963397979736
grad AddEdge W: 4.302602534916574e-15
grad ChooseDest W: 3.9746572971343994
grad AddEdge W: 1.9117344360605407e-16
grad ChooseDest W: 3.797435760498047
grad AddEdge W: 4.9589282507929447e-17
grad ChooseDest W: 5.134765625
grad AddEdge W: 6.567049087040551e-17
grad ChooseDest W: 4.998953819274902
grad AddEdge W: 1.296827691627729e-16
grad ChooseDest W: 8.540207862854004
grad AddEdge W: 1.6769363736860566e-16
grad ChooseDest W: 6.767282009124756
grad AddEdge W: 3.0985502825922693e-16
grad ChooseDest W: 3.1062769889831543
grad AddEdge W: 5.335436817832315e-15
grad ChooseDest W: 7.990493297576904
grad AddEdge W: 1.2746341049206864e-16
grad ChooseDest W: 5.193833827972412
grad AddEdge W: 3.8113153786960806e-15
grad ChooseDest W: 5.114317417144775
grad AddEdge W: 1.2099120527231893e-16
grad ChooseDest W: 5.449102878570557
grad AddEdge W: 1.351511455782153e-14
grad ChooseDest W: 6.843136787414551
grad AddEdge W: 3.6760482139562887e-17
grad ChooseDest W: 5.1691975593566895
grad AddEdge W: 6.37266495181304e-17
grad ChooseDest W: 7.569398403167725
grad AddEdge W: 1.0513468567951989e-11
grad ChooseDest W: 5.4187822341918945
grad AddEdge W: 1.0145886373665118e-16
grad ChooseDest W: 2.4494874477386475
grad AddEdge W: 2.263494645909941e-16
grad ChooseDest W: 4.8258843421936035
grad AddEdge W: 3.888745797073479e-17
grad ChooseDest W: 5.02709436416626
grad AddEdge W: 1.5182340519330484e-14
grad ChooseDest W: 4.543380260467529
grad AddEdge W: 2.613432477608213e-17
grad ChooseDest W: 7.129045486450195
grad AddEdge W: 4.9315384742965214e-15
grad ChooseDest W: 3.5714821815490723
grad AddEdge W: 6.982964101710584e-17
grad ChooseDest W: 9.392714500427246
grad AddEdge W: 1.8832382095418767e-15
grad ChooseDest W: 6.258235454559326
grad AddEdge W: 3.018971325439491e-15
grad ChooseDest W: 4.530770301818848
grad AddEdge W: 3.406312292438831e-16
grad ChooseDest W: 6.367822647094727
grad AddEdge W: 6.6206560023272625e-15
grad ChooseDest W: 4.859873294830322
grad AddEdge W: 1.3873474292530564e-15
grad ChooseDest W: 6.478305339813232
grad AddEdge W: 7.106369258238349e-15
grad ChooseDest W: 8.136704444885254
grad AddEdge W: 1.1318487922080955e-14
grad ChooseDest W: 2.738837718963623
grad AddEdge W: 5.64841604770815e-17
grad ChooseDest W: 6.823541641235352
grad AddEdge W: 6.128935588754249e-15
grad ChooseDest W: 2.926652669906616
grad AddEdge W: 6.399444454305856e-15
grad ChooseDest W: 4.587340354919434
grad AddEdge W: 6.4105299980030464e-15
grad ChooseDest W: 4.6529221534729
grad AddEdge W: 3.838366222899594e-15
grad ChooseDest W: 3.2228522300720215
grad AddEdge W: 4.54228564805611e-17
grad ChooseDest W: 4.78373384475708
grad AddEdge W: 1.3320010682816869e-14
grad ChooseDest W: 3.2096810340881348
grad AddEdge W: 8.738221509213407e-17
grad ChooseDest W: 3.4474689960479736
grad AddEdge W: 6.906032837202024e-15
grad ChooseDest W: 2.229440927505493
grad AddEdge W: 1.7040799415275146e-16
grad ChooseDest W: 9.260374069213867
grad AddEdge W: 3.334330585548213e-15
grad ChooseDest W: 4.5344109535217285
grad AddEdge W: 1.1654444111791146e-16
grad ChooseDest W: 3.0602355003356934
grad AddEdge W: 1.2580061864215944e-16
grad ChooseDest W: 5.065518856048584
grad AddEdge W: 5.2903297722921815e-15
grad ChooseDest W: 2.9291653633117676
grad AddEdge W: 4.221635502521482e-15
grad ChooseDest W: 9.327852249145508
grad AddEdge W: 6.5808760727238646e-18
grad ChooseDest W: 5.88364315032959
grad AddEdge W: 3.2147629911973225e-15
grad ChooseDest W: 3.9936583042144775
grad AddEdge W: 3.920535671502872e-16
grad ChooseDest W: 6.895097732543945
grad AddEdge W: 1.6698089884816057e-15
grad ChooseDest W: 6.667505741119385
grad AddEdge W: 2.702875848115817e-17
grad ChooseDest W: 3.970794916152954
grad AddEdge W: 4.461763916763258e-17
grad ChooseDest W: 5.629951000213623
=== Epoch 41: Train Loss: 4.0824, Train Log Prob: 0.0444 ===
Total mismatches: 61531
Predicted valid destination but wrong order: 40265
Epoch 41: Validation Loss: 3.9908, Validation Log Prob: 0.0258
Epoch 41: Edge Precision: 0.3690, Recall: 0.3663, F1: 0.3676, Jaccard: 0.2424
Epoch 41: TP: 2.5677881173944166, FP: 4.408876163206872, FN: 4.453686471009306
Epoch 41: Current Learning Rate: 6e-05
[Epoch 41] ‚è±Ô∏è Total: 1968.60s | Current time: 2025-07-16 14:15:40 | üèãÔ∏è Train: 1685.21s | ‚úÖ Val: 283.40s
grad AddEdge W: 8.243391321112459e-13
grad ChooseDest W: 9.572606086730957
grad AddEdge W: 7.697541610093503e-17
grad ChooseDest W: 5.435031414031982
grad AddEdge W: 2.814834214629277e-16
grad ChooseDest W: 4.954767227172852
grad AddEdge W: 2.5497608124397603e-16
grad ChooseDest W: 4.297958850860596
grad AddEdge W: 5.630960551549811e-17
grad ChooseDest W: 5.25248908996582
grad AddEdge W: 3.2027872158725676e-16
grad ChooseDest W: 6.6514506340026855
grad AddEdge W: 2.0400465934180928e-17
grad ChooseDest W: 8.337149620056152
grad AddEdge W: 1.8484171350665056e-16
grad ChooseDest W: 2.871251344680786
grad AddEdge W: 1.4418264973078645e-16
grad ChooseDest W: 3.982194423675537
grad AddEdge W: 1.1123755471023665e-16
grad ChooseDest W: 3.9264514446258545
grad AddEdge W: 3.369157192207521e-15
grad ChooseDest W: 4.030750274658203
grad AddEdge W: 6.047784141404672e-17
grad ChooseDest W: 4.577002048492432
grad AddEdge W: 1.7393117475729652e-15
grad ChooseDest W: 1.1829582452774048
grad AddEdge W: 4.965520483228298e-16
grad ChooseDest W: 5.787310600280762
grad AddEdge W: 8.185403682712655e-15
grad ChooseDest W: 7.382937908172607
grad AddEdge W: 3.58041276790646e-16
grad ChooseDest W: 5.583874225616455
grad AddEdge W: 1.431023915554616e-16
grad ChooseDest W: 3.0566413402557373
grad AddEdge W: 4.4617456261455534e-15
grad ChooseDest W: 5.564638614654541
grad AddEdge W: 5.209554142238115e-12
grad ChooseDest W: 1.215872883796692
grad AddEdge W: 2.759178889125974e-17
grad ChooseDest W: 6.372478485107422
grad AddEdge W: 5.841135895543205e-17
grad ChooseDest W: 5.491973400115967
grad AddEdge W: 1.2759241625691343e-15
grad ChooseDest W: 7.5001726150512695
grad AddEdge W: 6.044684186106179e-15
grad ChooseDest W: 4.116693496704102
grad AddEdge W: 3.373259081603498e-17
grad ChooseDest W: 7.949757099151611
grad AddEdge W: 6.187234748931394e-15
grad ChooseDest W: 5.239809036254883
grad AddEdge W: 1.2869796101269178e-16
grad ChooseDest W: 5.977292060852051
grad AddEdge W: 2.881154512119125e-16
grad ChooseDest W: 4.001979351043701
grad AddEdge W: 1.509752583977249e-16
grad ChooseDest W: 4.554681301116943
grad AddEdge W: 2.3837868240076483e-15
grad ChooseDest W: 2.736384391784668
grad AddEdge W: 2.642224546794969e-13
grad ChooseDest W: 4.408559322357178
grad AddEdge W: 7.081051840491612e-17
grad ChooseDest W: 5.659163951873779
grad AddEdge W: 1.7666289571686104e-16
grad ChooseDest W: 4.119006633758545
grad AddEdge W: 5.510872164285257e-15
grad ChooseDest W: 4.009746551513672
grad AddEdge W: 3.1546024893534756e-17
grad ChooseDest W: 7.161862850189209
grad AddEdge W: 8.508475733671499e-17
grad ChooseDest W: 5.196903228759766
grad AddEdge W: 6.459989098826172e-15
grad ChooseDest W: 7.654411315917969
grad AddEdge W: 9.86615676038926e-15
grad ChooseDest W: 5.092674255371094
grad AddEdge W: 9.792437450110644e-14
grad ChooseDest W: 1.8996970653533936
grad AddEdge W: 5.076411389704871e-16
grad ChooseDest W: 5.5766377449035645
grad AddEdge W: 1.8869352965984049e-16
grad ChooseDest W: 6.8470892906188965
grad AddEdge W: 2.85571864210867e-17
grad ChooseDest W: 3.6888844966888428
grad AddEdge W: 8.232924004528854e-17
grad ChooseDest W: 4.2643046379089355
grad AddEdge W: 6.684244697971554e-17
grad ChooseDest W: 6.6482086181640625
grad AddEdge W: 9.82614268573295e-17
grad ChooseDest W: 4.506814479827881
grad AddEdge W: 6.639984579499556e-17
grad ChooseDest W: 5.589404582977295
grad AddEdge W: 1.0436275076019003e-16
grad ChooseDest W: 6.2479963302612305
grad AddEdge W: 2.3456178650431444e-17
grad ChooseDest W: 4.519205093383789
grad AddEdge W: 3.0150025260953513e-17
grad ChooseDest W: 3.701395034790039
grad AddEdge W: 1.4078850930179966e-16
grad ChooseDest W: 6.210718154907227
grad AddEdge W: 4.710608630904526e-17
grad ChooseDest W: 5.4575419425964355
grad AddEdge W: 2.849706402249059e-15
grad ChooseDest W: 4.3473920822143555
grad AddEdge W: 5.599009542337093e-17
grad ChooseDest W: 7.229683876037598
grad AddEdge W: 1.4206111013060024e-16
grad ChooseDest W: 4.42666482925415
grad AddEdge W: 7.97679810252876e-15
grad ChooseDest W: 2.3921382427215576
grad AddEdge W: 1.4887093209521368e-15
grad ChooseDest W: 6.743464469909668
grad AddEdge W: 1.1864740952072911e-15
grad ChooseDest W: 3.8426716327667236
grad AddEdge W: 4.59212261821794e-17
grad ChooseDest W: 5.481710433959961
grad AddEdge W: 4.0337472497315745e-17
grad ChooseDest W: 7.186745643615723
grad AddEdge W: 1.0519701865623142e-16
grad ChooseDest W: 7.446134090423584
grad AddEdge W: 6.412689161086214e-18
grad ChooseDest W: 5.251955509185791
grad AddEdge W: 3.985243430019384e-15
grad ChooseDest W: 4.204803466796875
grad AddEdge W: 6.817520700010588e-17
grad ChooseDest W: 4.342294692993164
grad AddEdge W: 2.0308328535679912e-15
grad ChooseDest W: 3.08312726020813
grad AddEdge W: 4.3839311842056936e-17
grad ChooseDest W: 4.6796417236328125
grad AddEdge W: 2.4792834691506907e-17
grad ChooseDest W: 3.7975597381591797
grad AddEdge W: 5.1529014426921396e-17
grad ChooseDest W: 6.595119953155518
=== Epoch 42: Train Loss: 4.0402, Train Log Prob: 0.0457 ===
Total mismatches: 60919
Predicted valid destination but wrong order: 40504
Epoch 42: Validation Loss: 3.9829, Validation Log Prob: 0.0261
Epoch 42: Edge Precision: 0.3689, Recall: 0.3663, F1: 0.3675, Jaccard: 0.2420
Epoch 42: TP: 2.5683607730851827, FP: 4.411166785969936, FN: 4.45311381531854
Epoch 42: Current Learning Rate: 6e-05
[Epoch 42] ‚è±Ô∏è Total: 1943.14s | Current time: 2025-07-16 14:48:03 | üèãÔ∏è Train: 1659.80s | ‚úÖ Val: 283.34s
grad AddEdge W: 4.243704098929246e-15
grad ChooseDest W: 9.972338676452637
grad AddEdge W: 5.521790577794043e-17
grad ChooseDest W: 3.1932785511016846
grad AddEdge W: 3.9249154274102177e-17
grad ChooseDest W: 5.633835792541504
grad AddEdge W: 4.724261465290395e-15
grad ChooseDest W: 2.144322395324707
grad AddEdge W: 1.9940586600825947e-17
grad ChooseDest W: 6.137836933135986
grad AddEdge W: 4.2788686584996184e-17
grad ChooseDest W: 4.284711837768555
grad AddEdge W: 4.5855598201775035e-15
grad ChooseDest W: 4.953193187713623
grad AddEdge W: 3.020977311216826e-15
grad ChooseDest W: 3.1518445014953613
grad AddEdge W: 4.025967781506636e-17
grad ChooseDest W: 3.3731136322021484
grad AddEdge W: 1.7324035321436164e-16
grad ChooseDest W: 6.116264343261719
grad AddEdge W: 3.9572316777591876e-13
grad ChooseDest W: 2.502835512161255
grad AddEdge W: 4.8698482177150434e-15
grad ChooseDest W: 7.0646867752075195
grad AddEdge W: 2.6897499476324785e-16
grad ChooseDest W: 4.0279693603515625
grad AddEdge W: 1.3222938695358277e-16
grad ChooseDest W: 1.9539353847503662
grad AddEdge W: 7.293726593423895e-17
grad ChooseDest W: 7.52531099319458
grad AddEdge W: 9.278534561844065e-17
grad ChooseDest W: 5.038440227508545
grad AddEdge W: 1.18594682143279e-13
grad ChooseDest W: 6.121584892272949
grad AddEdge W: 9.014022054284308e-17
grad ChooseDest W: 5.079260349273682
grad AddEdge W: 9.973625018484214e-17
grad ChooseDest W: 5.505196571350098
grad AddEdge W: 5.3181866874590155e-17
grad ChooseDest W: 6.233670711517334
grad AddEdge W: 7.158374698919599e-17
grad ChooseDest W: 5.521358013153076
grad AddEdge W: 1.3221684027805156e-16
grad ChooseDest W: 8.137105941772461
grad AddEdge W: 7.107801590952156e-15
grad ChooseDest W: 3.473531723022461
grad AddEdge W: 2.1091234736079958e-15
grad ChooseDest W: 3.509423017501831
grad AddEdge W: 8.25444923459491e-14
grad ChooseDest W: 3.0751218795776367
grad AddEdge W: 1.3298811650432757e-13
grad ChooseDest W: 8.925047874450684
grad AddEdge W: 5.157168040291688e-17
grad ChooseDest W: 4.722267150878906
grad AddEdge W: 1.7637176784591178e-16
grad ChooseDest W: 3.9957435131073
grad AddEdge W: 2.558656412008829e-17
grad ChooseDest W: 2.636467456817627
grad AddEdge W: 4.9835434914612977e-17
grad ChooseDest W: 2.6728475093841553
grad AddEdge W: 1.3243188076753575e-16
grad ChooseDest W: 6.858463764190674
grad AddEdge W: 2.2118823428656648e-13
grad ChooseDest W: 1.5069597959518433
grad AddEdge W: 5.680117393959665e-15
grad ChooseDest W: 3.3497116565704346
grad AddEdge W: 2.5844969983319487e-15
grad ChooseDest W: 4.678364276885986
grad AddEdge W: 7.946675069825555e-15
grad ChooseDest W: 2.598148822784424
grad AddEdge W: 5.388714343181663e-15
grad ChooseDest W: 5.08559513092041
grad AddEdge W: 4.451050670516165e-16
grad ChooseDest W: 5.8036370277404785
grad AddEdge W: 3.4330186059910485e-15
grad ChooseDest W: 5.3772406578063965
grad AddEdge W: 7.738111179544533e-17
grad ChooseDest W: 6.302818775177002
grad AddEdge W: 2.9564046015158416e-15
grad ChooseDest W: 4.732275485992432
grad AddEdge W: 1.3330848045860514e-16
grad ChooseDest W: 7.31699275970459
grad AddEdge W: 2.0370674197239218e-17
grad ChooseDest W: 6.845504283905029
grad AddEdge W: 7.080715171364858e-15
grad ChooseDest W: 5.858888149261475
grad AddEdge W: 1.1182639083934416e-14
grad ChooseDest W: 6.22865629196167
grad AddEdge W: 3.078853903597245e-13
grad ChooseDest W: 4.323812007904053
grad AddEdge W: 2.6719940525628848e-17
grad ChooseDest W: 3.630948781967163
grad AddEdge W: 8.844268260734412e-15
grad ChooseDest W: 4.62315559387207
grad AddEdge W: 4.1362924993975283e-17
grad ChooseDest W: 5.665468692779541
grad AddEdge W: 2.1163846635483333e-15
grad ChooseDest W: 3.937880277633667
grad AddEdge W: 2.516899957492793e-15
grad ChooseDest W: 2.431053638458252
grad AddEdge W: 3.091494895188335e-17
grad ChooseDest W: 5.669315814971924
grad AddEdge W: 1.7063763325508158e-13
grad ChooseDest W: 3.0066864490509033
grad AddEdge W: 9.481670929696877e-17
grad ChooseDest W: 6.220595836639404
grad AddEdge W: 2.0736690037765357e-15
grad ChooseDest W: 7.51442813873291
grad AddEdge W: 1.5647655210168437e-16
grad ChooseDest W: 5.057106971740723
grad AddEdge W: 2.344954333842979e-16
grad ChooseDest W: 9.360107421875
grad AddEdge W: 2.1863334913655406e-15
grad ChooseDest W: 8.756213188171387
grad AddEdge W: 8.85199391589742e-17
grad ChooseDest W: 4.985318183898926
grad AddEdge W: 8.104309992422169e-17
grad ChooseDest W: 4.316315174102783
grad AddEdge W: 2.0484122018250866e-16
grad ChooseDest W: 5.794050693511963
grad AddEdge W: 1.510075701635638e-13
grad ChooseDest W: 4.5016069412231445
grad AddEdge W: 3.7708912056268196e-13
grad ChooseDest W: 3.2874488830566406
grad AddEdge W: 1.60953620765604e-13
grad ChooseDest W: 6.0292863845825195
grad AddEdge W: 1.7479258079487455e-16
grad ChooseDest W: 5.973520755767822
grad AddEdge W: 7.150684327972799e-15
grad ChooseDest W: 4.733236789703369
grad AddEdge W: 1.083650277580809e-16
grad ChooseDest W: 8.183273315429688
=== Epoch 43: Train Loss: 4.0133, Train Log Prob: 0.0471 ===
Total mismatches: 60641
Predicted valid destination but wrong order: 40701
Epoch 43: Validation Loss: 3.9465, Validation Log Prob: 0.0269
Epoch 43: Edge Precision: 0.3681, Recall: 0.3652, F1: 0.3665, Jaccard: 0.2412
Epoch 43: TP: 2.560200429491768, FP: 4.412741589119542, FN: 4.461274158911954
Epoch 43: Current Learning Rate: 6e-05
[Epoch 43] ‚è±Ô∏è Total: 1942.21s | Current time: 2025-07-16 15:20:26 | üèãÔ∏è Train: 1659.97s | ‚úÖ Val: 282.24s
grad AddEdge W: 6.256742696099556e-15
grad ChooseDest W: 9.37924861907959
grad AddEdge W: 1.593028924648087e-14
grad ChooseDest W: 4.803175449371338
grad AddEdge W: 3.1808626150658372e-15
grad ChooseDest W: 6.340878009796143
grad AddEdge W: 9.175884756548684e-17
grad ChooseDest W: 2.558328866958618
grad AddEdge W: 7.47025969457648e-15
grad ChooseDest W: 5.450275421142578
grad AddEdge W: 3.913001684013959e-15
grad ChooseDest W: 3.737409830093384
grad AddEdge W: 7.896891475974292e-17
grad ChooseDest W: 5.436221599578857
grad AddEdge W: 4.946645611313267e-17
grad ChooseDest W: 5.994145393371582
grad AddEdge W: 8.040525473348385e-15
grad ChooseDest W: 6.0482001304626465
grad AddEdge W: 3.220965866409625e-17
grad ChooseDest W: 6.4952287673950195
grad AddEdge W: 2.198557725276527e-17
grad ChooseDest W: 3.9909069538116455
grad AddEdge W: 8.489770698479837e-18
grad ChooseDest W: 6.382206439971924
grad AddEdge W: 1.1916156910762431e-14
grad ChooseDest W: 5.10589075088501
grad AddEdge W: 1.9005963498508406e-16
grad ChooseDest W: 5.961231231689453
grad AddEdge W: 7.094225770390037e-15
grad ChooseDest W: 4.426483631134033
grad AddEdge W: 3.4419841109245564e-13
grad ChooseDest W: 3.973482608795166
grad AddEdge W: 4.034250215248676e-15
grad ChooseDest W: 5.488424301147461
grad AddEdge W: 7.32047033524447e-17
grad ChooseDest W: 4.484280586242676
grad AddEdge W: 5.1430576625305136e-17
grad ChooseDest W: 9.586091995239258
grad AddEdge W: 5.8178792560175784e-15
grad ChooseDest W: 8.215431213378906
grad AddEdge W: 5.632837258923572e-17
grad ChooseDest W: 6.743821144104004
grad AddEdge W: 6.331286730595177e-17
grad ChooseDest W: 2.9326882362365723
grad AddEdge W: 6.035160703512623e-17
grad ChooseDest W: 6.509395122528076
grad AddEdge W: 2.2032106162221377e-17
grad ChooseDest W: 3.21101713180542
grad AddEdge W: 3.92861888044874e-17
grad ChooseDest W: 7.438416957855225
grad AddEdge W: 9.910543827924227e-15
grad ChooseDest W: 3.6303393840789795
grad AddEdge W: 8.882215786757658e-17
grad ChooseDest W: 5.485091686248779
grad AddEdge W: 5.574079642163725e-17
grad ChooseDest W: 7.120489597320557
grad AddEdge W: 2.8820084272090755e-17
grad ChooseDest W: 8.058534622192383
grad AddEdge W: 2.5071501847534223e-15
grad ChooseDest W: 4.565766334533691
grad AddEdge W: 2.250801129276396e-17
grad ChooseDest W: 5.072936534881592
grad AddEdge W: 3.468127062863555e-15
grad ChooseDest W: 8.113706588745117
grad AddEdge W: 4.5407534713336455e-15
grad ChooseDest W: 3.6287283897399902
grad AddEdge W: 7.520891088998605e-15
grad ChooseDest W: 6.539662837982178
grad AddEdge W: 1.7315418084686831e-15
grad ChooseDest W: 6.3933892250061035
grad AddEdge W: 2.83442403529135e-17
grad ChooseDest W: 5.773149490356445
grad AddEdge W: 8.725251978953066e-17
grad ChooseDest W: 6.596808433532715
grad AddEdge W: 1.0665440501643144e-16
grad ChooseDest W: 7.859239101409912
grad AddEdge W: 3.882637246920787e-15
grad ChooseDest W: 6.0016584396362305
grad AddEdge W: 2.880312773127791e-16
grad ChooseDest W: 3.767584800720215
grad AddEdge W: 2.8182444222144823e-15
grad ChooseDest W: 8.464296340942383
grad AddEdge W: 3.408682925899959e-15
grad ChooseDest W: 6.7865824699401855
grad AddEdge W: 7.830711177148758e-15
grad ChooseDest W: 7.471276760101318
grad AddEdge W: 1.641593389272323e-14
grad ChooseDest W: 6.634653091430664
grad AddEdge W: 2.4074803869502702e-17
grad ChooseDest W: 3.9969563484191895
grad AddEdge W: 1.5443390898832347e-17
grad ChooseDest W: 5.276054859161377
grad AddEdge W: 2.477984299280615e-15
grad ChooseDest W: 4.2869343757629395
grad AddEdge W: 7.691698162724456e-14
grad ChooseDest W: 3.379533290863037
grad AddEdge W: 3.860430742961299e-17
grad ChooseDest W: 5.148430824279785
grad AddEdge W: 8.863465918376796e-17
grad ChooseDest W: 4.122036457061768
grad AddEdge W: 2.0443060438912752e-11
grad ChooseDest W: 2.2138171195983887
grad AddEdge W: 7.125898926326277e-17
grad ChooseDest W: 2.4733591079711914
grad AddEdge W: 7.746301591097788e-17
grad ChooseDest W: 2.8916401863098145
grad AddEdge W: 1.3073352147477575e-15
grad ChooseDest W: 4.288802623748779
grad AddEdge W: 5.3632700160766256e-17
grad ChooseDest W: 10.268040657043457
grad AddEdge W: 5.361360552350608e-17
grad ChooseDest W: 5.93835973739624
grad AddEdge W: 2.75531770928825e-15
grad ChooseDest W: 7.5944037437438965
grad AddEdge W: 4.189847795967039e-13
grad ChooseDest W: 3.1190764904022217
grad AddEdge W: 1.2548427830613956e-16
grad ChooseDest W: 5.759994983673096
grad AddEdge W: 9.889066645802104e-17
grad ChooseDest W: 7.244060516357422
grad AddEdge W: 4.465459018586316e-15
grad ChooseDest W: 5.954104900360107
grad AddEdge W: 6.663720030868398e-17
grad ChooseDest W: 4.2655816078186035
grad AddEdge W: 1.7370025240005132e-16
grad ChooseDest W: 3.6843678951263428
grad AddEdge W: 8.448876774967012e-16
grad ChooseDest W: 5.837369918823242
grad AddEdge W: 3.1309431518358787e-15
grad ChooseDest W: 3.676435947418213
grad AddEdge W: 3.735876665649922e-16
grad ChooseDest W: 5.008542060852051
=== Epoch 44: Train Loss: 3.9807, Train Log Prob: 0.0483 ===
Total mismatches: 60081
Predicted valid destination but wrong order: 40674
Epoch 44: Validation Loss: 3.8550, Validation Log Prob: 0.0292
Epoch 44: Edge Precision: 0.3722, Recall: 0.3697, F1: 0.3708, Jaccard: 0.2447
Epoch 44: TP: 2.591123836793128, FP: 4.38740157480315, FN: 4.430350751610594
Epoch 44: Current Learning Rate: 6e-05
[Epoch 44] ‚è±Ô∏è Total: 1944.17s | Current time: 2025-07-16 15:52:50 | üèãÔ∏è Train: 1661.80s | ‚úÖ Val: 282.37s
grad AddEdge W: 2.1564195063937183e-14
grad ChooseDest W: 8.569085121154785
grad AddEdge W: 1.868202633574284e-17
grad ChooseDest W: 3.6617071628570557
grad AddEdge W: 2.6603953855706703e-15
grad ChooseDest W: 4.507566452026367
grad AddEdge W: 1.7843151374561782e-15
grad ChooseDest W: 7.493779182434082
grad AddEdge W: 3.455515906949241e-17
grad ChooseDest W: 4.337208271026611
grad AddEdge W: 2.6330259047777566e-17
grad ChooseDest W: 5.889077663421631
grad AddEdge W: 4.2528534973625807e-17
grad ChooseDest W: 5.318525314331055
grad AddEdge W: 1.8037194711376922e-16
grad ChooseDest W: 5.440664291381836
grad AddEdge W: 2.8680800291826626e-17
grad ChooseDest W: 4.848118305206299
grad AddEdge W: 1.6499913953660602e-17
grad ChooseDest W: 7.218230724334717
grad AddEdge W: 5.4950961359379765e-17
grad ChooseDest W: 6.376709461212158
grad AddEdge W: 5.942908518812365e-15
grad ChooseDest W: 8.255949020385742
grad AddEdge W: 4.121484642943849e-17
grad ChooseDest W: 2.053910255432129
grad AddEdge W: 1.1743788882369733e-16
grad ChooseDest W: 3.306152105331421
grad AddEdge W: 2.4289760317436652e-15
grad ChooseDest W: 5.1167426109313965
grad AddEdge W: 1.6458137478726764e-13
grad ChooseDest W: 2.0677566528320312
grad AddEdge W: 8.072713572376995e-15
grad ChooseDest W: 9.36799430847168
grad AddEdge W: 8.32619563573278e-12
grad ChooseDest W: 2.610776662826538
grad AddEdge W: 6.541970294356923e-17
grad ChooseDest W: 6.429583549499512
grad AddEdge W: 3.6641715594858126e-13
grad ChooseDest W: 4.999689102172852
grad AddEdge W: 5.6731139561451774e-14
grad ChooseDest W: 2.465956926345825
grad AddEdge W: 2.1741011378496616e-17
grad ChooseDest W: 4.44693660736084
grad AddEdge W: 1.0479474682771259e-14
grad ChooseDest W: 4.549764633178711
grad AddEdge W: 1.1128240086962903e-14
grad ChooseDest W: 3.6277267932891846
grad AddEdge W: 5.418089260503985e-17
grad ChooseDest W: 3.583543062210083
grad AddEdge W: 1.3232968260285482e-17
grad ChooseDest W: 4.068930149078369
grad AddEdge W: 6.93809896348623e-15
grad ChooseDest W: 2.040435314178467
grad AddEdge W: 1.5255837621103299e-16
grad ChooseDest W: 4.849425315856934
grad AddEdge W: 1.3004188466262913e-16
grad ChooseDest W: 5.586756706237793
grad AddEdge W: 2.175458011839269e-16
grad ChooseDest W: 5.843739032745361
grad AddEdge W: 5.0057202036838443e-17
grad ChooseDest W: 6.899410724639893
grad AddEdge W: 8.967208263825237e-17
grad ChooseDest W: 3.239959239959717
grad AddEdge W: 1.1413502148873312e-14
grad ChooseDest W: 3.3359732627868652
grad AddEdge W: 1.4985395882912762e-16
grad ChooseDest W: 5.8116559982299805
grad AddEdge W: 2.4221133774224558e-17
grad ChooseDest W: 9.161273956298828
grad AddEdge W: 5.902421376255014e-17
grad ChooseDest W: 2.3143670558929443
grad AddEdge W: 7.062450864621009e-17
grad ChooseDest W: 3.3337979316711426
grad AddEdge W: 3.5654310050007974e-17
grad ChooseDest W: 4.42787504196167
grad AddEdge W: 6.343110781143255e-17
grad ChooseDest W: 2.188913583755493
grad AddEdge W: 3.370527797395297e-16
grad ChooseDest W: 5.981757640838623
grad AddEdge W: 1.0517665181518913e-17
grad ChooseDest W: 2.732733964920044
grad AddEdge W: 4.61510390586126e-15
grad ChooseDest W: 5.863197326660156
grad AddEdge W: 5.786959588827481e-15
grad ChooseDest W: 10.231673240661621
grad AddEdge W: 6.681572077412395e-18
grad ChooseDest W: 5.462911605834961
grad AddEdge W: 4.779404580706071e-17
grad ChooseDest W: 5.569411754608154
grad AddEdge W: 3.7288020876557765e-15
grad ChooseDest W: 3.505290985107422
grad AddEdge W: 9.18461012234769e-16
grad ChooseDest W: 1.9933310747146606
grad AddEdge W: 3.8997334025861434e-17
grad ChooseDest W: 5.22805643081665
grad AddEdge W: 1.0183231923960662e-16
grad ChooseDest W: 10.230413436889648
grad AddEdge W: 4.220196393544097e-15
grad ChooseDest W: 4.625741958618164
grad AddEdge W: 7.717315488368036e-15
grad ChooseDest W: 4.339112758636475
grad AddEdge W: 4.6509073665018785e-17
grad ChooseDest W: 3.9606828689575195
grad AddEdge W: 1.6935531219381566e-15
grad ChooseDest W: 4.978659629821777
grad AddEdge W: 5.937272811567588e-17
grad ChooseDest W: 2.9267778396606445
grad AddEdge W: 2.795355906384106e-13
grad ChooseDest W: 2.0144450664520264
grad AddEdge W: 6.067959407417086e-17
grad ChooseDest W: 6.397870063781738
grad AddEdge W: 4.59648757857728e-13
grad ChooseDest W: 3.6610450744628906
grad AddEdge W: 4.146864516135556e-15
grad ChooseDest W: 3.846074342727661
grad AddEdge W: 5.610150672571447e-17
grad ChooseDest W: 6.154670238494873
grad AddEdge W: 1.3515980662244988e-17
grad ChooseDest W: 7.692343711853027
grad AddEdge W: 7.717295159577302e-15
grad ChooseDest W: 6.35070276260376
grad AddEdge W: 3.833205674668447e-15
grad ChooseDest W: 6.618132591247559
grad AddEdge W: 4.871353429143632e-13
grad ChooseDest W: 3.018916368484497
grad AddEdge W: 6.27003102838542e-17
grad ChooseDest W: 4.4823150634765625
grad AddEdge W: 3.507423030398721e-13
grad ChooseDest W: 4.105338096618652
grad AddEdge W: 8.829978338458199e-17
grad ChooseDest W: 4.330981254577637
=== Epoch 45: Train Loss: 3.9387, Train Log Prob: 0.0499 ===
Total mismatches: 59383
Predicted valid destination but wrong order: 40908
Epoch 45: Validation Loss: 3.8708, Validation Log Prob: 0.0287
Epoch 45: Edge Precision: 0.3682, Recall: 0.3653, F1: 0.3667, Jaccard: 0.2415
Epoch 45: TP: 2.5600572655690765, FP: 4.409878310665713, FN: 4.461417322834646
Epoch 45: Current Learning Rate: 6e-05
[Epoch 45] ‚è±Ô∏è Total: 1941.90s | Current time: 2025-07-16 16:25:12 | üèãÔ∏è Train: 1661.47s | ‚úÖ Val: 280.42s
grad AddEdge W: 1.7174735483620196e-13
grad ChooseDest W: 10.443034172058105
grad AddEdge W: 2.5999807606077573e-15
grad ChooseDest W: 3.2584853172302246
grad AddEdge W: 3.5376502566247673e-15
grad ChooseDest W: 2.9009315967559814
grad AddEdge W: 4.376280425284068e-17
grad ChooseDest W: 4.479518413543701
grad AddEdge W: 4.9217223285847995e-17
grad ChooseDest W: 4.7502593994140625
grad AddEdge W: 2.8791086628537097e-17
grad ChooseDest W: 7.304587364196777
grad AddEdge W: 9.13214344575688e-17
grad ChooseDest W: 2.65163254737854
grad AddEdge W: 1.6263887561163701e-16
grad ChooseDest W: 3.3689754009246826
grad AddEdge W: 2.8849217970311567e-15
grad ChooseDest W: 3.898688793182373
grad AddEdge W: 8.001435885390817e-13
grad ChooseDest W: 2.547780752182007
grad AddEdge W: 5.218563039716599e-17
grad ChooseDest W: 5.100979804992676
grad AddEdge W: 1.0426725441283201e-16
grad ChooseDest W: 6.286604404449463
grad AddEdge W: 3.3356954351471124e-13
grad ChooseDest W: 3.7464356422424316
grad AddEdge W: 2.034996232261983e-15
grad ChooseDest W: 5.804362773895264
grad AddEdge W: 1.0424933384264607e-14
grad ChooseDest W: 5.344327449798584
grad AddEdge W: 6.12589712275377e-17
grad ChooseDest W: 7.56821870803833
grad AddEdge W: 1.253412223822822e-16
grad ChooseDest W: 3.6887364387512207
grad AddEdge W: 2.6259037142675526e-17
grad ChooseDest W: 2.4318530559539795
grad AddEdge W: 3.996971461441957e-17
grad ChooseDest W: 3.216526746749878
grad AddEdge W: 4.849577448737827e-15
grad ChooseDest W: 3.478022336959839
grad AddEdge W: 2.0399231349249591e-13
grad ChooseDest W: 2.1537511348724365
grad AddEdge W: 9.834233835612699e-17
grad ChooseDest W: 4.565760612487793
grad AddEdge W: 1.6258147589457073e-16
grad ChooseDest W: 4.756359577178955
grad AddEdge W: 2.8569527691128197e-15
grad ChooseDest W: 6.529640197753906
grad AddEdge W: 3.645852151131163e-17
grad ChooseDest W: 4.77757453918457
grad AddEdge W: 3.402702653063777e-13
grad ChooseDest W: 2.8352901935577393
grad AddEdge W: 1.811268255233402e-16
grad ChooseDest W: 7.198837757110596
grad AddEdge W: 4.963524132450738e-16
grad ChooseDest W: 7.173762798309326
grad AddEdge W: 7.985491784001582e-17
grad ChooseDest W: 3.555307388305664
grad AddEdge W: 3.054034042532846e-15
grad ChooseDest W: 6.1503520011901855
grad AddEdge W: 1.712785110884684e-15
grad ChooseDest W: 5.6777663230896
grad AddEdge W: 4.219628299134286e-17
grad ChooseDest W: 5.712040424346924
grad AddEdge W: 4.392952151268401e-15
grad ChooseDest W: 5.145290851593018
grad AddEdge W: 9.950684322247914e-17
grad ChooseDest W: 7.597548484802246
grad AddEdge W: 9.84064878669917e-17
grad ChooseDest W: 4.667307376861572
grad AddEdge W: 2.668125408247313e-15
grad ChooseDest W: 6.083839416503906
grad AddEdge W: 4.56168958469093e-16
grad ChooseDest W: 5.945929527282715
grad AddEdge W: 2.2516736129167374e-15
grad ChooseDest W: 3.6510844230651855
grad AddEdge W: 8.541589427953222e-17
grad ChooseDest W: 6.518428802490234
grad AddEdge W: 8.287791887176232e-17
grad ChooseDest W: 2.8697073459625244
grad AddEdge W: 1.0021018497116563e-16
grad ChooseDest W: 4.839622974395752
grad AddEdge W: 3.033777143720141e-16
grad ChooseDest W: 4.324529647827148
grad AddEdge W: 2.5450271688140296e-15
grad ChooseDest W: 2.5800046920776367
grad AddEdge W: 1.374073787694871e-16
grad ChooseDest W: 4.120899677276611
grad AddEdge W: 1.348358502690941e-15
grad ChooseDest W: 6.586548328399658
grad AddEdge W: 3.9182526530122256e-17
grad ChooseDest W: 4.147963523864746
grad AddEdge W: 8.505976324732609e-17
grad ChooseDest W: 6.24393892288208
grad AddEdge W: 1.833540245427625e-15
grad ChooseDest W: 5.82002592086792
grad AddEdge W: 9.322638147568618e-13
grad ChooseDest W: 5.026368141174316
grad AddEdge W: 4.3604468384010435e-15
grad ChooseDest W: 6.234328269958496
grad AddEdge W: 1.9656297859181272e-17
grad ChooseDest W: 7.349848747253418
grad AddEdge W: 5.1085281262076756e-15
grad ChooseDest W: 3.714238405227661
grad AddEdge W: 2.7011728222009132e-15
grad ChooseDest W: 1.9336423873901367
grad AddEdge W: 4.651507179648587e-14
grad ChooseDest W: 5.141759872436523
grad AddEdge W: 1.0670592844242615e-16
grad ChooseDest W: 7.182745456695557
grad AddEdge W: 3.446110532512268e-17
grad ChooseDest W: 5.6629958152771
grad AddEdge W: 7.49695986713983e-15
grad ChooseDest W: 4.616403579711914
grad AddEdge W: 7.865100714807283e-15
grad ChooseDest W: 5.070295333862305
grad AddEdge W: 1.7020983079474132e-15
grad ChooseDest W: 5.324651718139648
grad AddEdge W: 1.6134475008745992e-14
grad ChooseDest W: 5.460122585296631
grad AddEdge W: 1.8317041162259942e-16
grad ChooseDest W: 4.4429450035095215
grad AddEdge W: 3.3736919948488837e-15
grad ChooseDest W: 4.5994720458984375
grad AddEdge W: 5.377013853564766e-15
grad ChooseDest W: 5.249870777130127
grad AddEdge W: 4.588906870868579e-17
grad ChooseDest W: 6.5389180183410645
grad AddEdge W: 2.087621965758419e-15
grad ChooseDest W: 5.325606346130371
grad AddEdge W: 1.93063266755607e-15
grad ChooseDest W: 5.14471435546875
=== Epoch 46: Train Loss: 3.9118, Train Log Prob: 0.0511 ===
Total mismatches: 58837
Predicted valid destination but wrong order: 40901
Epoch 46: Validation Loss: 3.8145, Validation Log Prob: 0.0303
Epoch 46: Edge Precision: 0.3657, Recall: 0.3627, F1: 0.3640, Jaccard: 0.2389
Epoch 46: TP: 2.5421617752326413, FP: 4.427201145311382, FN: 4.479312813171081
Epoch 46: Current Learning Rate: 6e-05
[Epoch 46] ‚è±Ô∏è Total: 1946.45s | Current time: 2025-07-16 16:57:38 | üèãÔ∏è Train: 1666.78s | ‚úÖ Val: 279.68s
grad AddEdge W: 2.3559024993840367e-15
grad ChooseDest W: 12.366344451904297
grad AddEdge W: 1.4431840867756146e-13
grad ChooseDest W: 3.244300603866577
grad AddEdge W: 5.540812423160312e-17
grad ChooseDest W: 4.156829833984375
grad AddEdge W: 1.6575001439201226e-16
grad ChooseDest W: 3.1820027828216553
grad AddEdge W: 5.385929470904658e-17
grad ChooseDest W: 5.866360187530518
grad AddEdge W: 6.78385643431315e-17
grad ChooseDest W: 4.331468105316162
grad AddEdge W: 9.288461390939191e-17
grad ChooseDest W: 4.680122375488281
grad AddEdge W: 2.3666552186899605e-17
grad ChooseDest W: 5.531354904174805
grad AddEdge W: 1.5581155182875094e-16
grad ChooseDest W: 4.111565113067627
grad AddEdge W: 2.3590383741734498e-16
grad ChooseDest W: 4.683295249938965
grad AddEdge W: 2.286457907633352e-17
grad ChooseDest W: 5.478458881378174
grad AddEdge W: 1.3023300970624319e-16
grad ChooseDest W: 6.4446234703063965
grad AddEdge W: 3.63479201842239e-15
grad ChooseDest W: 6.353958606719971
grad AddEdge W: 6.68812132954312e-15
grad ChooseDest W: 4.921701908111572
grad AddEdge W: 1.0787891025569574e-15
grad ChooseDest W: 8.239288330078125
grad AddEdge W: 3.628580725820174e-13
grad ChooseDest W: 6.238152503967285
grad AddEdge W: 7.45530075391479e-14
grad ChooseDest W: 1.82961905002594
grad AddEdge W: 8.59756970308836e-17
grad ChooseDest W: 14.563638687133789
grad AddEdge W: 1.8195575314134695e-16
grad ChooseDest W: 2.4719531536102295
grad AddEdge W: 9.1786349666493e-17
grad ChooseDest W: 5.646974086761475
grad AddEdge W: 2.7494452298649358e-15
grad ChooseDest W: 4.1349310874938965
grad AddEdge W: 1.9384094226285997e-17
grad ChooseDest W: 7.799890995025635
grad AddEdge W: 7.175854420220229e-14
grad ChooseDest W: 1.5240182876586914
grad AddEdge W: 2.9855109829241043e-15
grad ChooseDest W: 4.035955429077148
grad AddEdge W: 1.8725980781199997e-14
grad ChooseDest W: 5.039975166320801
grad AddEdge W: 6.35629604010735e-17
grad ChooseDest W: 4.321625232696533
grad AddEdge W: 3.54613255043769e-17
grad ChooseDest W: 4.807375431060791
grad AddEdge W: 5.099874864088305e-17
grad ChooseDest W: 4.029109477996826
grad AddEdge W: 2.2074387995540392e-15
grad ChooseDest W: 1.7576035261154175
grad AddEdge W: 3.2801700283513523e-16
grad ChooseDest W: 8.179550170898438
grad AddEdge W: 5.038450549382486e-16
grad ChooseDest W: 7.751173496246338
grad AddEdge W: 4.413774537217627e-17
grad ChooseDest W: 4.671938896179199
grad AddEdge W: 5.766145024698128e-17
grad ChooseDest W: 5.742349147796631
grad AddEdge W: 3.040631228450204e-17
grad ChooseDest W: 2.886861562728882
grad AddEdge W: 2.220537369989939e-16
grad ChooseDest W: 6.474530220031738
grad AddEdge W: 1.982861142600244e-15
grad ChooseDest W: 6.162919998168945
grad AddEdge W: 2.2568553369715656e-15
grad ChooseDest W: 6.441686153411865
grad AddEdge W: 3.023649064890703e-15
grad ChooseDest W: 5.209878444671631
grad AddEdge W: 1.5889412971693595e-16
grad ChooseDest W: 6.067085266113281
grad AddEdge W: 1.013324361283396e-15
grad ChooseDest W: 5.7206926345825195
grad AddEdge W: 4.7382460586589025e-17
grad ChooseDest W: 6.724676132202148
grad AddEdge W: 2.763073354711547e-16
grad ChooseDest W: 5.909324645996094
grad AddEdge W: 2.6734769624080657e-15
grad ChooseDest W: 4.707887649536133
grad AddEdge W: 7.045428202298717e-15
grad ChooseDest W: 3.9849324226379395
grad AddEdge W: 1.9555135986371752e-16
grad ChooseDest W: 6.416645050048828
grad AddEdge W: 4.1329676700864286e-15
grad ChooseDest W: 4.513830661773682
grad AddEdge W: 2.079166247604216e-15
grad ChooseDest W: 8.508008003234863
grad AddEdge W: 2.5542298376788128e-17
grad ChooseDest W: 4.393876075744629
grad AddEdge W: 3.948098983874364e-17
grad ChooseDest W: 6.130677223205566
grad AddEdge W: 3.776104642595193e-15
grad ChooseDest W: 3.0720252990722656
grad AddEdge W: 6.390408968569037e-17
grad ChooseDest W: 7.451804161071777
grad AddEdge W: 5.396312903697914e-17
grad ChooseDest W: 2.2030134201049805
grad AddEdge W: 1.8142696106028793e-15
grad ChooseDest W: 4.848455429077148
grad AddEdge W: 4.67381811218256e-15
grad ChooseDest W: 3.994652509689331
grad AddEdge W: 4.2646923048127866e-15
grad ChooseDest W: 6.57210636138916
grad AddEdge W: 1.07864140118678e-16
grad ChooseDest W: 4.08888578414917
grad AddEdge W: 1.0003047449060921e-14
grad ChooseDest W: 7.295608997344971
grad AddEdge W: 7.420438910684876e-14
grad ChooseDest W: 5.022611141204834
grad AddEdge W: 1.0413652811231311e-15
grad ChooseDest W: 3.1055121421813965
grad AddEdge W: 1.4599680901534795e-16
grad ChooseDest W: 5.489264965057373
grad AddEdge W: 9.279536443001989e-17
grad ChooseDest W: 6.855948448181152
grad AddEdge W: 6.011835533727608e-17
grad ChooseDest W: 3.06815505027771
grad AddEdge W: 2.7809586671510584e-15
grad ChooseDest W: 5.282444953918457
grad AddEdge W: 7.445948154924387e-15
grad ChooseDest W: 6.177104949951172
grad AddEdge W: 2.5887285631781944e-15
grad ChooseDest W: 7.225395202636719
grad AddEdge W: 5.5623662617641616e-15
grad ChooseDest W: 7.750044822692871
=== Epoch 47: Train Loss: 3.8858, Train Log Prob: 0.0522 ===
Total mismatches: 58577
Predicted valid destination but wrong order: 40844
Epoch 47: Validation Loss: 3.7819, Validation Log Prob: 0.0314
Epoch 47: Edge Precision: 0.3683, Recall: 0.3652, F1: 0.3666, Jaccard: 0.2409
Epoch 47: TP: 2.5603435934144594, FP: 4.410880458124552, FN: 4.461130994989262
Epoch 47: Current Learning Rate: 6e-05
[Epoch 47] ‚è±Ô∏è Total: 1941.55s | Current time: 2025-07-16 17:30:00 | üèãÔ∏è Train: 1661.75s | ‚úÖ Val: 279.80s
grad AddEdge W: 5.127530344761766e-12
grad ChooseDest W: 9.285784721374512
grad AddEdge W: 9.578107615975249e-17
grad ChooseDest W: 3.931267738342285
grad AddEdge W: 2.121789919597942e-14
grad ChooseDest W: 6.710013389587402
grad AddEdge W: 3.2199610206363853e-15
grad ChooseDest W: 4.964941501617432
grad AddEdge W: 3.615058268333733e-15
grad ChooseDest W: 4.508008003234863
grad AddEdge W: 3.2609239839518565e-17
grad ChooseDest W: 3.843804359436035
grad AddEdge W: 2.5582949340811434e-16
grad ChooseDest W: 4.35628604888916
grad AddEdge W: 2.444656076964341e-13
grad ChooseDest W: 4.596626281738281
grad AddEdge W: 3.844253075413232e-17
grad ChooseDest W: 3.651423215866089
grad AddEdge W: 5.3993195397884216e-17
grad ChooseDest W: 2.2989087104797363
grad AddEdge W: 1.1332084878762086e-17
grad ChooseDest W: 7.502456188201904
grad AddEdge W: 4.874475558705894e-15
grad ChooseDest W: 5.700238227844238
grad AddEdge W: 9.83908304629617e-15
grad ChooseDest W: 5.727850437164307
grad AddEdge W: 7.438787338388299e-15
grad ChooseDest W: 3.3784780502319336
grad AddEdge W: 7.387912686691633e-17
grad ChooseDest W: 4.65810489654541
grad AddEdge W: 3.309386179935623e-17
grad ChooseDest W: 4.558178424835205
grad AddEdge W: 1.3032140553522305e-16
grad ChooseDest W: 5.306901454925537
grad AddEdge W: 5.643196921854745e-15
grad ChooseDest W: 6.040165901184082
grad AddEdge W: 1.3881027708837704e-14
grad ChooseDest W: 2.901445150375366
grad AddEdge W: 4.7615228681565855e-15
grad ChooseDest W: 3.789625644683838
grad AddEdge W: 2.3532811441705215e-15
grad ChooseDest W: 3.759514570236206
grad AddEdge W: 5.242087725465362e-17
grad ChooseDest W: 7.60122537612915
grad AddEdge W: 1.3656923966817898e-14
grad ChooseDest W: 2.7434353828430176
grad AddEdge W: 1.2804852497016415e-16
grad ChooseDest W: 5.869811058044434
grad AddEdge W: 4.937645581846225e-15
grad ChooseDest W: 4.609687805175781
grad AddEdge W: 6.217373451744123e-15
grad ChooseDest W: 6.061622142791748
grad AddEdge W: 4.830085936837195e-17
grad ChooseDest W: 7.399298191070557
grad AddEdge W: 1.493654458116885e-16
grad ChooseDest W: 5.098216533660889
grad AddEdge W: 1.2073623512030558e-16
grad ChooseDest W: 2.1687560081481934
grad AddEdge W: 4.4368276109032264e-15
grad ChooseDest W: 2.717151641845703
grad AddEdge W: 1.003657081612154e-16
grad ChooseDest W: 5.560495376586914
grad AddEdge W: 1.8043428344473121e-16
grad ChooseDest W: 5.376596450805664
grad AddEdge W: 5.545730534680087e-15
grad ChooseDest W: 4.435700416564941
grad AddEdge W: 5.986355723883014e-17
grad ChooseDest W: 4.877606391906738
grad AddEdge W: 1.3454399712618375e-16
grad ChooseDest W: 6.129322528839111
grad AddEdge W: 5.694341777061584e-17
grad ChooseDest W: 3.886086940765381
grad AddEdge W: 6.044095577617176e-17
grad ChooseDest W: 5.279064655303955
grad AddEdge W: 1.9457953514053102e-16
grad ChooseDest W: 6.277737140655518
grad AddEdge W: 8.988883704596576e-17
grad ChooseDest W: 2.8737387657165527
grad AddEdge W: 4.239881650996524e-17
grad ChooseDest W: 6.361048221588135
grad AddEdge W: 8.668354525954728e-17
grad ChooseDest W: 4.208136081695557
grad AddEdge W: 7.71443832920445e-16
grad ChooseDest W: 10.175830841064453
grad AddEdge W: 1.8835790939810322e-17
grad ChooseDest W: 5.218385219573975
grad AddEdge W: 5.641960068463296e-17
grad ChooseDest W: 12.60607624053955
grad AddEdge W: 9.388230278362282e-15
grad ChooseDest W: 2.588866949081421
grad AddEdge W: 8.12351117054524e-17
grad ChooseDest W: 3.030963897705078
grad AddEdge W: 1.0003077280502533e-16
grad ChooseDest W: 6.531886100769043
grad AddEdge W: 1.5556096178998696e-13
grad ChooseDest W: 1.7141884565353394
grad AddEdge W: 1.791058282045975e-14
grad ChooseDest W: 3.525876760482788
grad AddEdge W: 3.1053091422902156e-17
grad ChooseDest W: 9.921339988708496
grad AddEdge W: 8.636831797775965e-16
grad ChooseDest W: 4.141353607177734
grad AddEdge W: 2.2064372085048994e-13
grad ChooseDest W: 5.217586994171143
grad AddEdge W: 3.198299794136692e-17
grad ChooseDest W: 8.752480506896973
grad AddEdge W: 7.031829961833276e-17
grad ChooseDest W: 4.689822196960449
grad AddEdge W: 1.6776307024747887e-15
grad ChooseDest W: 5.854284286499023
grad AddEdge W: 3.262348455141122e-15
grad ChooseDest W: 3.5027878284454346
grad AddEdge W: 8.375981648921899e-16
grad ChooseDest W: 2.547351837158203
grad AddEdge W: 7.552046496045835e-17
grad ChooseDest W: 4.200240135192871
grad AddEdge W: 2.162200040390608e-15
grad ChooseDest W: 4.977734565734863
grad AddEdge W: 2.534960605752386e-15
grad ChooseDest W: 4.324145317077637
grad AddEdge W: 2.031501486818175e-17
grad ChooseDest W: 4.657459735870361
grad AddEdge W: 4.921501239750676e-16
grad ChooseDest W: 5.042649745941162
grad AddEdge W: 2.201609856300725e-17
grad ChooseDest W: 4.440250396728516
grad AddEdge W: 3.032007797772445e-15
grad ChooseDest W: 7.163992881774902
grad AddEdge W: 9.637950493698766e-17
grad ChooseDest W: 5.204420566558838
grad AddEdge W: 7.946432977221318e-17
grad ChooseDest W: 3.768681764602661
=== Epoch 48: Train Loss: 3.8579, Train Log Prob: 0.0533 ===
Total mismatches: 58081
Predicted valid destination but wrong order: 41145
Epoch 48: Validation Loss: 3.7420, Validation Log Prob: 0.0322
Epoch 48: Edge Precision: 0.3696, Recall: 0.3670, F1: 0.3682, Jaccard: 0.2425
Epoch 48: TP: 2.571224051539012, FP: 4.407015032211882, FN: 4.4502505368647105
Epoch 48: Current Learning Rate: 6e-05
[Epoch 48] ‚è±Ô∏è Total: 1946.58s | Current time: 2025-07-16 18:02:26 | üèãÔ∏è Train: 1665.93s | ‚úÖ Val: 280.65s
grad AddEdge W: 2.147429056339914e-15
grad ChooseDest W: 11.48802375793457
grad AddEdge W: 2.5505419356358064e-17
grad ChooseDest W: 4.490684509277344
grad AddEdge W: 3.507115764432544e-17
grad ChooseDest W: 5.506333827972412
grad AddEdge W: 6.12796192438138e-15
grad ChooseDest W: 5.387846946716309
grad AddEdge W: 1.6358050824246533e-17
grad ChooseDest W: 5.428157806396484
grad AddEdge W: 1.4198740502929932e-17
grad ChooseDest W: 5.030606269836426
grad AddEdge W: 2.2243823701748814e-16
grad ChooseDest W: 4.841951370239258
grad AddEdge W: 4.527527091566939e-17
grad ChooseDest W: 12.706645965576172
grad AddEdge W: 2.7415204028453143e-17
grad ChooseDest W: 3.7887895107269287
grad AddEdge W: 5.0632181898068953e-17
grad ChooseDest W: 5.046143054962158
grad AddEdge W: 2.3794177010107685e-14
grad ChooseDest W: 3.2580862045288086
grad AddEdge W: 2.94036412683545e-15
grad ChooseDest W: 4.7129411697387695
grad AddEdge W: 3.2740555092633603e-16
grad ChooseDest W: 3.7860846519470215
grad AddEdge W: 2.8261310533421384e-17
grad ChooseDest W: 3.330044984817505
grad AddEdge W: 5.210180634910959e-14
grad ChooseDest W: 2.8105101585388184
grad AddEdge W: 1.4728694607313465e-16
grad ChooseDest W: 5.462357044219971
grad AddEdge W: 6.08862502609662e-17
grad ChooseDest W: 7.157488822937012
grad AddEdge W: 7.906895861523652e-15
grad ChooseDest W: 3.0758745670318604
grad AddEdge W: 7.331644552703326e-16
grad ChooseDest W: 4.69984769821167
grad AddEdge W: 2.5417267049350533e-16
grad ChooseDest W: 13.260811805725098
grad AddEdge W: 4.8610291091915787e-17
grad ChooseDest W: 1.3192436695098877
grad AddEdge W: 1.0893021710958124e-16
grad ChooseDest W: 5.4182963371276855
grad AddEdge W: 5.491235518583069e-17
grad ChooseDest W: 5.779780387878418
grad AddEdge W: 3.117752943022998e-15
grad ChooseDest W: 3.5129549503326416
grad AddEdge W: 1.3659717852054857e-16
grad ChooseDest W: 6.72946310043335
grad AddEdge W: 1.3827425081655288e-16
grad ChooseDest W: 4.609669208526611
grad AddEdge W: 4.5304818924174484e-13
grad ChooseDest W: 5.866531848907471
grad AddEdge W: 1.382576092661173e-15
grad ChooseDest W: 4.573114395141602
grad AddEdge W: 1.232609153634101e-11
grad ChooseDest W: 4.081292152404785
grad AddEdge W: 1.2424111981318995e-15
grad ChooseDest W: 4.1847686767578125
grad AddEdge W: 2.546715728994381e-16
grad ChooseDest W: 5.6823272705078125
grad AddEdge W: 1.5159991872654534e-16
grad ChooseDest W: 2.6085853576660156
grad AddEdge W: 3.252381841967254e-15
grad ChooseDest W: 5.7830047607421875
grad AddEdge W: 4.3748828209210985e-15
grad ChooseDest W: 3.5761559009552
grad AddEdge W: 6.977157566978587e-12
grad ChooseDest W: 5.615110874176025
grad AddEdge W: 7.13876668028405e-16
grad ChooseDest W: 5.200305461883545
grad AddEdge W: 2.0781911009236896e-15
grad ChooseDest W: 7.783915996551514
grad AddEdge W: 2.5881048689963294e-17
grad ChooseDest W: 2.7355055809020996
grad AddEdge W: 2.182547254091314e-15
grad ChooseDest W: 2.8576180934906006
grad AddEdge W: 7.231456436910904e-17
grad ChooseDest W: 5.2500505447387695
grad AddEdge W: 3.610926931307949e-17
grad ChooseDest W: 4.106965065002441
grad AddEdge W: 2.1782443201018172e-17
grad ChooseDest W: 3.47344708442688
grad AddEdge W: 1.0359302942170739e-16
grad ChooseDest W: 8.654383659362793
grad AddEdge W: 4.890850399642214e-15
grad ChooseDest W: 6.498960971832275
grad AddEdge W: 2.53079087431129e-15
grad ChooseDest W: 4.305597305297852
grad AddEdge W: 3.435442218716049e-17
grad ChooseDest W: 5.906579494476318
grad AddEdge W: 8.26031360897171e-17
grad ChooseDest W: 9.827126502990723
grad AddEdge W: 6.902470163686313e-17
grad ChooseDest W: 6.769970893859863
grad AddEdge W: 3.369093664736477e-15
grad ChooseDest W: 3.97304368019104
grad AddEdge W: 5.0151153210812216e-17
grad ChooseDest W: 4.035603046417236
grad AddEdge W: 9.115467484607811e-18
grad ChooseDest W: 8.782082557678223
grad AddEdge W: 5.396316212420364e-17
grad ChooseDest W: 4.300293922424316
grad AddEdge W: 6.618537440577281e-17
grad ChooseDest W: 3.4928205013275146
grad AddEdge W: 1.138198802337292e-16
grad ChooseDest W: 5.303738117218018
grad AddEdge W: 1.0013095430337285e-16
grad ChooseDest W: 3.7151596546173096
grad AddEdge W: 1.1583143366450561e-14
grad ChooseDest W: 2.0934720039367676
grad AddEdge W: 3.1962150872348214e-15
grad ChooseDest W: 5.783068656921387
grad AddEdge W: 1.2635699778162923e-11
grad ChooseDest W: 5.4346466064453125
grad AddEdge W: 1.1397998269565006e-15
grad ChooseDest W: 10.691386222839355
grad AddEdge W: 2.7017819405339427e-13
grad ChooseDest W: 3.4590673446655273
grad AddEdge W: 1.811964342674291e-13
grad ChooseDest W: 3.1921446323394775
grad AddEdge W: 1.0798137286670978e-13
grad ChooseDest W: 1.7671269178390503
grad AddEdge W: 7.419137762198775e-16
grad ChooseDest W: 5.827467441558838
grad AddEdge W: 1.6302758432508793e-15
grad ChooseDest W: 4.448183536529541
grad AddEdge W: 2.5336036589708845e-15
grad ChooseDest W: 7.449092388153076
grad AddEdge W: 1.420572190729988e-16
grad ChooseDest W: 4.174947738647461
=== Epoch 49: Train Loss: 3.8429, Train Log Prob: 0.0541 ===
Total mismatches: 57865
Predicted valid destination but wrong order: 41111
Epoch 49: Validation Loss: 3.7300, Validation Log Prob: 0.0328
Epoch 49: Edge Precision: 0.3678, Recall: 0.3643, F1: 0.3659, Jaccard: 0.2410
Epoch 49: TP: 2.552183249821045, FP: 4.410594130279169, FN: 4.469291338582678
Epoch 49: Current Learning Rate: 6e-05
[Epoch 49] ‚è±Ô∏è Total: 1946.41s | Current time: 2025-07-16 18:34:53 | üèãÔ∏è Train: 1664.58s | ‚úÖ Val: 281.83s
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:3638: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
grad AddEdge W: 3.9726777640095023e-13
grad ChooseDest W: 7.558438777923584
grad AddEdge W: 3.135234551618914e-17
grad ChooseDest W: 6.137080669403076
grad AddEdge W: 1.0197975591198807e-16
grad ChooseDest W: 9.93595027923584
grad AddEdge W: 3.046950729511432e-15
grad ChooseDest W: 4.32781457901001
grad AddEdge W: 5.416035205606893e-17
grad ChooseDest W: 6.763071060180664
grad AddEdge W: 2.834326097106824e-17
grad ChooseDest W: 7.005398750305176
grad AddEdge W: 1.4154994162880003e-15
grad ChooseDest W: 3.6509745121002197
grad AddEdge W: 7.722075787061825e-17
grad ChooseDest W: 9.812262535095215
grad AddEdge W: 7.18688905723729e-15
grad ChooseDest W: 2.5950427055358887
grad AddEdge W: 4.643974269479704e-17
grad ChooseDest W: 7.624765396118164
grad AddEdge W: 9.74596572331941e-17
grad ChooseDest W: 4.628538608551025
grad AddEdge W: 7.494153779334474e-17
grad ChooseDest W: 4.437931060791016
grad AddEdge W: 1.7558955727131967e-14
grad ChooseDest W: 5.627477169036865
grad AddEdge W: 1.5382659364433153e-15
grad ChooseDest W: 3.0079145431518555
grad AddEdge W: 2.9912869005914314e-15
grad ChooseDest W: 3.1249606609344482
grad AddEdge W: 4.049319354896804e-15
grad ChooseDest W: 5.316241264343262
grad AddEdge W: 3.8054735248077657e-17
grad ChooseDest W: 2.4389078617095947
grad AddEdge W: 7.887905647544006e-17
grad ChooseDest W: 3.3822600841522217
grad AddEdge W: 2.473655325645435e-15
grad ChooseDest W: 3.7150685787200928
grad AddEdge W: 3.422054399763552e-15
grad ChooseDest W: 2.1284070014953613
grad AddEdge W: 2.9518722340942023e-16
grad ChooseDest W: 7.313523292541504
grad AddEdge W: 8.173264175919195e-14
grad ChooseDest W: 3.5804648399353027
grad AddEdge W: 1.072044539186974e-14
grad ChooseDest W: 4.100022792816162
grad AddEdge W: 4.8373640806713674e-15
grad ChooseDest W: 5.5777201652526855
grad AddEdge W: 5.911340024384671e-15
grad ChooseDest W: 4.707961082458496
grad AddEdge W: 9.43882694443357e-15
grad ChooseDest W: 9.120923042297363
grad AddEdge W: 1.9385981653920496e-15
grad ChooseDest W: 13.776131629943848
grad AddEdge W: 9.059991458774085e-17
grad ChooseDest W: 1.890269160270691
grad AddEdge W: 2.7714598742444262e-17
grad ChooseDest W: 3.5494353771209717
grad AddEdge W: 1.2114320798168167e-16
grad ChooseDest W: 4.695502281188965
grad AddEdge W: 7.779605206048153e-17
grad ChooseDest W: 4.301734447479248
grad AddEdge W: 9.922882980383354e-16
grad ChooseDest W: 4.5802001953125
grad AddEdge W: 5.891174366902253e-17
grad ChooseDest W: 5.503998279571533
grad AddEdge W: 9.072191049320262e-18
grad ChooseDest W: 5.323158264160156
grad AddEdge W: 3.636793557276752e-15
grad ChooseDest W: 1.0661731958389282
grad AddEdge W: 3.0060426804007223e-13
grad ChooseDest W: 7.232350826263428
grad AddEdge W: 1.1258703205080669e-17
grad ChooseDest W: 2.4675519466400146
grad AddEdge W: 2.318577463167684e-16
grad ChooseDest W: 6.062071800231934
grad AddEdge W: 7.231969472061561e-13
grad ChooseDest W: 6.243387699127197
grad AddEdge W: 2.6612047255517718e-15
grad ChooseDest W: 2.0640406608581543
grad AddEdge W: 3.58051245309644e-15
grad ChooseDest W: 6.238238334655762
grad AddEdge W: 8.887505904559955e-15
grad ChooseDest W: 3.8955538272857666
grad AddEdge W: 4.612602617568018e-17
grad ChooseDest W: 5.141605377197266
grad AddEdge W: 3.579665843665659e-15
grad ChooseDest W: 3.3235785961151123
grad AddEdge W: 1.1037238996395519e-16
grad ChooseDest W: 5.953571796417236
grad AddEdge W: 5.111509986584476e-17
grad ChooseDest W: 5.091269016265869
grad AddEdge W: 3.539590213536886e-17
grad ChooseDest W: 4.89281702041626
grad AddEdge W: 4.28146567475079e-15
grad ChooseDest W: 7.925200939178467
grad AddEdge W: 1.4835002800847596e-15
grad ChooseDest W: 5.4160237312316895
grad AddEdge W: 1.5701142693809586e-16
grad ChooseDest W: 6.018374919891357
grad AddEdge W: 4.646270483155482e-15
grad ChooseDest W: 3.160304307937622
grad AddEdge W: 3.63922665941774e-15
grad ChooseDest W: 7.30534553527832
grad AddEdge W: 9.134583645042937e-10
grad ChooseDest W: 1.449153184890747
grad AddEdge W: 6.479614772804716e-17
grad ChooseDest W: 5.5006022453308105
grad AddEdge W: 2.695361964424512e-15
grad ChooseDest W: 7.0225372314453125
grad AddEdge W: 8.472975127014193e-17
grad ChooseDest W: 3.4085330963134766
grad AddEdge W: 2.4275297229862285e-15
grad ChooseDest W: 5.260007858276367
grad AddEdge W: 2.275384606209227e-15
grad ChooseDest W: 3.6424522399902344
grad AddEdge W: 2.6427978186936707e-13
grad ChooseDest W: 3.938664197921753
grad AddEdge W: 2.7915482730576317e-15
grad ChooseDest W: 2.5360896587371826
grad AddEdge W: 1.0640795978781107e-17
grad ChooseDest W: 5.71189546585083
grad AddEdge W: 6.43616793831098e-17
grad ChooseDest W: 2.273963212966919
grad AddEdge W: 2.102479347169733e-15
grad ChooseDest W: 5.090147018432617
grad AddEdge W: 1.8451426578936532e-17
grad ChooseDest W: 7.681207180023193
grad AddEdge W: 5.469257859103127e-15
grad ChooseDest W: 5.702805995941162
grad AddEdge W: 3.5583523241908136e-17
grad ChooseDest W: 3.840914487838745
=== Epoch 50: Train Loss: 3.7963, Train Log Prob: 0.0558 ===
Total mismatches: 57085
Predicted valid destination but wrong order: 41327
Epoch 50: Validation Loss: 3.7300, Validation Log Prob: 0.0327
Epoch 50: Edge Precision: 0.3673, Recall: 0.3640, F1: 0.3655, Jaccard: 0.2408
Epoch 50: TP: 2.5511811023622046, FP: 4.414173228346456, FN: 4.470293486041517
Epoch 50: Current Learning Rate: 6e-05
üíæ Checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/checkpoint_epoch_50.pth
[Epoch 50] ‚è±Ô∏è Total: 1944.75s | Current time: 2025-07-16 19:07:17 | üèãÔ∏è Train: 1662.77s | ‚úÖ Val: 281.98s
Training finished at: 2025-07-16 19:07:17
Training time: 200149.9854941368
‚úÖ Model saved to: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/model.pth
üìà Metrics saved to: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM
Device for model: cuda:3

Epoch-wise Validation Metrics:

Epoch 1:
  Validation Loss: 6.9717, Validation Log Prob: 0.0016
  Edge Precision: 0.3696, Recall: 0.3676, F1: 0.3685, Jaccard: 0.2428
  TP: 2.573514674302076, FP: 4.412884753042233, FN: 4.447959914101647

Epoch 2:
  Validation Loss: 6.7411, Validation Log Prob: 0.0019
  Edge Precision: 0.3704, Recall: 0.3691, F1: 0.3697, Jaccard: 0.2432
  TP: 2.584251968503937, FP: 4.417179670722978, FN: 4.437222619899785

Epoch 3:
  Validation Loss: 6.5174, Validation Log Prob: 0.0025
  Edge Precision: 0.3724, Recall: 0.3708, F1: 0.3715, Jaccard: 0.2450
  TP: 2.596707229778096, FP: 4.401288475304224, FN: 4.424767358625626

Epoch 4:
  Validation Loss: 6.3046, Validation Log Prob: 0.0030
  Edge Precision: 0.3700, Recall: 0.3680, F1: 0.3689, Jaccard: 0.2425
  TP: 2.577236936292054, FP: 4.409305654974946, FN: 4.444237652111668

Epoch 5:
  Validation Loss: 6.2361, Validation Log Prob: 0.0033
  Edge Precision: 0.3706, Recall: 0.3691, F1: 0.3698, Jaccard: 0.2436
  TP: 2.5858267716535432, FP: 4.413314244810308, FN: 4.435647816750179

Epoch 6:
  Validation Loss: 6.1808, Validation Log Prob: 0.0034
  Edge Precision: 0.3706, Recall: 0.3695, F1: 0.3700, Jaccard: 0.2436
  TP: 2.588690050107373, FP: 4.416320687186829, FN: 4.4327845382963496

Epoch 7:
  Validation Loss: 6.1699, Validation Log Prob: 0.0035
  Edge Precision: 0.3705, Recall: 0.3692, F1: 0.3698, Jaccard: 0.2436
  TP: 2.586256263421618, FP: 4.41445955619184, FN: 4.435218324982104

Epoch 8:
  Validation Loss: 6.2737, Validation Log Prob: 0.0033
  Edge Precision: 0.3706, Recall: 0.3693, F1: 0.3699, Jaccard: 0.2436
  TP: 2.5876879026485327, FP: 4.413314244810308, FN: 4.43378668575519

Epoch 9:
  Validation Loss: 6.0833, Validation Log Prob: 0.0038
  Edge Precision: 0.3696, Recall: 0.3682, F1: 0.3689, Jaccard: 0.2428
  TP: 2.5802433786685754, FP: 4.417895490336435, FN: 4.441231209735147

Epoch 10:
  Validation Loss: 6.1530, Validation Log Prob: 0.0036
  Edge Precision: 0.3664, Recall: 0.3650, F1: 0.3657, Jaccard: 0.2401
  TP: 2.5579098067287043, FP: 4.4380816034359345, FN: 4.463564781675018

Epoch 11:
  Validation Loss: 5.9639, Validation Log Prob: 0.0043
  Edge Precision: 0.3716, Recall: 0.3701, F1: 0.3708, Jaccard: 0.2442
  TP: 2.593128131710809, FP: 4.405010737294202, FN: 4.428346456692913

Epoch 12:
  Validation Loss: 6.0777, Validation Log Prob: 0.0039
  Edge Precision: 0.3713, Recall: 0.3700, F1: 0.3706, Jaccard: 0.2442
  TP: 2.5924123120973515, FP: 4.408876163206872, FN: 4.429062276306371

Epoch 13:
  Validation Loss: 5.8391, Validation Log Prob: 0.0049
  Edge Precision: 0.3696, Recall: 0.3679, F1: 0.3687, Jaccard: 0.2426
  TP: 2.5778095919828203, FP: 4.416607015032212, FN: 4.443664996420902

Epoch 14:
  Validation Loss: 5.8328, Validation Log Prob: 0.0049
  Edge Precision: 0.3679, Recall: 0.3663, F1: 0.3670, Jaccard: 0.2412
  TP: 2.5675017895490337, FP: 4.429205440229063, FN: 4.453972798854688

Epoch 15:
  Validation Loss: 5.7054, Validation Log Prob: 0.0054
  Edge Precision: 0.3726, Recall: 0.3706, F1: 0.3715, Jaccard: 0.2449
  TP: 2.597566213314245, FP: 4.390264853256979, FN: 4.423908375089478

Epoch 16:
  Validation Loss: 5.6697, Validation Log Prob: 0.0056
  Edge Precision: 0.3708, Recall: 0.3690, F1: 0.3698, Jaccard: 0.2439
  TP: 2.586113099498926, FP: 4.408017179670723, FN: 4.435361488904796

Epoch 17:
  Validation Loss: 5.6217, Validation Log Prob: 0.0060
  Edge Precision: 0.3714, Recall: 0.3696, F1: 0.3704, Jaccard: 0.2437
  TP: 2.5904080171796706, FP: 4.402433786685755, FN: 4.431066571224052

Epoch 18:
  Validation Loss: 5.4722, Validation Log Prob: 0.0067
  Edge Precision: 0.3673, Recall: 0.3653, F1: 0.3662, Jaccard: 0.2405
  TP: 2.561059413027917, FP: 4.425196850393701, FN: 4.460415175375806

Epoch 19:
  Validation Loss: 5.4861, Validation Log Prob: 0.0066
  Edge Precision: 0.3715, Recall: 0.3696, F1: 0.3705, Jaccard: 0.2444
  TP: 2.5908375089477453, FP: 4.401145311381532, FN: 4.430637079455977

Epoch 20:
  Validation Loss: 5.2805, Validation Log Prob: 0.0080
  Edge Precision: 0.3686, Recall: 0.3665, F1: 0.3675, Jaccard: 0.2419
  TP: 2.5695060844667146, FP: 4.41746599856836, FN: 4.451968503937008

Epoch 21:
  Validation Loss: 5.2971, Validation Log Prob: 0.0078
  Edge Precision: 0.3695, Recall: 0.3673, F1: 0.3683, Jaccard: 0.2424
  TP: 2.5739441660701505, FP: 4.41259842519685, FN: 4.447530422333572

Epoch 22:
  Validation Loss: 5.2251, Validation Log Prob: 0.0084
  Edge Precision: 0.3709, Recall: 0.3692, F1: 0.3700, Jaccard: 0.2442
  TP: 2.5875447387258412, FP: 4.404724409448819, FN: 4.433929849677881

Epoch 23:
  Validation Loss: 5.0754, Validation Log Prob: 0.0095
  Edge Precision: 0.3697, Recall: 0.3676, F1: 0.3686, Jaccard: 0.2428
  TP: 2.5769506084466713, FP: 4.409878310665713, FN: 4.444523979957051

Epoch 24:
  Validation Loss: 5.1174, Validation Log Prob: 0.0092
  Edge Precision: 0.3694, Recall: 0.3675, F1: 0.3684, Jaccard: 0.2425
  TP: 2.5760916249105223, FP: 4.415748031496063, FN: 4.445382963493199

Epoch 25:
  Validation Loss: 5.0194, Validation Log Prob: 0.0102
  Edge Precision: 0.3717, Recall: 0.3697, F1: 0.3706, Jaccard: 0.2445
  TP: 2.5906943450250535, FP: 4.398568360773085, FN: 4.430780243378669

Epoch 26:
  Validation Loss: 4.8927, Validation Log Prob: 0.0113
  Edge Precision: 0.3680, Recall: 0.3657, F1: 0.3668, Jaccard: 0.2414
  TP: 2.5633500357909806, FP: 4.419183965640658, FN: 4.458124552612742

Epoch 27:
  Validation Loss: 4.8115, Validation Log Prob: 0.0122
  Edge Precision: 0.3671, Recall: 0.3646, F1: 0.3657, Jaccard: 0.2407
  TP: 2.5549033643521835, FP: 4.426485325697924, FN: 4.466571224051539

Epoch 28:
  Validation Loss: 4.7522, Validation Log Prob: 0.0129
  Edge Precision: 0.3681, Recall: 0.3658, F1: 0.3669, Jaccard: 0.2415
  TP: 2.562634216177523, FP: 4.419183965640658, FN: 4.458840372226199

Epoch 29:
  Validation Loss: 4.7469, Validation Log Prob: 0.0130
  Edge Precision: 0.3672, Recall: 0.3648, F1: 0.3659, Jaccard: 0.2411
  TP: 2.5567644953471724, FP: 4.424767358625626, FN: 4.464710093056549

Epoch 30:
  Validation Loss: 4.5039, Validation Log Prob: 0.0162
  Edge Precision: 0.3664, Recall: 0.3640, F1: 0.3651, Jaccard: 0.2398
  TP: 2.5504652827487475, FP: 4.430207587687903, FN: 4.471009305654975

Epoch 31:
  Validation Loss: 4.5586, Validation Log Prob: 0.0154
  Edge Precision: 0.3683, Recall: 0.3661, F1: 0.3671, Jaccard: 0.2412
  TP: 2.5650680028632786, FP: 4.419899785254116, FN: 4.456406585540444

Epoch 32:
  Validation Loss: 4.5025, Validation Log Prob: 0.0161
  Edge Precision: 0.3673, Recall: 0.3649, F1: 0.3660, Jaccard: 0.2409
  TP: 2.5559055118110234, FP: 4.421474588403722, FN: 4.465569076592699

Epoch 33:
  Validation Loss: 4.3564, Validation Log Prob: 0.0184
  Edge Precision: 0.3649, Recall: 0.3626, F1: 0.3636, Jaccard: 0.2389
  TP: 2.5401574803149605, FP: 4.44151753758053, FN: 4.481317108088762

Epoch 34:
  Validation Loss: 4.4019, Validation Log Prob: 0.0178
  Edge Precision: 0.3669, Recall: 0.3648, F1: 0.3658, Jaccard: 0.2406
  TP: 2.5559055118110234, FP: 4.429921259842519, FN: 4.465569076592699

Epoch 35:
  Validation Loss: 4.2821, Validation Log Prob: 0.0199
  Edge Precision: 0.3652, Recall: 0.3627, F1: 0.3639, Jaccard: 0.2393
  TP: 2.541159627773801, FP: 4.4386542591267, FN: 4.480314960629921

Epoch 36:
  Validation Loss: 4.2529, Validation Log Prob: 0.0205
  Edge Precision: 0.3670, Recall: 0.3645, F1: 0.3657, Jaccard: 0.2403
  TP: 2.5547602004294916, FP: 4.423335719398712, FN: 4.46671438797423

Epoch 37:
  Validation Loss: 4.1874, Validation Log Prob: 0.0216
  Edge Precision: 0.3639, Recall: 0.3612, F1: 0.3624, Jaccard: 0.2380
  TP: 2.5312813171080886, FP: 4.444953471725126, FN: 4.490193271295634

Epoch 38:
  Validation Loss: 4.1238, Validation Log Prob: 0.0229
  Edge Precision: 0.3683, Recall: 0.3660, F1: 0.3671, Jaccard: 0.2416
  TP: 2.564638511095204, FP: 4.41646385110952, FN: 4.456836077308518

Epoch 39:
  Validation Loss: 4.1272, Validation Log Prob: 0.0227
  Edge Precision: 0.3709, Recall: 0.3681, F1: 0.3694, Jaccard: 0.2433
  TP: 2.579813886900501, FP: 4.395418754473873, FN: 4.4416607015032215

Epoch 40:
  Validation Loss: 4.0411, Validation Log Prob: 0.0246
  Edge Precision: 0.3664, Recall: 0.3638, F1: 0.3650, Jaccard: 0.2400
  TP: 2.5496062992125985, FP: 4.429062276306371, FN: 4.471868289191124

Epoch 41:
  Validation Loss: 3.9908, Validation Log Prob: 0.0258
  Edge Precision: 0.3690, Recall: 0.3663, F1: 0.3676, Jaccard: 0.2424
  TP: 2.5677881173944166, FP: 4.408876163206872, FN: 4.453686471009306

Epoch 42:
  Validation Loss: 3.9829, Validation Log Prob: 0.0261
  Edge Precision: 0.3689, Recall: 0.3663, F1: 0.3675, Jaccard: 0.2420
  TP: 2.5683607730851827, FP: 4.411166785969936, FN: 4.45311381531854

Epoch 43:
  Validation Loss: 3.9465, Validation Log Prob: 0.0269
  Edge Precision: 0.3681, Recall: 0.3652, F1: 0.3665, Jaccard: 0.2412
  TP: 2.560200429491768, FP: 4.412741589119542, FN: 4.461274158911954

Epoch 44:
  Validation Loss: 3.8550, Validation Log Prob: 0.0292
  Edge Precision: 0.3722, Recall: 0.3697, F1: 0.3708, Jaccard: 0.2447
  TP: 2.591123836793128, FP: 4.38740157480315, FN: 4.430350751610594

Epoch 45:
  Validation Loss: 3.8708, Validation Log Prob: 0.0287
  Edge Precision: 0.3682, Recall: 0.3653, F1: 0.3667, Jaccard: 0.2415
  TP: 2.5600572655690765, FP: 4.409878310665713, FN: 4.461417322834646

Epoch 46:
  Validation Loss: 3.8145, Validation Log Prob: 0.0303
  Edge Precision: 0.3657, Recall: 0.3627, F1: 0.3640, Jaccard: 0.2389
  TP: 2.5421617752326413, FP: 4.427201145311382, FN: 4.479312813171081

Epoch 47:
  Validation Loss: 3.7819, Validation Log Prob: 0.0314
  Edge Precision: 0.3683, Recall: 0.3652, F1: 0.3666, Jaccard: 0.2409
  TP: 2.5603435934144594, FP: 4.410880458124552, FN: 4.461130994989262

Epoch 48:
  Validation Loss: 3.7420, Validation Log Prob: 0.0322
  Edge Precision: 0.3696, Recall: 0.3670, F1: 0.3682, Jaccard: 0.2425
  TP: 2.571224051539012, FP: 4.407015032211882, FN: 4.4502505368647105

Epoch 49:
  Validation Loss: 3.7300, Validation Log Prob: 0.0328
  Edge Precision: 0.3678, Recall: 0.3643, F1: 0.3659, Jaccard: 0.2410/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4434: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4449: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))

  TP: 2.552183249821045, FP: 4.410594130279169, FN: 4.469291338582678

Epoch 50:
  Validation Loss: 3.7300, Validation Log Prob: 0.0327
  Edge Precision: 0.3673, Recall: 0.3640, F1: 0.3655, Jaccard: 0.2408
  TP: 2.5511811023622046, FP: 4.414173228346456, FN: 4.470293486041517
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 65.67%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 34.33%
  ‚ùå False Discovery rate (FP/TP+FP): 26.17%
  üéØ Precision (TP/TP+FP): 73.83%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 13.50%
  ‚ö†Ô∏è Std. False Negative rate: 13.50%
  ‚ùå Std. False Discovery rate: 13.69%
  üéØ Std. Precision: 13.69%
üìâ  Average detailed edge-metrics
  F1: 0.69
  Jaccard: 0.55
  TP: 3.80
  FP: 1.35
  FN: 1.95

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 51.66%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 48.34%
  ‚ùå False Discovery rate (FP/TP+FP): 45.10%
  üéØ Precision (TP/TP+FP): 54.90%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 20.02%
  ‚ö†Ô∏è Std. False Negative rate: 20.02%
  ‚ùå Std. False Discovery rate: 20.05%
  üéØ Std. Precision: 20.05%
üìâ  Average detailed edge-metrics
  F1: 0.53
  Jaccard: 0.39
  TP: 3.14
  FP: 2.52
  FN: 2.90

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 44.52%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 55.48%
  ‚ùå False Discovery rate (FP/TP+FP): 53.85%
  üéØ Precision (TP/TP+FP): 46.15%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.18%
  ‚ö†Ô∏è Std. False Negative rate: 19.18%
  ‚ùå Std. False Discovery rate: 19.75%
  üéØ Std. Precision: 19.75%
üìâ  Average detailed edge-metrics
  F1: 0.45
  Jaccard: 0.31
  TP: 2.91
  FP: 3.39
  FN: 3.62

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.80%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.20%
  ‚ùå False Discovery rate (FP/TP+FP): 62.23%
  üéØ Precision (TP/TP+FP): 37.77%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.41%
  ‚ö†Ô∏è Std. False Negative rate: 17.41%
  ‚ùå Std. False Discovery rate: 17.73%
  üéØ Std. Precision: 17.73%
üìâ  Average detailed edge-metrics
  F1: 0.37
  Jaccard: 0.24
  TP: 2.58
  FP: 4.25
  FN: 4.43

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 31.51%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 68.49%
  ‚ùå False Discovery rate (FP/TP+FP): 68.03%
  üéØ Precision (TP/TP+FP): 31.97%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.06%
  ‚ö†Ô∏è Std. False Negative rate: 17.06%
  ‚ùå Std. False Discovery rate: 17.26%
  üéØ Std. Precision: 17.26%
üìâ  Average detailed edge-metrics
  F1: 0.32
  Jaccard: 0.20
  TP: 2.24
  FP: 4.77
  FN: 4.87

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 34.78%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 65.22%
  ‚ùå False Discovery rate (FP/TP+FP): 64.45%
  üéØ Precision (TP/TP+FP): 35.55%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.42%
  ‚ö†Ô∏è Std. False Negative rate: 18.42%
  ‚ùå Std. False Discovery rate: 18.83%
  üéØ Std. Precision: 18.83%
üìâ  Average detailed edge-metrics
  F1: 0.35
  Jaccard: 0.23
  TP: 2.44
  FP: 4.44
  FN: 4.59
[6, 7, 8, 9, 10, 999]
[0.6566666666666666, 0.5165666266506602, 0.4452380952380952, 0.3679963798384851, 0.3151132966749086, 0.34779629818999896]
[np.float64(0.13694078201097648), np.float64(0.2005222823406475), np.float64(0.19753450720266305), np.float64(0.17727307859458774), np.float64(0.17256626663229052), np.float64(0.18829349243880122)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.FIXED

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 71.50%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 28.50%
  ‚ùå False Discovery rate (FP/TP+FP): 22.58%
  üéØ Precision (TP/TP+FP): 77.42%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 23.58%
  ‚ö†Ô∏è Std. False Negative rate: 23.58%
  ‚ùå Std. False Discovery rate: 20.85%
  üéØ Std. Precision: 20.85%
üìâ  Average detailed edge-metrics
  F1: 0.74
  Jaccard: 0.64
  TP: 4.15
  FP: 1.10
  FN: 1.60

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 56.31%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 43.69%
  ‚ùå False Discovery rate (FP/TP+FP): 42.55%
  üéØ Precision (TP/TP+FP): 57.45%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 20.86%
  ‚ö†Ô∏è Std. False Negative rate: 20.86%
  ‚ùå Std. False Discovery rate: 21.33%
  üéØ Std. Precision: 21.33%
üìâ  Average detailed edge-metrics
  F1: 0.57
  Jaccard: 0.43
  TP: 3.43
  FP: 2.50
  FN: 2.61

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 46.57%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 53.43%
  ‚ùå False Discovery rate (FP/TP+FP): 52.82%
  üéØ Precision (TP/TP+FP): 47.18%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.42%
  ‚ö†Ô∏è Std. False Negative rate: 19.42%
  ‚ùå Std. False Discovery rate: 19.37%
  üéØ Std. Precision: 19.37%
üìâ  Average detailed edge-metrics
  F1: 0.47
  Jaccard: 0.33
  TP: 3.06
  FP: 3.38
  FN: 3.47

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 38.52%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 61.48%
  ‚ùå False Discovery rate (FP/TP+FP): 61.23%
  üéØ Precision (TP/TP+FP): 38.77%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.93%
  ‚ö†Ô∏è Std. False Negative rate: 17.93%
  ‚ùå Std. False Discovery rate: 17.99%
  üéØ Std. Precision: 17.99%
üìâ  Average detailed edge-metrics
  F1: 0.39
  Jaccard: 0.26
  TP: 2.71
  FP: 4.26
  FN: 4.31

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 32.88%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 67.12%
  ‚ùå False Discovery rate (FP/TP+FP): 66.99%
  üéØ Precision (TP/TP+FP): 33.01%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.41%
  ‚ö†Ô∏è Std. False Negative rate: 17.41%
  ‚ùå Std. False Discovery rate: 17.45%
  üéØ Std. Precision: 17.45%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.34
  FP: 4.74
  FN: 4.77

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.27%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.73%
  ‚ùå False Discovery rate (FP/TP+FP): 63.50%
  üéØ Precision (TP/TP+FP): 36.50%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.49%
  ‚ö†Ô∏è Std. False Negative rate: 18.49%
  ‚ùå Std. False Discovery rate: 18.60%
  üéØ Std. Precision: 18.60%
üìâ  Average detailed edge-metrics
  F1: 0.36
  Jaccard: 0.24
  TP: 2.54
  FP: 4.44
  FN: 4.48
[6, 7, 8, 9, 10, 999]
[0.715, 0.5631052420968388, 0.4656910569105691, 0.38522927689594355, 0.3288402160140729, 0.3626766540546068]
[np.float64(0.20853157234976835), np.float64(0.21333128628620976), np.float64(0.1937027326512343), np.float64(0.1799137911563128), np.float64(0.17445198145680701), np.float64(0.18604928182880132)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_ASC

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 73.17%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 26.83%
  ‚ùå False Discovery rate (FP/TP+FP): 20.83%
  üéØ Precision (TP/TP+FP): 79.17%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 22.35%
  ‚ö†Ô∏è Std. False Negative rate: 22.35%
  ‚ùå Std. False Discovery rate: 19.86%
  üéØ Std. Precision: 19.86%
üìâ  Average detailed edge-metrics
  F1: 0.76
  Jaccard: 0.66
  TP: 4.20
  FP: 1.05/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4477: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4491: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))

  FN: 1.55

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 54.01%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 45.99%
  ‚ùå False Discovery rate (FP/TP+FP): 42.48%
  üéØ Precision (TP/TP+FP): 57.52%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 20.93%
  ‚ö†Ô∏è Std. False Negative rate: 20.93%
  ‚ùå Std. False Discovery rate: 21.16%
  üéØ Std. Precision: 21.16%
üìâ  Average detailed edge-metrics
  F1: 0.56
  Jaccard: 0.42
  TP: 3.28
  FP: 2.38
  FN: 2.76

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 44.24%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 55.76%
  ‚ùå False Discovery rate (FP/TP+FP): 53.52%
  üéØ Precision (TP/TP+FP): 46.48%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.40%
  ‚ö†Ô∏è Std. False Negative rate: 19.40%
  ‚ùå Std. False Discovery rate: 19.72%
  üéØ Std. Precision: 19.72%
üìâ  Average detailed edge-metrics
  F1: 0.45
  Jaccard: 0.32
  TP: 2.90
  FP: 3.31
  FN: 3.63

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 35.33%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 64.67%
  ‚ùå False Discovery rate (FP/TP+FP): 62.88%
  üéØ Precision (TP/TP+FP): 37.12%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.62%
  ‚ö†Ô∏è Std. False Negative rate: 17.62%
  ‚ùå Std. False Discovery rate: 18.23%
  üéØ Std. Precision: 18.23%
üìâ  Average detailed edge-metrics
  F1: 0.36
  Jaccard: 0.24
  TP: 2.48
  FP: 4.18
  FN: 4.54

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 30.84%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 69.16%
  ‚ùå False Discovery rate (FP/TP+FP): 68.10%
  üéØ Precision (TP/TP+FP): 31.90%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.27%
  ‚ö†Ô∏è Std. False Negative rate: 17.27%
  ‚ùå Std. False Discovery rate: 17.67%
  üéØ Std. Precision: 17.67%
üìâ  Average detailed edge-metrics
  F1: 0.31
  Jaccard: 0.20
  TP: 2.19
  FP: 4.67
  FN: 4.92

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 33.79%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 66.21%
  ‚ùå False Discovery rate (FP/TP+FP): 64.76%
  üéØ Precision (TP/TP+FP): 35.24%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.27%
  ‚ö†Ô∏è Std. False Negative rate: 18.27%
  ‚ùå Std. False Discovery rate: 18.86%
  üéØ Std. Precision: 18.86%
üìâ  Average detailed edge-metrics
  F1: 0.34
  Jaccard: 0.22
  TP: 2.36
  FP: 4.36
  FN: 4.66
[6, 7, 8, 9, 10, 999]
[0.7316666666666667, 0.5400560224089636, 0.44236933797909406, 0.3533231226213682, 0.308404374407991, 0.3379210553226301]
[np.float64(0.19864121760937062), np.float64(0.21158236437566416), np.float64(0.19717800546825423), np.float64(0.18229378505343038), np.float64(0.17670867970147217), np.float64(0.1885588657541175)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 70.83%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 29.17%
  ‚ùå False Discovery rate (FP/TP+FP): 23.17%
  üéØ Precision (TP/TP+FP): 76.83%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 20.81%
  ‚ö†Ô∏è Std. False Negative rate: 20.81%
  ‚ùå Std. False Discovery rate: 17.26%
  üéØ Std. Precision: 17.26%
üìâ  Average detailed edge-metrics
  F1: 0.74
  Jaccard: 0.62
  TP: 4.10
  FP: 1.15
  FN: 1.65

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 54.96%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 45.04%
  ‚ùå False Discovery rate (FP/TP+FP): 43.53%
  üéØ Precision (TP/TP+FP): 56.47%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 22.73%
  ‚ö†Ô∏è Std. False Negative rate: 22.73%
  ‚ùå Std. False Discovery rate: 22.94%
  üéØ Std. Precision: 22.94%
üìâ  Average detailed edge-metrics
  F1: 0.56
  Jaccard: 0.42
  TP: 3.37
  FP: 2.51
  FN: 2.67

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 45.92%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 54.08%
  ‚ùå False Discovery rate (FP/TP+FP): 53.52%
  üéØ Precision (TP/TP+FP): 46.48%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.19%
  ‚ö†Ô∏è Std. False Negative rate: 19.19%
  ‚ùå Std. False Discovery rate: 19.41%
  üéØ Std. Precision: 19.41%
üìâ  Average detailed edge-metrics
  F1: 0.46
  Jaccard: 0.32
  TP: 3.02
  FP: 3.44
  FN: 3.51

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 38.84%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 61.16%
  ‚ùå False Discovery rate (FP/TP+FP): 60.88%
  üéØ Precision (TP/TP+FP): 39.12%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.62%
  ‚ö†Ô∏è Std. False Negative rate: 17.62%
  ‚ùå Std. False Discovery rate: 17.73%
  üéØ Std. Precision: 17.73%
üìâ  Average detailed edge-metrics
  F1: 0.39
  Jaccard: 0.26
  TP: 2.73
  FP: 4.24
  FN: 4.29

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 32.85%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 67.15%
  ‚ùå False Discovery rate (FP/TP+FP): 67.01%
  üéØ Precision (TP/TP+FP): 32.99%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.31%
  ‚ö†Ô∏è Std. False Negative rate: 17.31%
  ‚ùå Std. False Discovery rate: 17.35%
  üéØ Std. Precision: 17.35%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.34
  FP: 4.74
  FN: 4.77

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.30%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.70%
  ‚ùå False Discovery rate (FP/TP+FP): 63.44%
  üéØ Precision (TP/TP+FP): 36.56%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.32%
  ‚ö†Ô∏è Std. False Negative rate: 18.32%
  ‚ùå Std. False Discovery rate: 18.46%
  üéØ Std. Precision: 18.46%
üìâ  Average detailed edge-metrics
  F1: 0.36
  Jaccard: 0.24
  TP: 2.54
  FP: 4.43
  FN: 4.48
[6, 7, 8, 9, 10, 999]
[0.7083333333333333, 0.5495798319327732, 0.459198606271777, 0.3883945976051239, 0.3285403673223358, 0.363042233357194]
[np.float64(0.17263481044614895), np.float64(0.2293548327261791), np.float64(0.19408869341303617), np.float64(0.17732050937425864), np.float64(0.17351471209546748), np.float64(0.18463437614472553)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 66.83%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 33.17%
  ‚ùå False Discovery rate (FP/TP+FP): 27.50%
  üéØ Precision (TP/TP+FP): 72.50%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 22.84%
  ‚ö†Ô∏è Std. False Negative rate: 22.84%
  ‚ùå Std. False Discovery rate: 21.63%
  üéØ Std. Precision: 21.63%
üìâ  Average detailed edge-metrics
  F1: 0.69
  Jaccard: 0.58
  TP: 3.85
  FP: 1.40
  FN: 1.90

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 51.35%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 48.65%
  ‚ùå False Discovery rate (FP/TP+FP): 46.15%
  üéØ Precision (TP/TP+FP): 53.85%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 20.95%
  ‚ö†Ô∏è Std. False Negative rate: 20.95%
  ‚ùå Std. False Discovery rate: 21.81%
  üéØ Std. Precision: 21.81%
üìâ  Average detailed edge-metrics
  F1: 0.52
  Jaccard: 0.39
  TP: 3.13
  FP: 2.64
  FN: 2.91

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 44.26%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 55.74%
  ‚ùå False Discovery rate (FP/TP+FP): 54.08%
  üéØ Precision (TP/TP+FP): 45.92%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.75%
  ‚ö†Ô∏è Std. False Negative rate: 19.75%/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4505: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4525: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5344: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5361: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5377: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5409: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5663: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))

  ‚ùå Std. False Discovery rate: 20.14%
  üéØ Std. Precision: 20.14%
üìâ  Average detailed edge-metrics
  F1: 0.45
  Jaccard: 0.31
  TP: 2.90
  FP: 3.39
  FN: 3.63

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.29%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.71%
  ‚ùå False Discovery rate (FP/TP+FP): 62.34%
  üéØ Precision (TP/TP+FP): 37.66%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.77%
  ‚ö†Ô∏è Std. False Negative rate: 17.77%
  ‚ùå Std. False Discovery rate: 18.25%
  üéØ Std. Precision: 18.25%
üìâ  Average detailed edge-metrics
  F1: 0.37
  Jaccard: 0.24
  TP: 2.55
  FP: 4.21
  FN: 4.47

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 30.53%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 69.47%
  ‚ùå False Discovery rate (FP/TP+FP): 68.48%
  üéØ Precision (TP/TP+FP): 31.52%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 16.90%
  ‚ö†Ô∏è Std. False Negative rate: 16.90%
  ‚ùå Std. False Discovery rate: 17.31%
  üéØ Std. Precision: 17.31%
üìâ  Average detailed edge-metrics
  F1: 0.31
  Jaccard: 0.20
  TP: 2.17
  FP: 4.71
  FN: 4.94

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 33.91%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 66.09%
  ‚ùå False Discovery rate (FP/TP+FP): 64.88%
  üéØ Precision (TP/TP+FP): 35.12%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.14%
  ‚ö†Ô∏è Std. False Negative rate: 18.14%
  ‚ùå Std. False Discovery rate: 18.65%
  üéØ Std. Precision: 18.65%
üìâ  Average detailed edge-metrics
  F1: 0.34
  Jaccard: 0.22
  TP: 2.38
  FP: 4.40
  FN: 4.65
[6, 7, 8, 9, 10, 999]
[0.6683333333333333, 0.513485394157663, 0.44262485481997677, 0.36294903926482874, 0.3052936364419185, 0.3391128949790367]
[np.float64(0.21631381524689233), np.float64(0.2181491095855423), np.float64(0.20142764558255452), np.float64(0.18245824510920103), np.float64(0.17306835726067257), np.float64(0.1865167656009433)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 74.33%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 25.67%
  ‚ùå False Discovery rate (FP/TP+FP): 20.50%
  üéØ Precision (TP/TP+FP): 79.50%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 23.67%
  ‚ö†Ô∏è Std. False Negative rate: 23.67%
  ‚ùå Std. False Discovery rate: 20.59%
  üéØ Std. Precision: 20.59%
üìâ  Average detailed edge-metrics
  F1: 0.77
  Jaccard: 0.67
  TP: 4.30
  FP: 1.00
  FN: 1.45

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 54.38%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 45.62%
  ‚ùå False Discovery rate (FP/TP+FP): 43.73%
  üéØ Precision (TP/TP+FP): 56.27%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 23.05%
  ‚ö†Ô∏è Std. False Negative rate: 23.05%
  ‚ùå Std. False Discovery rate: 23.38%
  üéØ Std. Precision: 23.38%
üìâ  Average detailed edge-metrics
  F1: 0.55
  Jaccard: 0.42
  TP: 3.33
  FP: 2.51
  FN: 2.71

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 46.12%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 53.88%
  ‚ùå False Discovery rate (FP/TP+FP): 53.09%
  üéØ Precision (TP/TP+FP): 46.91%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.70%
  ‚ö†Ô∏è Std. False Negative rate: 18.70%
  ‚ùå Std. False Discovery rate: 19.05%
  üéØ Std. Precision: 19.05%
üìâ  Average detailed edge-metrics
  F1: 0.46
  Jaccard: 0.32
  TP: 3.03
  FP: 3.41
  FN: 3.51

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 39.39%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 60.61%
  ‚ùå False Discovery rate (FP/TP+FP): 60.23%
  üéØ Precision (TP/TP+FP): 39.77%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.79%
  ‚ö†Ô∏è Std. False Negative rate: 17.79%
  ‚ùå Std. False Discovery rate: 17.96%
  üéØ Std. Precision: 17.96%
üìâ  Average detailed edge-metrics
  F1: 0.40
  Jaccard: 0.26
  TP: 2.77
  FP: 4.18
  FN: 4.25

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 32.63%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 67.37%
  ‚ùå False Discovery rate (FP/TP+FP): 67.19%
  üéØ Precision (TP/TP+FP): 32.81%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.35%
  ‚ö†Ô∏è Std. False Negative rate: 17.35%
  ‚ùå Std. False Discovery rate: 17.43%
  üéØ Std. Precision: 17.43%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.32
  FP: 4.75
  FN: 4.79

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.40%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.60%
  ‚ùå False Discovery rate (FP/TP+FP): 63.27%
  üéØ Precision (TP/TP+FP): 36.73%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.45%
  ‚ö†Ô∏è Std. False Negative rate: 18.45%
  ‚ùå Std. False Discovery rate: 18.66%
  üéØ Std. Precision: 18.66%
üìâ  Average detailed edge-metrics
  F1: 0.37
  Jaccard: 0.24
  TP: 2.55
  FP: 4.41
  FN: 4.47
[6, 7, 8, 9, 10, 999]
[0.7433333333333334, 0.5438175270108043, 0.4611730545876887, 0.3939130232989882, 0.32632763774587586, 0.36396035722807374]
[np.float64(0.20589236670325267), np.float64(0.23379142070950623), np.float64(0.1904579663245825), np.float64(0.1795617966598595), np.float64(0.1742737655097083), np.float64(0.1866166199960248)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM
Device for model: cuda:3
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/random
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.FIXED
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/fixed
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_ASC
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/deg_asc
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/deg_desc
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/neigh_min_rem
56596
5636
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_neigh_max_rem__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/neigh_max_rem
56418
6918
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM
Device for model: cuda:3
‚úÖ Code finished successfully at: 2025-07-16 20:23:30
