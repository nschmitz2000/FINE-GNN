nohup: ignoring input
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:2859: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  metadata = torch.load(load_path_metadata)
/home/nschmitz/.pyenv/versions/3.12.9/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
‚úÖ Using device: cuda:3
/home/nschmitz/GNNs/kirigami-database/data/cleaned
Model without constraints:  False
üíø Folder that model data gets saved into:  18__constraints_deg_desc__
Metrics---
Amount unfoldings:  3000
Max files per dir:  50
Batch size:  1
Num prop rounds:  2
Learning rate:  6e-05
Num epochs:  50
Accum steps:  4
Training node ordering strategy:  NodeOrder.DEG_DESC
Add edge is deterministic:  True
Add edge is deterministic threshold:  0.5
Calc PR-Curve:  False
---------------
6_7_8_9_10
Found existing dataset files; skipping save.
Loading dataset from disk‚Ä¶
6_7_8_9_10
Max amount of features per node in a graph:  128
Loaded 46571 graphs with actions!
Train size: 32599, Validation size: 6985, Test size: 6987
Graph(num_nodes=10, num_edges=18,
      ndata_schemes={'a': Scheme(shape=(256,), dtype=torch.float32), 'hv': Scheme(shape=(128,), dtype=torch.float32), 'deg_rem': Scheme(shape=(1,), dtype=torch.int16), 'deg_target': Scheme(shape=(1,), dtype=torch.int16)}
      edata_schemes={'he': Scheme(shape=(1,), dtype=torch.float32)})
[[0. 0. 0. 1. 0. 1. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]]
--------
Graph(num_nodes=10, num_edges=32,
      ndata_schemes={'deg_target': Scheme(shape=(1,), dtype=torch.int16), 'hv': Scheme(shape=(128,), dtype=torch.float32)}
      edata_schemes={})
[[0. 0. 1. 1. 0. 1. 1. 0. 1. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 1. 1.]
 [1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 0. 1. 1. 0. 0. 0. 1. 0. 0.]
 [1. 0. 0. 1. 0. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]
 [1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]
 [0. 1. 0. 0. 0. 0. 0. 1. 1. 0.]]
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC
Device for model: cuda:3
Dir_name to save to:  6_7_8_9_10
Training started at: 2025-07-14 11:33:27
grad AddEdge W: 0.219546839594841
grad ChooseDest W: 3.0705974102020264
grad AddEdge W: 0.04121628776192665
grad ChooseDest W: 2.7127883434295654
grad AddEdge W: 0.0024111380334943533
grad ChooseDest W: 5.693978309631348
grad AddEdge W: 0.0012851187493652105
grad ChooseDest W: 4.007417678833008
grad AddEdge W: 0.002369473222643137
grad ChooseDest W: 3.555184841156006
grad AddEdge W: 0.0005464248824864626
grad ChooseDest W: 3.800445795059204
grad AddEdge W: 0.0006323776324279606
grad ChooseDest W: 2.2109601497650146
grad AddEdge W: 0.0005297975731082261
grad ChooseDest W: 2.498861789703369
grad AddEdge W: 0.00023455572954844683
grad ChooseDest W: 4.295387268066406
grad AddEdge W: 0.0001724790781736374
grad ChooseDest W: 3.933302164077759
grad AddEdge W: 0.00011077587259933352
grad ChooseDest W: 3.3455111980438232
grad AddEdge W: 0.0005242248298600316
grad ChooseDest W: 2.395885705947876
grad AddEdge W: 0.0002348213893128559
grad ChooseDest W: 3.1279869079589844
grad AddEdge W: 0.00013601391401607543
grad ChooseDest W: 2.8502492904663086
grad AddEdge W: 0.0003104816423729062
grad ChooseDest W: 2.375802516937256
grad AddEdge W: 6.623351509915665e-05
grad ChooseDest W: 6.6310248374938965
grad AddEdge W: 9.05316264834255e-05
grad ChooseDest W: 3.227008819580078
grad AddEdge W: 5.161251829122193e-05
grad ChooseDest W: 4.197941780090332
grad AddEdge W: 6.23730375082232e-05
grad ChooseDest W: 1.9944963455200195
grad AddEdge W: 0.00014232123794499785
grad ChooseDest W: 3.446751356124878
grad AddEdge W: 6.201855285326019e-05
grad ChooseDest W: 3.227649688720703
grad AddEdge W: 2.4894487069104798e-05
grad ChooseDest W: 4.731752395629883
grad AddEdge W: 5.5804059229558334e-05
grad ChooseDest W: 2.1991846561431885
grad AddEdge W: 3.230597940273583e-05
grad ChooseDest W: 2.663106679916382
grad AddEdge W: 0.00026126066222786903
grad ChooseDest W: 3.2151215076446533
grad AddEdge W: 0.00221447111107409
grad ChooseDest W: 7.510789714615385e-07
grad AddEdge W: 0.00010531675070524216
grad ChooseDest W: 3.005702257156372
grad AddEdge W: 7.725963951088488e-05
grad ChooseDest W: 4.206911563873291
grad AddEdge W: 1.9591090676840395e-05
grad ChooseDest W: 3.5164194107055664
grad AddEdge W: 4.389452806208283e-05
grad ChooseDest W: 4.976144790649414
grad AddEdge W: 5.974376472295262e-05
grad ChooseDest W: 3.2377755641937256
grad AddEdge W: 1.3355186638364103e-05
grad ChooseDest W: 4.999741077423096
grad AddEdge W: 2.0067223886144347e-05
grad ChooseDest W: 2.733656644821167
grad AddEdge W: 2.5055798687390052e-05
grad ChooseDest W: 2.0130786895751953
grad AddEdge W: 1.0596178981359117e-05
grad ChooseDest W: 3.593296527862549
grad AddEdge W: 7.303192887775367e-06
grad ChooseDest W: 3.22296142578125
grad AddEdge W: 8.547923243895639e-06
grad ChooseDest W: 2.6953070163726807
grad AddEdge W: 6.05803916187142e-06
grad ChooseDest W: 3.299427032470703
grad AddEdge W: 6.323676643660292e-06
grad ChooseDest W: 2.8690226078033447
grad AddEdge W: 2.7090954972663894e-05
grad ChooseDest W: 2.407137155532837
grad AddEdge W: 9.514143130218145e-06
grad ChooseDest W: 3.256734609603882
grad AddEdge W: 4.45152318206965e-06
grad ChooseDest W: 2.4234657287597656
grad AddEdge W: 1.5912506569293328e-05
grad ChooseDest W: 3.367826461791992
grad AddEdge W: 1.5916250049485825e-05
grad ChooseDest W: 5.236166477203369
grad AddEdge W: 5.354041149985278e-06
grad ChooseDest W: 2.3569328784942627
grad AddEdge W: 1.762932151905261e-05
grad ChooseDest W: 1.9296324253082275
grad AddEdge W: 2.3537268134532496e-06
grad ChooseDest W: 3.109168767929077
grad AddEdge W: 0.00016191124450415373
grad ChooseDest W: 3.1155855655670166
grad AddEdge W: 7.404260486509884e-06
grad ChooseDest W: 3.077740430831909
grad AddEdge W: 1.580097341502551e-05
grad ChooseDest W: 1.972703218460083
grad AddEdge W: 2.8005713375023333e-06
grad ChooseDest W: 2.7913920879364014
grad AddEdge W: 2.786805680443649e-06
grad ChooseDest W: 4.411892414093018
grad AddEdge W: 9.252149197891413e-07
grad ChooseDest W: 2.883255958557129
grad AddEdge W: 1.4286902114690747e-06
grad ChooseDest W: 3.6031832695007324
grad AddEdge W: 1.4249843616198632e-06
grad ChooseDest W: 3.2852942943573
grad AddEdge W: 1.255437950931082e-06
grad ChooseDest W: 4.8642578125
grad AddEdge W: 1.1431366147007793e-06
grad ChooseDest W: 2.987117052078247
grad AddEdge W: 1.571987013448961e-06
grad ChooseDest W: 3.7082161903381348
grad AddEdge W: 1.6664242821207154e-06
grad ChooseDest W: 2.9678306579589844
grad AddEdge W: 1.5278897080861498e-06
grad ChooseDest W: 2.8285040855407715
grad AddEdge W: 5.204520221013809e-06
grad ChooseDest W: 2.839951992034912
grad AddEdge W: 0.00012498233991209418
grad ChooseDest W: 2.7893478870391846
grad AddEdge W: 8.353476346201205e-07
grad ChooseDest W: 3.3394007682800293
grad AddEdge W: 3.840307726932224e-06
grad ChooseDest W: 3.994734525680542
grad AddEdge W: 2.207379111496266e-06
grad ChooseDest W: 3.46394681930542
grad AddEdge W: 1.8495203448765096e-06
grad ChooseDest W: 4.084573268890381
=== Epoch 1: Train Loss: 5.5605, Train Log Prob: 0.0111 ===
Total mismatches: 95551
Predicted valid destination but wrong order: 43213
Epoch 1: Validation Loss: 7.1203, Validation Log Prob: 0.0014
Epoch 1: Edge Precision: 0.3770, Recall: 0.3746, F1: 0.3757, Jaccard: 0.2487
Epoch 1: TP: 2.6231925554760203, FP: 4.359198282032928, FN: 4.398282032927702
Epoch 1: warmup, skipping learning rate scheduler
Epoch 1: Current Learning Rate: 6e-05
[Epoch 1] ‚è±Ô∏è Total: 3337.56s | Current time: 2025-07-14 12:29:05 | üèãÔ∏è Train: 2854.84s | ‚úÖ Val: 482.72s
grad AddEdge W: 2.2857827843836276e-06
grad ChooseDest W: 7.95841646194458
grad AddEdge W: 1.923213858390227e-06
grad ChooseDest W: 2.862996816635132
grad AddEdge W: 4.944907777826302e-05
grad ChooseDest W: 1.0282622575759888
grad AddEdge W: 2.0668319393735146e-06
grad ChooseDest W: 2.8707056045532227
grad AddEdge W: 3.9479073166148737e-07
grad ChooseDest W: 5.154532432556152
grad AddEdge W: 3.7572749533865135e-07
grad ChooseDest W: 2.226128578186035
grad AddEdge W: 6.350079502226436e-07
grad ChooseDest W: 2.2167141437530518
grad AddEdge W: 2.7635482524601684e-07
grad ChooseDest W: 3.7203667163848877
grad AddEdge W: 1.1139224625367206e-05
grad ChooseDest W: 4.525994300842285
grad AddEdge W: 1.1651230806819513e-06
grad ChooseDest W: 3.4932901859283447
grad AddEdge W: 4.920262881569215e-07
grad ChooseDest W: 4.470978260040283
grad AddEdge W: 2.3923038838802313e-07
grad ChooseDest W: 4.353015899658203
grad AddEdge W: 1.7493032373749884e-06
grad ChooseDest W: 5.386050701141357
grad AddEdge W: 6.202176905389933e-07
grad ChooseDest W: 3.047133445739746
grad AddEdge W: 2.737340594194393e-07
grad ChooseDest W: 3.904371500015259
grad AddEdge W: 5.132282240083441e-06
grad ChooseDest W: 2.0688514709472656
grad AddEdge W: 1.163759179689805e-06
grad ChooseDest W: 2.489457607269287
grad AddEdge W: 9.294332699028018e-07
grad ChooseDest W: 4.1665778160095215
grad AddEdge W: 1.2521397820819402e-06
grad ChooseDest W: 3.032503843307495
grad AddEdge W: 1.497421919793851e-07
grad ChooseDest W: 2.981095790863037
grad AddEdge W: 8.250589189628954e-08
grad ChooseDest W: 4.642056465148926
grad AddEdge W: 4.039017085233354e-07
grad ChooseDest W: 3.5156710147857666
grad AddEdge W: 5.134249292382265e-08
grad ChooseDest W: 4.699088096618652
grad AddEdge W: 5.068873178970534e-07
grad ChooseDest W: 2.4584672451019287
grad AddEdge W: 1.951850521209053e-07
grad ChooseDest W: 6.412541389465332
grad AddEdge W: 8.219568030654045e-07
grad ChooseDest W: 4.4269232749938965
grad AddEdge W: 7.015901104523437e-08
grad ChooseDest W: 4.968799591064453
grad AddEdge W: 1.0900573954586434e-07
grad ChooseDest W: 2.7455103397369385
grad AddEdge W: 2.6272806508131907e-07
grad ChooseDest W: 3.1658504009246826
grad AddEdge W: 6.761997184412394e-08
grad ChooseDest W: 1.6161655187606812
grad AddEdge W: 6.274806310102576e-07
grad ChooseDest W: 2.7991793155670166
grad AddEdge W: 3.4715071706159506e-07
grad ChooseDest W: 2.71412992477417
grad AddEdge W: 2.4666025666419955e-08
grad ChooseDest W: 2.390235424041748
grad AddEdge W: 4.5518683577938646e-07
grad ChooseDest W: 7.7422943115234375
grad AddEdge W: 2.111930719195243e-08
grad ChooseDest W: 2.1716043949127197
grad AddEdge W: 2.713638203033497e-08
grad ChooseDest W: 3.1034772396087646
grad AddEdge W: 2.3126010262330965e-07
grad ChooseDest W: 2.091111421585083
grad AddEdge W: 1.8245704325181578e-07
grad ChooseDest W: 3.7907357215881348
grad AddEdge W: 1.390373114418253e-07
grad ChooseDest W: 6.701866149902344
grad AddEdge W: 2.0779002625204157e-06
grad ChooseDest W: 2.1767852306365967
grad AddEdge W: 2.0313389370585355e-07
grad ChooseDest W: 4.973896026611328
grad AddEdge W: 2.7890981968425876e-08
grad ChooseDest W: 3.183554172515869
grad AddEdge W: 7.562150727835615e-08
grad ChooseDest W: 5.207027912139893
grad AddEdge W: 1.6236216993092967e-07
grad ChooseDest W: 3.1012887954711914
grad AddEdge W: 1.2671780780237896e-07
grad ChooseDest W: 2.7304108142852783
grad AddEdge W: 1.4437277151557737e-08
grad ChooseDest W: 4.226711273193359
grad AddEdge W: 9.397915334830031e-09
grad ChooseDest W: 3.725571870803833
grad AddEdge W: 1.1426493529143045e-07
grad ChooseDest W: 3.734400749206543
grad AddEdge W: 2.2136443789122495e-08
grad ChooseDest W: 2.673977851867676
grad AddEdge W: 4.080738946754536e-08
grad ChooseDest W: 1.7242510318756104
grad AddEdge W: 1.3443201218876766e-08
grad ChooseDest W: 4.555453300476074
grad AddEdge W: 1.4170419149195368e-07
grad ChooseDest W: 3.4566402435302734
grad AddEdge W: 6.6616308025402304e-09
grad ChooseDest W: 3.835740804672241
grad AddEdge W: 6.507080740902893e-08
grad ChooseDest W: 2.565844774246216
grad AddEdge W: 3.6980523532292864e-07
grad ChooseDest W: 1.34501051902771
grad AddEdge W: 2.9931981337938396e-09
grad ChooseDest W: 4.382454872131348
grad AddEdge W: 3.607637921732021e-08
grad ChooseDest W: 2.480252504348755
grad AddEdge W: 2.4724853275870373e-08
grad ChooseDest W: 2.3059709072113037
grad AddEdge W: 1.0043136988713286e-08
grad ChooseDest W: 4.038061618804932
grad AddEdge W: 4.186503588243795e-09
grad ChooseDest W: 2.9562501907348633
grad AddEdge W: 4.210228166101615e-09
grad ChooseDest W: 2.7326102256774902
grad AddEdge W: 5.500194788510271e-08
grad ChooseDest W: 4.6466240882873535
grad AddEdge W: 3.8029539517481226e-09
grad ChooseDest W: 3.238429307937622
grad AddEdge W: 4.0417131863534905e-09
grad ChooseDest W: 3.99342942237854
grad AddEdge W: 7.242999089385194e-08
grad ChooseDest W: 3.2600815296173096
grad AddEdge W: 1.6978347616714018e-09
grad ChooseDest W: 3.9327569007873535
=== Epoch 2: Train Loss: 5.4514, Train Log Prob: 0.0121 ===
Total mismatches: 93003
Predicted valid destination but wrong order: 44046
Epoch 2: Validation Loss: 6.8909, Validation Log Prob: 0.0018
Epoch 2: Edge Precision: 0.3796, Recall: 0.3774, F1: 0.3784, Jaccard: 0.2510
Epoch 2: TP: 2.643092340730136, FP: 4.3440229062276305, FN: 4.378382247673586
Epoch 2: warmup, skipping learning rate scheduler
Epoch 2: Current Learning Rate: 6e-05
[Epoch 2] ‚è±Ô∏è Total: 3362.35s | Current time: 2025-07-14 13:25:07 | üèãÔ∏è Train: 2878.55s | ‚úÖ Val: 483.81s
grad AddEdge W: 1.650817154086326e-07
grad ChooseDest W: 8.421631813049316
grad AddEdge W: 2.3884103583782235e-08
grad ChooseDest W: 2.7662644386291504
grad AddEdge W: 3.5608131998543513e-09
grad ChooseDest W: 3.005311965942383
grad AddEdge W: 5.992460305748182e-09
grad ChooseDest W: 4.263571262359619
grad AddEdge W: 1.3147920085998521e-08
grad ChooseDest W: 3.3026390075683594
grad AddEdge W: 2.27040675149226e-09
grad ChooseDest W: 4.101308822631836
grad AddEdge W: 2.5422062677193935e-08
grad ChooseDest W: 6.121169090270996
grad AddEdge W: 1.1564929014440395e-09
grad ChooseDest W: 3.7130560874938965
grad AddEdge W: 3.94241048695676e-08
grad ChooseDest W: 1.8988275527954102
grad AddEdge W: 8.190773392868778e-09
grad ChooseDest W: 3.2514445781707764
grad AddEdge W: 1.455177089937365e-09
grad ChooseDest W: 3.399718999862671
grad AddEdge W: 8.004351070844962e-10
grad ChooseDest W: 3.299193859100342
grad AddEdge W: 1.318423814566927e-09
grad ChooseDest W: 3.272184371948242
grad AddEdge W: 7.510119637288426e-09
grad ChooseDest W: 2.740218162536621
grad AddEdge W: 4.370653616803111e-09
grad ChooseDest W: 4.297962188720703
grad AddEdge W: 1.2174510288787133e-08
grad ChooseDest W: 3.0458662509918213
grad AddEdge W: 4.013989141071761e-10
grad ChooseDest W: 2.7647225856781006
grad AddEdge W: 4.7730841501447685e-09
grad ChooseDest W: 3.130476713180542
grad AddEdge W: 5.721292883897888e-10
grad ChooseDest W: 4.2039055824279785
grad AddEdge W: 3.387277125455057e-09
grad ChooseDest W: 5.136993408203125
grad AddEdge W: 3.2406894945324893e-09
grad ChooseDest W: 3.5258188247680664
grad AddEdge W: 1.0162718444561847e-09
grad ChooseDest W: 2.5848000049591064
grad AddEdge W: 4.047322477163107e-09
grad ChooseDest W: 3.5256054401397705
grad AddEdge W: 4.203027703653106e-09
grad ChooseDest W: 3.118046522140503
grad AddEdge W: 1.3162637646502162e-09
grad ChooseDest W: 4.077821254730225
grad AddEdge W: 3.830738226628938e-10
grad ChooseDest W: 4.582640647888184
grad AddEdge W: 1.5910187300960388e-07
grad ChooseDest W: 1.2160669565200806
grad AddEdge W: 1.503403512792545e-09
grad ChooseDest W: 3.116058349609375
grad AddEdge W: 5.02383045386523e-07
grad ChooseDest W: 1.4276225566864014
grad AddEdge W: 3.4147884520052685e-10
grad ChooseDest W: 4.084029197692871
grad AddEdge W: 2.4458773895119634e-10
grad ChooseDest W: 4.239091396331787
grad AddEdge W: 1.796098964002013e-10
grad ChooseDest W: 4.443323135375977
grad AddEdge W: 1.6824561188677478e-10
grad ChooseDest W: 4.161739826202393
grad AddEdge W: 1.0776191744499286e-10
grad ChooseDest W: 3.794239044189453
grad AddEdge W: 1.2189996234646117e-10
grad ChooseDest W: 2.499056816101074
grad AddEdge W: 2.373010010714438e-09
grad ChooseDest W: 4.1750946044921875
grad AddEdge W: 1.4283188520813894e-10
grad ChooseDest W: 7.116795063018799
grad AddEdge W: 7.193364792712487e-10
grad ChooseDest W: 4.420731544494629
grad AddEdge W: 2.4782422780589286e-09
grad ChooseDest W: 2.9570906162261963
grad AddEdge W: 6.729258927862247e-09
grad ChooseDest W: 3.12927508354187
grad AddEdge W: 1.4108819668123829e-09
grad ChooseDest W: 4.616720676422119
grad AddEdge W: 3.441989054886463e-11
grad ChooseDest W: 2.858267068862915
grad AddEdge W: 3.171363172072006e-11
grad ChooseDest W: 4.250199794769287
grad AddEdge W: 5.242813960748549e-10
grad ChooseDest W: 4.027656555175781
grad AddEdge W: 5.220441856579328e-10
grad ChooseDest W: 4.925133228302002
grad AddEdge W: 3.379940494152578e-10
grad ChooseDest W: 2.399667263031006
grad AddEdge W: 4.828285340985339e-11
grad ChooseDest W: 2.6337954998016357
grad AddEdge W: 3.667318948519238e-10
grad ChooseDest W: 3.037348747253418
grad AddEdge W: 8.238327381393518e-11
grad ChooseDest W: 4.63631534576416
grad AddEdge W: 1.8025261838694462e-10
grad ChooseDest W: 4.145137310028076
grad AddEdge W: 1.32253569207208e-10
grad ChooseDest W: 3.559727668762207
grad AddEdge W: 9.686569255040745e-12
grad ChooseDest W: 5.711680889129639
grad AddEdge W: 2.1481009679358287e-11
grad ChooseDest W: 3.684234619140625
grad AddEdge W: 1.0020953684908296e-11
grad ChooseDest W: 4.035366058349609
grad AddEdge W: 2.5839592618370766e-11
grad ChooseDest W: 2.554001569747925
grad AddEdge W: 2.8577892829950713e-10
grad ChooseDest W: 3.3431310653686523
grad AddEdge W: 6.93862300948922e-10
grad ChooseDest W: 3.7717182636260986
grad AddEdge W: 1.8796395551134992e-09
grad ChooseDest W: 2.704418420791626
grad AddEdge W: 1.2661699466676168e-10
grad ChooseDest W: 3.6178677082061768
grad AddEdge W: 1.5470277836548973e-10
grad ChooseDest W: 4.326344013214111
grad AddEdge W: 2.5050653218450236e-10
grad ChooseDest W: 1.8270751237869263
grad AddEdge W: 8.07337269953523e-12
grad ChooseDest W: 4.755082607269287
grad AddEdge W: 6.0007185852251066e-12
grad ChooseDest W: 3.920332193374634
grad AddEdge W: 1.69462412852317e-11
grad ChooseDest W: 5.675825119018555
grad AddEdge W: 1.2560536378353193e-12
grad ChooseDest W: 3.9218316078186035
grad AddEdge W: 1.4662115271901754e-11
grad ChooseDest W: 3.4453189373016357
=== Epoch 3: Train Loss: 5.3931, Train Log Prob: 0.0127 ===
Total mismatches: 92345
Predicted valid destination but wrong order: 44261
Epoch 3: Validation Loss: 6.6188, Validation Log Prob: 0.0022
Epoch 3: Edge Precision: 0.3765, Recall: 0.3751, F1: 0.3757, Jaccard: 0.2486
Epoch 3: TP: 2.62705798138869, FP: 4.37294201861131, FN: 4.394416607015033
Epoch 3: warmup, skipping learning rate scheduler
Epoch 3: Current Learning Rate: 6e-05
[Epoch 3] ‚è±Ô∏è Total: 3368.72s | Current time: 2025-07-14 14:21:16 | üèãÔ∏è Train: 2885.23s | ‚úÖ Val: 483.49s
grad AddEdge W: 4.041028900392263e-10
grad ChooseDest W: 9.550227165222168
grad AddEdge W: 1.2383959135942035e-10
grad ChooseDest W: 5.2117767333984375
grad AddEdge W: 6.225635007517383e-10
grad ChooseDest W: 4.836671352386475
grad AddEdge W: 1.0871276968915655e-11
grad ChooseDest W: 6.361399173736572
grad AddEdge W: 9.834573433398219e-11
grad ChooseDest W: 3.8731961250305176
grad AddEdge W: 1.235445218350506e-10
grad ChooseDest W: 4.933943271636963
grad AddEdge W: 3.8137507041291485e-11
grad ChooseDest W: 3.8652334213256836
grad AddEdge W: 1.3894106134712536e-12
grad ChooseDest W: 2.7973132133483887
grad AddEdge W: 7.386353681471114e-12
grad ChooseDest W: 6.68543815612793
grad AddEdge W: 5.664910381064647e-12
grad ChooseDest W: 2.78052020072937
grad AddEdge W: 1.5981788375335482e-12
grad ChooseDest W: 5.626186847686768
grad AddEdge W: 5.626165852645748e-11
grad ChooseDest W: 3.8507354259490967
grad AddEdge W: 5.597321109211684e-13
grad ChooseDest W: 5.437411308288574
grad AddEdge W: 4.974791065204265e-11
grad ChooseDest W: 3.471268653869629
grad AddEdge W: 1.1410244277199055e-11
grad ChooseDest W: 7.224409580230713
grad AddEdge W: 3.0864507997996338e-12
grad ChooseDest W: 4.308084964752197
grad AddEdge W: 9.453220428667919e-10
grad ChooseDest W: 2.923616647720337
grad AddEdge W: 2.4768235422703566e-13
grad ChooseDest W: 4.341049671173096
grad AddEdge W: 9.300599570008505e-13
grad ChooseDest W: 4.906638145446777
grad AddEdge W: 4.082264248861378e-10
grad ChooseDest W: 3.307749032974243
grad AddEdge W: 5.28619717443668e-11
grad ChooseDest W: 3.867098093032837
grad AddEdge W: 1.3355697624228835e-11
grad ChooseDest W: 2.921349048614502
grad AddEdge W: 1.7382577582186753e-12
grad ChooseDest W: 5.513904571533203
grad AddEdge W: 2.0209526326725458e-12
grad ChooseDest W: 5.371892929077148
grad AddEdge W: 2.3114457396722354e-12
grad ChooseDest W: 5.084347248077393
grad AddEdge W: 2.6477664505364373e-09
grad ChooseDest W: 4.044951915740967
grad AddEdge W: 1.7955761660512493e-12
grad ChooseDest W: 6.447128772735596
grad AddEdge W: 1.4073680101086422e-13
grad ChooseDest W: 2.580801010131836
grad AddEdge W: 4.184971325645742e-13
grad ChooseDest W: 3.766732931137085
grad AddEdge W: 3.0873903694023097e-13
grad ChooseDest W: 4.911546230316162
grad AddEdge W: 8.648747408350477e-13
grad ChooseDest W: 5.089802265167236
grad AddEdge W: 5.2774538211730615e-12
grad ChooseDest W: 2.4449472427368164
grad AddEdge W: 8.302257535966473e-13
grad ChooseDest W: 4.434101104736328
grad AddEdge W: 1.616383486476372e-13
grad ChooseDest W: 6.484283924102783
grad AddEdge W: 1.4312066072205698e-12
grad ChooseDest W: 4.384076118469238
grad AddEdge W: 4.416930146286524e-13
grad ChooseDest W: 5.598381519317627
grad AddEdge W: 5.290339021891965e-13
grad ChooseDest W: 3.762335777282715
grad AddEdge W: 2.838410387805135e-13
grad ChooseDest W: 4.296109676361084
grad AddEdge W: 1.12533099158596e-11
grad ChooseDest W: 5.20004940032959
grad AddEdge W: 2.7267410876961884e-13
grad ChooseDest W: 3.5789921283721924
grad AddEdge W: 6.780889218183095e-12
grad ChooseDest W: 3.7919552326202393
grad AddEdge W: 1.032826059919889e-12
grad ChooseDest W: 4.657336711883545
grad AddEdge W: 2.965537158287207e-13
grad ChooseDest W: 4.2188801765441895
grad AddEdge W: 1.3133610518578642e-11
grad ChooseDest W: 1.875762939453125
grad AddEdge W: 7.1387435893188744e-12
grad ChooseDest W: 4.06106424331665
grad AddEdge W: 2.521757217106846e-13
grad ChooseDest W: 3.169842481613159
grad AddEdge W: 2.6884164382526277e-13
grad ChooseDest W: 3.3416993618011475
grad AddEdge W: 4.3750994580676883e-13
grad ChooseDest W: 3.993953227996826
grad AddEdge W: 3.717457385758044e-13
grad ChooseDest W: 4.637465953826904
grad AddEdge W: 5.280548134173335e-13
grad ChooseDest W: 5.874289512634277
grad AddEdge W: 2.5361218116901063e-13
grad ChooseDest W: 3.000939130783081
grad AddEdge W: 2.2068197963465153e-13
grad ChooseDest W: 2.90159010887146
grad AddEdge W: 7.888861005417303e-14
grad ChooseDest W: 6.111151695251465
grad AddEdge W: 4.644085680252652e-13
grad ChooseDest W: 3.3390955924987793
grad AddEdge W: 2.1987293442101569e-13
grad ChooseDest W: 5.826591968536377
grad AddEdge W: 8.594090716851355e-12
grad ChooseDest W: 5.425343036651611
grad AddEdge W: 1.8831420867115867e-09
grad ChooseDest W: 2.3638453483581543
grad AddEdge W: 1.6912247427156024e-13
grad ChooseDest W: 4.79948091506958
grad AddEdge W: 1.7487061603448095e-13
grad ChooseDest W: 4.279766082763672
grad AddEdge W: 5.226952564892952e-14
grad ChooseDest W: 5.177708625793457
grad AddEdge W: 1.3394515531450768e-12
grad ChooseDest W: 2.6531269550323486
grad AddEdge W: 3.7759428916928073e-10
grad ChooseDest W: 1.944037675857544
grad AddEdge W: 3.9210401418798835e-12
grad ChooseDest W: 4.940837860107422
grad AddEdge W: 1.0755112756334964e-13
grad ChooseDest W: 3.9032232761383057
grad AddEdge W: 4.400048034258752e-12
grad ChooseDest W: 3.8621163368225098
grad AddEdge W: 2.009568076180579e-12
grad ChooseDest W: 5.316255569458008
=== Epoch 4: Train Loss: 5.3464, Train Log Prob: 0.0132 ===
Total mismatches: 91357
Predicted valid destination but wrong order: 44317
Epoch 4: Validation Loss: 6.7102, Validation Log Prob: 0.0021
Epoch 4: Edge Precision: 0.3763, Recall: 0.3740, F1: 0.3751, Jaccard: 0.2483
Epoch 4: TP: 2.6197566213314243, FP: 4.363636363636363, FN: 4.4017179670722975
Epoch 4: warmup, skipping learning rate scheduler
Epoch 4: Current Learning Rate: 6e-05
[Epoch 4] ‚è±Ô∏è Total: 3360.19s | Current time: 2025-07-14 15:17:16 | üèãÔ∏è Train: 2871.05s | ‚úÖ Val: 489.14s
grad AddEdge W: 4.108524284673898e-12
grad ChooseDest W: 8.869027137756348
grad AddEdge W: 4.245358838778079e-12
grad ChooseDest W: 2.7957711219787598
grad AddEdge W: 1.2110679212945508e-13
grad ChooseDest W: 4.131247520446777
grad AddEdge W: 1.5934529493414826e-14
grad ChooseDest W: 4.693556785583496
grad AddEdge W: 1.0534694454668661e-13
grad ChooseDest W: 3.8261446952819824
grad AddEdge W: 1.2810005877231245e-12
grad ChooseDest W: 4.6173787117004395
grad AddEdge W: 1.7213140635075064e-13
grad ChooseDest W: 4.247127532958984
grad AddEdge W: 6.412886443662308e-14
grad ChooseDest W: 3.51651930809021
grad AddEdge W: 5.27129609493443e-14
grad ChooseDest W: 4.017980098724365
grad AddEdge W: 7.661211075260521e-14
grad ChooseDest W: 1.7309073209762573
grad AddEdge W: 2.655636669769701e-14
grad ChooseDest W: 5.6805949211120605
grad AddEdge W: 1.02069203783417e-11
grad ChooseDest W: 4.171437740325928
grad AddEdge W: 1.7753848521526172e-12
grad ChooseDest W: 4.540997505187988
grad AddEdge W: 5.5452894880125836e-12
grad ChooseDest W: 4.43333101272583
grad AddEdge W: 3.403326543651407e-14
grad ChooseDest W: 3.292257785797119
grad AddEdge W: 1.2032374066290458e-13
grad ChooseDest W: 3.5818190574645996
grad AddEdge W: 1.1879630308977984e-13
grad ChooseDest W: 2.6345789432525635
grad AddEdge W: 5.473588907969028e-14
grad ChooseDest W: 5.693434238433838
grad AddEdge W: 2.4903106920354245e-12
grad ChooseDest W: 4.180191993713379
grad AddEdge W: 2.8440823914704927e-12
grad ChooseDest W: 1.4893132448196411
grad AddEdge W: 8.180228955149971e-13
grad ChooseDest W: 4.350406646728516
grad AddEdge W: 9.551803521311991e-14
grad ChooseDest W: 6.0385355949401855
grad AddEdge W: 6.470538216557867e-14
grad ChooseDest W: 6.232551097869873
grad AddEdge W: 2.3307176501285998e-12
grad ChooseDest W: 5.091914653778076
grad AddEdge W: 6.83135785353034e-13
grad ChooseDest W: 5.653298377990723
grad AddEdge W: 1.6519212664434235e-09
grad ChooseDest W: 4.624783515930176
grad AddEdge W: 2.138779604810015e-12
grad ChooseDest W: 2.3463528156280518
grad AddEdge W: 1.2396570315403865e-12
grad ChooseDest W: 3.553035259246826
grad AddEdge W: 6.007324086960975e-13
grad ChooseDest W: 5.640851974487305
grad AddEdge W: 3.8638207217056575e-12
grad ChooseDest W: 5.050503253936768
grad AddEdge W: 9.657817487363982e-14
grad ChooseDest W: 7.776330947875977
grad AddEdge W: 1.2061807715818551e-12
grad ChooseDest W: 4.719998836517334
grad AddEdge W: 9.894552692913013e-15
grad ChooseDest W: 3.687241554260254
grad AddEdge W: 7.412814112539579e-11
grad ChooseDest W: 5.288699626922607
grad AddEdge W: 4.9941452205269285e-14
grad ChooseDest W: 2.487694501876831
grad AddEdge W: 2.411939571678179e-12
grad ChooseDest W: 2.9984326362609863
grad AddEdge W: 3.653302053235885e-12
grad ChooseDest W: 4.476529598236084
grad AddEdge W: 6.904834370284954e-14
grad ChooseDest W: 3.2687950134277344
grad AddEdge W: 6.706093471330055e-13
grad ChooseDest W: 5.412241458892822
grad AddEdge W: 9.023170089523993e-15
grad ChooseDest W: 4.123291492462158
grad AddEdge W: 1.632412521129778e-12
grad ChooseDest W: 4.810131072998047
grad AddEdge W: 3.0592303863763437e-12
grad ChooseDest W: 2.591438055038452
grad AddEdge W: 1.5274579561255774e-12
grad ChooseDest W: 3.2947118282318115
grad AddEdge W: 1.8043442174403568e-12
grad ChooseDest W: 4.673420429229736
grad AddEdge W: 6.9618285745629e-13
grad ChooseDest W: 7.40673303604126
grad AddEdge W: 6.8901152144862365e-12
grad ChooseDest W: 3.279388666152954
grad AddEdge W: 1.4116049908835526e-12
grad ChooseDest W: 4.199773788452148
grad AddEdge W: 1.0698470646712532e-13
grad ChooseDest W: 5.923007965087891
grad AddEdge W: 5.235694148196524e-13
grad ChooseDest W: 4.69971227645874
grad AddEdge W: 4.1835025027525674e-13
grad ChooseDest W: 5.123319625854492
grad AddEdge W: 1.051928943257563e-12
grad ChooseDest W: 4.078550338745117
grad AddEdge W: 8.391043936073483e-13
grad ChooseDest W: 3.0024704933166504
grad AddEdge W: 6.211209389699046e-13
grad ChooseDest W: 3.6971421241760254
grad AddEdge W: 3.2341899883041314e-13
grad ChooseDest W: 3.9952704906463623
grad AddEdge W: 2.0244619376068654e-14
grad ChooseDest W: 4.083967685699463
grad AddEdge W: 1.934284329929703e-12
grad ChooseDest W: 8.082213401794434
grad AddEdge W: 5.974696293264969e-10
grad ChooseDest W: 5.012424468994141
grad AddEdge W: 1.5588100255037318e-10
grad ChooseDest W: 3.836488723754883
grad AddEdge W: 1.829683662067129e-14
grad ChooseDest W: 3.4892735481262207
grad AddEdge W: 2.7004450514926323e-14
grad ChooseDest W: 3.9296207427978516
grad AddEdge W: 3.041855233410634e-13
grad ChooseDest W: 7.741159915924072
grad AddEdge W: 2.370401204693836e-14
grad ChooseDest W: 4.981879234313965
grad AddEdge W: 5.12148862815559e-13
grad ChooseDest W: 3.1789000034332275
grad AddEdge W: 1.00784503220255e-14
grad ChooseDest W: 4.488290786743164
grad AddEdge W: 5.920087553859532e-13
grad ChooseDest W: 6.063211441040039
grad AddEdge W: 5.647581469321594e-13
grad ChooseDest W: 4.0799174308776855
=== Epoch 5: Train Loss: 5.3060, Train Log Prob: 0.0138 ===
Total mismatches: 90649
Predicted valid destination but wrong order: 44215
Epoch 5: Validation Loss: 6.3180, Validation Log Prob: 0.0030
Epoch 5: Edge Precision: 0.3789, Recall: 0.3770, F1: 0.3779, Jaccard: 0.2507
Epoch 5: TP: 2.641088045812455, FP: 4.350894774516822, FN: 4.380386542591267
Epoch 5: warmup, skipping learning rate scheduler
Epoch 5: Current Learning Rate: 6e-05
[Epoch 5] ‚è±Ô∏è Total: 3348.06s | Current time: 2025-07-14 16:13:04 | üèãÔ∏è Train: 2871.40s | ‚úÖ Val: 476.66s
grad AddEdge W: 3.401962956131599e-13
grad ChooseDest W: 11.842402458190918
grad AddEdge W: 6.026859783805905e-13
grad ChooseDest W: 7.562082290649414
grad AddEdge W: 4.295665335078561e-15
grad ChooseDest W: 3.4886436462402344
grad AddEdge W: 2.7455360034067677e-13
grad ChooseDest W: 7.318906784057617
grad AddEdge W: 5.710513171271886e-14
grad ChooseDest W: 3.505932092666626
grad AddEdge W: 2.6760476226996878e-14
grad ChooseDest W: 3.056345224380493
grad AddEdge W: 6.099228788041325e-14
grad ChooseDest W: 4.732020854949951
grad AddEdge W: 1.0847821359444895e-14
grad ChooseDest W: 2.950773239135742
grad AddEdge W: 1.4432584223870656e-14
grad ChooseDest W: 3.708948850631714
grad AddEdge W: 1.92401162276562e-14
grad ChooseDest W: 4.306902885437012
grad AddEdge W: 4.673115193420951e-13
grad ChooseDest W: 4.903709888458252
grad AddEdge W: 5.709048685187401e-13
grad ChooseDest W: 3.931971311569214
grad AddEdge W: 1.1037575298189495e-09
grad ChooseDest W: 1.9547600746154785
grad AddEdge W: 3.369296486775697e-14
grad ChooseDest W: 4.886451244354248
grad AddEdge W: 4.472564693277538e-14
grad ChooseDest W: 4.969743728637695
grad AddEdge W: 3.595894198724081e-13
grad ChooseDest W: 4.301012992858887
grad AddEdge W: 1.0630830153083182e-14
grad ChooseDest W: 5.111103057861328
grad AddEdge W: 1.816384397762289e-14
grad ChooseDest W: 3.3533341884613037
grad AddEdge W: 2.052685244815968e-14
grad ChooseDest W: 6.604626178741455
grad AddEdge W: 6.452113691414463e-13
grad ChooseDest W: 3.8731191158294678
grad AddEdge W: 5.906307801645033e-15
grad ChooseDest W: 5.725602626800537
grad AddEdge W: 8.272932679350125e-15
grad ChooseDest W: 3.5664291381835938
grad AddEdge W: 4.8503753097284275e-12
grad ChooseDest W: 4.158447742462158
grad AddEdge W: 1.1886533966311286e-13
grad ChooseDest W: 4.552828788757324
grad AddEdge W: 1.899966051458965e-13
grad ChooseDest W: 4.132716178894043
grad AddEdge W: 1.4574848277801463e-15
grad ChooseDest W: 3.357966661453247
grad AddEdge W: 2.2079534652431204e-13
grad ChooseDest W: 5.057732582092285
grad AddEdge W: 1.8499008985620787e-15
grad ChooseDest W: 3.506279468536377
grad AddEdge W: 1.8512596241127693e-13
grad ChooseDest W: 4.806607723236084
grad AddEdge W: 1.1585383133321339e-15
grad ChooseDest W: 3.930483341217041
grad AddEdge W: 2.4755905333496975e-13
grad ChooseDest W: 4.158899784088135
grad AddEdge W: 2.448337722610241e-15
grad ChooseDest W: 4.749431610107422
grad AddEdge W: 4.8360289026366104e-14
grad ChooseDest W: 6.818006992340088
grad AddEdge W: 5.842751954997227e-15
grad ChooseDest W: 2.9754135608673096
grad AddEdge W: 3.2605430044140493e-15
grad ChooseDest W: 4.593372344970703
grad AddEdge W: 9.305944415668316e-15
grad ChooseDest W: 4.160609722137451
grad AddEdge W: 1.4486498209776572e-13
grad ChooseDest W: 3.9471218585968018
grad AddEdge W: 1.0434129282159768e-13
grad ChooseDest W: 5.747550010681152
grad AddEdge W: 8.041819739691789e-15
grad ChooseDest W: 3.310359239578247
grad AddEdge W: 2.507678521554272e-15
grad ChooseDest W: 4.987229824066162
grad AddEdge W: 4.323953270901539e-15
grad ChooseDest W: 6.062969207763672
grad AddEdge W: 1.5599922341315992e-13
grad ChooseDest W: 6.237387657165527
grad AddEdge W: 1.906974402064547e-15
grad ChooseDest W: 2.3827736377716064
grad AddEdge W: 4.100022747119446e-15
grad ChooseDest W: 8.541522979736328
grad AddEdge W: 7.560141748741422e-14
grad ChooseDest W: 6.411123275756836
grad AddEdge W: 2.1543584211231644e-15
grad ChooseDest W: 3.618605613708496
grad AddEdge W: 1.383432707668643e-16
grad ChooseDest W: 4.854328155517578
grad AddEdge W: 3.8959727642357694e-14
grad ChooseDest W: 3.8935248851776123
grad AddEdge W: 3.3607170594595476e-14
grad ChooseDest W: 5.283888339996338
grad AddEdge W: 6.207296724287112e-16
grad ChooseDest W: 5.176281452178955
grad AddEdge W: 1.3780634187555573e-15
grad ChooseDest W: 2.722316026687622
grad AddEdge W: 6.968395045020559e-14
grad ChooseDest W: 4.412186145782471
grad AddEdge W: 8.058142233991969e-14
grad ChooseDest W: 7.69998836517334
grad AddEdge W: 1.1029922770489434e-13
grad ChooseDest W: 3.8704473972320557
grad AddEdge W: 1.659687791794864e-15
grad ChooseDest W: 3.396219253540039
grad AddEdge W: 1.001694493038476e-15
grad ChooseDest W: 4.565978050231934
grad AddEdge W: 8.565478377573973e-14
grad ChooseDest W: 6.712141513824463
grad AddEdge W: 4.623284177371345e-13
grad ChooseDest W: 7.8247599601745605
grad AddEdge W: 3.883844692387098e-15
grad ChooseDest W: 3.7864346504211426
grad AddEdge W: 3.730563831482771e-14
grad ChooseDest W: 4.07489538192749
grad AddEdge W: 3.9811162619838875e-15
grad ChooseDest W: 5.631305694580078
grad AddEdge W: 4.397486742151527e-15
grad ChooseDest W: 3.2157726287841797
grad AddEdge W: 2.2891973418866923e-14
grad ChooseDest W: 6.627519130706787
grad AddEdge W: 1.5012914659880074e-15
grad ChooseDest W: 7.009413719177246
grad AddEdge W: 7.532194743679714e-15
grad ChooseDest W: 3.933281183242798
grad AddEdge W: 4.384738049262402e-15
grad ChooseDest W: 5.446662902832031
=== Epoch 6: Train Loss: 5.2703, Train Log Prob: 0.0143 ===
Total mismatches: 89668
Predicted valid destination but wrong order: 44228
Epoch 6: Validation Loss: 6.4447, Validation Log Prob: 0.0027
Epoch 6: Edge Precision: 0.3790, Recall: 0.3768, F1: 0.3778, Jaccard: 0.2506
Epoch 6: TP: 2.6402290622763065, FP: 4.348174659985683, FN: 4.381245526127416
Epoch 6: Current Learning Rate: 6e-05
[Epoch 6] ‚è±Ô∏è Total: 3342.87s | Current time: 2025-07-14 17:08:47 | üèãÔ∏è Train: 2864.68s | ‚úÖ Val: 478.19s
grad AddEdge W: 1.6931716445567346e-12
grad ChooseDest W: 8.840937614440918
grad AddEdge W: 2.698393707100972e-15
grad ChooseDest W: 5.849322319030762
grad AddEdge W: 1.5680378002980166e-12
grad ChooseDest W: 3.4145963191986084
grad AddEdge W: 8.001569242258033e-14
grad ChooseDest W: 1.9447070360183716
grad AddEdge W: 1.026549654200952e-13
grad ChooseDest W: 3.9323806762695312
grad AddEdge W: 3.848949052542589e-15
grad ChooseDest W: 3.3631672859191895
grad AddEdge W: 1.4588354938123438e-13
grad ChooseDest W: 7.63004207611084
grad AddEdge W: 8.626849641547515e-14
grad ChooseDest W: 3.6550309658050537
grad AddEdge W: 1.997795815767995e-15
grad ChooseDest W: 3.942174196243286
grad AddEdge W: 1.3158472605929528e-15
grad ChooseDest W: 3.8135616779327393
grad AddEdge W: 6.127504526589863e-15
grad ChooseDest W: 6.6399760246276855
grad AddEdge W: 1.2581395411712278e-15
grad ChooseDest W: 3.8477723598480225
grad AddEdge W: 2.398974971784865e-15
grad ChooseDest W: 5.766434192657471
grad AddEdge W: 9.98988455706059e-15
grad ChooseDest W: 6.028645992279053
grad AddEdge W: 1.2144054301594753e-15
grad ChooseDest W: 5.468656063079834
grad AddEdge W: 1.901756963712531e-12
grad ChooseDest W: 2.3679206371307373
grad AddEdge W: 6.077876103880581e-14
grad ChooseDest W: 7.5492844581604
grad AddEdge W: 3.8109465127882103e-13
grad ChooseDest W: 4.96983528137207
grad AddEdge W: 1.186966750745238e-15
grad ChooseDest W: 8.702467918395996
grad AddEdge W: 3.5327914638810798e-15
grad ChooseDest W: 5.467248439788818
grad AddEdge W: 7.208523019863993e-16
grad ChooseDest W: 4.141901016235352
grad AddEdge W: 2.0866127260017655e-15
grad ChooseDest W: 4.579952239990234
grad AddEdge W: 7.812102879737118e-14
grad ChooseDest W: 4.573408126831055
grad AddEdge W: 1.9794537408116767e-15
grad ChooseDest W: 5.247115135192871
grad AddEdge W: 1.2587146765424134e-15
grad ChooseDest W: 4.512152671813965
grad AddEdge W: 2.2547790474596085e-15
grad ChooseDest W: 4.508115291595459
grad AddEdge W: 3.796519576580032e-14
grad ChooseDest W: 2.7709760665893555
grad AddEdge W: 3.525229577244467e-15
grad ChooseDest W: 3.88869047164917
grad AddEdge W: 9.234914943217001e-14
grad ChooseDest W: 3.761963129043579
grad AddEdge W: 7.21197710926158e-12
grad ChooseDest W: 7.942039966583252
grad AddEdge W: 3.066977552999839e-15
grad ChooseDest W: 5.960533142089844
grad AddEdge W: 1.091619741648564e-13
grad ChooseDest W: 4.541012287139893
grad AddEdge W: 1.6590292236783738e-15
grad ChooseDest W: 5.177310943603516
grad AddEdge W: 8.024895174372701e-14
grad ChooseDest W: 9.699671745300293
grad AddEdge W: 3.086753442977622e-15
grad ChooseDest W: 4.415379047393799
grad AddEdge W: 1.7836910859322886e-15
grad ChooseDest W: 7.707038402557373
grad AddEdge W: 2.6009266846516036e-15
grad ChooseDest W: 7.159747123718262
grad AddEdge W: 2.4159279719999927e-13
grad ChooseDest W: 4.839887619018555
grad AddEdge W: 3.801000211464418e-15
grad ChooseDest W: 5.265199661254883
grad AddEdge W: 1.613360595294211e-15
grad ChooseDest W: 4.047558307647705
grad AddEdge W: 1.835744013398144e-15
grad ChooseDest W: 4.331874370574951
grad AddEdge W: 5.03047149901935e-15
grad ChooseDest W: 4.036385536193848
grad AddEdge W: 7.122564528189851e-14
grad ChooseDest W: 4.8876447677612305
grad AddEdge W: 1.3640497965091564e-13
grad ChooseDest W: 6.408026218414307
grad AddEdge W: 2.1568336631532783e-15
grad ChooseDest W: 5.74534797668457
grad AddEdge W: 1.2286749707053949e-15
grad ChooseDest W: 4.945672512054443
grad AddEdge W: 3.0926663682241673e-15
grad ChooseDest W: 3.689823627471924
grad AddEdge W: 9.686321501291804e-16
grad ChooseDest W: 4.4964752197265625
grad AddEdge W: 8.355626917803786e-16
grad ChooseDest W: 5.2812299728393555
grad AddEdge W: 6.745080907114183e-15
grad ChooseDest W: 4.57872200012207
grad AddEdge W: 2.075628190983535e-15
grad ChooseDest W: 4.236868381500244
grad AddEdge W: 1.9561738872893395e-15
grad ChooseDest W: 4.594554424285889
grad AddEdge W: 7.117509435560637e-14
grad ChooseDest W: 6.256263256072998
grad AddEdge W: 2.552765661820145e-15
grad ChooseDest W: 6.936981678009033
grad AddEdge W: 3.424684494324204e-11
grad ChooseDest W: 2.209244966506958
grad AddEdge W: 2.030282282152276e-15
grad ChooseDest W: 3.733015537261963
grad AddEdge W: 1.522356561287335e-14
grad ChooseDest W: 8.598782539367676
grad AddEdge W: 4.489073026606345e-15
grad ChooseDest W: 4.7416839599609375
grad AddEdge W: 1.1848923231472241e-10
grad ChooseDest W: 3.774824380874634
grad AddEdge W: 2.707421807769282e-15
grad ChooseDest W: 5.465807914733887
grad AddEdge W: 2.289958019824974e-15
grad ChooseDest W: 5.8450751304626465
grad AddEdge W: 1.1485434304336515e-15
grad ChooseDest W: 4.930784702301025
grad AddEdge W: 6.544854176844528e-16
grad ChooseDest W: 5.272402763366699
grad AddEdge W: 5.629991889965287e-16
grad ChooseDest W: 10.930140495300293
grad AddEdge W: 1.7476335286423835e-15
grad ChooseDest W: 5.7673821449279785
grad AddEdge W: 1.205360410008044e-13
grad ChooseDest W: 4.073808193206787
=== Epoch 7: Train Loss: 5.2383, Train Log Prob: 0.0148 ===
Total mismatches: 89101
Predicted valid destination but wrong order: 44157
Epoch 7: Validation Loss: 6.2905, Validation Log Prob: 0.0031
Epoch 7: Edge Precision: 0.3772, Recall: 0.3751, F1: 0.3760, Jaccard: 0.2492
Epoch 7: TP: 2.6277738010021476, FP: 4.358911954187545, FN: 4.393700787401575
Epoch 7: Current Learning Rate: 6e-05
[Epoch 7] ‚è±Ô∏è Total: 3357.26s | Current time: 2025-07-14 18:04:44 | üèãÔ∏è Train: 2880.25s | ‚úÖ Val: 477.02s
grad AddEdge W: 1.6528389713745678e-13
grad ChooseDest W: 11.484603881835938
grad AddEdge W: 3.7922594137839046e-16
grad ChooseDest W: 2.817523717880249
grad AddEdge W: 3.202320543070278e-14
grad ChooseDest W: 3.138437032699585
grad AddEdge W: 4.231073990652737e-15
grad ChooseDest W: 6.395505428314209
grad AddEdge W: 2.3557124133322116e-12
grad ChooseDest W: 4.817483901977539
grad AddEdge W: 2.8882741416781524e-15
grad ChooseDest W: 4.734241485595703
grad AddEdge W: 1.3854139494960064e-14
grad ChooseDest W: 5.6095991134643555
grad AddEdge W: 1.1630536553915334e-14
grad ChooseDest W: 6.0788044929504395
grad AddEdge W: 4.493385377219935e-16
grad ChooseDest W: 7.183411121368408
grad AddEdge W: 2.481420288431152e-14
grad ChooseDest W: 5.174076557159424
grad AddEdge W: 4.947934491056523e-16
grad ChooseDest W: 5.831279754638672
grad AddEdge W: 1.919816692094343e-15
grad ChooseDest W: 6.443490982055664
grad AddEdge W: 4.020685406114872e-16
grad ChooseDest W: 3.507159948348999
grad AddEdge W: 1.8090988979763654e-15
grad ChooseDest W: 6.487912178039551
grad AddEdge W: 1.0460615494200642e-15
grad ChooseDest W: 5.081112384796143
grad AddEdge W: 8.572619881758864e-14
grad ChooseDest W: 6.058749675750732
grad AddEdge W: 3.5452307250466604e-16
grad ChooseDest W: 5.632938385009766
grad AddEdge W: 2.0991217932701117e-13
grad ChooseDest W: 5.797317028045654
grad AddEdge W: 2.1128313603346015e-15
grad ChooseDest W: 3.9243297576904297
grad AddEdge W: 4.6316171519811614e-14
grad ChooseDest W: 2.8297250270843506
grad AddEdge W: 7.129326561614371e-14
grad ChooseDest W: 6.131531715393066
grad AddEdge W: 1.9748636692705056e-15
grad ChooseDest W: 6.355570316314697
grad AddEdge W: 1.1466456531153283e-15
grad ChooseDest W: 6.37758207321167
grad AddEdge W: 1.753461856473322e-15
grad ChooseDest W: 6.162084102630615
grad AddEdge W: 6.940732388919244e-15
grad ChooseDest W: 3.6928772926330566
grad AddEdge W: 2.541456977880912e-16
grad ChooseDest W: 7.763180255889893
grad AddEdge W: 1.985135002547148e-15
grad ChooseDest W: 5.3211588859558105
grad AddEdge W: 1.0785707798148026e-15
grad ChooseDest W: 5.653378486633301
grad AddEdge W: 6.32527888471898e-16
grad ChooseDest W: 3.1970717906951904
grad AddEdge W: 1.1867327155419116e-14
grad ChooseDest W: 3.7742502689361572
grad AddEdge W: 2.2841737588831165e-14
grad ChooseDest W: 4.378504276275635
grad AddEdge W: 3.06623690739076e-14
grad ChooseDest W: 5.654263496398926
grad AddEdge W: 2.5981270290026913e-14
grad ChooseDest W: 6.736514568328857
grad AddEdge W: 4.539173860766135e-16
grad ChooseDest W: 5.974490642547607
grad AddEdge W: 5.865889506984426e-16
grad ChooseDest W: 3.268010139465332
grad AddEdge W: 1.0546150541398958e-12
grad ChooseDest W: 6.359279632568359
grad AddEdge W: 9.53637295270527e-15
grad ChooseDest W: 3.694429636001587
grad AddEdge W: 3.5608587476212983e-16
grad ChooseDest W: 5.632143497467041
grad AddEdge W: 8.784672928389731e-16
grad ChooseDest W: 3.7444982528686523
grad AddEdge W: 8.77268906390033e-14
grad ChooseDest W: 6.065950870513916
grad AddEdge W: 6.0371150996895145e-15
grad ChooseDest W: 4.346437931060791
grad AddEdge W: 2.0376854771662206e-14
grad ChooseDest W: 3.3372933864593506
grad AddEdge W: 3.821218940854937e-16
grad ChooseDest W: 6.267195701599121
grad AddEdge W: 4.59946937172084e-16
grad ChooseDest W: 8.165282249450684
grad AddEdge W: 7.251478178201627e-16
grad ChooseDest W: 3.3539698123931885
grad AddEdge W: 1.6070525715294187e-14
grad ChooseDest W: 5.451294422149658
grad AddEdge W: 1.6418312784755593e-15
grad ChooseDest W: 4.703768730163574
grad AddEdge W: 6.933199809622606e-14
grad ChooseDest W: 3.420306444168091
grad AddEdge W: 1.0114257369321254e-15
grad ChooseDest W: 5.414577484130859
grad AddEdge W: 2.5567819214791104e-16
grad ChooseDest W: 6.911198139190674
grad AddEdge W: 7.184782803934823e-16
grad ChooseDest W: 6.374648571014404
grad AddEdge W: 1.538177103862972e-15
grad ChooseDest W: 6.691450595855713
grad AddEdge W: 1.0872905393144883e-14
grad ChooseDest W: 4.643683910369873
grad AddEdge W: 6.341730911532618e-16
grad ChooseDest W: 4.515133380889893
grad AddEdge W: 1.7584345751484151e-15
grad ChooseDest W: 3.8309335708618164
grad AddEdge W: 3.013159832388379e-16
grad ChooseDest W: 3.511930465698242
grad AddEdge W: 7.500748476106309e-14
grad ChooseDest W: 3.205029249191284
grad AddEdge W: 4.4331827434278964e-14
grad ChooseDest W: 4.06415319442749
grad AddEdge W: 6.223404643965929e-16
grad ChooseDest W: 4.659587383270264
grad AddEdge W: 2.935692846010461e-16
grad ChooseDest W: 4.380533695220947
grad AddEdge W: 2.654959392812988e-16
grad ChooseDest W: 3.3099095821380615
grad AddEdge W: 3.8367319575311615e-14
grad ChooseDest W: 5.09165620803833
grad AddEdge W: 7.178380823040357e-16
grad ChooseDest W: 5.026878833770752
grad AddEdge W: 1.3610523501967307e-15
grad ChooseDest W: 7.268754959106445
grad AddEdge W: 5.177672060966755e-16
grad ChooseDest W: 4.765310287475586
grad AddEdge W: 2.158077488684674e-14
grad ChooseDest W: 5.714570045471191
=== Epoch 8: Train Loss: 5.2123, Train Log Prob: 0.0152 ===
Total mismatches: 88330
Predicted valid destination but wrong order: 44191
Epoch 8: Validation Loss: 6.3083, Validation Log Prob: 0.0031
Epoch 8: Edge Precision: 0.3791, Recall: 0.3770, F1: 0.3779, Jaccard: 0.2511
Epoch 8: TP: 2.640801717967072, FP: 4.345454545454546, FN: 4.3806728704366495
Epoch 8: Current Learning Rate: 6e-05
[Epoch 8] ‚è±Ô∏è Total: 3342.90s | Current time: 2025-07-14 19:00:27 | üèãÔ∏è Train: 2864.48s | ‚úÖ Val: 478.42s
grad AddEdge W: 1.4676951535087628e-14
grad ChooseDest W: 9.3037109375
grad AddEdge W: 2.1360507357045047e-14
grad ChooseDest W: 7.18049430847168
grad AddEdge W: 2.8077210966360316e-14
grad ChooseDest W: 8.192684173583984
grad AddEdge W: 5.794233907778501e-15
grad ChooseDest W: 3.056272029876709
grad AddEdge W: 1.9659973518951213e-14
grad ChooseDest W: 6.79302453994751
grad AddEdge W: 7.049180240617698e-16
grad ChooseDest W: 4.759387493133545
grad AddEdge W: 6.306686511526748e-16
grad ChooseDest W: 3.660336971282959
grad AddEdge W: 3.6233184277562384e-16
grad ChooseDest W: 4.385748386383057
grad AddEdge W: 3.2525678927541185e-14
grad ChooseDest W: 3.566654682159424
grad AddEdge W: 7.438484947626129e-15
grad ChooseDest W: 3.714001178741455
grad AddEdge W: 1.9024395774003278e-12
grad ChooseDest W: 4.86384916305542
grad AddEdge W: 1.4645126866132948e-16
grad ChooseDest W: 3.818310260772705
grad AddEdge W: 2.038428494467552e-14
grad ChooseDest W: 4.744612693786621
grad AddEdge W: 5.708069650625647e-14
grad ChooseDest W: 3.0504140853881836
grad AddEdge W: 8.414180187865108e-16
grad ChooseDest W: 4.173367977142334
grad AddEdge W: 2.3934649695234167e-16
grad ChooseDest W: 4.682469844818115
grad AddEdge W: 7.795580485661386e-15
grad ChooseDest W: 8.112142562866211
grad AddEdge W: 7.219405805049435e-16
grad ChooseDest W: 8.418703079223633
grad AddEdge W: 4.6169343440602764e-15
grad ChooseDest W: 4.378528594970703
grad AddEdge W: 6.417988228982739e-16
grad ChooseDest W: 3.7382659912109375
grad AddEdge W: 2.4430583356565942e-14
grad ChooseDest W: 5.100648880004883
grad AddEdge W: 5.688623129409404e-14
grad ChooseDest W: 3.907674551010132
grad AddEdge W: 7.492015418189351e-16
grad ChooseDest W: 3.975771903991699
grad AddEdge W: 1.3667938783263993e-14
grad ChooseDest W: 3.031423568725586
grad AddEdge W: 9.785780956991352e-15
grad ChooseDest W: 1.3607808351516724
grad AddEdge W: 1.007332979610111e-15
grad ChooseDest W: 7.046005725860596
grad AddEdge W: 1.2627201894708606e-15
grad ChooseDest W: 4.28196382522583
grad AddEdge W: 4.045124688923323e-16
grad ChooseDest W: 4.8528852462768555
grad AddEdge W: 8.012575927187834e-15
grad ChooseDest W: 5.987408638000488
grad AddEdge W: 4.500849325672022e-16
grad ChooseDest W: 6.698386192321777
grad AddEdge W: 1.9193690351817193e-14
grad ChooseDest W: 2.978736162185669
grad AddEdge W: 2.82317563627516e-16
grad ChooseDest W: 6.142880439758301
grad AddEdge W: 2.816608674538392e-14
grad ChooseDest W: 3.554697275161743
grad AddEdge W: 5.161120401902695e-15
grad ChooseDest W: 4.73220682144165
grad AddEdge W: 2.545972161121634e-14
grad ChooseDest W: 3.758941173553467
grad AddEdge W: 1.8850862236745487e-15
grad ChooseDest W: 6.377750396728516
grad AddEdge W: 5.4258667329075416e-14
grad ChooseDest W: 5.910069942474365
grad AddEdge W: 5.02784252050931e-16
grad ChooseDest W: 2.996009349822998
grad AddEdge W: 3.216147170088078e-14
grad ChooseDest W: 6.405479907989502
grad AddEdge W: 1.903828691105034e-14
grad ChooseDest W: 8.276187896728516
grad AddEdge W: 1.241986834625325e-15
grad ChooseDest W: 3.529492139816284
grad AddEdge W: 2.7381879925893367e-14
grad ChooseDest W: 4.144575595855713
grad AddEdge W: 1.6647107818753767e-14
grad ChooseDest W: 5.3670172691345215
grad AddEdge W: 8.478273053401473e-16
grad ChooseDest W: 5.671230792999268
grad AddEdge W: 3.165498178556473e-16
grad ChooseDest W: 4.377003192901611
grad AddEdge W: 1.4414483129787832e-14
grad ChooseDest W: 3.7669625282287598
grad AddEdge W: 3.019007800795782e-16
grad ChooseDest W: 6.040581703186035
grad AddEdge W: 9.028533713904244e-16
grad ChooseDest W: 4.180019378662109
grad AddEdge W: 1.9548119091321314e-13
grad ChooseDest W: 2.792447805404663
grad AddEdge W: 3.318426398469035e-14
grad ChooseDest W: 6.702916622161865
grad AddEdge W: 6.040463675039895e-14
grad ChooseDest W: 6.171857833862305
grad AddEdge W: 7.654906439627518e-14
grad ChooseDest W: 5.515594482421875
grad AddEdge W: 6.91892764327455e-14
grad ChooseDest W: 6.0648016929626465
grad AddEdge W: 7.877250767160935e-16
grad ChooseDest W: 5.4745774269104
grad AddEdge W: 3.809443944102417e-14
grad ChooseDest W: 6.748487949371338
grad AddEdge W: 3.7109283689797307e-16
grad ChooseDest W: 6.379103183746338
grad AddEdge W: 7.039490183701109e-16
grad ChooseDest W: 5.911977767944336
grad AddEdge W: 1.2955840090331434e-16
grad ChooseDest W: 3.498711109161377
grad AddEdge W: 2.221895746139539e-15
grad ChooseDest W: 4.029110431671143
grad AddEdge W: 2.6434389506369417e-16
grad ChooseDest W: 4.726973533630371
grad AddEdge W: 1.6300166511690195e-14
grad ChooseDest W: 4.437598705291748
grad AddEdge W: 6.956415309816776e-16
grad ChooseDest W: 5.030975341796875
grad AddEdge W: 5.2836168244269543e-17
grad ChooseDest W: 4.184281826019287
grad AddEdge W: 1.692931135057076e-16
grad ChooseDest W: 3.4331867694854736
grad AddEdge W: 2.9843840055877825e-16
grad ChooseDest W: 4.446890354156494
grad AddEdge W: 4.553329793018004e-15
grad ChooseDest W: 4.26316499710083
=== Epoch 9: Train Loss: 5.1829, Train Log Prob: 0.0157 ===
Total mismatches: 87792
Predicted valid destination but wrong order: 44166
Epoch 9: Validation Loss: 6.0553, Validation Log Prob: 0.0039
Epoch 9: Edge Precision: 0.3810, Recall: 0.3791, F1: 0.3800, Jaccard: 0.2525
Epoch 9: TP: 2.655261274158912, FP: 4.336005726556908, FN: 4.366213314244811
Epoch 9: Current Learning Rate: 6e-05
[Epoch 9] ‚è±Ô∏è Total: 3349.18s | Current time: 2025-07-14 19:56:16 | üèãÔ∏è Train: 2872.34s | ‚úÖ Val: 476.85s
grad AddEdge W: 1.4133839938538801e-14
grad ChooseDest W: 8.576823234558105
grad AddEdge W: 1.9512977303702333e-15
grad ChooseDest W: 7.191869735717773
grad AddEdge W: 1.0316149260486092e-16
grad ChooseDest W: 4.424424648284912
grad AddEdge W: 4.130129011217273e-17
grad ChooseDest W: 6.543941974639893
grad AddEdge W: 1.1883684314542662e-16
grad ChooseDest W: 3.0460546016693115
grad AddEdge W: 9.151288375598297e-17
grad ChooseDest W: 7.6283650398254395
grad AddEdge W: 1.0434995923919751e-16
grad ChooseDest W: 3.9296984672546387
grad AddEdge W: 1.9144471384834614e-15
grad ChooseDest W: 5.979695796966553
grad AddEdge W: 3.952366523798066e-14
grad ChooseDest W: 3.4504222869873047
grad AddEdge W: 3.7224083785674357e-17
grad ChooseDest W: 6.547173500061035
grad AddEdge W: 3.2547567105931183e-15
grad ChooseDest W: 4.2960615158081055
grad AddEdge W: 6.005815974973937e-18
grad ChooseDest W: 4.783298015594482
grad AddEdge W: 1.1277205018539438e-15
grad ChooseDest W: 4.75845193862915
grad AddEdge W: 8.654963629890352e-18
grad ChooseDest W: 7.216153621673584
grad AddEdge W: 3.784298362870898e-17
grad ChooseDest W: 5.660890102386475
grad AddEdge W: 5.635764419500825e-16
grad ChooseDest W: 8.029558181762695
grad AddEdge W: 2.962291599613267e-17
grad ChooseDest W: 4.660354137420654
grad AddEdge W: 2.2204066622182658e-15
grad ChooseDest W: 3.1292662620544434
grad AddEdge W: 3.5101138507842404e-15
grad ChooseDest W: 5.572015285491943
grad AddEdge W: 1.1252577239136488e-13
grad ChooseDest W: 1.5995643138885498
grad AddEdge W: 8.25978196344841e-16
grad ChooseDest W: 6.203479290008545
grad AddEdge W: 4.5726902819787085e-10
grad ChooseDest W: 3.661405086517334
grad AddEdge W: 4.085591687978397e-16
grad ChooseDest W: 3.1721489429473877
grad AddEdge W: 1.4867495514099863e-16
grad ChooseDest W: 5.458765506744385
grad AddEdge W: 6.863986968733268e-15
grad ChooseDest W: 4.74850606918335
grad AddEdge W: 3.2480011993222917e-15
grad ChooseDest W: 4.9442033767700195
grad AddEdge W: 1.5428118563921107e-15
grad ChooseDest W: 5.593714237213135
grad AddEdge W: 8.907487147087888e-17
grad ChooseDest W: 5.11781644821167
grad AddEdge W: 3.4228595194691683e-18
grad ChooseDest W: 4.688905239105225
grad AddEdge W: 3.7999949421747045e-17
grad ChooseDest W: 4.437437057495117
grad AddEdge W: 1.2795038991665208e-17
grad ChooseDest W: 4.117025852203369
grad AddEdge W: 7.45275173531497e-17
grad ChooseDest W: 5.7062578201293945
grad AddEdge W: 3.4540567603486976e-17
grad ChooseDest W: 4.71730899810791
grad AddEdge W: 1.7871013795442774e-17
grad ChooseDest W: 5.080779075622559
grad AddEdge W: 2.0959108978313516e-16
grad ChooseDest W: 6.438584327697754
grad AddEdge W: 9.246342677636971e-17
grad ChooseDest W: 5.728191375732422
grad AddEdge W: 1.868489460106048e-15
grad ChooseDest W: 5.887871265411377
grad AddEdge W: 4.7395877456124635e-17
grad ChooseDest W: 4.463912010192871
grad AddEdge W: 2.5200399814201585e-17
grad ChooseDest W: 6.668493270874023
grad AddEdge W: 8.281477124205553e-16
grad ChooseDest W: 4.837422847747803
grad AddEdge W: 1.3508485413278522e-16
grad ChooseDest W: 5.120129585266113
grad AddEdge W: 9.644818078016426e-17
grad ChooseDest W: 6.921236991882324
grad AddEdge W: 7.074563435766746e-17
grad ChooseDest W: 6.905667304992676
grad AddEdge W: 8.545395120515456e-17
grad ChooseDest W: 3.0457143783569336
grad AddEdge W: 9.613182059180968e-17
grad ChooseDest W: 3.764793634414673
grad AddEdge W: 4.7789747114853395e-15
grad ChooseDest W: 5.560680866241455
grad AddEdge W: 7.3888818114973e-18
grad ChooseDest W: 10.393148422241211
grad AddEdge W: 6.169384985405888e-17
grad ChooseDest W: 5.38194465637207
grad AddEdge W: 2.759069429969876e-15
grad ChooseDest W: 7.776364326477051
grad AddEdge W: 1.723716514437809e-17
grad ChooseDest W: 6.056155204772949
grad AddEdge W: 9.190764081407288e-18
grad ChooseDest W: 4.965518951416016
grad AddEdge W: 2.5364686155660742e-17
grad ChooseDest W: 3.8749711513519287
grad AddEdge W: 1.4998416632196632e-15
grad ChooseDest W: 4.200424671173096
grad AddEdge W: 1.5104888408968702e-16
grad ChooseDest W: 3.892714738845825
grad AddEdge W: 1.3159977843033803e-17
grad ChooseDest W: 3.5751852989196777
grad AddEdge W: 3.970535761681497e-17
grad ChooseDest W: 4.454837799072266
grad AddEdge W: 8.296258411617957e-18
grad ChooseDest W: 4.566503047943115
grad AddEdge W: 2.421025145226026e-15
grad ChooseDest W: 3.77616810798645
grad AddEdge W: 1.800564405758068e-15
grad ChooseDest W: 4.758915424346924
grad AddEdge W: 2.827545863061849e-17
grad ChooseDest W: 4.635776996612549
grad AddEdge W: 4.594943251167187e-15
grad ChooseDest W: 4.674731731414795
grad AddEdge W: 1.6725554589317598e-13
grad ChooseDest W: 3.7915632724761963
grad AddEdge W: 1.0772823368229104e-15
grad ChooseDest W: 4.214475631713867
grad AddEdge W: 3.3339485736890015e-15
grad ChooseDest W: 4.9412360191345215
grad AddEdge W: 7.049445600158205e-17
grad ChooseDest W: 3.3450748920440674
grad AddEdge W: 4.7513527129654926e-14
grad ChooseDest W: 7.362792015075684
=== Epoch 10: Train Loss: 5.1581, Train Log Prob: 0.0161 ===
Total mismatches: 87110
Predicted valid destination but wrong order: 44127
Epoch 10: Validation Loss: 6.0239, Validation Log Prob: 0.0039
Epoch 10: Edge Precision: 0.3784, Recall: 0.3757, F1: 0.3769, Jaccard: 0.2502
Epoch 10: TP: 2.6319255547602003, FP: 4.343450250536865, FN: 4.3895490336435214
Epoch 10: Current Learning Rate: 6e-05
[Epoch 10] ‚è±Ô∏è Total: 3355.38s | Current time: 2025-07-14 20:52:12 | üèãÔ∏è Train: 2877.03s | ‚úÖ Val: 478.35s
grad AddEdge W: 7.779709629328682e-15
grad ChooseDest W: 8.63235855102539
grad AddEdge W: 6.596230033817951e-17
grad ChooseDest W: 5.8357696533203125
grad AddEdge W: 5.190161958820713e-17
grad ChooseDest W: 5.826263427734375
grad AddEdge W: 1.4992183727019372e-17
grad ChooseDest W: 5.241990566253662
grad AddEdge W: 1.454772098887446e-15
grad ChooseDest W: 4.425224781036377
grad AddEdge W: 1.5744596542794915e-15
grad ChooseDest W: 2.845391035079956
grad AddEdge W: 5.814041032096214e-16
grad ChooseDest W: 10.370667457580566
grad AddEdge W: 2.1893752346556064e-18
grad ChooseDest W: 4.786984443664551
grad AddEdge W: 9.823094690611814e-17
grad ChooseDest W: 4.392612457275391
grad AddEdge W: 6.192141187278365e-16
grad ChooseDest W: 6.300844192504883
grad AddEdge W: 3.802655313843353e-15
grad ChooseDest W: 5.632210731506348
grad AddEdge W: 1.2510414414691837e-17
grad ChooseDest W: 6.43293571472168
grad AddEdge W: 6.19286455341532e-14
grad ChooseDest W: 4.2947468757629395
grad AddEdge W: 8.063588883482919e-17
grad ChooseDest W: 6.152766704559326
grad AddEdge W: 3.188748068288301e-15
grad ChooseDest W: 4.746481418609619
grad AddEdge W: 5.903572811667688e-17
grad ChooseDest W: 5.880204200744629
grad AddEdge W: 6.948838269231341e-18
grad ChooseDest W: 5.252711296081543
grad AddEdge W: 4.033442746464999e-10
grad ChooseDest W: 2.1921281814575195
grad AddEdge W: 3.189408264296237e-17
grad ChooseDest W: 3.0799059867858887
grad AddEdge W: 1.3409963404675904e-17
grad ChooseDest W: 5.552915573120117
grad AddEdge W: 1.8217875441704635e-17
grad ChooseDest W: 4.868320465087891
grad AddEdge W: 5.371353026499147e-16
grad ChooseDest W: 3.439298152923584
grad AddEdge W: 9.147265498494431e-16
grad ChooseDest W: 4.528036594390869
grad AddEdge W: 1.023137797151431e-17
grad ChooseDest W: 6.468921184539795
grad AddEdge W: 2.164015231996574e-15
grad ChooseDest W: 9.599699974060059
grad AddEdge W: 5.656447640583795e-17
grad ChooseDest W: 6.181757926940918
grad AddEdge W: 1.3028564486298116e-17
grad ChooseDest W: 5.119156360626221
grad AddEdge W: 5.4707430288057416e-18
grad ChooseDest W: 6.068403244018555
grad AddEdge W: 8.335705000669386e-18
grad ChooseDest W: 7.352304458618164
grad AddEdge W: 5.801718714416914e-15
grad ChooseDest W: 8.239934921264648
grad AddEdge W: 1.0398207893357068e-17
grad ChooseDest W: 9.096344947814941
grad AddEdge W: 2.5407191592442943e-15
grad ChooseDest W: 4.566465854644775
grad AddEdge W: 1.998332834656554e-15
grad ChooseDest W: 3.808910846710205
grad AddEdge W: 5.950797148408381e-16
grad ChooseDest W: 4.780070781707764
grad AddEdge W: 2.2779903905748915e-17
grad ChooseDest W: 6.961492538452148
grad AddEdge W: 2.994933800945835e-17
grad ChooseDest W: 3.8606603145599365
grad AddEdge W: 8.565236867304888e-17
grad ChooseDest W: 5.025010585784912
grad AddEdge W: 7.756571203838757e-17
grad ChooseDest W: 6.170902252197266
grad AddEdge W: 3.2600662307438635e-16
grad ChooseDest W: 6.877410888671875
grad AddEdge W: 3.827853696991221e-15
grad ChooseDest W: 5.054327964782715
grad AddEdge W: 4.442094885285727e-17
grad ChooseDest W: 5.8631792068481445
grad AddEdge W: 5.904346142822363e-18
grad ChooseDest W: 12.248968124389648
grad AddEdge W: 2.976003938063681e-17
grad ChooseDest W: 5.31602144241333
grad AddEdge W: 2.7341280551470504e-17
grad ChooseDest W: 5.966694355010986
grad AddEdge W: 2.196739747726258e-17
grad ChooseDest W: 5.48591947555542
grad AddEdge W: 2.1944077601433484e-17
grad ChooseDest W: 5.324488639831543
grad AddEdge W: 2.49217747133601e-13
grad ChooseDest W: 4.414774417877197
grad AddEdge W: 2.6507114696429487e-15
grad ChooseDest W: 4.469794750213623
grad AddEdge W: 2.19018334875504e-17
grad ChooseDest W: 7.6596174240112305
grad AddEdge W: 1.6862449018169118e-17
grad ChooseDest W: 4.145384788513184
grad AddEdge W: 1.1614782588129351e-15
grad ChooseDest W: 7.199390888214111
grad AddEdge W: 6.24218448937219e-18
grad ChooseDest W: 6.169991970062256
grad AddEdge W: 1.6430516808989853e-17
grad ChooseDest W: 5.436478614807129
grad AddEdge W: 8.619615720774124e-18
grad ChooseDest W: 6.443975448608398
grad AddEdge W: 5.536795848193332e-12
grad ChooseDest W: 4.2467217445373535
grad AddEdge W: 1.302574710913176e-17
grad ChooseDest W: 7.13554048538208
grad AddEdge W: 5.001560808691683e-18
grad ChooseDest W: 5.0235466957092285
grad AddEdge W: 9.453174623768874e-16
grad ChooseDest W: 6.457394599914551
grad AddEdge W: 1.615545587932421e-12
grad ChooseDest W: 4.984646320343018
grad AddEdge W: 7.845912136299302e-16
grad ChooseDest W: 3.9255013465881348
grad AddEdge W: 6.779378409349032e-17
grad ChooseDest W: 7.599501609802246
grad AddEdge W: 1.035320700698561e-12
grad ChooseDest W: 5.240497589111328
grad AddEdge W: 9.10179676988582e-16
grad ChooseDest W: 3.9957847595214844
grad AddEdge W: 4.107237824586135e-13
grad ChooseDest W: 5.3523359298706055
grad AddEdge W: 1.3334830589637712e-17
grad ChooseDest W: 6.173600196838379
grad AddEdge W: 6.107668709031061e-17
grad ChooseDest W: 4.9158124923706055
=== Epoch 11: Train Loss: 5.1290, Train Log Prob: 0.0165 ===
Total mismatches: 86492
Predicted valid destination but wrong order: 44294
Epoch 11: Validation Loss: 6.0367, Validation Log Prob: 0.0038
Epoch 11: Edge Precision: 0.3793, Recall: 0.3772, F1: 0.3782, Jaccard: 0.2509
Epoch 11: TP: 2.6418038654259126, FP: 4.345168217609163, FN: 4.37967072297781
Epoch 11: Current Learning Rate: 6e-05
[Epoch 11] ‚è±Ô∏è Total: 3375.44s | Current time: 2025-07-14 21:48:27 | üèãÔ∏è Train: 2886.23s | ‚úÖ Val: 489.21s
grad AddEdge W: 4.429104788006635e-15
grad ChooseDest W: 9.941750526428223
grad AddEdge W: 3.0984498820829653e-12
grad ChooseDest W: 2.8357014656066895
grad AddEdge W: 1.7738193404485134e-17
grad ChooseDest W: 7.109845161437988
grad AddEdge W: 9.458583657056032e-18
grad ChooseDest W: 7.259794235229492
grad AddEdge W: 2.036588865961058e-15
grad ChooseDest W: 8.410335540771484
grad AddEdge W: 5.91344405422565e-15
grad ChooseDest W: 4.6919169425964355
grad AddEdge W: 8.467701519736922e-18
grad ChooseDest W: 5.276347637176514
grad AddEdge W: 2.7234286393597995e-17
grad ChooseDest W: 4.861065864562988
grad AddEdge W: 1.292352260702013e-15
grad ChooseDest W: 4.083676338195801
grad AddEdge W: 2.0748158069777792e-17
grad ChooseDest W: 4.285850524902344
grad AddEdge W: 2.3977970136530304e-16
grad ChooseDest W: 5.716725826263428
grad AddEdge W: 3.306714386557077e-17
grad ChooseDest W: 6.936604022979736
grad AddEdge W: 1.2238506912455856e-17
grad ChooseDest W: 5.636195182800293
grad AddEdge W: 2.7275602410833794e-17
grad ChooseDest W: 5.757485389709473
grad AddEdge W: 2.0651690612380633e-17
grad ChooseDest W: 6.212716102600098
grad AddEdge W: 5.389999199518419e-17
grad ChooseDest W: 4.843506813049316
grad AddEdge W: 1.0431663213231775e-17
grad ChooseDest W: 5.281347274780273
grad AddEdge W: 5.009071277781419e-17
grad ChooseDest W: 7.32343864440918
grad AddEdge W: 3.391174331363739e-15
grad ChooseDest W: 5.6126556396484375
grad AddEdge W: 1.6423888114433098e-16
grad ChooseDest W: 5.90306282043457
grad AddEdge W: 1.190615897722185e-17
grad ChooseDest W: 3.9655697345733643
grad AddEdge W: 1.432339278312363e-15
grad ChooseDest W: 3.4647421836853027
grad AddEdge W: 2.6998467127126478e-17
grad ChooseDest W: 4.454047679901123
grad AddEdge W: 3.31284544925732e-17
grad ChooseDest W: 4.561609745025635
grad AddEdge W: 3.474749179680079e-17
grad ChooseDest W: 4.6829705238342285
grad AddEdge W: 1.0050368638725177e-14
grad ChooseDest W: 3.44161057472229
grad AddEdge W: 2.2443788990005343e-17
grad ChooseDest W: 4.796777248382568
grad AddEdge W: 2.5440053889992898e-17
grad ChooseDest W: 5.488113880157471
grad AddEdge W: 2.064050944660463e-15
grad ChooseDest W: 5.530987739562988
grad AddEdge W: 5.1259740667755784e-17
grad ChooseDest W: 6.295032978057861
grad AddEdge W: 3.046084664792644e-17
grad ChooseDest W: 5.133131504058838
grad AddEdge W: 1.5097025031542426e-15
grad ChooseDest W: 5.384931564331055
grad AddEdge W: 1.3350602567907544e-10
grad ChooseDest W: 5.239294052124023
grad AddEdge W: 1.849778078784727e-15
grad ChooseDest W: 7.370615482330322
grad AddEdge W: 5.5698591185242956e-18
grad ChooseDest W: 5.033567428588867
grad AddEdge W: 1.44829233654562e-18
grad ChooseDest W: 5.209705352783203
grad AddEdge W: 9.374683554705391e-18
grad ChooseDest W: 4.317785739898682
grad AddEdge W: 1.075510174490665e-14
grad ChooseDest W: 8.247528076171875
grad AddEdge W: 7.470812065937208e-17
grad ChooseDest W: 9.336520195007324
grad AddEdge W: 3.935310771604294e-17
grad ChooseDest W: 6.561938285827637
grad AddEdge W: 4.946598825977821e-16
grad ChooseDest W: 4.431845664978027
grad AddEdge W: 5.3503588847675304e-18
grad ChooseDest W: 3.8066298961639404
grad AddEdge W: 3.7073526988022355e-17
grad ChooseDest W: 4.467061996459961
grad AddEdge W: 1.97366981603602e-17
grad ChooseDest W: 5.022997856140137
grad AddEdge W: 4.8702266958586783e-17
grad ChooseDest W: 7.912644863128662
grad AddEdge W: 1.3361406877015081e-15
grad ChooseDest W: 6.342488765716553
grad AddEdge W: 8.337947487310017e-17
grad ChooseDest W: 7.241783142089844
grad AddEdge W: 8.161155829350283e-18
grad ChooseDest W: 3.8821802139282227
grad AddEdge W: 5.063703992980648e-18
grad ChooseDest W: 5.055819988250732
grad AddEdge W: 1.2588257437376221e-15
grad ChooseDest W: 7.741737365722656
grad AddEdge W: 3.0662772685106964e-15
grad ChooseDest W: 3.179994583129883
grad AddEdge W: 4.084247552570223e-16
grad ChooseDest W: 5.308986663818359
grad AddEdge W: 2.0882110043813405e-17
grad ChooseDest W: 4.872072696685791
grad AddEdge W: 8.318065241757794e-16
grad ChooseDest W: 5.4032063484191895
grad AddEdge W: 2.2124520434617026e-17
grad ChooseDest W: 6.193576335906982
grad AddEdge W: 5.519606026883515e-16
grad ChooseDest W: 6.1072893142700195
grad AddEdge W: 5.733491904581474e-17
grad ChooseDest W: 6.1768717765808105
grad AddEdge W: 1.4927467034952244e-15
grad ChooseDest W: 3.126373291015625
grad AddEdge W: 6.361918883039241e-16
grad ChooseDest W: 3.3560471534729004
grad AddEdge W: 6.459794201838965e-17
grad ChooseDest W: 5.924283027648926
grad AddEdge W: 6.234957412360314e-18
grad ChooseDest W: 5.163624286651611
grad AddEdge W: 4.5029664448190145e-17
grad ChooseDest W: 4.359679698944092
grad AddEdge W: 5.259163049414172e-18
grad ChooseDest W: 4.627021312713623
grad AddEdge W: 4.628749183125053e-16
grad ChooseDest W: 6.780491828918457
grad AddEdge W: 2.959985433300359e-15
grad ChooseDest W: 6.087014198303223
grad AddEdge W: 1.4766296782122248e-15
grad ChooseDest W: 4.843982696533203
=== Epoch 12: Train Loss: 5.1056, Train Log Prob: 0.0169 ===
Total mismatches: 85699
Predicted valid destination but wrong order: 44269
Epoch 12: Validation Loss: 5.9138, Validation Log Prob: 0.0043
Epoch 12: Edge Precision: 0.3793, Recall: 0.3767, F1: 0.3779, Jaccard: 0.2510
Epoch 12: TP: 2.6382247673586257, FP: 4.340730136005726, FN: 4.383249821045097
Epoch 12: Current Learning Rate: 6e-05
[Epoch 12] ‚è±Ô∏è Total: 3378.27s | Current time: 2025-07-14 22:44:45 | üèãÔ∏è Train: 2896.70s | ‚úÖ Val: 481.57s
grad AddEdge W: 5.5446891923747327e-14
grad ChooseDest W: 11.5757474899292
grad AddEdge W: 2.816538709616949e-14
grad ChooseDest W: 2.9110026359558105
grad AddEdge W: 6.311166389375437e-18
grad ChooseDest W: 4.577375888824463
grad AddEdge W: 8.009207713907967e-18
grad ChooseDest W: 3.6345784664154053
grad AddEdge W: 4.4903128975526684e-17
grad ChooseDest W: 3.6289494037628174
grad AddEdge W: 7.174011276527004e-14
grad ChooseDest W: 2.2526228427886963
grad AddEdge W: 1.5276004152088444e-15
grad ChooseDest W: 4.536646366119385
grad AddEdge W: 1.1275340383587462e-10
grad ChooseDest W: 0.08162367343902588
grad AddEdge W: 1.3476456525011132e-18
grad ChooseDest W: 7.356235027313232
grad AddEdge W: 2.5731383593011625e-17
grad ChooseDest W: 6.5286641120910645
grad AddEdge W: 9.822390925346654e-18
grad ChooseDest W: 4.597142219543457
grad AddEdge W: 3.040734129718406e-18
grad ChooseDest W: 3.9629831314086914
grad AddEdge W: 2.3185291889071354e-17
grad ChooseDest W: 4.067629337310791
grad AddEdge W: 1.9916775379712995e-17
grad ChooseDest W: 5.475376129150391
grad AddEdge W: 2.5967933259964026e-18
grad ChooseDest W: 5.962076663970947
grad AddEdge W: 2.7817302294627123e-14
grad ChooseDest W: 3.3878562450408936
grad AddEdge W: 5.004917378974294e-13
grad ChooseDest W: 1.9456387758255005
grad AddEdge W: 2.596352389099075e-16
grad ChooseDest W: 5.0913496017456055
grad AddEdge W: 1.0103217353644978e-15
grad ChooseDest W: 5.8673577308654785
grad AddEdge W: 1.0144274099389578e-15
grad ChooseDest W: 3.3883931636810303
grad AddEdge W: 1.8058338028051346e-12
grad ChooseDest W: 1.9798214435577393
grad AddEdge W: 4.2008924597366126e-16
grad ChooseDest W: 4.898253917694092
grad AddEdge W: 2.6417057177404287e-18
grad ChooseDest W: 4.709293842315674
grad AddEdge W: 3.2402838821398577e-15
grad ChooseDest W: 4.701161861419678
grad AddEdge W: 1.2206193929007084e-17
grad ChooseDest W: 4.764864921569824
grad AddEdge W: 2.3487008982115386e-14
grad ChooseDest W: 2.59967041015625
grad AddEdge W: 2.3654601226993227e-14
grad ChooseDest W: 4.795156955718994
grad AddEdge W: 2.0822973048937416e-15
grad ChooseDest W: 5.844348907470703
grad AddEdge W: 1.2793133994714498e-17
grad ChooseDest W: 6.477284908294678
grad AddEdge W: 6.85709328521292e-16
grad ChooseDest W: 5.276950359344482
grad AddEdge W: 1.2092987644734302e-17
grad ChooseDest W: 4.2426276206970215
grad AddEdge W: 1.9000568626553335e-17
grad ChooseDest W: 4.3892927169799805
grad AddEdge W: 8.896232461499043e-16
grad ChooseDest W: 2.3305375576019287
grad AddEdge W: 6.5159477713124335e-18
grad ChooseDest W: 4.367333889007568
grad AddEdge W: 5.220482694307763e-16
grad ChooseDest W: 5.778538227081299
grad AddEdge W: 4.9303757131269005e-18
grad ChooseDest W: 5.540156364440918
grad AddEdge W: 1.3268569286669152e-17
grad ChooseDest W: 4.905916690826416
grad AddEdge W: 9.147036832685897e-18
grad ChooseDest W: 4.736355304718018
grad AddEdge W: 1.2333788622988532e-15
grad ChooseDest W: 8.298195838928223
grad AddEdge W: 9.386197611047109e-16
grad ChooseDest W: 3.3871138095855713
grad AddEdge W: 9.32610571887199e-18
grad ChooseDest W: 3.5439062118530273
grad AddEdge W: 3.1580662585377047e-16
grad ChooseDest W: 6.160830974578857
grad AddEdge W: 1.0523385929548106e-15
grad ChooseDest W: 6.201119422912598
grad AddEdge W: 1.262442471852002e-17
grad ChooseDest W: 5.318233489990234
grad AddEdge W: 4.2708257332495815e-18
grad ChooseDest W: 7.626877307891846
grad AddEdge W: 7.768286066545978e-17
grad ChooseDest W: 5.453155040740967
grad AddEdge W: 7.657927467337195e-16
grad ChooseDest W: 6.697525501251221
grad AddEdge W: 9.020854566143996e-18
grad ChooseDest W: 4.739687442779541
grad AddEdge W: 3.4006540627203354e-18
grad ChooseDest W: 5.700992584228516
grad AddEdge W: 2.7055969414588714e-18
grad ChooseDest W: 5.903906345367432
grad AddEdge W: 9.549479225847034e-18
grad ChooseDest W: 8.908547401428223
grad AddEdge W: 3.446278284740494e-17
grad ChooseDest W: 5.387049674987793
grad AddEdge W: 1.069295769042368e-15
grad ChooseDest W: 5.093605041503906
grad AddEdge W: 1.798547243116296e-17
grad ChooseDest W: 5.599544525146484
grad AddEdge W: 1.0637858197204788e-15
grad ChooseDest W: 3.6589035987854004
grad AddEdge W: 1.7085012417539453e-15
grad ChooseDest W: 3.6582047939300537
grad AddEdge W: 3.847422434473841e-16
grad ChooseDest W: 4.995916843414307
grad AddEdge W: 4.796119664612908e-12
grad ChooseDest W: 1.7706108093261719
grad AddEdge W: 5.3027947587748125e-18
grad ChooseDest W: 5.211841583251953
grad AddEdge W: 1.3354546439537914e-17
grad ChooseDest W: 6.669247150421143
grad AddEdge W: 6.652514463264326e-18
grad ChooseDest W: 4.5722455978393555
grad AddEdge W: 1.5463784830039689e-18
grad ChooseDest W: 6.261122703552246
grad AddEdge W: 2.0410446695451993e-17
grad ChooseDest W: 6.05065393447876
grad AddEdge W: 1.5013414144621158e-17
grad ChooseDest W: 4.902291297912598
grad AddEdge W: 2.4493969373107823e-17
grad ChooseDest W: 6.348598957061768
grad AddEdge W: 4.885052797931437e-14
grad ChooseDest W: 4.508080005645752
=== Epoch 13: Train Loss: 5.0724, Train Log Prob: 0.0174 ===
Total mismatches: 85208
Predicted valid destination but wrong order: 44324
Epoch 13: Validation Loss: 5.8936, Validation Log Prob: 0.0045
Epoch 13: Edge Precision: 0.3810, Recall: 0.3794, F1: 0.3801, Jaccard: 0.2522
Epoch 13: TP: 2.657408732999284, FP: 4.337294201861131, FN: 4.364065855404438
Epoch 13: Current Learning Rate: 6e-05
[Epoch 13] ‚è±Ô∏è Total: 3355.25s | Current time: 2025-07-14 23:40:40 | üèãÔ∏è Train: 2876.32s | ‚úÖ Val: 478.93s
grad AddEdge W: 8.536757862024832e-14
grad ChooseDest W: 13.568643569946289
grad AddEdge W: 1.0886328165441345e-16
grad ChooseDest W: 6.126760959625244
grad AddEdge W: 3.979889354612124e-18
grad ChooseDest W: 2.6528990268707275
grad AddEdge W: 7.910029585643472e-18
grad ChooseDest W: 5.005152702331543
grad AddEdge W: 6.267570248781136e-18
grad ChooseDest W: 5.244237422943115
grad AddEdge W: 9.497099634831114e-16
grad ChooseDest W: 7.6526031494140625
grad AddEdge W: 7.556672090031232e-18
grad ChooseDest W: 5.146100997924805
grad AddEdge W: 3.840619436418409e-18
grad ChooseDest W: 4.824556350708008
grad AddEdge W: 2.2699432467037307e-18
grad ChooseDest W: 4.74884033203125
grad AddEdge W: 2.0641206925297136e-17
grad ChooseDest W: 4.700175762176514
grad AddEdge W: 1.8018957115949414e-18
grad ChooseDest W: 4.283717632293701
grad AddEdge W: 1.0788936581863841e-17
grad ChooseDest W: 2.7611939907073975
grad AddEdge W: 6.221104420118541e-16
grad ChooseDest W: 7.00648307800293
grad AddEdge W: 1.3706166227206527e-15
grad ChooseDest W: 3.6904373168945312
grad AddEdge W: 3.397877630994301e-18
grad ChooseDest W: 9.185334205627441
grad AddEdge W: 4.8436201833017264e-18
grad ChooseDest W: 5.436437606811523
grad AddEdge W: 1.4414704125977727e-17
grad ChooseDest W: 6.249669075012207
grad AddEdge W: 6.142169419763913e-16
grad ChooseDest W: 3.1199934482574463
grad AddEdge W: 3.479834024341565e-17
grad ChooseDest W: 8.160128593444824
grad AddEdge W: 1.7023196680967773e-17
grad ChooseDest W: 5.626651763916016
grad AddEdge W: 6.192135926409669e-18
grad ChooseDest W: 3.159560203552246
grad AddEdge W: 5.124197365537876e-18
grad ChooseDest W: 6.107376575469971
grad AddEdge W: 1.702478916908306e-16
grad ChooseDest W: 4.875985622406006
grad AddEdge W: 6.467154785801707e-16
grad ChooseDest W: 5.1026387214660645
grad AddEdge W: 9.0862357489408e-18
grad ChooseDest W: 6.6794233322143555
grad AddEdge W: 1.0353631130146585e-17
grad ChooseDest W: 4.867331504821777
grad AddEdge W: 4.6884676528844414e-17
grad ChooseDest W: 6.376035213470459
grad AddEdge W: 2.1159237055714595e-17
grad ChooseDest W: 5.35086727142334
grad AddEdge W: 7.238209539431786e-17
grad ChooseDest W: 6.664399147033691
grad AddEdge W: 6.276971536945883e-16
grad ChooseDest W: 5.90347957611084
grad AddEdge W: 2.4247390141228216e-16
grad ChooseDest W: 4.161961555480957
grad AddEdge W: 1.9434378535723095e-17
grad ChooseDest W: 6.31163215637207
grad AddEdge W: 1.3140949611833205e-15
grad ChooseDest W: 10.446377754211426
grad AddEdge W: 9.33696660031481e-19
grad ChooseDest W: 3.132603645324707
grad AddEdge W: 6.527243749757458e-18
grad ChooseDest W: 3.795257806777954
grad AddEdge W: 4.214842827680095e-16
grad ChooseDest W: 4.958990573883057
grad AddEdge W: 1.1721336222694839e-17
grad ChooseDest W: 5.455060005187988
grad AddEdge W: 1.4771025409724744e-17
grad ChooseDest W: 5.205804824829102
grad AddEdge W: 1.6055089969198576e-16
grad ChooseDest W: 5.030834674835205
grad AddEdge W: 2.4221201603034787e-17
grad ChooseDest W: 5.284654140472412
grad AddEdge W: 1.3609070120815633e-18
grad ChooseDest W: 3.129289388656616
grad AddEdge W: 2.1906890042634937e-18
grad ChooseDest W: 4.187011241912842
grad AddEdge W: 1.1923031641920584e-15
grad ChooseDest W: 8.19242000579834
grad AddEdge W: 2.687707837223432e-17
grad ChooseDest W: 6.33067512512207
grad AddEdge W: 8.271836731212941e-18
grad ChooseDest W: 5.092076301574707
grad AddEdge W: 9.019411963155703e-18
grad ChooseDest W: 4.222830295562744
grad AddEdge W: 1.785721973154784e-17
grad ChooseDest W: 6.05424690246582
grad AddEdge W: 1.1244195284317101e-17
grad ChooseDest W: 4.136062145233154
grad AddEdge W: 5.028431737803243e-16
grad ChooseDest W: 5.078357696533203
grad AddEdge W: 1.0411014938875324e-14
grad ChooseDest W: 4.0929460525512695
grad AddEdge W: 1.2181417388119283e-17
grad ChooseDest W: 4.0828704833984375
grad AddEdge W: 5.2456187940642284e-18
grad ChooseDest W: 5.827157497406006
grad AddEdge W: 9.655314801466377e-16
grad ChooseDest W: 4.215263843536377
grad AddEdge W: 9.268237685230107e-16
grad ChooseDest W: 4.937122344970703
grad AddEdge W: 5.111395968008842e-16
grad ChooseDest W: 6.1912946701049805
grad AddEdge W: 4.6684626206420914e-18
grad ChooseDest W: 5.1460041999816895
grad AddEdge W: 3.812017251371999e-16
grad ChooseDest W: 5.058736324310303
grad AddEdge W: 6.287435570218026e-16
grad ChooseDest W: 4.486903190612793
grad AddEdge W: 3.0041780405994824e-17
grad ChooseDest W: 4.071838855743408
grad AddEdge W: 2.681967865516804e-17
grad ChooseDest W: 3.7730460166931152
grad AddEdge W: 7.376467418689656e-16
grad ChooseDest W: 3.5706820487976074
grad AddEdge W: 2.0573865738038888e-18
grad ChooseDest W: 5.002232074737549
grad AddEdge W: 2.0438253198923575e-17
grad ChooseDest W: 5.445130348205566
grad AddEdge W: 4.4904707236135435e-18
grad ChooseDest W: 4.851851463317871
grad AddEdge W: 7.736363777044127e-16
grad ChooseDest W: 3.4428212642669678
grad AddEdge W: 9.859661202206456e-18
grad ChooseDest W: 4.230375289916992
=== Epoch 14: Train Loss: 5.0469, Train Log Prob: 0.0179 ===
Total mismatches: 84668
Predicted valid destination but wrong order: 44362
Epoch 14: Validation Loss: 5.7643, Validation Log Prob: 0.0050
Epoch 14: Edge Precision: 0.3817, Recall: 0.3796, F1: 0.3806, Jaccard: 0.2530
Epoch 14: TP: 2.659699355762348, FP: 4.32655690765927, FN: 4.361775232641374
Epoch 14: Current Learning Rate: 6e-05
[Epoch 14] ‚è±Ô∏è Total: 3345.87s | Current time: 2025-07-15 00:36:26 | üèãÔ∏è Train: 2864.90s | ‚úÖ Val: 480.97s
grad AddEdge W: 5.271602720861336e-14
grad ChooseDest W: 10.871491432189941
grad AddEdge W: 7.526019026461283e-16
grad ChooseDest W: 5.88315486907959
grad AddEdge W: 2.533620301844809e-18
grad ChooseDest W: 4.42111873626709
grad AddEdge W: 2.598706801548391e-18
grad ChooseDest W: 10.132699966430664
grad AddEdge W: 2.9761615159703726e-18
grad ChooseDest W: 7.281848907470703
grad AddEdge W: 9.129738004535576e-18
grad ChooseDest W: 4.942592620849609
grad AddEdge W: 5.891863242916387e-16
grad ChooseDest W: 5.192148685455322
grad AddEdge W: 1.8627689668484844e-18
grad ChooseDest W: 4.06761360168457
grad AddEdge W: 1.694803015831962e-18
grad ChooseDest W: 5.486594200134277
grad AddEdge W: 6.161133958057345e-16
grad ChooseDest W: 5.650292873382568
grad AddEdge W: 4.630593051428495e-18
grad ChooseDest W: 5.070801258087158
grad AddEdge W: 8.823555446437847e-18
grad ChooseDest W: 4.608273983001709
grad AddEdge W: 1.4876831736237627e-17
grad ChooseDest W: 6.4529547691345215
grad AddEdge W: 3.7275249207899947e-16
grad ChooseDest W: 6.589694499969482
grad AddEdge W: 9.787493234094226e-16
grad ChooseDest W: 6.682126045227051
grad AddEdge W: 2.4939906808848795e-15
grad ChooseDest W: 3.916569232940674
grad AddEdge W: 3.3642054905374315e-16
grad ChooseDest W: 1.1829992532730103
grad AddEdge W: 1.199202353426563e-15
grad ChooseDest W: 3.102950096130371
grad AddEdge W: 1.6704412853418412e-17
grad ChooseDest W: 8.125948905944824
grad AddEdge W: 6.22682043667463e-17
grad ChooseDest W: 6.1449971199035645
grad AddEdge W: 1.9105022512764694e-18
grad ChooseDest W: 12.367984771728516
grad AddEdge W: 2.457035123087097e-17
grad ChooseDest W: 4.099253177642822
grad AddEdge W: 1.7449980518014508e-17
grad ChooseDest W: 6.079096794128418
grad AddEdge W: 7.693453848942388e-18
grad ChooseDest W: 4.4674296379089355
grad AddEdge W: 7.908541487721489e-18
grad ChooseDest W: 2.769646406173706
grad AddEdge W: 4.0220178948200216e-16
grad ChooseDest W: 5.4867167472839355
grad AddEdge W: 5.46500310672995e-16
grad ChooseDest W: 3.2791271209716797
grad AddEdge W: 1.7985584514135962e-18
grad ChooseDest W: 3.66253399848938
grad AddEdge W: 1.7347618571572296e-16
grad ChooseDest W: 4.194180488586426
grad AddEdge W: 3.430372387382681e-18
grad ChooseDest W: 5.578034400939941
grad AddEdge W: 3.923965162322517e-17
grad ChooseDest W: 4.682423114776611
grad AddEdge W: 1.25687712326664e-14
grad ChooseDest W: 3.765004873275757
grad AddEdge W: 4.2116413078372697e-16
grad ChooseDest W: 7.278090476989746
grad AddEdge W: 4.2109385351888446e-16
grad ChooseDest W: 7.4941511154174805
grad AddEdge W: 1.1973330178864219e-17
grad ChooseDest W: 3.9391679763793945
grad AddEdge W: 1.3087574103631577e-18
grad ChooseDest W: 5.6218953132629395
grad AddEdge W: 2.1487952675442758e-16
grad ChooseDest W: 7.588327407836914
grad AddEdge W: 3.327542049918611e-18
grad ChooseDest W: 7.337869167327881
grad AddEdge W: 1.1648086070730818e-16
grad ChooseDest W: 3.3741424083709717
grad AddEdge W: 9.022532750170743e-17
grad ChooseDest W: 5.766878128051758
grad AddEdge W: 1.2544962233360573e-18
grad ChooseDest W: 4.309274196624756
grad AddEdge W: 6.679478069691717e-18
grad ChooseDest W: 4.82843542098999
grad AddEdge W: 6.943157192784327e-18
grad ChooseDest W: 3.3443684577941895
grad AddEdge W: 6.939606933595249e-18
grad ChooseDest W: 6.777944564819336
grad AddEdge W: 1.6141276683312444e-14
grad ChooseDest W: 4.268027305603027
grad AddEdge W: 1.415267779246706e-16
grad ChooseDest W: 7.100619316101074
grad AddEdge W: 4.350399862976305e-16
grad ChooseDest W: 5.113217353820801
grad AddEdge W: 7.196450682783251e-16
grad ChooseDest W: 7.2631988525390625
grad AddEdge W: 8.712140835371855e-18
grad ChooseDest W: 4.926806449890137
grad AddEdge W: 1.279071419363776e-15
grad ChooseDest W: 5.444275379180908
grad AddEdge W: 5.859378999993592e-16
grad ChooseDest W: 5.81088399887085
grad AddEdge W: 1.779209646366603e-14
grad ChooseDest W: 1.3793704509735107
grad AddEdge W: 5.6998753672053805e-18
grad ChooseDest W: 4.0894999504089355
grad AddEdge W: 2.94786408080524e-18
grad ChooseDest W: 6.273441314697266
grad AddEdge W: 7.028060532869097e-16
grad ChooseDest W: 9.14845085144043
grad AddEdge W: 2.4854000148311835e-18
grad ChooseDest W: 7.8561787605285645
grad AddEdge W: 2.8724309992046916e-17
grad ChooseDest W: 5.478603363037109
grad AddEdge W: 7.749050213011629e-16
grad ChooseDest W: 4.159936904907227
grad AddEdge W: 1.3220112219205183e-17
grad ChooseDest W: 4.485633373260498
grad AddEdge W: 9.27996351635225e-19
grad ChooseDest W: 6.8446455001831055
grad AddEdge W: 1.0838771235919956e-15
grad ChooseDest W: 5.204859733581543
grad AddEdge W: 2.2040572769381163e-18
grad ChooseDest W: 5.239394664764404
grad AddEdge W: 1.8339638495823972e-19
grad ChooseDest W: 7.949737548828125
grad AddEdge W: 5.823677214739508e-19
grad ChooseDest W: 4.770236015319824
grad AddEdge W: 1.3934907937833694e-18
grad ChooseDest W: 7.0445122718811035
grad AddEdge W: 4.1734573210689873e-19
grad ChooseDest W: 6.554184436798096
=== Epoch 15: Train Loss: 5.0122, Train Log Prob: 0.0186 ===
Total mismatches: 83836
Predicted valid destination but wrong order: 44402
Epoch 15: Validation Loss: 5.7658, Validation Log Prob: 0.0050
Epoch 15: Edge Precision: 0.3776, Recall: 0.3753, F1: 0.3764, Jaccard: 0.2493
Epoch 15: TP: 2.6284896206156048, FP: 4.3546170365068, FN: 4.392984967788117
Epoch 15: Current Learning Rate: 6e-05
[Epoch 15] ‚è±Ô∏è Total: 3324.53s | Current time: 2025-07-15 01:31:51 | üèãÔ∏è Train: 2840.38s | ‚úÖ Val: 484.15s
grad AddEdge W: 1.2160918939332823e-14
grad ChooseDest W: 13.429454803466797
grad AddEdge W: 7.175679317334208e-17
grad ChooseDest W: 7.125171661376953
grad AddEdge W: 1.402840933234939e-18
grad ChooseDest W: 8.841363906860352
grad AddEdge W: 7.783680096268937e-16
grad ChooseDest W: 4.971561431884766
grad AddEdge W: 1.919445612254107e-16
grad ChooseDest W: 3.094031810760498
grad AddEdge W: 2.4313414870541586e-18
grad ChooseDest W: 7.79492712020874
grad AddEdge W: 5.149213271815434e-19
grad ChooseDest W: 6.081915378570557
grad AddEdge W: 6.319418963551725e-19
grad ChooseDest W: 4.828883647918701
grad AddEdge W: 5.277211716789772e-19
grad ChooseDest W: 6.590792179107666
grad AddEdge W: 6.471212189424341e-18
grad ChooseDest W: 8.227747917175293
grad AddEdge W: 9.21976648124947e-19
grad ChooseDest W: 5.3292107582092285
grad AddEdge W: 7.209038270667553e-19
grad ChooseDest W: 8.339524269104004
grad AddEdge W: 1.2849730684842112e-16
grad ChooseDest W: 9.3881196975708
grad AddEdge W: 1.736751391966542e-17
grad ChooseDest W: 6.311285495758057
grad AddEdge W: 7.629304958463135e-19
grad ChooseDest W: 6.546282768249512
grad AddEdge W: 1.0177041536900862e-19
grad ChooseDest W: 7.3564324378967285
grad AddEdge W: 9.074610400410747e-15
grad ChooseDest W: 10.655134201049805
grad AddEdge W: 2.7744535070557046e-16
grad ChooseDest W: 4.598881244659424
grad AddEdge W: 1.5271244356320466e-16
grad ChooseDest W: 6.099298477172852
grad AddEdge W: 1.512870856363227e-16
grad ChooseDest W: 4.727816104888916
grad AddEdge W: 1.418622298551532e-18
grad ChooseDest W: 4.617493152618408
grad AddEdge W: 1.368727315731802e-13
grad ChooseDest W: 6.256183624267578
grad AddEdge W: 1.1113317907531124e-15
grad ChooseDest W: 4.879719257354736
grad AddEdge W: 7.356479161670128e-17
grad ChooseDest W: 7.576995849609375
grad AddEdge W: 7.923794698216967e-18
grad ChooseDest W: 4.188084602355957
grad AddEdge W: 7.620312988216801e-19
grad ChooseDest W: 4.135311126708984
grad AddEdge W: 1.315496297785214e-16
grad ChooseDest W: 6.800349235534668
grad AddEdge W: 6.167402295516144e-19
grad ChooseDest W: 3.6658291816711426
grad AddEdge W: 1.305547631121916e-16
grad ChooseDest W: 5.748645782470703
grad AddEdge W: 3.3211690699764206e-16
grad ChooseDest W: 8.483906745910645
grad AddEdge W: 3.816464108170635e-17
grad ChooseDest W: 3.8769447803497314
grad AddEdge W: 3.385239634723471e-17
grad ChooseDest W: 4.406374454498291
grad AddEdge W: 5.338811340018714e-19
grad ChooseDest W: 5.464494228363037
grad AddEdge W: 1.6160506652283916e-18
grad ChooseDest W: 4.934704303741455
grad AddEdge W: 9.174335654056525e-19
grad ChooseDest W: 4.973557949066162
grad AddEdge W: 5.510611893559879e-18
grad ChooseDest W: 7.090664863586426
grad AddEdge W: 2.2278023649610942e-17
grad ChooseDest W: 8.119965553283691
grad AddEdge W: 2.664272445621794e-18
grad ChooseDest W: 5.97912073135376
grad AddEdge W: 2.201796592324009e-18
grad ChooseDest W: 7.479712963104248
grad AddEdge W: 2.2772677242327346e-18
grad ChooseDest W: 3.8920412063598633
grad AddEdge W: 3.2386615758178493e-17
grad ChooseDest W: 6.799223899841309
grad AddEdge W: 5.009802836315161e-17
grad ChooseDest W: 5.756682872772217
grad AddEdge W: 1.5841545808083468e-18
grad ChooseDest W: 3.5485379695892334
grad AddEdge W: 4.040358325341282e-18
grad ChooseDest W: 9.1990385055542
grad AddEdge W: 4.1099092987391395e-13
grad ChooseDest W: 2.324882984161377
grad AddEdge W: 2.9360140620798836e-14
grad ChooseDest W: 3.089102268218994
grad AddEdge W: 3.304125178890888e-16
grad ChooseDest W: 5.6837639808654785
grad AddEdge W: 1.0444627614972319e-18
grad ChooseDest W: 5.070718288421631
grad AddEdge W: 5.045116169281785e-17
grad ChooseDest W: 5.319093227386475
grad AddEdge W: 1.4539320539620065e-16
grad ChooseDest W: 7.539222240447998
grad AddEdge W: 1.176848675838128e-18
grad ChooseDest W: 6.364615440368652
grad AddEdge W: 3.6611305079172624e-16
grad ChooseDest W: 10.510674476623535
grad AddEdge W: 6.703231470879851e-17
grad ChooseDest W: 7.788554668426514
grad AddEdge W: 3.2091795352974793e-18
grad ChooseDest W: 4.688343048095703
grad AddEdge W: 2.111226477745016e-17
grad ChooseDest W: 5.825778007507324
grad AddEdge W: 2.1977138356156003e-18
grad ChooseDest W: 7.463745594024658
grad AddEdge W: 2.628389557444397e-16
grad ChooseDest W: 5.886477470397949
grad AddEdge W: 1.224726394272871e-16
grad ChooseDest W: 6.450202465057373
grad AddEdge W: 8.063671456787567e-19
grad ChooseDest W: 5.125321865081787
grad AddEdge W: 2.038299820608928e-19
grad ChooseDest W: 5.811302185058594
grad AddEdge W: 1.6802296113152017e-16
grad ChooseDest W: 4.290851593017578
grad AddEdge W: 8.08800156416287e-16
grad ChooseDest W: 8.895702362060547
grad AddEdge W: 2.611166254192724e-14
grad ChooseDest W: 3.3020122051239014
grad AddEdge W: 2.7482162644678086e-17
grad ChooseDest W: 6.8475728034973145
grad AddEdge W: 7.480969182114869e-17
grad ChooseDest W: 3.8679137229919434
grad AddEdge W: 1.654590321048508e-16
grad ChooseDest W: 8.468819618225098
=== Epoch 16: Train Loss: 4.9785, Train Log Prob: 0.0191 ===
Total mismatches: 83036
Predicted valid destination but wrong order: 44491
Epoch 16: Validation Loss: 5.7518, Validation Log Prob: 0.0052
Epoch 16: Edge Precision: 0.3767, Recall: 0.3741, F1: 0.3753, Jaccard: 0.2486
Epoch 16: TP: 2.619899785254116, FP: 4.358339298496778, FN: 4.4015748031496065
Epoch 16: Current Learning Rate: 6e-05
[Epoch 16] ‚è±Ô∏è Total: 3328.15s | Current time: 2025-07-15 02:27:19 | üèãÔ∏è Train: 2847.43s | ‚úÖ Val: 480.71s
grad AddEdge W: 9.741737837772529e-17
grad ChooseDest W: 9.12507152557373
grad AddEdge W: 1.2506535558004422e-18
grad ChooseDest W: 5.313848495483398
grad AddEdge W: 1.2041715516209693e-16
grad ChooseDest W: 4.316203594207764
grad AddEdge W: 1.4153573794506576e-16
grad ChooseDest W: 9.674551010131836
grad AddEdge W: 1.0832182411468726e-18
grad ChooseDest W: 5.849167346954346
grad AddEdge W: 2.1075308663787005e-16
grad ChooseDest W: 5.782273292541504
grad AddEdge W: 4.938653412914295e-19
grad ChooseDest W: 8.002176284790039
grad AddEdge W: 2.722648856196346e-18
grad ChooseDest W: 5.577891826629639
grad AddEdge W: 8.150994742705682e-17
grad ChooseDest W: 5.256863117218018
grad AddEdge W: 8.447668859662113e-19
grad ChooseDest W: 6.316964149475098
grad AddEdge W: 5.94557373445068e-17
grad ChooseDest W: 4.920261859893799
grad AddEdge W: 7.244783441544766e-16
grad ChooseDest W: 5.567641258239746
grad AddEdge W: 1.5149367275353689e-18
grad ChooseDest W: 7.217463493347168
grad AddEdge W: 1.2440959043123409e-14
grad ChooseDest W: 6.87121057510376
grad AddEdge W: 4.0772076189680826e-19
grad ChooseDest W: 4.28009033203125
grad AddEdge W: 3.1804939042708754e-17
grad ChooseDest W: 4.77791166305542
grad AddEdge W: 2.573609624775649e-19
grad ChooseDest W: 4.426366806030273
grad AddEdge W: 1.5770658497539205e-18
grad ChooseDest W: 5.871732234954834
grad AddEdge W: 2.3870430701198e-18
grad ChooseDest W: 5.63237190246582
grad AddEdge W: 9.028808734914306e-17
grad ChooseDest W: 3.7317988872528076
grad AddEdge W: 8.364581379545244e-17
grad ChooseDest W: 7.928814888000488
grad AddEdge W: 6.544245371913689e-17
grad ChooseDest W: 6.446127414703369
grad AddEdge W: 1.5852283811846295e-16
grad ChooseDest W: 6.958515167236328
grad AddEdge W: 1.0410353386291184e-18
grad ChooseDest W: 5.000279426574707
grad AddEdge W: 9.688544830429449e-17
grad ChooseDest W: 3.4993271827697754
grad AddEdge W: 1.1153692626317062e-17
grad ChooseDest W: 5.182965278625488
grad AddEdge W: 1.6694817796540813e-14
grad ChooseDest W: 2.4362001419067383
grad AddEdge W: 6.269658466237526e-17
grad ChooseDest W: 9.478034019470215
grad AddEdge W: 2.1835351327358289e-16
grad ChooseDest W: 4.959758758544922
grad AddEdge W: 4.8882308443737115e-16
grad ChooseDest W: 5.08400821685791
grad AddEdge W: 2.566838717192081e-17
grad ChooseDest W: 3.669802188873291
grad AddEdge W: 1.2781498475171633e-14
grad ChooseDest W: 3.5614829063415527
grad AddEdge W: 9.738088192832853e-19
grad ChooseDest W: 5.7093634605407715
grad AddEdge W: 3.910444068029725e-17
grad ChooseDest W: 4.311075687408447
grad AddEdge W: 7.6589180243925335e-19
grad ChooseDest W: 4.812864780426025
grad AddEdge W: 2.3517925615206996e-18
grad ChooseDest W: 4.827086448669434
grad AddEdge W: 1.6870302064109543e-18
grad ChooseDest W: 5.261436462402344
grad AddEdge W: 9.459605431907688e-19
grad ChooseDest W: 6.203853607177734
grad AddEdge W: 5.910244767770479e-18
grad ChooseDest W: 8.047530174255371
grad AddEdge W: 8.684907361269619e-19
grad ChooseDest W: 7.7514448165893555
grad AddEdge W: 1.569763478626787e-17
grad ChooseDest W: 5.9536309242248535
grad AddEdge W: 7.427869480744885e-17
grad ChooseDest W: 3.5629959106445312
grad AddEdge W: 9.061763610518418e-19
grad ChooseDest W: 4.118261337280273
grad AddEdge W: 3.6055500091721705e-18
grad ChooseDest W: 5.554141521453857
grad AddEdge W: 1.368019355006575e-16
grad ChooseDest W: 2.347809076309204
grad AddEdge W: 6.089642136906936e-13
grad ChooseDest W: 6.64520788192749
grad AddEdge W: 1.2720869428450334e-18
grad ChooseDest W: 4.666497707366943
grad AddEdge W: 7.610643308894602e-17
grad ChooseDest W: 5.503584861755371
grad AddEdge W: 7.252709022953106e-17
grad ChooseDest W: 7.604299068450928
grad AddEdge W: 4.8810379464647464e-17
grad ChooseDest W: 7.792267322540283
grad AddEdge W: 3.2283908050240234e-18
grad ChooseDest W: 3.8362162113189697
grad AddEdge W: 4.978181272460907e-19
grad ChooseDest W: 4.666417121887207
grad AddEdge W: 1.9952481100692435e-14
grad ChooseDest W: 5.046277046203613
grad AddEdge W: 8.529329287786206e-17
grad ChooseDest W: 4.180630207061768
grad AddEdge W: 1.8499805853888773e-11
grad ChooseDest W: 3.5684213638305664
grad AddEdge W: 1.2338197065519521e-18
grad ChooseDest W: 4.633814811706543
grad AddEdge W: 2.6136784811223863e-17
grad ChooseDest W: 6.154366493225098
grad AddEdge W: 4.104685597319632e-17
grad ChooseDest W: 6.46111536026001
grad AddEdge W: 1.665258502495829e-18
grad ChooseDest W: 4.598001003265381
grad AddEdge W: 1.8865167018494224e-18
grad ChooseDest W: 4.60189151763916
grad AddEdge W: 8.494080929856698e-19
grad ChooseDest W: 7.276321887969971
grad AddEdge W: 1.0975423458347186e-16
grad ChooseDest W: 5.52683687210083
grad AddEdge W: 2.1954028997792803e-18
grad ChooseDest W: 6.2565813064575195
grad AddEdge W: 7.631185843098989e-17
grad ChooseDest W: 7.064241409301758
grad AddEdge W: 3.3344521099607258e-18
grad ChooseDest W: 3.6727051734924316
grad AddEdge W: 2.4688376669392486e-16
grad ChooseDest W: 4.876889705657959
=== Epoch 17: Train Loss: 4.9457, Train Log Prob: 0.0198 ===
Total mismatches: 82353
Predicted valid destination but wrong order: 44662
Epoch 17: Validation Loss: 5.5605, Validation Log Prob: 0.0060
Epoch 17: Edge Precision: 0.3764, Recall: 0.3743, F1: 0.3753, Jaccard: 0.2486
Epoch 17: TP: 2.621760916249105, FP: 4.365640658554044, FN: 4.399713672154617
Epoch 17: Current Learning Rate: 6e-05
[Epoch 17] ‚è±Ô∏è Total: 3337.60s | Current time: 2025-07-15 03:22:57 | üèãÔ∏è Train: 2858.41s | ‚úÖ Val: 479.19s
grad AddEdge W: 5.352649747047966e-12
grad ChooseDest W: 9.127405166625977
grad AddEdge W: 5.106094931422459e-17
grad ChooseDest W: 6.541923999786377
grad AddEdge W: 2.051484185183986e-17
grad ChooseDest W: 6.76929235458374
grad AddEdge W: 4.4208430023470455e-19
grad ChooseDest W: 6.020756721496582
grad AddEdge W: 1.4367045949549361e-16
grad ChooseDest W: 6.024857044219971
grad AddEdge W: 6.326178063132049e-19
grad ChooseDest W: 6.273771286010742
grad AddEdge W: 3.9042177865012194e-19
grad ChooseDest W: 6.943516254425049
grad AddEdge W: 4.272967552009665e-17
grad ChooseDest W: 6.815558433532715
grad AddEdge W: 5.311973982032313e-19
grad ChooseDest W: 3.2923214435577393
grad AddEdge W: 2.2553666699493212e-18
grad ChooseDest W: 7.803760528564453
grad AddEdge W: 1.4968554899231488e-18
grad ChooseDest W: 6.2022318840026855
grad AddEdge W: 1.5459082308257325e-18
grad ChooseDest W: 7.711821556091309
grad AddEdge W: 1.1813647717899379e-18
grad ChooseDest W: 4.066461086273193
grad AddEdge W: 1.7585748649803041e-16
grad ChooseDest W: 4.04665470123291
grad AddEdge W: 2.566544158175951e-18
grad ChooseDest W: 5.044654369354248
grad AddEdge W: 9.084216394270404e-19
grad ChooseDest W: 4.775125980377197
grad AddEdge W: 6.357284190067106e-18
grad ChooseDest W: 5.550114631652832
grad AddEdge W: 3.558796209987025e-19
grad ChooseDest W: 6.603640079498291
grad AddEdge W: 3.634960002261187e-18
grad ChooseDest W: 6.119176387786865
grad AddEdge W: 3.910580916790266e-16
grad ChooseDest W: 5.928370475769043
grad AddEdge W: 2.5448485507412774e-16
grad ChooseDest W: 4.575672626495361
grad AddEdge W: 1.635745955554468e-16
grad ChooseDest W: 5.964765548706055
grad AddEdge W: 4.4611147454185266e-17
grad ChooseDest W: 1.751563549041748
grad AddEdge W: 2.9759141889672193e-18
grad ChooseDest W: 7.662936210632324
grad AddEdge W: 1.1445558549167876e-18
grad ChooseDest W: 4.678135871887207
grad AddEdge W: 2.1205420203674656e-18
grad ChooseDest W: 3.486034631729126
grad AddEdge W: 9.173778754709124e-18
grad ChooseDest W: 3.619147539138794
grad AddEdge W: 5.507854486987933e-18
grad ChooseDest W: 4.6757378578186035
grad AddEdge W: 5.09712570864297e-19
grad ChooseDest W: 5.425728797912598
grad AddEdge W: 5.890073554943067e-18
grad ChooseDest W: 4.885679721832275
grad AddEdge W: 3.984962784181157e-17
grad ChooseDest W: 3.368335723876953
grad AddEdge W: 5.4064160876996365e-18
grad ChooseDest W: 7.19119930267334
grad AddEdge W: 9.278469843232938e-16
grad ChooseDest W: 5.710390567779541
grad AddEdge W: 2.7261064711568175e-18
grad ChooseDest W: 5.826432228088379
grad AddEdge W: 9.7651839413631e-18
grad ChooseDest W: 7.9585957527160645
grad AddEdge W: 1.853701206178525e-18
grad ChooseDest W: 6.5139265060424805
grad AddEdge W: 1.8466268515231448e-16
grad ChooseDest W: 4.693313121795654
grad AddEdge W: 3.817801824657256e-17
grad ChooseDest W: 5.957790374755859
grad AddEdge W: 1.6067644254791416e-18
grad ChooseDest W: 3.386251449584961
grad AddEdge W: 1.287902346643833e-16
grad ChooseDest W: 3.1819889545440674
grad AddEdge W: 3.130771683568234e-11
grad ChooseDest W: 2.882537841796875
grad AddEdge W: 1.1547573700138275e-18
grad ChooseDest W: 4.59710693359375
grad AddEdge W: 1.789987826291781e-18
grad ChooseDest W: 4.04348087310791
grad AddEdge W: 2.5325888076209555e-17
grad ChooseDest W: 2.880842924118042
grad AddEdge W: 6.442872568047968e-19
grad ChooseDest W: 4.471410274505615
grad AddEdge W: 7.431912077834554e-17
grad ChooseDest W: 5.047135829925537
grad AddEdge W: 3.5058863258881064e-18
grad ChooseDest W: 8.081267356872559
grad AddEdge W: 4.178906925497239e-16
grad ChooseDest W: 3.9114320278167725
grad AddEdge W: 8.821172959478542e-19
grad ChooseDest W: 4.353523254394531
grad AddEdge W: 1.8597700235376734e-16
grad ChooseDest W: 4.335296630859375
grad AddEdge W: 1.2197250659019314e-18
grad ChooseDest W: 8.384989738464355
grad AddEdge W: 2.15898606651648e-18
grad ChooseDest W: 5.369234561920166
grad AddEdge W: 5.270160002067758e-19
grad ChooseDest W: 5.567577362060547
grad AddEdge W: 5.604676722149816e-17
grad ChooseDest W: 4.108747482299805
grad AddEdge W: 2.775565254342588e-18
grad ChooseDest W: 6.840023994445801
grad AddEdge W: 1.434859916014494e-16
grad ChooseDest W: 3.7634639739990234
grad AddEdge W: 4.098419890295181e-19
grad ChooseDest W: 4.4111151695251465
grad AddEdge W: 4.575906073181083e-18
grad ChooseDest W: 7.108105659484863
grad AddEdge W: 8.3973783463704205e-19
grad ChooseDest W: 4.670225143432617
grad AddEdge W: 7.886216916964449e-19
grad ChooseDest W: 6.370853424072266
grad AddEdge W: 1.0489582565158861e-16
grad ChooseDest W: 5.562841892242432
grad AddEdge W: 2.1278822887743631e-16
grad ChooseDest W: 3.120396137237549
grad AddEdge W: 2.1162654966005664e-18
grad ChooseDest W: 7.920506000518799
grad AddEdge W: 4.986008985995073e-19
grad ChooseDest W: 5.757779598236084
grad AddEdge W: 8.959975934216471e-19
grad ChooseDest W: 5.717771530151367
grad AddEdge W: 2.706389115787901e-16
grad ChooseDest W: 4.5614213943481445
=== Epoch 18: Train Loss: 4.9053, Train Log Prob: 0.0205 ===
Total mismatches: 81408
Predicted valid destination but wrong order: 44795
Epoch 18: Validation Loss: 5.6465, Validation Log Prob: 0.0056
Epoch 18: Edge Precision: 0.3773, Recall: 0.3754, F1: 0.3763, Jaccard: 0.2494
Epoch 18: TP: 2.6307802433786684, FP: 4.359484609878311, FN: 4.390694345025054
Epoch 18: Current Learning Rate: 6e-05
[Epoch 18] ‚è±Ô∏è Total: 3331.46s | Current time: 2025-07-15 04:18:28 | üèãÔ∏è Train: 2846.13s | ‚úÖ Val: 485.34s
grad AddEdge W: 1.2650840731381901e-16
grad ChooseDest W: 10.957282066345215
grad AddEdge W: 1.4648151038452442e-16
grad ChooseDest W: 3.0503485202789307
grad AddEdge W: 1.6779563452839773e-18
grad ChooseDest W: 4.564881801605225
grad AddEdge W: 1.363407948823045e-14
grad ChooseDest W: 6.020803451538086
grad AddEdge W: 5.630754749013408e-17
grad ChooseDest W: 7.820271968841553
grad AddEdge W: 5.397907046174426e-17
grad ChooseDest W: 6.8596367835998535
grad AddEdge W: 6.556139443300619e-19
grad ChooseDest W: 6.142486572265625
grad AddEdge W: 7.813833960475113e-19
grad ChooseDest W: 3.8681576251983643
grad AddEdge W: 2.422716722961252e-16
grad ChooseDest W: 5.259641647338867
grad AddEdge W: 2.5814803434298617e-16
grad ChooseDest W: 4.144011974334717
grad AddEdge W: 4.308334651716105e-18
grad ChooseDest W: 5.8881707191467285
grad AddEdge W: 7.775559796544402e-19
grad ChooseDest W: 4.563398838043213
grad AddEdge W: 1.8152749444126803e-18
grad ChooseDest W: 5.250404357910156
grad AddEdge W: 5.241451664933339e-19
grad ChooseDest W: 6.452397346496582
grad AddEdge W: 8.955038699935295e-19
grad ChooseDest W: 10.810098648071289
grad AddEdge W: 3.240793054820276e-17
grad ChooseDest W: 7.949797630310059
grad AddEdge W: 8.577023198417033e-17
grad ChooseDest W: 4.976189613342285
grad AddEdge W: 1.0394651347595335e-16
grad ChooseDest W: 9.678914070129395
grad AddEdge W: 3.096542662041346e-13
grad ChooseDest W: 3.2948572635650635
grad AddEdge W: 4.807730553602337e-17
grad ChooseDest W: 3.0146355628967285
grad AddEdge W: 8.59287661116498e-17
grad ChooseDest W: 2.5029313564300537
grad AddEdge W: 7.871190147161657e-18
grad ChooseDest W: 4.355798721313477
grad AddEdge W: 9.795995592000087e-17
grad ChooseDest W: 5.13828182220459
grad AddEdge W: 4.127330369691792e-19
grad ChooseDest W: 6.598421573638916
grad AddEdge W: 8.333571184881728e-19
grad ChooseDest W: 4.732907295227051
grad AddEdge W: 1.3952234380581584e-14
grad ChooseDest W: 5.002879619598389
grad AddEdge W: 1.5223439231256281e-18
grad ChooseDest W: 5.8168110847473145
grad AddEdge W: 2.2160643742839462e-16
grad ChooseDest W: 5.336847305297852
grad AddEdge W: 1.4752762347474127e-19
grad ChooseDest W: 6.0483012199401855
grad AddEdge W: 5.24431319286489e-19
grad ChooseDest W: 4.832170009613037
grad AddEdge W: 4.728045991127935e-19
grad ChooseDest W: 3.9593074321746826
grad AddEdge W: 2.2113980785443133e-11
grad ChooseDest W: 1.6889405250549316
grad AddEdge W: 4.086665381097809e-11
grad ChooseDest W: 4.805890083312988
grad AddEdge W: 3.6037043624304116e-19
grad ChooseDest W: 7.40175199508667
grad AddEdge W: 2.8267125199537326e-18
grad ChooseDest W: 6.277645587921143
grad AddEdge W: 2.4968868982897787e-16
grad ChooseDest W: 8.263675689697266
grad AddEdge W: 4.3482081652252845e-16
grad ChooseDest W: 6.703763961791992
grad AddEdge W: 1.476418528780342e-16
grad ChooseDest W: 7.80266809463501
grad AddEdge W: 1.0342595217286148e-16
grad ChooseDest W: 5.806238651275635
grad AddEdge W: 1.386347824496178e-16
grad ChooseDest W: 4.506965160369873
grad AddEdge W: 3.65163870964989e-16
grad ChooseDest W: 9.26660442352295
grad AddEdge W: 2.260650906497463e-18
grad ChooseDest W: 5.601124286651611
grad AddEdge W: 6.169905778319551e-17
grad ChooseDest W: 5.1126484870910645
grad AddEdge W: 7.051768488754377e-19
grad ChooseDest W: 5.6687541007995605
grad AddEdge W: 4.963890176415405e-18
grad ChooseDest W: 4.7374420166015625
grad AddEdge W: 2.0467781892430493e-18
grad ChooseDest W: 5.703843116760254
grad AddEdge W: 1.8497383906589366e-18
grad ChooseDest W: 3.505756378173828
grad AddEdge W: 1.598507161056431e-13
grad ChooseDest W: 4.340192794799805
grad AddEdge W: 6.470019394981039e-18
grad ChooseDest W: 5.711355686187744
grad AddEdge W: 8.955392319647162e-19
grad ChooseDest W: 6.185812950134277
grad AddEdge W: 5.714516381329508e-17
grad ChooseDest W: 4.065577507019043
grad AddEdge W: 6.550383713803322e-17
grad ChooseDest W: 6.434337139129639
grad AddEdge W: 2.036195447281199e-18
grad ChooseDest W: 5.209229469299316
grad AddEdge W: 1.0387935757715231e-18
grad ChooseDest W: 6.976073741912842
grad AddEdge W: 5.635445545510589e-19
grad ChooseDest W: 5.963466167449951
grad AddEdge W: 1.6340489779842032e-18
grad ChooseDest W: 7.148219108581543
grad AddEdge W: 2.5948115563450904e-19
grad ChooseDest W: 4.2442626953125
grad AddEdge W: 8.184545188350833e-17
grad ChooseDest W: 3.861013650894165
grad AddEdge W: 9.304568262166581e-20
grad ChooseDest W: 4.520204544067383
grad AddEdge W: 1.9512386216980215e-18
grad ChooseDest W: 5.210336685180664
grad AddEdge W: 2.1401801483773115e-16
grad ChooseDest W: 5.861922740936279
grad AddEdge W: 3.095750574003798e-17
grad ChooseDest W: 9.282709121704102
grad AddEdge W: 2.3497334525500652e-16
grad ChooseDest W: 6.043500900268555
grad AddEdge W: 1.865793630306467e-19
grad ChooseDest W: 3.5237600803375244
grad AddEdge W: 9.952661945656406e-19
grad ChooseDest W: 4.444473743438721
grad AddEdge W: 1.4156783255283282e-16
grad ChooseDest W: 5.61367654800415
=== Epoch 19: Train Loss: 4.8647, Train Log Prob: 0.0213 ===
Total mismatches: 80492
Predicted valid destination but wrong order: 44965
Epoch 19: Validation Loss: 5.4784, Validation Log Prob: 0.0066
Epoch 19: Edge Precision: 0.3778, Recall: 0.3756, F1: 0.3767, Jaccard: 0.2502
Epoch 19: TP: 2.6307802433786684, FP: 4.354044380816035, FN: 4.390694345025054
Epoch 19: Current Learning Rate: 6e-05
[Epoch 19] ‚è±Ô∏è Total: 3323.58s | Current time: 2025-07-15 05:13:52 | üèãÔ∏è Train: 2841.65s | ‚úÖ Val: 481.92s
grad AddEdge W: 1.3124485673618745e-16
grad ChooseDest W: 11.285490989685059
grad AddEdge W: 7.836783569582514e-18
grad ChooseDest W: 6.410142421722412
grad AddEdge W: 2.1400613321541244e-17
grad ChooseDest W: 9.211929321289062
grad AddEdge W: 6.394264333327055e-19
grad ChooseDest W: 6.0769877433776855
grad AddEdge W: 3.331546224468827e-18
grad ChooseDest W: 4.4455718994140625
grad AddEdge W: 1.0591199387305285e-16
grad ChooseDest W: 4.687502861022949
grad AddEdge W: 9.126065110857604e-15
grad ChooseDest W: 5.922069072723389
grad AddEdge W: 7.62543788909945e-19
grad ChooseDest W: 6.9582438468933105
grad AddEdge W: 1.2096958111674556e-18
grad ChooseDest W: 4.950136184692383
grad AddEdge W: 5.983612379381482e-18
grad ChooseDest W: 7.341645240783691
grad AddEdge W: 3.9313278555958206e-19
grad ChooseDest W: 5.424792289733887
grad AddEdge W: 1.1864685630233544e-16
grad ChooseDest W: 5.6495361328125
grad AddEdge W: 4.874059665528792e-15
grad ChooseDest W: 4.471673488616943
grad AddEdge W: 2.5600567874268507e-19
grad ChooseDest W: 4.191569805145264
grad AddEdge W: 1.5833461151571528e-18
grad ChooseDest W: 5.704313278198242
grad AddEdge W: 8.09454442218746e-19
grad ChooseDest W: 6.828845024108887
grad AddEdge W: 1.4993200497428322e-16
grad ChooseDest W: 4.6587114334106445
grad AddEdge W: 2.012665823264179e-16
grad ChooseDest W: 6.223430633544922
grad AddEdge W: 3.069636750899198e-19
grad ChooseDest W: 4.448692798614502
grad AddEdge W: 5.509560877873569e-17
grad ChooseDest W: 5.9393415451049805
grad AddEdge W: 3.0434366015715105e-15
grad ChooseDest W: 2.5231761932373047
grad AddEdge W: 5.906581391632635e-19
grad ChooseDest W: 6.087015628814697
grad AddEdge W: 1.3825854100235927e-16
grad ChooseDest W: 6.124715805053711
grad AddEdge W: 2.2703680039482767e-18
grad ChooseDest W: 6.42739725112915
grad AddEdge W: 4.890604042094738e-18
grad ChooseDest W: 6.503864765167236
grad AddEdge W: 1.0057949629925218e-18
grad ChooseDest W: 3.702646493911743
grad AddEdge W: 1.1613988031520157e-18
grad ChooseDest W: 4.7974772453308105
grad AddEdge W: 7.06952443759071e-19
grad ChooseDest W: 4.584427356719971
grad AddEdge W: 7.6346408110815e-17
grad ChooseDest W: 5.072998523712158
grad AddEdge W: 1.675860269611768e-18
grad ChooseDest W: 6.194164276123047
grad AddEdge W: 8.134543774683227e-17
grad ChooseDest W: 5.956973552703857
grad AddEdge W: 1.825397823584788e-16
grad ChooseDest W: 5.321549415588379
grad AddEdge W: 5.660099146679849e-17
grad ChooseDest W: 6.164503574371338
grad AddEdge W: 1.5632418005617812e-18
grad ChooseDest W: 7.043670177459717
grad AddEdge W: 5.559393546696213e-17
grad ChooseDest W: 4.692824840545654
grad AddEdge W: 3.6411739881634774e-15
grad ChooseDest W: 3.660325527191162
grad AddEdge W: 2.6159282883114387e-18
grad ChooseDest W: 3.8340651988983154
grad AddEdge W: 7.378257834581914e-17
grad ChooseDest W: 6.109802722930908
grad AddEdge W: 6.00352695942385e-19
grad ChooseDest W: 5.973823547363281
grad AddEdge W: 6.068221375517081e-18
grad ChooseDest W: 7.501523017883301
grad AddEdge W: 8.816304050315963e-17
grad ChooseDest W: 5.876304626464844
grad AddEdge W: 4.3110789784946144e-19
grad ChooseDest W: 5.333112716674805
grad AddEdge W: 8.413101498851405e-19
grad ChooseDest W: 7.348191261291504
grad AddEdge W: 1.575534883196692e-16
grad ChooseDest W: 4.275053977966309
grad AddEdge W: 5.596346268918856e-19
grad ChooseDest W: 6.32535457611084
grad AddEdge W: 1.007417312327922e-16
grad ChooseDest W: 3.558960437774658
grad AddEdge W: 2.535586923751154e-18
grad ChooseDest W: 5.606551647186279
grad AddEdge W: 1.7541495148775944e-16
grad ChooseDest W: 7.683114528656006
grad AddEdge W: 1.2367832465325459e-16
grad ChooseDest W: 6.679876327514648
grad AddEdge W: 6.69090939556447e-19
grad ChooseDest W: 5.983659267425537
grad AddEdge W: 7.8478052342569e-19
grad ChooseDest W: 4.8216447830200195
grad AddEdge W: 4.0023270423179315e-18
grad ChooseDest W: 5.601411819458008
grad AddEdge W: 1.3440239174624855e-15
grad ChooseDest W: 5.560129165649414
grad AddEdge W: 2.2418150733714415e-19
grad ChooseDest W: 8.344732284545898
grad AddEdge W: 9.104720754089522e-17
grad ChooseDest W: 4.77684211730957
grad AddEdge W: 1.3372209723466126e-16
grad ChooseDest W: 7.8608317375183105
grad AddEdge W: 9.682183440412606e-13
grad ChooseDest W: 4.95358943939209
grad AddEdge W: 3.865443126601125e-18
grad ChooseDest W: 5.463302135467529
grad AddEdge W: 7.717338358257612e-17
grad ChooseDest W: 5.853584289550781
grad AddEdge W: 7.562302811858457e-19
grad ChooseDest W: 4.190731525421143
grad AddEdge W: 4.15971957783401e-17
grad ChooseDest W: 8.923367500305176
grad AddEdge W: 6.212231452211032e-18
grad ChooseDest W: 3.232239007949829
grad AddEdge W: 8.369316823115988e-17
grad ChooseDest W: 5.357102394104004
grad AddEdge W: 3.970237139140608e-19
grad ChooseDest W: 9.813918113708496
grad AddEdge W: 4.8431883691725796e-20
grad ChooseDest W: 5.811183452606201
grad AddEdge W: 1.27949514759564e-16
grad ChooseDest W: 4.720394134521484
=== Epoch 20: Train Loss: 4.8253, Train Log Prob: 0.0220 ===
Total mismatches: 79768
Predicted valid destination but wrong order: 44934
Epoch 20: Validation Loss: 5.4806, Validation Log Prob: 0.0065
Epoch 20: Edge Precision: 0.3805, Recall: 0.3787, F1: 0.3795, Jaccard: 0.2521
Epoch 20: TP: 2.651682176091625, FP: 4.340300644237652, FN: 4.369792412312098
Epoch 20: Current Learning Rate: 6e-05
[Epoch 20] ‚è±Ô∏è Total: 3337.03s | Current time: 2025-07-15 06:09:29 | üèãÔ∏è Train: 2855.74s | ‚úÖ Val: 481.29s
grad AddEdge W: 3.0198836858028022e-16
grad ChooseDest W: 16.293935775756836
grad AddEdge W: 3.8815288943831376e-18
grad ChooseDest W: 5.337788105010986
grad AddEdge W: 2.280435304855027e-16
grad ChooseDest W: 3.244597911834717
grad AddEdge W: 2.479699795108689e-14
grad ChooseDest W: 10.062464714050293
grad AddEdge W: 1.0885791986968288e-18
grad ChooseDest W: 4.36625862121582
grad AddEdge W: 3.190499150088072e-16
grad ChooseDest W: 5.417850017547607
grad AddEdge W: 1.4899930961638323e-18
grad ChooseDest W: 4.51872444152832
grad AddEdge W: 4.169581204417352e-19
grad ChooseDest W: 8.475510597229004
grad AddEdge W: 9.30013787246599e-17
grad ChooseDest W: 7.262149333953857
grad AddEdge W: 6.042261883635268e-17
grad ChooseDest W: 6.4292311668396
grad AddEdge W: 1.4382462021567693e-19
grad ChooseDest W: 6.631091117858887
grad AddEdge W: 4.0146234957138386e-17
grad ChooseDest W: 4.50246000289917
grad AddEdge W: 1.1780081762617743e-17
grad ChooseDest W: 5.8706135749816895
grad AddEdge W: 9.538477723700079e-17
grad ChooseDest W: 6.886213779449463
grad AddEdge W: 4.1123343710077875e-15
grad ChooseDest W: 5.273426532745361
grad AddEdge W: 3.3803986842655962e-18
grad ChooseDest W: 6.602751731872559
grad AddEdge W: 3.216399829428332e-16
grad ChooseDest W: 7.6604485511779785
grad AddEdge W: 6.025558398051317e-19
grad ChooseDest W: 4.761322975158691
grad AddEdge W: 3.7161380980613775e-15
grad ChooseDest W: 3.4636387825012207
grad AddEdge W: 5.812196511974449e-15
grad ChooseDest W: 5.022881031036377
grad AddEdge W: 6.262310455420095e-17
grad ChooseDest W: 3.463299512863159
grad AddEdge W: 5.263764344969074e-19
grad ChooseDest W: 9.945401191711426
grad AddEdge W: 6.95451372085019e-17
grad ChooseDest W: 6.212334156036377
grad AddEdge W: 2.449265456952417e-18
grad ChooseDest W: 6.054037094116211
grad AddEdge W: 5.642165354011817e-19
grad ChooseDest W: 8.792984962463379
grad AddEdge W: 6.745209894350182e-17
grad ChooseDest W: 7.349615573883057
grad AddEdge W: 2.7530119681461767e-18
grad ChooseDest W: 8.24908447265625
grad AddEdge W: 6.266814412496416e-19
grad ChooseDest W: 4.03819465637207
grad AddEdge W: 2.8859202470439e-18
grad ChooseDest W: 5.157908916473389
grad AddEdge W: 5.240390599002587e-18
grad ChooseDest W: 5.84394645690918
grad AddEdge W: 9.036498412683752e-19
grad ChooseDest W: 5.5487775802612305
grad AddEdge W: 1.501802960564405e-18
grad ChooseDest W: 4.508988857269287
grad AddEdge W: 1.987730935191717e-18
grad ChooseDest W: 6.007355213165283
grad AddEdge W: 9.954447828598908e-18
grad ChooseDest W: 5.351700305938721
grad AddEdge W: 1.0880731709570995e-18
grad ChooseDest W: 8.608562469482422
grad AddEdge W: 3.4490503324092816e-18
grad ChooseDest W: 8.25705337524414
grad AddEdge W: 1.4345033391319376e-18
grad ChooseDest W: 6.468080520629883
grad AddEdge W: 1.5595331073340782e-16
grad ChooseDest W: 6.343263626098633
grad AddEdge W: 1.3777920431304551e-18
grad ChooseDest W: 6.965080261230469
grad AddEdge W: 2.573623066460603e-19
grad ChooseDest W: 4.150076866149902
grad AddEdge W: 4.347870679671266e-19
grad ChooseDest W: 5.777479648590088
grad AddEdge W: 1.1506521718954003e-16
grad ChooseDest W: 8.110160827636719
grad AddEdge W: 1.0428380133380552e-17
grad ChooseDest W: 3.0428903102874756
grad AddEdge W: 3.013812709502255e-17
grad ChooseDest W: 5.088531017303467
grad AddEdge W: 4.56952726988738e-18
grad ChooseDest W: 4.019845008850098
grad AddEdge W: 8.614731943040034e-19
grad ChooseDest W: 3.3383584022521973
grad AddEdge W: 3.1208027314717013e-18
grad ChooseDest W: 4.532419204711914
grad AddEdge W: 4.704581979077102e-19
grad ChooseDest W: 4.788698196411133
grad AddEdge W: 1.6184214682615451e-18
grad ChooseDest W: 6.7019171714782715
grad AddEdge W: 4.364300037600218e-19
grad ChooseDest W: 5.074380874633789
grad AddEdge W: 2.311167513065985e-16
grad ChooseDest W: 3.6658146381378174
grad AddEdge W: 3.757133645675538e-15
grad ChooseDest W: 3.5331127643585205
grad AddEdge W: 1.96288346356639e-18
grad ChooseDest W: 6.012119293212891
grad AddEdge W: 2.3108340870166277e-13
grad ChooseDest W: 5.4278764724731445
grad AddEdge W: 2.973385911424951e-18
grad ChooseDest W: 4.673416614532471
grad AddEdge W: 5.391682656821572e-19
grad ChooseDest W: 8.132755279541016
grad AddEdge W: 2.3287330407894057e-18
grad ChooseDest W: 7.358368396759033
grad AddEdge W: 4.9337958568056235e-17
grad ChooseDest W: 9.432721138000488
grad AddEdge W: 2.334548947676266e-18
grad ChooseDest W: 7.04727029800415
grad AddEdge W: 1.9306701222942066e-16
grad ChooseDest W: 8.639826774597168
grad AddEdge W: 3.848796426966225e-19
grad ChooseDest W: 4.974302768707275
grad AddEdge W: 5.834581729959641e-18
grad ChooseDest W: 4.116381645202637
grad AddEdge W: 1.0587273877631323e-18
grad ChooseDest W: 4.023625373840332
grad AddEdge W: 5.775895829758903e-17
grad ChooseDest W: 4.745151996612549
grad AddEdge W: 9.31946552650472e-19
grad ChooseDest W: 7.875429630279541
grad AddEdge W: 2.2707907759593525e-16
grad ChooseDest W: 3.366354465484619
=== Epoch 21: Train Loss: 4.7809, Train Log Prob: 0.0229 ===
Total mismatches: 78919
Predicted valid destination but wrong order: 45037
Epoch 21: Validation Loss: 5.4035, Validation Log Prob: 0.0069
Epoch 21: Edge Precision: 0.3783, Recall: 0.3761, F1: 0.3771, Jaccard: 0.2507
Epoch 21: TP: 2.6346456692913387, FP: 4.348890479599141, FN: 4.386828919112384
Epoch 21: Current Learning Rate: 6e-05
[Epoch 21] ‚è±Ô∏è Total: 3337.88s | Current time: 2025-07-15 07:05:07 | üèãÔ∏è Train: 2855.75s | ‚úÖ Val: 482.13s
grad AddEdge W: 2.4914362412741973e-16
grad ChooseDest W: 11.389872550964355
grad AddEdge W: 3.666890414676653e-18
grad ChooseDest W: 7.883750915527344
grad AddEdge W: 1.8893665458548207e-18
grad ChooseDest W: 5.173954010009766
grad AddEdge W: 1.3264353974267582e-17
grad ChooseDest W: 9.13582706451416
grad AddEdge W: 3.1844649295580434e-19
grad ChooseDest W: 6.8483757972717285
grad AddEdge W: 1.1803418595649395e-18
grad ChooseDest W: 4.681206703186035
grad AddEdge W: 6.883946611921046e-17
grad ChooseDest W: 6.443482398986816
grad AddEdge W: 5.402442312036932e-17
grad ChooseDest W: 5.32487678527832
grad AddEdge W: 1.7296852222633232e-18
grad ChooseDest W: 5.034215450286865
grad AddEdge W: 8.782664930909146e-17
grad ChooseDest W: 7.191685676574707
grad AddEdge W: 3.515739287754532e-17
grad ChooseDest W: 3.942221164703369
grad AddEdge W: 9.93924408350406e-17
grad ChooseDest W: 2.4160706996917725
grad AddEdge W: 1.2868339188056288e-18
grad ChooseDest W: 4.959494590759277
grad AddEdge W: 5.203989171978696e-19
grad ChooseDest W: 6.033297538757324
grad AddEdge W: 1.3476120813759527e-16
grad ChooseDest W: 5.545446395874023
grad AddEdge W: 4.447316711056652e-17
grad ChooseDest W: 5.711671829223633
grad AddEdge W: 2.178633880811299e-18
grad ChooseDest W: 6.058515548706055
grad AddEdge W: 1.144127900619174e-16
grad ChooseDest W: 6.964597702026367
grad AddEdge W: 4.350823197470197e-19
grad ChooseDest W: 4.81374454498291
grad AddEdge W: 1.8075764003010435e-19
grad ChooseDest W: 6.612174987792969
grad AddEdge W: 2.25414342491972e-19
grad ChooseDest W: 3.7414164543151855
grad AddEdge W: 3.3646595926084544e-11
grad ChooseDest W: 4.727181911468506
grad AddEdge W: 4.73998024281312e-18
grad ChooseDest W: 3.774052381515503
grad AddEdge W: 1.7237877724079857e-14
grad ChooseDest W: 5.580563545227051
grad AddEdge W: 1.737310896932873e-16
grad ChooseDest W: 8.549036979675293
grad AddEdge W: 3.747163330041249e-18
grad ChooseDest W: 6.458789825439453
grad AddEdge W: 8.823374293883698e-19
grad ChooseDest W: 6.190933704376221
grad AddEdge W: 5.083502043954222e-19
grad ChooseDest W: 6.834946632385254
grad AddEdge W: 5.558041602703057e-19
grad ChooseDest W: 10.25980281829834
grad AddEdge W: 4.350723522206385e-17
grad ChooseDest W: 4.797999858856201
grad AddEdge W: 8.89360916571981e-19
grad ChooseDest W: 4.177301406860352
grad AddEdge W: 2.986692145553632e-18
grad ChooseDest W: 5.055401802062988
grad AddEdge W: 6.3310485025787615e-18
grad ChooseDest W: 4.800891399383545
grad AddEdge W: 2.2137890568448027e-18
grad ChooseDest W: 6.046751499176025
grad AddEdge W: 1.3814724798684333e-18
grad ChooseDest W: 5.390929222106934
grad AddEdge W: 4.711562866459167e-17
grad ChooseDest W: 5.093774318695068
grad AddEdge W: 4.839391739407932e-19
grad ChooseDest W: 4.8548903465271
grad AddEdge W: 1.1573455147529293e-18
grad ChooseDest W: 7.019049167633057
grad AddEdge W: 1.2162236879171769e-19
grad ChooseDest W: 7.466828346252441
grad AddEdge W: 7.537106687740842e-16
grad ChooseDest W: 3.4906251430511475
grad AddEdge W: 2.349490231274426e-13
grad ChooseDest W: 3.579563856124878
grad AddEdge W: 8.509504229365632e-19
grad ChooseDest W: 5.519031524658203
grad AddEdge W: 5.39810468030228e-19
grad ChooseDest W: 5.3617143630981445
grad AddEdge W: 1.2893923305376125e-16
grad ChooseDest W: 4.304910659790039
grad AddEdge W: 7.068824787549098e-17
grad ChooseDest W: 7.782113552093506
grad AddEdge W: 4.726442191317771e-18
grad ChooseDest W: 5.83140754699707
grad AddEdge W: 3.493282194391554e-16
grad ChooseDest W: 5.595998287200928
grad AddEdge W: 1.8398439916760337e-14
grad ChooseDest W: 5.423069477081299
grad AddEdge W: 6.323319326935066e-17
grad ChooseDest W: 6.6464457511901855
grad AddEdge W: 7.292772071995635e-13
grad ChooseDest W: 2.823944091796875
grad AddEdge W: 1.3649326312891598e-17
grad ChooseDest W: 4.748078346252441
grad AddEdge W: 2.5478871511586057e-19
grad ChooseDest W: 5.00418758392334
grad AddEdge W: 1.5492379512557943e-16
grad ChooseDest W: 4.766839981079102
grad AddEdge W: 4.656422489838499e-19
grad ChooseDest W: 6.535192966461182
grad AddEdge W: 7.990095809252506e-19
grad ChooseDest W: 6.117346286773682
grad AddEdge W: 2.0554776477452695e-18
grad ChooseDest W: 6.0621795654296875
grad AddEdge W: 7.034409586612069e-19
grad ChooseDest W: 5.351885795593262
grad AddEdge W: 8.12877783612281e-15
grad ChooseDest W: 2.5998051166534424
grad AddEdge W: 1.6710748792212772e-14
grad ChooseDest W: 3.996752977371216
grad AddEdge W: 2.440335628649601e-18
grad ChooseDest W: 5.048562049865723
grad AddEdge W: 8.312500102945435e-17
grad ChooseDest W: 5.913482666015625
grad AddEdge W: 1.967068790670755e-18
grad ChooseDest W: 4.261484622955322
grad AddEdge W: 3.8489939452887936e-16
grad ChooseDest W: 4.848491668701172
grad AddEdge W: 1.6195451269492494e-16
grad ChooseDest W: 3.6244874000549316
grad AddEdge W: 8.558017338316101e-19
grad ChooseDest W: 8.671049118041992
grad AddEdge W: 5.106924179986543e-19
grad ChooseDest W: 8.394116401672363
=== Epoch 22: Train Loss: 4.7366, Train Log Prob: 0.0241 ===
Total mismatches: 77848
Predicted valid destination but wrong order: 45215
Epoch 22: Validation Loss: 5.3037, Validation Log Prob: 0.0075
Epoch 22: Edge Precision: 0.3776, Recall: 0.3752, F1: 0.3763, Jaccard: 0.2499
Epoch 22: TP: 2.6272011453113815, FP: 4.354187544738726, FN: 4.394273443092341
Epoch 22: Current Learning Rate: 6e-05
[Epoch 22] ‚è±Ô∏è Total: 3328.14s | Current time: 2025-07-15 08:00:35 | üèãÔ∏è Train: 2842.65s | ‚úÖ Val: 485.50s
grad AddEdge W: 1.105441325114559e-16
grad ChooseDest W: 8.59547233581543
grad AddEdge W: 7.491616816395774e-18
grad ChooseDest W: 7.352794647216797
grad AddEdge W: 7.599524905447578e-19
grad ChooseDest W: 4.1171040534973145
grad AddEdge W: 1.2970826940728736e-13
grad ChooseDest W: 2.9249608516693115
grad AddEdge W: 2.3282780914525016e-18
grad ChooseDest W: 5.249331474304199
grad AddEdge W: 5.777583781134323e-15
grad ChooseDest W: 6.926932334899902
grad AddEdge W: 3.291225925895389e-18
grad ChooseDest W: 5.286901473999023
grad AddEdge W: 6.513170512405787e-19
grad ChooseDest W: 5.18026065826416
grad AddEdge W: 6.254789791329309e-19
grad ChooseDest W: 4.847105979919434
grad AddEdge W: 1.8351485790147262e-18
grad ChooseDest W: 4.360436916351318
grad AddEdge W: 2.689630833624271e-16
grad ChooseDest W: 5.726254940032959
grad AddEdge W: 3.0306894687450213e-18
grad ChooseDest W: 4.932528495788574
grad AddEdge W: 2.911122802490778e-16
grad ChooseDest W: 5.447262287139893
grad AddEdge W: 4.775574238161583e-19
grad ChooseDest W: 7.110260486602783
grad AddEdge W: 1.1896206546886765e-18
grad ChooseDest W: 4.3866729736328125
grad AddEdge W: 8.203834213054957e-18
grad ChooseDest W: 5.0259528160095215
grad AddEdge W: 3.3378843122784186e-15
grad ChooseDest W: 2.8446786403656006
grad AddEdge W: 1.3186712992515765e-19
grad ChooseDest W: 6.936395645141602
grad AddEdge W: 4.074397634728452e-18
grad ChooseDest W: 4.821682453155518
grad AddEdge W: 9.993358197818249e-19
grad ChooseDest W: 8.247727394104004
grad AddEdge W: 1.3564172204732327e-17
grad ChooseDest W: 3.60740327835083
grad AddEdge W: 3.105591965681405e-19
grad ChooseDest W: 6.02595329284668
grad AddEdge W: 2.046101943242893e-19
grad ChooseDest W: 6.345261573791504
grad AddEdge W: 2.4363657820948057e-18
grad ChooseDest W: 4.330658435821533
grad AddEdge W: 4.638422729516616e-18
grad ChooseDest W: 5.9920220375061035
grad AddEdge W: 9.077160543645327e-19
grad ChooseDest W: 5.257778644561768
grad AddEdge W: 1.1287343713404858e-18
grad ChooseDest W: 6.457868576049805
grad AddEdge W: 8.579133191403049e-19
grad ChooseDest W: 6.517625331878662
grad AddEdge W: 2.393019747830516e-16
grad ChooseDest W: 3.10821795463562
grad AddEdge W: 1.1632675312107151e-14
grad ChooseDest W: 8.67729377746582
grad AddEdge W: 4.863111619101742e-17
grad ChooseDest W: 5.763795375823975
grad AddEdge W: 5.927874571563398e-19
grad ChooseDest W: 4.7197465896606445
grad AddEdge W: 5.378162906697275e-19
grad ChooseDest W: 7.979477882385254
grad AddEdge W: 1.2468425597544908e-11
grad ChooseDest W: 1.174143671989441
grad AddEdge W: 8.605628820398888e-19
grad ChooseDest W: 5.638190269470215
grad AddEdge W: 1.715068341133567e-16
grad ChooseDest W: 6.043684959411621
grad AddEdge W: 5.031499055863488e-16
grad ChooseDest W: 5.817903518676758
grad AddEdge W: 4.562687389502796e-15
grad ChooseDest W: 3.787198066711426
grad AddEdge W: 8.658593195020658e-20
grad ChooseDest W: 4.409928321838379
grad AddEdge W: 8.453141176402034e-19
grad ChooseDest W: 7.851596355438232
grad AddEdge W: 8.044240136338568e-17
grad ChooseDest W: 7.212554454803467
grad AddEdge W: 2.2260834422891783e-18
grad ChooseDest W: 8.018527030944824
grad AddEdge W: 8.775753671455143e-17
grad ChooseDest W: 5.605189800262451
grad AddEdge W: 1.2006712706845021e-18
grad ChooseDest W: 7.26746940612793
grad AddEdge W: 2.2313230110842424e-18
grad ChooseDest W: 5.552787780761719
grad AddEdge W: 7.647208662436386e-17
grad ChooseDest W: 5.092440605163574
grad AddEdge W: 1.0235692380153265e-16
grad ChooseDest W: 3.94472599029541
grad AddEdge W: 2.888462224765064e-19
grad ChooseDest W: 7.754220962524414
grad AddEdge W: 8.865013531943852e-19
grad ChooseDest W: 7.604656219482422
grad AddEdge W: 7.646406628114455e-17
grad ChooseDest W: 6.371025085449219
grad AddEdge W: 7.630667035418796e-17
grad ChooseDest W: 7.304348468780518
grad AddEdge W: 3.08109081596699e-16
grad ChooseDest W: 7.414367198944092
grad AddEdge W: 2.229041580780372e-16
grad ChooseDest W: 6.896467685699463
grad AddEdge W: 1.015980815424663e-16
grad ChooseDest W: 4.872561931610107
grad AddEdge W: 1.140007482377476e-16
grad ChooseDest W: 3.362828493118286
grad AddEdge W: 2.7070357725035707e-16
grad ChooseDest W: 6.943378448486328
grad AddEdge W: 2.7586643000669046e-18
grad ChooseDest W: 4.078947067260742
grad AddEdge W: 1.5603281602516397e-18
grad ChooseDest W: 8.291291236877441
grad AddEdge W: 1.1270253789718237e-16
grad ChooseDest W: 4.4815874099731445
grad AddEdge W: 2.5028967459430583e-18
grad ChooseDest W: 4.866259574890137
grad AddEdge W: 8.8354410392235e-17
grad ChooseDest W: 7.403443813323975
grad AddEdge W: 7.691383312471591e-19
grad ChooseDest W: 6.206601619720459
grad AddEdge W: 5.488378002477489e-19
grad ChooseDest W: 4.538797378540039
grad AddEdge W: 1.4618458976774545e-18
grad ChooseDest W: 4.132464408874512
grad AddEdge W: 9.95002840938119e-19
grad ChooseDest W: 5.624209880828857
grad AddEdge W: 3.0957353332010115e-19
grad ChooseDest W: 7.041207790374756
=== Epoch 23: Train Loss: 4.6942, Train Log Prob: 0.0249 ===
Total mismatches: 77020
Predicted valid destination but wrong order: 45361
Epoch 23: Validation Loss: 5.2827, Validation Log Prob: 0.0077
Epoch 23: Edge Precision: 0.3776, Recall: 0.3755, F1: 0.3765, Jaccard: 0.2498
Epoch 23: TP: 2.6283464566929133, FP: 4.356335003579098, FN: 4.393128131710809
Epoch 23: Current Learning Rate: 6e-05
[Epoch 23] ‚è±Ô∏è Total: 3330.10s | Current time: 2025-07-15 08:56:05 | üèãÔ∏è Train: 2850.65s | ‚úÖ Val: 479.45s
grad AddEdge W: 3.2287238941130863e-16
grad ChooseDest W: 9.984670639038086
grad AddEdge W: 6.708707737407197e-19
grad ChooseDest W: 5.977032661437988
grad AddEdge W: 2.012636671351441e-19
grad ChooseDest W: 5.240180015563965
grad AddEdge W: 6.338084169996893e-17
grad ChooseDest W: 6.888853073120117
grad AddEdge W: 1.2898298635867193e-18
grad ChooseDest W: 8.615468978881836
grad AddEdge W: 2.996487471129336e-15
grad ChooseDest W: 4.482245445251465
grad AddEdge W: 6.950680536774861e-14
grad ChooseDest W: 2.055284023284912
grad AddEdge W: 7.939694143566002e-19
grad ChooseDest W: 3.083585739135742
grad AddEdge W: 4.456453748102913e-18
grad ChooseDest W: 6.694583892822266
grad AddEdge W: 8.84028699412408e-17
grad ChooseDest W: 5.824364185333252
grad AddEdge W: 2.495968587520013e-14
grad ChooseDest W: 3.6675262451171875
grad AddEdge W: 2.0362124127555625e-16
grad ChooseDest W: 4.75579833984375
grad AddEdge W: 4.848442232682604e-18
grad ChooseDest W: 4.131318092346191
grad AddEdge W: 2.9015181850596594e-18
grad ChooseDest W: 9.216276168823242
grad AddEdge W: 3.4818854322606966e-17
grad ChooseDest W: 5.841732978820801
grad AddEdge W: 4.352208595956048e-15
grad ChooseDest W: 3.999589204788208
grad AddEdge W: 2.2444414338548433e-16
grad ChooseDest W: 5.952815055847168
grad AddEdge W: 5.074588614507049e-17
grad ChooseDest W: 4.343113899230957
grad AddEdge W: 9.207186098108303e-19
grad ChooseDest W: 6.718359470367432
grad AddEdge W: 1.9095212150699815e-18
grad ChooseDest W: 9.03287410736084
grad AddEdge W: 3.747131897177972e-18
grad ChooseDest W: 4.820441722869873
grad AddEdge W: 2.479546016277115e-16
grad ChooseDest W: 3.137935161590576
grad AddEdge W: 1.8667898918061047e-18
grad ChooseDest W: 6.949287414550781
grad AddEdge W: 2.7612370385670977e-18
grad ChooseDest W: 4.736535549163818
grad AddEdge W: 1.4663761590972543e-16
grad ChooseDest W: 3.5659444332122803
grad AddEdge W: 3.960705878154162e-17
grad ChooseDest W: 4.839967727661133
grad AddEdge W: 1.256189830145326e-16
grad ChooseDest W: 8.645991325378418
grad AddEdge W: 1.9284700653867878e-14
grad ChooseDest W: 4.797914981842041
grad AddEdge W: 1.443862023359235e-16
grad ChooseDest W: 5.340842247009277
grad AddEdge W: 2.702457294725865e-17
grad ChooseDest W: 4.261874198913574
grad AddEdge W: 3.7171623991226244e-17
grad ChooseDest W: 9.831650733947754
grad AddEdge W: 8.364838136407381e-17
grad ChooseDest W: 7.041621208190918
grad AddEdge W: 7.058247198922464e-16
grad ChooseDest W: 5.357203960418701
grad AddEdge W: 9.056147054159183e-19
grad ChooseDest W: 7.022104263305664
grad AddEdge W: 2.716705149904846e-19
grad ChooseDest W: 6.353135108947754
grad AddEdge W: 1.8856446466886384e-18
grad ChooseDest W: 4.762406349182129
grad AddEdge W: 8.325774697415686e-17
grad ChooseDest W: 5.676590442657471
grad AddEdge W: 2.294229501098859e-16
grad ChooseDest W: 3.934126853942871
grad AddEdge W: 3.110164723505175e-19
grad ChooseDest W: 5.706623077392578
grad AddEdge W: 1.1094673124175281e-16
grad ChooseDest W: 6.75890588760376
grad AddEdge W: 7.534036193307045e-15
grad ChooseDest W: 5.2118964195251465
grad AddEdge W: 9.762884251030002e-11
grad ChooseDest W: 0.16813088953495026
grad AddEdge W: 1.1364472970210477e-16
grad ChooseDest W: 2.423072099685669
grad AddEdge W: 9.828769149613928e-17
grad ChooseDest W: 3.4874930381774902
grad AddEdge W: 1.886484119205094e-16
grad ChooseDest W: 5.424308776855469
grad AddEdge W: 1.4505314813765765e-16
grad ChooseDest W: 6.9240641593933105
grad AddEdge W: 1.148820798155111e-18
grad ChooseDest W: 5.14298152923584
grad AddEdge W: 4.061038047450261e-19
grad ChooseDest W: 5.198498725891113
grad AddEdge W: 1.4937757931050373e-18
grad ChooseDest W: 3.6926026344299316
grad AddEdge W: 7.63277005940815e-17
grad ChooseDest W: 3.6535804271698
grad AddEdge W: 8.057197424325844e-18
grad ChooseDest W: 4.570120334625244
grad AddEdge W: 7.315775258087619e-17
grad ChooseDest W: 3.7450315952301025
grad AddEdge W: 1.7438024449440666e-17
grad ChooseDest W: 5.241732120513916
grad AddEdge W: 6.40417656268437e-17
grad ChooseDest W: 4.240698337554932
grad AddEdge W: 4.905635320031868e-17
grad ChooseDest W: 6.893165588378906
grad AddEdge W: 7.067946652610812e-17
grad ChooseDest W: 5.929714679718018
grad AddEdge W: 3.567605828267322e-17
grad ChooseDest W: 9.305098533630371
grad AddEdge W: 3.014890691276534e-17
grad ChooseDest W: 4.568691253662109
grad AddEdge W: 9.106369971285194e-20
grad ChooseDest W: 9.48501968383789
grad AddEdge W: 1.2260005750166415e-18
grad ChooseDest W: 5.810159683227539
grad AddEdge W: 8.621657864270395e-17
grad ChooseDest W: 4.332540988922119
grad AddEdge W: 9.859870478901432e-17
grad ChooseDest W: 4.444155693054199
grad AddEdge W: 2.074453038648338e-16
grad ChooseDest W: 6.597798824310303
grad AddEdge W: 7.345756750261848e-19
grad ChooseDest W: 5.818244934082031
grad AddEdge W: 2.2991888376460024e-18
grad ChooseDest W: 3.7135391235351562
grad AddEdge W: 9.887187953194873e-17
grad ChooseDest W: 7.403876304626465
=== Epoch 24: Train Loss: 4.6452, Train Log Prob: 0.0260 ===
Total mismatches: 76105
Predicted valid destination but wrong order: 45389
Epoch 24: Validation Loss: 5.1643, Validation Log Prob: 0.0086
Epoch 24: Edge Precision: 0.3780, Recall: 0.3758, F1: 0.3768, Jaccard: 0.2499
Epoch 24: TP: 2.631782390837509, FP: 4.355476020042949, FN: 4.389692197566213
Epoch 24: Current Learning Rate: 6e-05
[Epoch 24] ‚è±Ô∏è Total: 3333.14s | Current time: 2025-07-15 09:51:38 | üèãÔ∏è Train: 2852.80s | ‚úÖ Val: 480.34s
grad AddEdge W: 2.7412696678580373e-16
grad ChooseDest W: 9.56302547454834
grad AddEdge W: 5.927517849924235e-19
grad ChooseDest W: 4.7843017578125
grad AddEdge W: 3.80468571799237e-17
grad ChooseDest W: 4.568716049194336
grad AddEdge W: 3.1427088522363666e-19
grad ChooseDest W: 5.247970104217529
grad AddEdge W: 1.3281446172700887e-16
grad ChooseDest W: 2.2636938095092773
grad AddEdge W: 3.749555536372752e-18
grad ChooseDest W: 2.8009254932403564
grad AddEdge W: 5.494588060926231e-19
grad ChooseDest W: 6.381075859069824
grad AddEdge W: 1.2294849699494446e-18
grad ChooseDest W: 3.2620232105255127
grad AddEdge W: 3.1289574915405553e-19
grad ChooseDest W: 6.201070785522461
grad AddEdge W: 1.1651107596072352e-14
grad ChooseDest W: 3.539642333984375
grad AddEdge W: 7.437801789911569e-19
grad ChooseDest W: 7.601561069488525
grad AddEdge W: 2.492739613221785e-16
grad ChooseDest W: 4.464560508728027
grad AddEdge W: 2.1272782173845794e-19
grad ChooseDest W: 4.128364562988281
grad AddEdge W: 4.0837905973534205e-13
grad ChooseDest W: 7.84211540222168
grad AddEdge W: 1.1903973400607362e-16
grad ChooseDest W: 5.973260879516602
grad AddEdge W: 1.7493871134906878e-18
grad ChooseDest W: 6.059617519378662
grad AddEdge W: 7.535570381732759e-17
grad ChooseDest W: 7.852072715759277
grad AddEdge W: 1.2967055335948673e-16
grad ChooseDest W: 9.22517204284668
grad AddEdge W: 7.14138639437123e-17
grad ChooseDest W: 4.806748390197754
grad AddEdge W: 6.447682106322081e-19
grad ChooseDest W: 3.729919672012329
grad AddEdge W: 4.1697574972854024e-19
grad ChooseDest W: 5.4227519035339355
grad AddEdge W: 2.5395757954600378e-18
grad ChooseDest W: 6.958310604095459
grad AddEdge W: 1.617614394755322e-15
grad ChooseDest W: 2.498757839202881
grad AddEdge W: 2.099494403063609e-15
grad ChooseDest W: 3.8005967140197754
grad AddEdge W: 2.534353918403915e-15
grad ChooseDest W: 3.572587251663208
grad AddEdge W: 6.9088105857995936e-18
grad ChooseDest W: 7.419402122497559
grad AddEdge W: 9.648167456393745e-19
grad ChooseDest W: 4.950300693511963
grad AddEdge W: 3.730637833045603e-17
grad ChooseDest W: 5.522712230682373
grad AddEdge W: 3.536294312386168e-18
grad ChooseDest W: 2.9952902793884277
grad AddEdge W: 1.845270672365252e-16
grad ChooseDest W: 6.46693229675293
grad AddEdge W: 1.4763517504894907e-18
grad ChooseDest W: 5.134607791900635
grad AddEdge W: 9.167086449963264e-19
grad ChooseDest W: 8.44258975982666
grad AddEdge W: 6.808569116857661e-19
grad ChooseDest W: 4.298751354217529
grad AddEdge W: 8.31226249531448e-19
grad ChooseDest W: 6.816620349884033
grad AddEdge W: 3.0177589814095924e-18
grad ChooseDest W: 4.536335468292236
grad AddEdge W: 6.758928974322588e-19
grad ChooseDest W: 4.513644218444824
grad AddEdge W: 3.5304743523143064e-17
grad ChooseDest W: 6.150811672210693
grad AddEdge W: 1.5055190074277536e-15
grad ChooseDest W: 3.0785796642303467
grad AddEdge W: 1.8785292390594572e-18
grad ChooseDest W: 5.151489734649658
grad AddEdge W: 1.1256957754726508e-14
grad ChooseDest W: 8.353240966796875
grad AddEdge W: 4.232470747982759e-15
grad ChooseDest W: 4.985074043273926
grad AddEdge W: 1.018580176732871e-18
grad ChooseDest W: 6.016470909118652
grad AddEdge W: 5.2108987150329276e-18
grad ChooseDest W: 6.495723724365234
grad AddEdge W: 9.420853054185344e-19
grad ChooseDest W: 6.269143104553223
grad AddEdge W: 1.5210224896893655e-16
grad ChooseDest W: 1.70823073387146
grad AddEdge W: 2.304125658336872e-18
grad ChooseDest W: 6.489743709564209
grad AddEdge W: 1.7293179540713496e-18
grad ChooseDest W: 7.539542198181152
grad AddEdge W: 1.1932885017377316e-16
grad ChooseDest W: 4.768563270568848
grad AddEdge W: 2.73866224567476e-18
grad ChooseDest W: 8.968623161315918
grad AddEdge W: 4.622916815344003e-18
grad ChooseDest W: 2.6340126991271973
grad AddEdge W: 3.4553429021241257e-18
grad ChooseDest W: 4.854665756225586
grad AddEdge W: 5.632101564486767e-18
grad ChooseDest W: 6.932295799255371
grad AddEdge W: 1.5516963320363019e-16
grad ChooseDest W: 4.876861095428467
grad AddEdge W: 3.8379444004918613e-16
grad ChooseDest W: 4.535731315612793
grad AddEdge W: 3.516145847025602e-18
grad ChooseDest W: 3.735421657562256
grad AddEdge W: 8.146968689228264e-17
grad ChooseDest W: 6.055483818054199
grad AddEdge W: 2.8765187189967755e-18
grad ChooseDest W: 5.098016262054443
grad AddEdge W: 2.6188324780830818e-17
grad ChooseDest W: 7.042683124542236
grad AddEdge W: 8.192036135978113e-17
grad ChooseDest W: 5.53784704208374
grad AddEdge W: 8.971549307679915e-17
grad ChooseDest W: 4.267634868621826
grad AddEdge W: 3.74878951542674e-19
grad ChooseDest W: 3.342862367630005
grad AddEdge W: 1.2561535830908839e-18
grad ChooseDest W: 3.433147668838501
grad AddEdge W: 1.303469621074285e-16
grad ChooseDest W: 5.110311508178711
grad AddEdge W: 4.730002442848648e-16
grad ChooseDest W: 4.852812767028809
grad AddEdge W: 1.5768566516411028e-16
grad ChooseDest W: 6.72229528427124
grad AddEdge W: 2.8597174317130985e-16
grad ChooseDest W: 5.57789421081543
=== Epoch 25: Train Loss: 4.5974, Train Log Prob: 0.0272 ===
Total mismatches: 75098
Predicted valid destination but wrong order: 45633
Epoch 25: Validation Loss: 5.1714, Validation Log Prob: 0.0086
Epoch 25: Edge Precision: 0.3777, Recall: 0.3752, F1: 0.3763, Jaccard: 0.2502
Epoch 25: TP: 2.6260558339298496, FP: 4.351753758052971, FN: 4.395418754473873
Epoch 25: Current Learning Rate: 6e-05
üíæ Checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/checkpoint_epoch_25.pth
[Epoch 25] ‚è±Ô∏è Total: 3338.79s | Current time: 2025-07-15 10:47:17 | üèãÔ∏è Train: 2855.61s | ‚úÖ Val: 483.18s
grad AddEdge W: 2.8550193105315934e-16
grad ChooseDest W: 11.279585838317871
grad AddEdge W: 1.4296902357093845e-16
grad ChooseDest W: 4.724831581115723
grad AddEdge W: 3.8847369075937714e-19
grad ChooseDest W: 6.051661014556885
grad AddEdge W: 8.727433750536735e-17
grad ChooseDest W: 5.006932735443115
grad AddEdge W: 8.956390064902023e-17
grad ChooseDest W: 8.242006301879883
grad AddEdge W: 1.294016017871697e-18
grad ChooseDest W: 4.361248016357422
grad AddEdge W: 1.9333226589080926e-16
grad ChooseDest W: 5.900671005249023
grad AddEdge W: 2.156536619286588e-16
grad ChooseDest W: 6.653804302215576
grad AddEdge W: 1.2317110120740622e-14
grad ChooseDest W: 3.619164228439331
grad AddEdge W: 1.7869433880472797e-18
grad ChooseDest W: 4.814645290374756
grad AddEdge W: 5.451578825656052e-17
grad ChooseDest W: 4.968820095062256
grad AddEdge W: 2.3092704239619313e-16
grad ChooseDest W: 5.4405837059021
grad AddEdge W: 9.54759573559225e-19
grad ChooseDest W: 3.268401861190796
grad AddEdge W: 7.237772126323868e-17
grad ChooseDest W: 5.965939044952393
grad AddEdge W: 3.3799427009529264e-18
grad ChooseDest W: 4.726953983306885
grad AddEdge W: 1.5773959982159058e-18
grad ChooseDest W: 5.297364711761475
grad AddEdge W: 5.545749698800519e-18
grad ChooseDest W: 4.725069999694824
grad AddEdge W: 5.482141825800906e-17
grad ChooseDest W: 3.3020620346069336
grad AddEdge W: 1.2682000055956513e-14
grad ChooseDest W: 7.014658451080322
grad AddEdge W: 1.5929190432990045e-16
grad ChooseDest W: 5.03085470199585
grad AddEdge W: 2.0608109112883174e-16
grad ChooseDest W: 4.6786274909973145
grad AddEdge W: 3.999597263578445e-17
grad ChooseDest W: 4.782651424407959
grad AddEdge W: 1.7351217932202534e-15
grad ChooseDest W: 5.474445343017578
grad AddEdge W: 5.709960932260056e-17
grad ChooseDest W: 4.688599109649658
grad AddEdge W: 1.1745595568904641e-18
grad ChooseDest W: 4.767240047454834
grad AddEdge W: 1.075647999986073e-16
grad ChooseDest W: 3.796367883682251
grad AddEdge W: 3.976302658447936e-14
grad ChooseDest W: 5.855518341064453
grad AddEdge W: 7.467164116717788e-19
grad ChooseDest W: 5.159616947174072
grad AddEdge W: 4.3340355669384337e-19
grad ChooseDest W: 2.761693239212036
grad AddEdge W: 1.3548382981199915e-18
grad ChooseDest W: 5.176071643829346
grad AddEdge W: 2.1846165555814562e-16
grad ChooseDest W: 4.256419658660889
grad AddEdge W: 2.179135152262506e-18
grad ChooseDest W: 6.324350833892822
grad AddEdge W: 8.841107557291733e-17
grad ChooseDest W: 4.3064985275268555
grad AddEdge W: 5.088104787075196e-19
grad ChooseDest W: 4.94932746887207
grad AddEdge W: 1.782248517685582e-18
grad ChooseDest W: 12.322834014892578
grad AddEdge W: 1.4417166642661297e-18
grad ChooseDest W: 5.073491096496582
grad AddEdge W: 3.491403634133222e-17
grad ChooseDest W: 8.195402145385742
grad AddEdge W: 1.46746907148159e-18
grad ChooseDest W: 4.160579681396484
grad AddEdge W: 1.3309298998286773e-16
grad ChooseDest W: 3.5885510444641113
grad AddEdge W: 7.162660838661119e-19
grad ChooseDest W: 4.4461188316345215
grad AddEdge W: 3.4506552695927876e-18
grad ChooseDest W: 4.682827949523926
grad AddEdge W: 7.365440505554283e-17
grad ChooseDest W: 6.854600429534912
grad AddEdge W: 5.4258035468966544e-17
grad ChooseDest W: 5.702056884765625
grad AddEdge W: 2.7841183018763864e-18
grad ChooseDest W: 5.9351725578308105
grad AddEdge W: 1.850246279555044e-18
grad ChooseDest W: 9.18191146850586
grad AddEdge W: 1.389307431232959e-18
grad ChooseDest W: 7.652999401092529
grad AddEdge W: 5.212401205897569e-17
grad ChooseDest W: 4.724834442138672
grad AddEdge W: 4.641768079524352e-15
grad ChooseDest W: 8.734585762023926
grad AddEdge W: 6.606876840918244e-17
grad ChooseDest W: 5.680561065673828
grad AddEdge W: 4.0938417557974295e-18
grad ChooseDest W: 3.470395565032959
grad AddEdge W: 1.0796877308949197e-18
grad ChooseDest W: 7.18753719329834
grad AddEdge W: 6.733167985108273e-19
grad ChooseDest W: 3.8199918270111084
grad AddEdge W: 5.901225397166354e-19
grad ChooseDest W: 8.116908073425293
grad AddEdge W: 2.570244343851054e-18
grad ChooseDest W: 6.183919906616211
grad AddEdge W: 1.4500730247938751e-16
grad ChooseDest W: 4.616563320159912
grad AddEdge W: 1.626496612196441e-19
grad ChooseDest W: 5.689513206481934
grad AddEdge W: 6.280100484097666e-19
grad ChooseDest W: 5.250702381134033
grad AddEdge W: 1.9878779069885943e-15
grad ChooseDest W: 3.4435784816741943
grad AddEdge W: 2.1489583627456529e-19
grad ChooseDest W: 4.959888935089111
grad AddEdge W: 1.5657777295247156e-18
grad ChooseDest W: 3.4293880462646484
grad AddEdge W: 2.9470300759526332e-18
grad ChooseDest W: 5.088665008544922
grad AddEdge W: 8.216965084893777e-19
grad ChooseDest W: 8.773420333862305
grad AddEdge W: 6.658280491160249e-17
grad ChooseDest W: 4.351096153259277
grad AddEdge W: 2.9476767243964966e-19
grad ChooseDest W: 6.200699806213379
grad AddEdge W: 9.988869709019383e-19
grad ChooseDest W: 6.3810715675354
grad AddEdge W: 1.4601854876941191e-19
grad ChooseDest W: 3.331866502761841
=== Epoch 26: Train Loss: 4.5514, Train Log Prob: 0.0282 ===
Total mismatches: 74219
Predicted valid destination but wrong order: 45636
Epoch 26: Validation Loss: 4.9784, Validation Log Prob: 0.0103
Epoch 26: Edge Precision: 0.3782, Recall: 0.3762, F1: 0.3771, Jaccard: 0.2502
Epoch 26: TP: 2.635361488904796, FP: 4.352469577666428, FN: 4.386113099498926
Epoch 26: Current Learning Rate: 6e-05
[Epoch 26] ‚è±Ô∏è Total: 3325.34s | Current time: 2025-07-15 11:42:42 | üèãÔ∏è Train: 2845.16s | ‚úÖ Val: 480.18s
grad AddEdge W: 4.6283795988273643e-17
grad ChooseDest W: 10.700751304626465
grad AddEdge W: 2.1693218950654833e-18
grad ChooseDest W: 4.5769195556640625
grad AddEdge W: 4.2746954392514695e-19
grad ChooseDest W: 5.722718238830566
grad AddEdge W: 2.3196791353947066e-18
grad ChooseDest W: 5.867377281188965
grad AddEdge W: 8.01610714984909e-20
grad ChooseDest W: 6.393040657043457
grad AddEdge W: 4.3886216953255633e-16
grad ChooseDest W: 3.990873098373413
grad AddEdge W: 2.311613297241702e-18
grad ChooseDest W: 5.713184833526611
grad AddEdge W: 2.4693673934035276e-17
grad ChooseDest W: 9.019777297973633
grad AddEdge W: 5.475211355077176e-19
grad ChooseDest W: 5.0003790855407715
grad AddEdge W: 3.1024037531066843e-17
grad ChooseDest W: 3.681419849395752
grad AddEdge W: 6.368804748048438e-19
grad ChooseDest W: 11.932918548583984
grad AddEdge W: 1.452442467114921e-16
grad ChooseDest W: 6.486644268035889
grad AddEdge W: 5.313701052433263e-17
grad ChooseDest W: 6.079156875610352
grad AddEdge W: 1.322014427245392e-18
grad ChooseDest W: 4.865005970001221
grad AddEdge W: 2.127742834394893e-18
grad ChooseDest W: 6.521313667297363
grad AddEdge W: 7.684512522829057e-17
grad ChooseDest W: 6.619213104248047
grad AddEdge W: 1.812984158696082e-16
grad ChooseDest W: 6.919172286987305
grad AddEdge W: 9.16794878575185e-19
grad ChooseDest W: 4.872585296630859
grad AddEdge W: 3.851131144245156e-19
grad ChooseDest W: 7.84045934677124
grad AddEdge W: 9.973671299239487e-19
grad ChooseDest W: 4.8218889236450195
grad AddEdge W: 1.5055653915806025e-18
grad ChooseDest W: 3.15777587890625
grad AddEdge W: 1.1355149776167361e-18
grad ChooseDest W: 6.2748637199401855
grad AddEdge W: 1.188285068192133e-18
grad ChooseDest W: 5.7263407707214355
grad AddEdge W: 1.2217600336063884e-18
grad ChooseDest W: 6.531944274902344
grad AddEdge W: 1.5462332094088893e-18
grad ChooseDest W: 10.769424438476562
grad AddEdge W: 5.647111377087001e-19
grad ChooseDest W: 7.397030830383301
grad AddEdge W: 4.54047089651023e-19
grad ChooseDest W: 6.623382568359375
grad AddEdge W: 1.1727748630200927e-19
grad ChooseDest W: 7.877440929412842
grad AddEdge W: 7.553637382739457e-15
grad ChooseDest W: 3.7183682918548584
grad AddEdge W: 1.0864546183807108e-16
grad ChooseDest W: 7.636317253112793
grad AddEdge W: 2.2525474833253756e-19
grad ChooseDest W: 8.305608749389648
grad AddEdge W: 5.425021654422639e-19
grad ChooseDest W: 8.571784973144531
grad AddEdge W: 1.695556267177268e-18
grad ChooseDest W: 5.304300785064697
grad AddEdge W: 8.387476994438161e-19
grad ChooseDest W: 5.228348255157471
grad AddEdge W: 1.1155450385118737e-16
grad ChooseDest W: 7.655599594116211
grad AddEdge W: 1.82234940660154e-18
grad ChooseDest W: 8.106760025024414
grad AddEdge W: 7.008203698305292e-17
grad ChooseDest W: 5.650012969970703
grad AddEdge W: 7.034793708609023e-19
grad ChooseDest W: 5.097463130950928
grad AddEdge W: 1.6067058809441075e-15
grad ChooseDest W: 4.022190093994141
grad AddEdge W: 2.425279527022288e-16
grad ChooseDest W: 5.140135765075684
grad AddEdge W: 2.8126229292413515e-16
grad ChooseDest W: 7.356225490570068
grad AddEdge W: 3.0022944262676073e-19
grad ChooseDest W: 4.48298978805542
grad AddEdge W: 5.543425321279245e-18
grad ChooseDest W: 7.274936199188232
grad AddEdge W: 4.822474344917574e-19
grad ChooseDest W: 7.659778594970703
grad AddEdge W: 2.5837980373317863e-16
grad ChooseDest W: 3.7956061363220215
grad AddEdge W: 2.953166722122011e-18
grad ChooseDest W: 6.294078826904297
grad AddEdge W: 1.2975282672844472e-14
grad ChooseDest W: 6.28373908996582
grad AddEdge W: 2.172260299095213e-19
grad ChooseDest W: 4.848842144012451
grad AddEdge W: 1.176088174254753e-16
grad ChooseDest W: 5.094759941101074
grad AddEdge W: 6.196150957705425e-19
grad ChooseDest W: 7.9252214431762695
grad AddEdge W: 8.08537247977371e-15
grad ChooseDest W: 2.6841886043548584
grad AddEdge W: 1.2495044985320295e-18
grad ChooseDest W: 6.637866020202637
grad AddEdge W: 5.509607861732362e-17
grad ChooseDest W: 4.645460605621338
grad AddEdge W: 1.360221225587017e-16
grad ChooseDest W: 5.884983062744141
grad AddEdge W: 1.0227863901477032e-18
grad ChooseDest W: 7.254920482635498
grad AddEdge W: 2.4881452535763183e-16
grad ChooseDest W: 5.84193229675293
grad AddEdge W: 7.789298232543142e-19
grad ChooseDest W: 6.017983913421631
grad AddEdge W: 4.001937192095235e-17
grad ChooseDest W: 5.222196102142334
grad AddEdge W: 5.257000075509922e-19
grad ChooseDest W: 5.506629943847656
grad AddEdge W: 1.8501909411720643e-17
grad ChooseDest W: 6.031820774078369
grad AddEdge W: 3.454390676618373e-16
grad ChooseDest W: 5.244049072265625
grad AddEdge W: 3.400932574432582e-17
grad ChooseDest W: 3.5099565982818604
grad AddEdge W: 7.962626526637146e-17
grad ChooseDest W: 9.6211576461792
grad AddEdge W: 4.431706158536551e-17
grad ChooseDest W: 5.334387302398682
grad AddEdge W: 1.7003557758864539e-18
grad ChooseDest W: 6.639882564544678
grad AddEdge W: 1.0264524933966295e-19
grad ChooseDest W: 6.309967517852783
=== Epoch 27: Train Loss: 4.5034, Train Log Prob: 0.0296 ===
Total mismatches: 73410
Predicted valid destination but wrong order: 45970
Epoch 27: Validation Loss: 5.0627, Validation Log Prob: 0.0096
Epoch 27: Edge Precision: 0.3774, Recall: 0.3750, F1: 0.3761, Jaccard: 0.2495
Epoch 27: TP: 2.626485325697924, FP: 4.353185397279885, FN: 4.394989262705798
Epoch 27: Current Learning Rate: 6e-05
[Epoch 27] ‚è±Ô∏è Total: 3335.69s | Current time: 2025-07-15 12:38:18 | üèãÔ∏è Train: 2852.26s | ‚úÖ Val: 483.43s
grad AddEdge W: 8.86696323800667e-17
grad ChooseDest W: 12.690475463867188
grad AddEdge W: 2.696370775701608e-18
grad ChooseDest W: 7.478646755218506
grad AddEdge W: 6.808592277914813e-18
grad ChooseDest W: 4.600288391113281
grad AddEdge W: 2.1393810588183608e-17
grad ChooseDest W: 6.155816555023193
grad AddEdge W: 9.218283387770192e-17
grad ChooseDest W: 5.386889934539795
grad AddEdge W: 1.0615672022036034e-18
grad ChooseDest W: 8.239742279052734
grad AddEdge W: 6.364017440253288e-20
grad ChooseDest W: 5.764952659606934
grad AddEdge W: 1.972933360593052e-16
grad ChooseDest W: 2.588205575942993
grad AddEdge W: 4.2714769725458445e-17
grad ChooseDest W: 7.387593746185303
grad AddEdge W: 1.6283946360145867e-16
grad ChooseDest W: 6.876335620880127
grad AddEdge W: 5.613981908470997e-16
grad ChooseDest W: 6.355790615081787
grad AddEdge W: 1.4627021536885388e-16
grad ChooseDest W: 4.89401388168335
grad AddEdge W: 3.1379254032387735e-16
grad ChooseDest W: 4.776788234710693
grad AddEdge W: 6.219548691909676e-18
grad ChooseDest W: 3.6021244525909424
grad AddEdge W: 1.1609627755716237e-18
grad ChooseDest W: 3.5779786109924316
grad AddEdge W: 1.1563293725314061e-12
grad ChooseDest W: 2.0448503494262695
grad AddEdge W: 1.6043887627598893e-18
grad ChooseDest W: 4.475246906280518
grad AddEdge W: 3.662110551507015e-17
grad ChooseDest W: 3.484743595123291
grad AddEdge W: 1.0529219240315054e-18
grad ChooseDest W: 4.198157787322998
grad AddEdge W: 2.0463207583643075e-18
grad ChooseDest W: 7.336947441101074
grad AddEdge W: 5.074155937008138e-19
grad ChooseDest W: 6.259278774261475
grad AddEdge W: 2.863282174834702e-18
grad ChooseDest W: 5.213781356811523
grad AddEdge W: 3.744850946638857e-18
grad ChooseDest W: 6.159236907958984
grad AddEdge W: 2.701607035774222e-18
grad ChooseDest W: 4.892605304718018
grad AddEdge W: 4.41607844201874e-19
grad ChooseDest W: 5.455559253692627
grad AddEdge W: 1.0394537527543048e-16
grad ChooseDest W: 7.655812740325928
grad AddEdge W: 1.2405254788321605e-19
grad ChooseDest W: 6.001045227050781
grad AddEdge W: 3.8213027838818254e-17
grad ChooseDest W: 4.823198318481445
grad AddEdge W: 1.0976660879184535e-18
grad ChooseDest W: 8.921940803527832
grad AddEdge W: 8.891467636472941e-17
grad ChooseDest W: 5.677526950836182
grad AddEdge W: 1.305080582448756e-13
grad ChooseDest W: 3.072903633117676
grad AddEdge W: 6.651103396536887e-19
grad ChooseDest W: 5.446719646453857
grad AddEdge W: 6.179050425427447e-17
grad ChooseDest W: 4.580638408660889
grad AddEdge W: 1.1326103328957562e-18
grad ChooseDest W: 6.334169864654541
grad AddEdge W: 4.42039322288897e-19
grad ChooseDest W: 5.168217182159424
grad AddEdge W: 5.832918890133256e-19
grad ChooseDest W: 6.547910690307617
grad AddEdge W: 2.5047568269865063e-17
grad ChooseDest W: 3.386233329772949
grad AddEdge W: 6.866409721190432e-17
grad ChooseDest W: 3.5471484661102295
grad AddEdge W: 1.4494042740840444e-18
grad ChooseDest W: 4.921697616577148
grad AddEdge W: 6.287454760808237e-17
grad ChooseDest W: 4.153499603271484
grad AddEdge W: 8.081298675641072e-19
grad ChooseDest W: 8.805261611938477
grad AddEdge W: 1.2286604652661732e-16
grad ChooseDest W: 7.807326316833496
grad AddEdge W: 3.716485963904903e-16
grad ChooseDest W: 5.649152755737305
grad AddEdge W: 1.2160305670935399e-18
grad ChooseDest W: 5.846418380737305
grad AddEdge W: 1.2266124818747776e-18
grad ChooseDest W: 6.712452411651611
grad AddEdge W: 5.321311300184389e-19
grad ChooseDest W: 4.489996910095215
grad AddEdge W: 4.1816440826877894e-18
grad ChooseDest W: 5.4592790603637695
grad AddEdge W: 2.5366362023581774e-18
grad ChooseDest W: 6.801168441772461
grad AddEdge W: 2.0963493697304537e-15
grad ChooseDest W: 6.952113628387451
grad AddEdge W: 7.829037023145955e-19
grad ChooseDest W: 9.46643352508545
grad AddEdge W: 5.535344800670367e-19
grad ChooseDest W: 5.388835906982422
grad AddEdge W: 7.38178718678101e-19
grad ChooseDest W: 3.8156027793884277
grad AddEdge W: 9.85343439335031e-19
grad ChooseDest W: 5.640262603759766
grad AddEdge W: 2.958001510084322e-17
grad ChooseDest W: 4.8843865394592285
grad AddEdge W: 6.018526845856733e-19
grad ChooseDest W: 5.376653671264648
grad AddEdge W: 1.1285486692929676e-18
grad ChooseDest W: 4.939975261688232
grad AddEdge W: 1.0335461280811246e-17
grad ChooseDest W: 4.027275085449219
grad AddEdge W: 1.7983063681219207e-18
grad ChooseDest W: 4.227674961090088
grad AddEdge W: 2.8379216444344387e-18
grad ChooseDest W: 7.474393367767334
grad AddEdge W: 1.124863459722855e-16
grad ChooseDest W: 7.914926052093506
grad AddEdge W: 2.542066047509547e-16
grad ChooseDest W: 8.918509483337402
grad AddEdge W: 4.473692605658824e-18
grad ChooseDest W: 5.964324474334717
grad AddEdge W: 4.416010240977235e-17
grad ChooseDest W: 3.8278138637542725
grad AddEdge W: 8.95822437926939e-19
grad ChooseDest W: 5.102417469024658
grad AddEdge W: 3.2459513531201567e-17
grad ChooseDest W: 3.251065254211426
grad AddEdge W: 8.694049402758585e-17
grad ChooseDest W: 4.497756004333496
=== Epoch 28: Train Loss: 4.4602, Train Log Prob: 0.0306 ===
Total mismatches: 72558
Predicted valid destination but wrong order: 46065
Epoch 28: Validation Loss: 4.9516, Validation Log Prob: 0.0104
Epoch 28: Edge Precision: 0.3782, Recall: 0.3756, F1: 0.3768, Jaccard: 0.2503
Epoch 28: TP: 2.62992125984252, FP: 4.347745168217609, FN: 4.391553328561202
Epoch 28: Current Learning Rate: 6e-05
[Epoch 28] ‚è±Ô∏è Total: 3336.97s | Current time: 2025-07-15 13:33:55 | üèãÔ∏è Train: 2856.01s | ‚úÖ Val: 480.96s
grad AddEdge W: 1.8379325877151715e-16
grad ChooseDest W: 10.169563293457031
grad AddEdge W: 8.333670446555235e-19
grad ChooseDest W: 4.973940849304199
grad AddEdge W: 7.345958086022944e-17
grad ChooseDest W: 5.089266777038574
grad AddEdge W: 3.9992280101530016e-18
grad ChooseDest W: 7.638141632080078
grad AddEdge W: 2.9989859623113366e-19
grad ChooseDest W: 6.616426467895508
grad AddEdge W: 7.649008607449301e-18
grad ChooseDest W: 5.189107418060303
grad AddEdge W: 1.864449384262886e-18
grad ChooseDest W: 8.208641052246094
grad AddEdge W: 3.38998136486687e-17
grad ChooseDest W: 8.349896430969238
grad AddEdge W: 5.562744461975111e-15
grad ChooseDest W: 5.8774285316467285
grad AddEdge W: 4.842307551067181e-19
grad ChooseDest W: 7.951145172119141
grad AddEdge W: 7.960375209843476e-19
grad ChooseDest W: 6.49714994430542
grad AddEdge W: 4.3434638542783744e-17
grad ChooseDest W: 5.200404167175293
grad AddEdge W: 1.2145019125061235e-16
grad ChooseDest W: 4.28040075302124
grad AddEdge W: 1.644167712981442e-16
grad ChooseDest W: 5.181674003601074
grad AddEdge W: 2.88246933224639e-17
grad ChooseDest W: 4.143801689147949
grad AddEdge W: 9.444781321354972e-19
grad ChooseDest W: 5.929296493530273
grad AddEdge W: 2.2001773449159057e-16
grad ChooseDest W: 5.793563365936279
grad AddEdge W: 1.024129834860066e-18
grad ChooseDest W: 3.349518299102783
grad AddEdge W: 4.872043143124814e-19
grad ChooseDest W: 4.541926383972168
grad AddEdge W: 1.0866494897253191e-18
grad ChooseDest W: 7.058390140533447
grad AddEdge W: 1.8934214513920047e-16
grad ChooseDest W: 4.3680596351623535
grad AddEdge W: 3.2214115513236685e-17
grad ChooseDest W: 4.0682268142700195
grad AddEdge W: 1.0064394400872771e-18
grad ChooseDest W: 5.3068084716796875
grad AddEdge W: 7.583881699972796e-17
grad ChooseDest W: 7.968301773071289
grad AddEdge W: 7.977482855874721e-19
grad ChooseDest W: 9.543539047241211
grad AddEdge W: 1.6988485494128057e-19
grad ChooseDest W: 4.8563361167907715
grad AddEdge W: 2.6985166890057238e-18
grad ChooseDest W: 4.729534149169922
grad AddEdge W: 2.4374315381396313e-16
grad ChooseDest W: 7.10262393951416
grad AddEdge W: 2.4474036801887133e-18
grad ChooseDest W: 6.007746696472168
grad AddEdge W: 4.897584735089359e-15
grad ChooseDest W: 8.864513397216797
grad AddEdge W: 2.2734314673468668e-18
grad ChooseDest W: 4.821344375610352
grad AddEdge W: 5.489632732069156e-19
grad ChooseDest W: 5.162074089050293
grad AddEdge W: 4.322087976965871e-19
grad ChooseDest W: 7.575777053833008
grad AddEdge W: 3.178008009395243e-19
grad ChooseDest W: 5.618451118469238
grad AddEdge W: 3.430084060345286e-15
grad ChooseDest W: 3.621703863143921
grad AddEdge W: 8.6289145543482e-17
grad ChooseDest W: 4.0384979248046875
grad AddEdge W: 8.116841977225695e-16
grad ChooseDest W: 8.010246276855469
grad AddEdge W: 6.16459064657553e-17
grad ChooseDest W: 4.439743518829346
grad AddEdge W: 7.927261101971447e-19
grad ChooseDest W: 6.325277805328369
grad AddEdge W: 1.5547762273777604e-18
grad ChooseDest W: 7.908757209777832
grad AddEdge W: 7.719453399526906e-15
grad ChooseDest W: 4.677167892456055
grad AddEdge W: 1.2340762359394202e-18
grad ChooseDest W: 5.960953712463379
grad AddEdge W: 8.017494616079677e-19
grad ChooseDest W: 4.140313625335693
grad AddEdge W: 5.553359243448124e-19
grad ChooseDest W: 6.789046287536621
grad AddEdge W: 1.3809792237973618e-16
grad ChooseDest W: 6.673808574676514
grad AddEdge W: 3.8206178783346315e-17
grad ChooseDest W: 9.346587181091309
grad AddEdge W: 9.170590138883855e-17
grad ChooseDest W: 6.6263427734375
grad AddEdge W: 3.4585149329781134e-17
grad ChooseDest W: 4.201277732849121
grad AddEdge W: 5.940303601331982e-17
grad ChooseDest W: 8.478536605834961
grad AddEdge W: 1.6073981492259338e-18
grad ChooseDest W: 3.9236462116241455
grad AddEdge W: 2.355710864383575e-19
grad ChooseDest W: 4.537895202636719
grad AddEdge W: 1.530071341010522e-18
grad ChooseDest W: 6.5469255447387695
grad AddEdge W: 4.395045513788201e-18
grad ChooseDest W: 7.0719709396362305
grad AddEdge W: 4.8855467425476504e-17
grad ChooseDest W: 4.41958475112915
grad AddEdge W: 8.793935101319059e-17
grad ChooseDest W: 6.568432807922363
grad AddEdge W: 1.9946909900300547e-16
grad ChooseDest W: 5.89055061340332
grad AddEdge W: 2.1555974384190954e-17
grad ChooseDest W: 6.869259357452393
grad AddEdge W: 2.3145314870452125e-19
grad ChooseDest W: 8.266182899475098
grad AddEdge W: 3.527908768926412e-17
grad ChooseDest W: 6.0815653800964355
grad AddEdge W: 7.417627888747167e-19
grad ChooseDest W: 5.513801097869873
grad AddEdge W: 5.54506909459251e-17
grad ChooseDest W: 5.082900047302246
grad AddEdge W: 1.073022313363703e-12
grad ChooseDest W: 5.266970634460449
grad AddEdge W: 6.198335645100754e-17
grad ChooseDest W: 6.705549240112305
grad AddEdge W: 4.5545514130512915e-17
grad ChooseDest W: 5.7681989669799805
grad AddEdge W: 1.8382654451936628e-16
grad ChooseDest W: 2.4617462158203125
grad AddEdge W: 3.84401015314684e-19
grad ChooseDest W: 3.2672080993652344
=== Epoch 29: Train Loss: 4.4122, Train Log Prob: 0.0320 ===
Total mismatches: 71502
Predicted valid destination but wrong order: 46361
Epoch 29: Validation Loss: 4.8405, Validation Log Prob: 0.0117
Epoch 29: Edge Precision: 0.3755, Recall: 0.3727, F1: 0.3740, Jaccard: 0.2479
Epoch 29: TP: 2.6103078024337867, FP: 4.362061560486757, FN: 4.411166785969936
Epoch 29: Current Learning Rate: 6e-05
[Epoch 29] ‚è±Ô∏è Total: 3314.69s | Current time: 2025-07-15 14:29:09 | üèãÔ∏è Train: 2830.73s | ‚úÖ Val: 483.96s
grad AddEdge W: 1.885833306734027e-14
grad ChooseDest W: 10.947633743286133
grad AddEdge W: 9.24838072658921e-19
grad ChooseDest W: 5.520870685577393
grad AddEdge W: 8.839489840167763e-19
grad ChooseDest W: 6.776988983154297
grad AddEdge W: 3.020477717287901e-18
grad ChooseDest W: 6.864467620849609
grad AddEdge W: 5.240914783356762e-17
grad ChooseDest W: 6.7615461349487305
grad AddEdge W: 2.0705712123825246e-18
grad ChooseDest W: 6.727528095245361
grad AddEdge W: 5.262212368024287e-17
grad ChooseDest W: 5.3302836418151855
grad AddEdge W: 4.310537020097027e-18
grad ChooseDest W: 9.139272689819336
grad AddEdge W: 2.1976036882452327e-15
grad ChooseDest W: 5.032537460327148
grad AddEdge W: 9.18102754521208e-19
grad ChooseDest W: 6.57979679107666
grad AddEdge W: 5.797750272414799e-19
grad ChooseDest W: 4.578625679016113
grad AddEdge W: 1.8038811518602218e-18
grad ChooseDest W: 4.987392425537109
grad AddEdge W: 2.0912348458285893e-17
grad ChooseDest W: 5.02791166305542
grad AddEdge W: 4.3279894887743244e-16
grad ChooseDest W: 4.448436260223389
grad AddEdge W: 1.2319391445165373e-16
grad ChooseDest W: 4.594707012176514
grad AddEdge W: 5.768261283577283e-19
grad ChooseDest W: 4.532037258148193
grad AddEdge W: 4.865353940306251e-17
grad ChooseDest W: 5.758625507354736
grad AddEdge W: 4.444541354665414e-17
grad ChooseDest W: 9.236948013305664
grad AddEdge W: 1.1374225760504722e-16
grad ChooseDest W: 3.8814697265625
grad AddEdge W: 2.0682544862819167e-19
grad ChooseDest W: 2.948183059692383
grad AddEdge W: 4.415632715745666e-17
grad ChooseDest W: 5.100130558013916
grad AddEdge W: 5.608558515328405e-17
grad ChooseDest W: 5.990151882171631
grad AddEdge W: 7.23868752574875e-19
grad ChooseDest W: 4.51339864730835
grad AddEdge W: 3.529813352286815e-18
grad ChooseDest W: 5.0932698249816895
grad AddEdge W: 1.2576563935781139e-14
grad ChooseDest W: 3.252124309539795
grad AddEdge W: 3.637182967439362e-17
grad ChooseDest W: 5.397211074829102
grad AddEdge W: 5.426594331562255e-17
grad ChooseDest W: 5.22507381439209
grad AddEdge W: 9.878573446141562e-19
grad ChooseDest W: 5.454247951507568
grad AddEdge W: 1.9910042129526814e-17
grad ChooseDest W: 7.215184688568115
grad AddEdge W: 2.5501958432675142e-17
grad ChooseDest W: 6.0597333908081055
grad AddEdge W: 1.74409695432936e-16
grad ChooseDest W: 5.011632919311523
grad AddEdge W: 5.715738416607463e-19
grad ChooseDest W: 5.934698581695557
grad AddEdge W: 1.5821385638117234e-16
grad ChooseDest W: 3.0137813091278076
grad AddEdge W: 1.997987147606762e-18
grad ChooseDest W: 5.420217990875244
grad AddEdge W: 2.4336619768265536e-16
grad ChooseDest W: 9.042525291442871
grad AddEdge W: 1.4675621293005022e-18
grad ChooseDest W: 9.18559741973877
grad AddEdge W: 1.1136067889005395e-18
grad ChooseDest W: 6.292990684509277
grad AddEdge W: 5.047980840499663e-19
grad ChooseDest W: 8.432741165161133
grad AddEdge W: 3.4297927405684346e-19
grad ChooseDest W: 6.114622592926025
grad AddEdge W: 4.210366636988986e-19
grad ChooseDest W: 7.780890464782715
grad AddEdge W: 1.1709977171728108e-18
grad ChooseDest W: 5.674995422363281
grad AddEdge W: 3.308879416005149e-16
grad ChooseDest W: 11.116938591003418
grad AddEdge W: 6.780872214817242e-18
grad ChooseDest W: 5.340065002441406
grad AddEdge W: 1.8228690001033153e-17
grad ChooseDest W: 4.117315292358398
grad AddEdge W: 3.1021576151756615e-19
grad ChooseDest W: 5.438947677612305
grad AddEdge W: 3.005313215626547e-14
grad ChooseDest W: 5.971756935119629
grad AddEdge W: 1.4202262017592723e-18
grad ChooseDest W: 6.042553424835205
grad AddEdge W: 2.754057731235597e-18
grad ChooseDest W: 8.514403343200684
grad AddEdge W: 5.1227338348800857e-17
grad ChooseDest W: 5.656621932983398
grad AddEdge W: 1.5773293067790187e-18
grad ChooseDest W: 3.5459134578704834
grad AddEdge W: 6.066086670510266e-17
grad ChooseDest W: 3.1010384559631348
grad AddEdge W: 6.514542970478135e-17
grad ChooseDest W: 5.527712821960449
grad AddEdge W: 1.1963922653757653e-18
grad ChooseDest W: 3.7213573455810547
grad AddEdge W: 1.7456236380431617e-13
grad ChooseDest W: 1.7240978479385376
grad AddEdge W: 4.0610171445961817e-16
grad ChooseDest W: 3.51798152923584
grad AddEdge W: 1.1697486885099261e-14
grad ChooseDest W: 2.7675044536590576
grad AddEdge W: 3.7227397140336e-16
grad ChooseDest W: 6.262851715087891
grad AddEdge W: 1.738279293819601e-16
grad ChooseDest W: 7.70559549331665
grad AddEdge W: 4.5403113333700686e-17
grad ChooseDest W: 3.247570276260376
grad AddEdge W: 8.599483468153563e-17
grad ChooseDest W: 4.244630813598633
grad AddEdge W: 2.410177657491683e-17
grad ChooseDest W: 7.185039520263672
grad AddEdge W: 1.5264933299118932e-17
grad ChooseDest W: 6.003338813781738
grad AddEdge W: 1.3947138630370903e-16
grad ChooseDest W: 8.226859092712402
grad AddEdge W: 3.815867545512862e-17
grad ChooseDest W: 3.7750048637390137
grad AddEdge W: 2.8758325726786628e-18
grad ChooseDest W: 4.608254432678223
grad AddEdge W: 3.788188903484465e-19
grad ChooseDest W: 6.552164554595947
=== Epoch 30: Train Loss: 4.3683, Train Log Prob: 0.0333 ===
Total mismatches: 70808
Predicted valid destination but wrong order: 46434
Epoch 30: Validation Loss: 4.8249, Validation Log Prob: 0.0117
Epoch 30: Edge Precision: 0.3760, Recall: 0.3733, F1: 0.3746, Jaccard: 0.2484
Epoch 30: TP: 2.6141732283464565, FP: 4.359055118110236, FN: 4.407301360057265
Epoch 30: Current Learning Rate: 6e-05
[Epoch 30] ‚è±Ô∏è Total: 3280.74s | Current time: 2025-07-15 15:23:50 | üèãÔ∏è Train: 2798.80s | ‚úÖ Val: 481.94s
grad AddEdge W: 4.060757740756085e-16
grad ChooseDest W: 15.979106903076172
grad AddEdge W: 1.2320841782292393e-18
grad ChooseDest W: 5.526381015777588
grad AddEdge W: 5.688755991821136e-17
grad ChooseDest W: 6.014806270599365
grad AddEdge W: 2.624406527698589e-19
grad ChooseDest W: 5.658448219299316
grad AddEdge W: 7.176478373805934e-19
grad ChooseDest W: 8.650782585144043
grad AddEdge W: 7.733862138853486e-19
grad ChooseDest W: 5.077801704406738
grad AddEdge W: 4.851454410883216e-18
grad ChooseDest W: 4.77016544342041
grad AddEdge W: 1.3859064781444472e-18
grad ChooseDest W: 1.960604190826416
grad AddEdge W: 1.673279569498179e-18
grad ChooseDest W: 5.243673801422119
grad AddEdge W: 5.4795452644990715e-19
grad ChooseDest W: 4.758798599243164
grad AddEdge W: 5.914780302466684e-19
grad ChooseDest W: 3.179459810256958
grad AddEdge W: 8.370823780755937e-19
grad ChooseDest W: 4.421311855316162
grad AddEdge W: 3.2811729414170907e-19
grad ChooseDest W: 5.052597522735596
grad AddEdge W: 7.083237582542222e-17
grad ChooseDest W: 7.741854190826416
grad AddEdge W: 7.643283855865944e-17
grad ChooseDest W: 6.6166486740112305
grad AddEdge W: 1.9548600442691728e-19
grad ChooseDest W: 4.405178070068359
grad AddEdge W: 2.2687086796394953e-18
grad ChooseDest W: 4.156465530395508
grad AddEdge W: 8.82645037178663e-19
grad ChooseDest W: 3.1803910732269287
grad AddEdge W: 6.037596584980469e-17
grad ChooseDest W: 5.119865417480469
grad AddEdge W: 1.1241383201106666e-16
grad ChooseDest W: 3.54899001121521
grad AddEdge W: 2.5453066061413817e-19
grad ChooseDest W: 6.9082159996032715
grad AddEdge W: 2.7400245294255735e-17
grad ChooseDest W: 8.178130149841309
grad AddEdge W: 1.2113551851070738e-16
grad ChooseDest W: 8.159729957580566
grad AddEdge W: 4.342642898199931e-19
grad ChooseDest W: 6.1555585861206055
grad AddEdge W: 6.607341406229769e-19
grad ChooseDest W: 4.451849937438965
grad AddEdge W: 2.3526086000104016e-16
grad ChooseDest W: 4.130383014678955
grad AddEdge W: 4.999503445072141e-17
grad ChooseDest W: 5.90303373336792
grad AddEdge W: 9.620379357690792e-19
grad ChooseDest W: 5.29824161529541
grad AddEdge W: 5.462997888576224e-17
grad ChooseDest W: 7.132264137268066
grad AddEdge W: 2.3684238962757214e-17
grad ChooseDest W: 4.771319389343262
grad AddEdge W: 1.8290367896227557e-17
grad ChooseDest W: 5.939861297607422
grad AddEdge W: 7.245826714820542e-18
grad ChooseDest W: 8.010208129882812
grad AddEdge W: 6.278318716378711e-17
grad ChooseDest W: 6.704321384429932
grad AddEdge W: 2.993218186996369e-19
grad ChooseDest W: 5.934725284576416
grad AddEdge W: 1.9236622812318818e-18
grad ChooseDest W: 4.604039192199707
grad AddEdge W: 6.25231176087335e-16
grad ChooseDest W: 4.263264179229736
grad AddEdge W: 7.759973645852371e-19
grad ChooseDest W: 10.271915435791016
grad AddEdge W: 4.895116829324098e-19
grad ChooseDest W: 5.1809821128845215
grad AddEdge W: 5.006603198308229e-19
grad ChooseDest W: 5.087018013000488
grad AddEdge W: 9.065277473760543e-17
grad ChooseDest W: 6.612253665924072
grad AddEdge W: 1.8181722354980147e-16
grad ChooseDest W: 3.6310977935791016
grad AddEdge W: 7.088080228720352e-17
grad ChooseDest W: 7.716540813446045
grad AddEdge W: 9.949418363679432e-19
grad ChooseDest W: 7.562608242034912
grad AddEdge W: 8.590762316839779e-19
grad ChooseDest W: 2.8124613761901855
grad AddEdge W: 3.290380960899666e-18
grad ChooseDest W: 8.415252685546875
grad AddEdge W: 1.6247263422880009e-18
grad ChooseDest W: 6.274851322174072
grad AddEdge W: 1.2645799892729003e-17
grad ChooseDest W: 7.197020530700684
grad AddEdge W: 5.008432032604038e-17
grad ChooseDest W: 2.6571078300476074
grad AddEdge W: 9.84989716225588e-19
grad ChooseDest W: 3.957979440689087
grad AddEdge W: 6.040279297143101e-17
grad ChooseDest W: 6.212738990783691
grad AddEdge W: 4.784447404592439e-18
grad ChooseDest W: 8.505258560180664
grad AddEdge W: 1.7349687184848168e-16
grad ChooseDest W: 6.033599853515625
grad AddEdge W: 7.697772248727798e-19
grad ChooseDest W: 5.249618053436279
grad AddEdge W: 1.3188710623355322e-16
grad ChooseDest W: 4.397884368896484
grad AddEdge W: 6.656538490149243e-19
grad ChooseDest W: 3.305453062057495
grad AddEdge W: 1.120428857604994e-18
grad ChooseDest W: 5.611774921417236
grad AddEdge W: 9.306879540042085e-15
grad ChooseDest W: 4.6861701011657715
grad AddEdge W: 5.1229032414695365e-18
grad ChooseDest W: 5.001845359802246
grad AddEdge W: 3.1888635658628707e-18
grad ChooseDest W: 6.918403148651123
grad AddEdge W: 3.840785699721532e-19
grad ChooseDest W: 7.308774948120117
grad AddEdge W: 7.829208249532753e-18
grad ChooseDest W: 8.1072416305542
grad AddEdge W: 5.8421957207030385e-19
grad ChooseDest W: 5.290853977203369
grad AddEdge W: 4.44115612341367e-11
grad ChooseDest W: 2.114760160446167
grad AddEdge W: 5.628571653940758e-17
grad ChooseDest W: 5.306982040405273
grad AddEdge W: 1.6217509860323568e-16
grad ChooseDest W: 5.26625919342041
grad AddEdge W: 1.1110172503621055e-16
grad ChooseDest W: 5.95469856262207
=== Epoch 31: Train Loss: 4.3259, Train Log Prob: 0.0345 ===
Total mismatches: 70090
Predicted valid destination but wrong order: 46467
Epoch 31: Validation Loss: 4.7704, Validation Log Prob: 0.0123
Epoch 31: Edge Precision: 0.3757, Recall: 0.3735, F1: 0.3745, Jaccard: 0.2483
Epoch 31: TP: 2.6158911954187545, FP: 4.368790264853257, FN: 4.405583392984968
Epoch 31: Current Learning Rate: 6e-05
[Epoch 31] ‚è±Ô∏è Total: 3268.49s | Current time: 2025-07-15 16:18:19 | üèãÔ∏è Train: 2786.36s | ‚úÖ Val: 482.14s
grad AddEdge W: 2.176953951433459e-16
grad ChooseDest W: 9.82054328918457
grad AddEdge W: 1.9756213729154588e-19
grad ChooseDest W: 7.585085868835449
grad AddEdge W: 7.837855926528628e-17
grad ChooseDest W: 2.6297662258148193
grad AddEdge W: 2.902717535736496e-15
grad ChooseDest W: 5.395986557006836
grad AddEdge W: 9.638289885904096e-19
grad ChooseDest W: 4.1118268966674805
grad AddEdge W: 1.6399568053166335e-18
grad ChooseDest W: 4.629018306732178
grad AddEdge W: 1.1972548129225886e-15
grad ChooseDest W: 7.04341459274292
grad AddEdge W: 2.2777771434129754e-17
grad ChooseDest W: 3.837228298187256
grad AddEdge W: 5.62095431311588e-17
grad ChooseDest W: 6.485380172729492
grad AddEdge W: 5.836158336207167e-19
grad ChooseDest W: 4.4039740562438965
grad AddEdge W: 1.6197277932439195e-18
grad ChooseDest W: 4.9409661293029785
grad AddEdge W: 1.0431959178454947e-16
grad ChooseDest W: 3.941176652908325
grad AddEdge W: 2.0867428249685078e-16
grad ChooseDest W: 4.412381649017334
grad AddEdge W: 1.16747558212456e-18
grad ChooseDest W: 5.188139915466309
grad AddEdge W: 3.3699189262900088e-18
grad ChooseDest W: 7.311432361602783
grad AddEdge W: 2.2342129816211556e-16
grad ChooseDest W: 4.595632553100586
grad AddEdge W: 1.5849178617185801e-18
grad ChooseDest W: 6.156348705291748
grad AddEdge W: 5.0777113867555594e-17
grad ChooseDest W: 3.9055042266845703
grad AddEdge W: 5.81316219819031e-19
grad ChooseDest W: 5.218801498413086
grad AddEdge W: 5.058005297586586e-17
grad ChooseDest W: 6.993402481079102
grad AddEdge W: 1.451502934695668e-18
grad ChooseDest W: 6.10357666015625
grad AddEdge W: 3.211271526765414e-19
grad ChooseDest W: 3.3883588314056396
grad AddEdge W: 5.897546966341361e-17
grad ChooseDest W: 3.9827053546905518
grad AddEdge W: 4.246823019824824e-18
grad ChooseDest W: 4.840095520019531
grad AddEdge W: 1.4777951806583957e-18
grad ChooseDest W: 4.803534507751465
grad AddEdge W: 1.0830905079166822e-16
grad ChooseDest W: 3.498314619064331
grad AddEdge W: 1.582314414138146e-18
grad ChooseDest W: 5.326456546783447
grad AddEdge W: 5.988415787420076e-15
grad ChooseDest W: 4.855614185333252
grad AddEdge W: 1.6321644537536543e-18
grad ChooseDest W: 6.045372009277344
grad AddEdge W: 1.4544291895101509e-18
grad ChooseDest W: 6.276587963104248
grad AddEdge W: 4.684279347929871e-19
grad ChooseDest W: 4.591484546661377
grad AddEdge W: 2.407055671064755e-18
grad ChooseDest W: 4.8466691970825195
grad AddEdge W: 2.187158855903059e-19
grad ChooseDest W: 4.765749454498291
grad AddEdge W: 6.877496526338056e-19
grad ChooseDest W: 3.317215919494629
grad AddEdge W: 1.4293625398379155e-16
grad ChooseDest W: 7.427911758422852
grad AddEdge W: 1.4391756688219434e-18
grad ChooseDest W: 4.440944194793701
grad AddEdge W: 4.673265438073727e-19
grad ChooseDest W: 5.626773357391357
grad AddEdge W: 1.2009509735368308e-16
grad ChooseDest W: 3.3139004707336426
grad AddEdge W: 6.677369482912743e-19
grad ChooseDest W: 5.389160633087158
grad AddEdge W: 7.290324895872832e-18
grad ChooseDest W: 7.833258152008057
grad AddEdge W: 6.378626607923832e-17
grad ChooseDest W: 3.9065778255462646
grad AddEdge W: 1.6387494318150358e-18
grad ChooseDest W: 5.976560115814209
grad AddEdge W: 7.343153199283838e-19
grad ChooseDest W: 9.686893463134766
grad AddEdge W: 1.1861402054073953e-16
grad ChooseDest W: 4.011068820953369
grad AddEdge W: 3.872005977581121e-17
grad ChooseDest W: 3.0807156562805176
grad AddEdge W: 2.2189735354110707e-16
grad ChooseDest W: 8.690244674682617
grad AddEdge W: 4.538775176254496e-18
grad ChooseDest W: 3.5926735401153564
grad AddEdge W: 1.3359543851208653e-18
grad ChooseDest W: 5.343385219573975
grad AddEdge W: 1.8164853164439986e-17
grad ChooseDest W: 4.027377605438232
grad AddEdge W: 2.867142678452275e-19
grad ChooseDest W: 6.317428112030029
grad AddEdge W: 2.5104832329310884e-18
grad ChooseDest W: 4.347837924957275
grad AddEdge W: 3.4585393348061837e-19
grad ChooseDest W: 7.66087007522583
grad AddEdge W: 1.1060936728328428e-17
grad ChooseDest W: 6.338087558746338
grad AddEdge W: 1.0615558863728237e-16
grad ChooseDest W: 3.8492913246154785
grad AddEdge W: 4.366330766004035e-17
grad ChooseDest W: 8.20698070526123
grad AddEdge W: 4.515303926373304e-18
grad ChooseDest W: 9.18893814086914
grad AddEdge W: 3.0641336398794326e-15
grad ChooseDest W: 6.166811943054199
grad AddEdge W: 8.396924141496068e-17
grad ChooseDest W: 5.625852584838867
grad AddEdge W: 7.688224909418195e-17
grad ChooseDest W: 7.187861442565918
grad AddEdge W: 5.891472151922772e-17
grad ChooseDest W: 3.546781301498413
grad AddEdge W: 5.911718700224472e-19
grad ChooseDest W: 4.606475830078125
grad AddEdge W: 5.567419183871448e-17
grad ChooseDest W: 7.743590831756592
grad AddEdge W: 9.406987439167424e-19
grad ChooseDest W: 3.6170291900634766
grad AddEdge W: 8.02788348758546e-19
grad ChooseDest W: 7.504186153411865
grad AddEdge W: 3.67866579598544e-19
grad ChooseDest W: 5.928664684295654
grad AddEdge W: 2.6219608751597573e-17
grad ChooseDest W: 4.834881782531738
=== Epoch 32: Train Loss: 4.2821, Train Log Prob: 0.0360 ===
Total mismatches: 69175
Predicted valid destination but wrong order: 46880
Epoch 32: Validation Loss: 4.7355, Validation Log Prob: 0.0126
Epoch 32: Edge Precision: 0.3753, Recall: 0.3727, F1: 0.3739, Jaccard: 0.2478
Epoch 32: TP: 2.610021474588404, FP: 4.366785969935576, FN: 4.411453113815319
Epoch 32: Current Learning Rate: 6e-05
[Epoch 32] ‚è±Ô∏è Total: 3267.36s | Current time: 2025-07-15 17:12:46 | üèãÔ∏è Train: 2784.44s | ‚úÖ Val: 482.92s
grad AddEdge W: 1.2241477648907779e-16
grad ChooseDest W: 12.593196868896484
grad AddEdge W: 3.89822615543298e-19
grad ChooseDest W: 4.011329650878906
grad AddEdge W: 2.0967795036489812e-16
grad ChooseDest W: 3.875765323638916
grad AddEdge W: 4.704742596872545e-17
grad ChooseDest W: 7.255505084991455
grad AddEdge W: 6.015292052673768e-19
grad ChooseDest W: 9.374542236328125
grad AddEdge W: 3.2269459272890464e-18
grad ChooseDest W: 6.228054523468018
grad AddEdge W: 6.872263512949347e-17
grad ChooseDest W: 3.8797574043273926
grad AddEdge W: 2.376748509611557e-16
grad ChooseDest W: 6.761002063751221
grad AddEdge W: 4.532288529288432e-19
grad ChooseDest W: 1.7996071577072144
grad AddEdge W: 9.405313556479862e-17
grad ChooseDest W: 7.909748554229736
grad AddEdge W: 1.010650513188208e-18
grad ChooseDest W: 2.732356071472168
grad AddEdge W: 6.932467041419936e-17
grad ChooseDest W: 8.192700386047363
grad AddEdge W: 7.194249852958268e-17
grad ChooseDest W: 4.876176357269287
grad AddEdge W: 2.071526544050739e-12
grad ChooseDest W: 7.658326148986816
grad AddEdge W: 4.2437392905012264e-17
grad ChooseDest W: 5.418465614318848
grad AddEdge W: 6.067261742608943e-19
grad ChooseDest W: 4.211249828338623
grad AddEdge W: 7.247063246438732e-19
grad ChooseDest W: 7.85968017578125
grad AddEdge W: 1.1099437746542132e-19
grad ChooseDest W: 6.200940132141113
grad AddEdge W: 6.67828515944313e-15
grad ChooseDest W: 4.571101188659668
grad AddEdge W: 1.5492327731051597e-18
grad ChooseDest W: 6.1338677406311035
grad AddEdge W: 4.641460633034278e-17
grad ChooseDest W: 7.914980888366699
grad AddEdge W: 3.267669476411104e-19
grad ChooseDest W: 5.107670783996582
grad AddEdge W: 2.8334185145387306e-17
grad ChooseDest W: 4.644633769989014
grad AddEdge W: 6.399281183742189e-19
grad ChooseDest W: 5.432027816772461
grad AddEdge W: 4.304599600378183e-17
grad ChooseDest W: 6.861458778381348
grad AddEdge W: 2.945801919538145e-17
grad ChooseDest W: 7.755502223968506
grad AddEdge W: 1.4022711174622727e-16
grad ChooseDest W: 7.016857624053955
grad AddEdge W: 6.565663394078402e-17
grad ChooseDest W: 5.679560661315918
grad AddEdge W: 1.6967739804365227e-18
grad ChooseDest W: 6.23438835144043
grad AddEdge W: 1.03596737308434e-14
grad ChooseDest W: 2.923297166824341
grad AddEdge W: 2.0382878419996826e-17
grad ChooseDest W: 7.975255012512207
grad AddEdge W: 5.7771793245563705e-19
grad ChooseDest W: 6.964996337890625
grad AddEdge W: 3.7400925901651456e-18
grad ChooseDest W: 6.6049981117248535
grad AddEdge W: 1.3535925641174866e-18
grad ChooseDest W: 2.6577510833740234
grad AddEdge W: 3.3134069808161516e-18
grad ChooseDest W: 5.460916519165039
grad AddEdge W: 2.817397070389102e-19
grad ChooseDest W: 5.009325981140137
grad AddEdge W: 7.05868371867532e-19
grad ChooseDest W: 7.046107769012451
grad AddEdge W: 7.962975762291766e-18
grad ChooseDest W: 4.919919967651367
grad AddEdge W: 1.0009186298839451e-18
grad ChooseDest W: 4.0782976150512695
grad AddEdge W: 1.4776882841078355e-16
grad ChooseDest W: 5.5521626472473145
grad AddEdge W: 2.153281001831702e-18
grad ChooseDest W: 6.040497303009033
grad AddEdge W: 6.715432715787253e-17
grad ChooseDest W: 5.382708549499512
grad AddEdge W: 1.0693132316522796e-18
grad ChooseDest W: 5.96750545501709
grad AddEdge W: 2.5261351444818167e-17
grad ChooseDest W: 6.782232284545898
grad AddEdge W: 1.427322067246482e-18
grad ChooseDest W: 3.6460161209106445
grad AddEdge W: 5.407368172584685e-18
grad ChooseDest W: 5.531418323516846
grad AddEdge W: 2.7142818415823107e-17
grad ChooseDest W: 5.813188552856445
grad AddEdge W: 6.56907137820212e-17
grad ChooseDest W: 7.336101531982422
grad AddEdge W: 1.318165245662453e-16
grad ChooseDest W: 3.6458559036254883
grad AddEdge W: 4.316081529024909e-17
grad ChooseDest W: 6.9116644859313965
grad AddEdge W: 1.8612986533096714e-16
grad ChooseDest W: 4.2512593269348145
grad AddEdge W: 4.418087456931478e-19
grad ChooseDest W: 6.318211078643799
grad AddEdge W: 3.5197904875225715e-17
grad ChooseDest W: 9.63520336151123
grad AddEdge W: 5.778167805388371e-19
grad ChooseDest W: 6.797187805175781
grad AddEdge W: 1.0485571070060224e-16
grad ChooseDest W: 3.3060739040374756
grad AddEdge W: 2.4087819721877346e-16
grad ChooseDest W: 5.902063369750977
grad AddEdge W: 5.59537656508676e-17
grad ChooseDest W: 5.030622959136963
grad AddEdge W: 1.9604692707742049e-16
grad ChooseDest W: 6.138912200927734
grad AddEdge W: 2.473445062951169e-17
grad ChooseDest W: 1.860673189163208
grad AddEdge W: 1.2529913750066702e-18
grad ChooseDest W: 3.933116912841797
grad AddEdge W: 1.234541111443675e-18
grad ChooseDest W: 6.301877975463867
grad AddEdge W: 2.12903418740818e-17
grad ChooseDest W: 3.0801494121551514
grad AddEdge W: 1.8824069790530446e-16
grad ChooseDest W: 7.528293609619141
grad AddEdge W: 1.6686363979247658e-18
grad ChooseDest W: 7.434023857116699
grad AddEdge W: 2.1274065596264959e-19
grad ChooseDest W: 3.241795063018799
grad AddEdge W: 9.41294210560204e-19
grad ChooseDest W: 6.8468708992004395
=== Epoch 33: Train Loss: 4.2308, Train Log Prob: 0.0375 ===
Total mismatches: 68266
Predicted valid destination but wrong order: 46866
Epoch 33: Validation Loss: 4.6315, Validation Log Prob: 0.0141
Epoch 33: Edge Precision: 0.3749, Recall: 0.3723, F1: 0.3735, Jaccard: 0.2478
Epoch 33: TP: 2.606871868289191, FP: 4.372083035075161, FN: 4.414602720114531
Epoch 33: Current Learning Rate: 6e-05
[Epoch 33] ‚è±Ô∏è Total: 3293.28s | Current time: 2025-07-15 18:07:39 | üèãÔ∏è Train: 2810.66s | ‚úÖ Val: 482.62s
grad AddEdge W: 1.5545581122579393e-16
grad ChooseDest W: 18.716087341308594
grad AddEdge W: 8.99478886427153e-19
grad ChooseDest W: 7.183957576751709
grad AddEdge W: 7.968222051929307e-19
grad ChooseDest W: 7.516134262084961
grad AddEdge W: 1.2535266642605686e-18
grad ChooseDest W: 8.697615623474121
grad AddEdge W: 4.0310038948902847e-19
grad ChooseDest W: 3.549738883972168
grad AddEdge W: 7.962868642402441e-19
grad ChooseDest W: 4.026955604553223
grad AddEdge W: 6.837075249691341e-17
grad ChooseDest W: 4.831589698791504
grad AddEdge W: 4.000930678725881e-17
grad ChooseDest W: 3.5202391147613525
grad AddEdge W: 3.382874022248661e-19
grad ChooseDest W: 3.4275078773498535
grad AddEdge W: 7.637845598287745e-19
grad ChooseDest W: 7.533417701721191
grad AddEdge W: 4.775394739968659e-17
grad ChooseDest W: 3.103121280670166
grad AddEdge W: 4.0561074306139855e-18
grad ChooseDest W: 4.119424343109131
grad AddEdge W: 7.786781825042664e-17
grad ChooseDest W: 6.307814121246338
grad AddEdge W: 1.8814356704905603e-16
grad ChooseDest W: 6.548200607299805
grad AddEdge W: 4.807312000212385e-17
grad ChooseDest W: 2.470851421356201
grad AddEdge W: 4.6646374272969165e-19
grad ChooseDest W: 6.120307922363281
grad AddEdge W: 6.086937784442166e-19
grad ChooseDest W: 7.693700313568115
grad AddEdge W: 1.3090244102028194e-15
grad ChooseDest W: 4.093318462371826
grad AddEdge W: 1.2936694291950373e-18
grad ChooseDest W: 3.3321468830108643
grad AddEdge W: 2.4801172258491136e-18
grad ChooseDest W: 7.297321319580078
grad AddEdge W: 7.563581322892734e-19
grad ChooseDest W: 7.825125217437744
grad AddEdge W: 5.266906080333127e-19
grad ChooseDest W: 3.5739758014678955
grad AddEdge W: 1.9708238184204702e-17
grad ChooseDest W: 9.151531219482422
grad AddEdge W: 2.5028134819425987e-16
grad ChooseDest W: 4.9600443840026855
grad AddEdge W: 5.614697787796228e-19
grad ChooseDest W: 5.29686975479126
grad AddEdge W: 9.477623171000185e-16
grad ChooseDest W: 2.6803436279296875
grad AddEdge W: 6.147376769874118e-19
grad ChooseDest W: 4.7489495277404785
grad AddEdge W: 1.043248857404698e-16
grad ChooseDest W: 6.281109809875488
grad AddEdge W: 1.4473398082917631e-15
grad ChooseDest W: 5.653285026550293
grad AddEdge W: 1.4671263126511664e-16
grad ChooseDest W: 4.911532878875732
grad AddEdge W: 2.4202330201142426e-19
grad ChooseDest W: 3.611605644226074
grad AddEdge W: 2.7098426941069836e-17
grad ChooseDest W: 4.270700931549072
grad AddEdge W: 2.954158759830646e-17
grad ChooseDest W: 4.118619441986084
grad AddEdge W: 5.725229280160743e-18
grad ChooseDest W: 5.324317455291748
grad AddEdge W: 3.769952218782811e-15
grad ChooseDest W: 4.421648025512695
grad AddEdge W: 5.937493834227262e-17
grad ChooseDest W: 7.064126968383789
grad AddEdge W: 5.5091862477741435e-18
grad ChooseDest W: 5.006329536437988
grad AddEdge W: 9.561778858925216e-15
grad ChooseDest W: 1.6023845672607422
grad AddEdge W: 1.7976330431033025e-18
grad ChooseDest W: 3.529989242553711
grad AddEdge W: 7.905604479559043e-19
grad ChooseDest W: 8.106282234191895
grad AddEdge W: 3.836384933426627e-17
grad ChooseDest W: 8.380693435668945
grad AddEdge W: 6.698034522565849e-19
grad ChooseDest W: 5.589109897613525
grad AddEdge W: 7.725750081983755e-19
grad ChooseDest W: 5.017871379852295
grad AddEdge W: 2.2808554133445302e-18
grad ChooseDest W: 3.569669723510742
grad AddEdge W: 2.4550974859864627e-13
grad ChooseDest W: 1.9952665567398071
grad AddEdge W: 1.4526190205448644e-16
grad ChooseDest W: 5.209836006164551
grad AddEdge W: 1.0348605180744713e-16
grad ChooseDest W: 5.302932262420654
grad AddEdge W: 1.1890743018940852e-18
grad ChooseDest W: 6.353621482849121
grad AddEdge W: 1.4602449929994347e-18
grad ChooseDest W: 4.510165214538574
grad AddEdge W: 3.8420587823830393e-19
grad ChooseDest W: 4.142414093017578
grad AddEdge W: 1.2989693821748177e-18
grad ChooseDest W: 5.404422283172607
grad AddEdge W: 7.647821437834165e-17
grad ChooseDest W: 1.960254192352295
grad AddEdge W: 1.2123084611322044e-18
grad ChooseDest W: 7.142523765563965
grad AddEdge W: 5.749001416989752e-19
grad ChooseDest W: 3.5857388973236084
grad AddEdge W: 2.5573893367465203e-18
grad ChooseDest W: 4.159520626068115
grad AddEdge W: 9.658188129129365e-18
grad ChooseDest W: 5.916250705718994
grad AddEdge W: 1.1560866399937773e-14
grad ChooseDest W: 1.781103491783142
grad AddEdge W: 1.3267248072435752e-18
grad ChooseDest W: 5.529364585876465
grad AddEdge W: 3.3717975568586933e-19
grad ChooseDest W: 5.158291339874268
grad AddEdge W: 1.011971107036149e-18
grad ChooseDest W: 3.318981409072876
grad AddEdge W: 1.2059381447115865e-16
grad ChooseDest W: 7.829939842224121
grad AddEdge W: 6.976639292631365e-19
grad ChooseDest W: 4.888255596160889
grad AddEdge W: 2.480026856367192e-18
grad ChooseDest W: 6.672062873840332
grad AddEdge W: 1.3545502076562515e-16
grad ChooseDest W: 4.6653642654418945
grad AddEdge W: 1.2363323710601585e-18
grad ChooseDest W: 6.914878845214844
grad AddEdge W: 4.875526276607183e-17
grad ChooseDest W: 6.4482421875
=== Epoch 34: Train Loss: 4.1972, Train Log Prob: 0.0387 ===
Total mismatches: 67698
Predicted valid destination but wrong order: 46993
Epoch 34: Validation Loss: 4.5300, Validation Log Prob: 0.0153
Epoch 34: Edge Precision: 0.3763, Recall: 0.3736, F1: 0.3748, Jaccard: 0.2485
Epoch 34: TP: 2.615748031496063, FP: 4.3596277738010025, FN: 4.405726556907659
Epoch 34: Current Learning Rate: 6e-05
[Epoch 34] ‚è±Ô∏è Total: 3302.25s | Current time: 2025-07-15 19:02:42 | üèãÔ∏è Train: 2819.99s | ‚úÖ Val: 482.26s
grad AddEdge W: 7.79982679417487e-18
grad ChooseDest W: 16.816267013549805
grad AddEdge W: 1.4133537784350286e-18
grad ChooseDest W: 7.3397979736328125
grad AddEdge W: 7.418729589925511e-19
grad ChooseDest W: 2.9710776805877686
grad AddEdge W: 2.5835399280293483e-19
grad ChooseDest W: 4.302983283996582
grad AddEdge W: 6.484516148806337e-18
grad ChooseDest W: 4.882325172424316
grad AddEdge W: 1.8007738478891664e-17
grad ChooseDest W: 3.0645079612731934
grad AddEdge W: 7.465019650979744e-19
grad ChooseDest W: 5.085540294647217
grad AddEdge W: 5.805376050481925e-18
grad ChooseDest W: 5.040817737579346
grad AddEdge W: 6.350076345004472e-19
grad ChooseDest W: 5.0970458984375
grad AddEdge W: 1.204590372355695e-14
grad ChooseDest W: 2.5907704830169678
grad AddEdge W: 1.2904895293718824e-15
grad ChooseDest W: 4.5401811599731445
grad AddEdge W: 1.779118325626977e-16
grad ChooseDest W: 1.7915821075439453
grad AddEdge W: 5.802397476493698e-19
grad ChooseDest W: 5.555165767669678
grad AddEdge W: 3.7572169349050067e-19
grad ChooseDest W: 3.689540386199951
grad AddEdge W: 1.040130201206916e-14
grad ChooseDest W: 5.730259418487549
grad AddEdge W: 3.5075510268708694e-18
grad ChooseDest W: 3.991947889328003
grad AddEdge W: 1.9475158870794205e-16
grad ChooseDest W: 7.696194648742676
grad AddEdge W: 2.1879582329072724e-16
grad ChooseDest W: 3.6028122901916504
grad AddEdge W: 1.7966865830464193e-16
grad ChooseDest W: 9.958989143371582
grad AddEdge W: 6.925526003463881e-17
grad ChooseDest W: 4.705236434936523
grad AddEdge W: 9.51114395394857e-19
grad ChooseDest W: 5.332311630249023
grad AddEdge W: 4.449504305991834e-16
grad ChooseDest W: 7.078218460083008
grad AddEdge W: 6.994851204332464e-13
grad ChooseDest W: 4.95159387588501
grad AddEdge W: 3.909467798451517e-19
grad ChooseDest W: 5.468599319458008
grad AddEdge W: 1.8598049719185538e-18
grad ChooseDest W: 4.567101955413818
grad AddEdge W: 1.2727972345652266e-16
grad ChooseDest W: 7.561153888702393
grad AddEdge W: 8.911951895803174e-19
grad ChooseDest W: 5.836884021759033
grad AddEdge W: 6.107687093120175e-19
grad ChooseDest W: 4.4548492431640625
grad AddEdge W: 1.2999158837561677e-14
grad ChooseDest W: 7.9928765296936035
grad AddEdge W: 5.581742395204232e-19
grad ChooseDest W: 4.71664571762085
grad AddEdge W: 4.310704405057061e-15
grad ChooseDest W: 1.5885837078094482
grad AddEdge W: 7.852784861544469e-19
grad ChooseDest W: 5.270843029022217
grad AddEdge W: 5.209233765895981e-17
grad ChooseDest W: 4.196345329284668
grad AddEdge W: 9.943948502631954e-20
grad ChooseDest W: 5.2886857986450195
grad AddEdge W: 4.459922612719715e-17
grad ChooseDest W: 4.53116512298584
grad AddEdge W: 1.868578504444628e-17
grad ChooseDest W: 5.293766975402832
grad AddEdge W: 4.569190607378071e-18
grad ChooseDest W: 6.982100009918213
grad AddEdge W: 6.33720231274585e-18
grad ChooseDest W: 3.022292375564575
grad AddEdge W: 2.6846428538804664e-19
grad ChooseDest W: 6.722323894500732
grad AddEdge W: 2.668775294276094e-16
grad ChooseDest W: 4.507887840270996
grad AddEdge W: 5.441860466754804e-19
grad ChooseDest W: 6.394296646118164
grad AddEdge W: 1.0746982808375764e-18
grad ChooseDest W: 3.040095090866089
grad AddEdge W: 1.7944320195688448e-17
grad ChooseDest W: 10.663557052612305
grad AddEdge W: 3.0530742685904134e-19
grad ChooseDest W: 8.346541404724121
grad AddEdge W: 3.298772129187588e-17
grad ChooseDest W: 6.480304718017578
grad AddEdge W: 7.863535624568245e-19
grad ChooseDest W: 2.7181904315948486
grad AddEdge W: 4.112374685722892e-19
grad ChooseDest W: 3.575291395187378
grad AddEdge W: 7.487326773395738e-20
grad ChooseDest W: 7.895703315734863
grad AddEdge W: 8.245903481636062e-19
grad ChooseDest W: 8.420010566711426
grad AddEdge W: 2.5148871425123207e-16
grad ChooseDest W: 3.36175799369812
grad AddEdge W: 5.037691726975754e-17
grad ChooseDest W: 4.521949768066406
grad AddEdge W: 1.1518841581561245e-18
grad ChooseDest W: 6.0629425048828125
grad AddEdge W: 1.4680810558879085e-19
grad ChooseDest W: 6.632876396179199
grad AddEdge W: 2.842486134587986e-16
grad ChooseDest W: 7.886839866638184
grad AddEdge W: 2.0193217124175474e-19
grad ChooseDest W: 5.048468112945557
grad AddEdge W: 4.1030526187133606e-19
grad ChooseDest W: 4.242093086242676
grad AddEdge W: 8.71697653323284e-18
grad ChooseDest W: 5.917993068695068
grad AddEdge W: 1.2754447419209883e-16
grad ChooseDest W: 9.424731254577637
grad AddEdge W: 1.0588757550137028e-16
grad ChooseDest W: 2.9245223999023438
grad AddEdge W: 4.109717884992948e-19
grad ChooseDest W: 7.278125762939453
grad AddEdge W: 4.758313968539207e-13
grad ChooseDest W: 3.8653740882873535
grad AddEdge W: 3.017225656709649e-18
grad ChooseDest W: 3.753894567489624
grad AddEdge W: 8.789807304626297e-19
grad ChooseDest W: 7.309521198272705
grad AddEdge W: 1.5690558979910516e-19
grad ChooseDest W: 5.647103309631348
grad AddEdge W: 1.654141892996758e-19
grad ChooseDest W: 7.762310981750488
grad AddEdge W: 9.067013229557925e-17
grad ChooseDest W: 7.215550422668457
=== Epoch 35: Train Loss: 4.1466, Train Log Prob: 0.0404 ===
Total mismatches: 66778
Predicted valid destination but wrong order: 47274
Epoch 35: Validation Loss: 4.4638, Validation Log Prob: 0.0163
Epoch 35: Edge Precision: 0.3744, Recall: 0.3715, F1: 0.3728, Jaccard: 0.2470
Epoch 35: TP: 2.6001431639226915, FP: 4.371653543307087, FN: 4.421331424481031
Epoch 35: Current Learning Rate: 6e-05
[Epoch 35] ‚è±Ô∏è Total: 3292.02s | Current time: 2025-07-15 19:57:34 | üèãÔ∏è Train: 2810.98s | ‚úÖ Val: 481.04s
grad AddEdge W: 2.623827449571014e-18
grad ChooseDest W: 13.037047386169434
grad AddEdge W: 6.146797226457448e-19
grad ChooseDest W: 4.2539167404174805
grad AddEdge W: 6.150788889900899e-19
grad ChooseDest W: 4.029824256896973
grad AddEdge W: 2.952141272934074e-15
grad ChooseDest W: 5.449707508087158
grad AddEdge W: 1.2721690834018723e-13
grad ChooseDest W: 9.567743301391602
grad AddEdge W: 7.467568401242173e-19
grad ChooseDest W: 4.911616325378418
grad AddEdge W: 1.7144616868723207e-18
grad ChooseDest W: 3.843026638031006
grad AddEdge W: 4.5776079145733495e-17
grad ChooseDest W: 3.01692795753479
grad AddEdge W: 6.390464472388139e-19
grad ChooseDest W: 5.300456523895264
grad AddEdge W: 3.113190717689968e-16
grad ChooseDest W: 2.199751853942871
grad AddEdge W: 1.2160743332462198e-12
grad ChooseDest W: 2.475027084350586
grad AddEdge W: 4.9330606586771864e-17
grad ChooseDest W: 4.578664302825928
grad AddEdge W: 7.099270162610847e-18
grad ChooseDest W: 7.534785270690918
grad AddEdge W: 7.929370557290064e-17
grad ChooseDest W: 5.461100101470947
grad AddEdge W: 2.8509705219950393e-19
grad ChooseDest W: 5.138455390930176
grad AddEdge W: 2.9836398871436196e-15
grad ChooseDest W: 2.899585247039795
grad AddEdge W: 1.1118871838046183e-18
grad ChooseDest W: 6.347045421600342
grad AddEdge W: 1.5553722110091048e-18
grad ChooseDest W: 5.7242431640625
grad AddEdge W: 6.628165161162909e-17
grad ChooseDest W: 5.210334777832031
grad AddEdge W: 7.088152544985405e-19
grad ChooseDest W: 6.816107273101807
grad AddEdge W: 8.295846641109028e-17
grad ChooseDest W: 6.7565016746521
grad AddEdge W: 2.269080497324838e-18
grad ChooseDest W: 6.319817543029785
grad AddEdge W: 6.033052385567348e-17
grad ChooseDest W: 4.567765712738037
grad AddEdge W: 2.6577532284191106e-18
grad ChooseDest W: 6.830129146575928
grad AddEdge W: 2.5006617006192175e-15
grad ChooseDest W: 3.1271684169769287
grad AddEdge W: 2.922653832578665e-16
grad ChooseDest W: 2.3140242099761963
grad AddEdge W: 3.4359269465550053e-17
grad ChooseDest W: 4.669506072998047
grad AddEdge W: 2.799299547658572e-18
grad ChooseDest W: 4.95920991897583
grad AddEdge W: 3.806353314107277e-17
grad ChooseDest W: 4.51039457321167
grad AddEdge W: 9.33286792037961e-19
grad ChooseDest W: 6.883058071136475
grad AddEdge W: 2.770417792108732e-17
grad ChooseDest W: 7.461101055145264
grad AddEdge W: 8.226552625181149e-19
grad ChooseDest W: 6.161074161529541
grad AddEdge W: 1.5250375333928305e-18
grad ChooseDest W: 8.149093627929688
grad AddEdge W: 8.95017591190925e-19
grad ChooseDest W: 3.3311376571655273
grad AddEdge W: 2.3127072953005917e-19
grad ChooseDest W: 5.878434658050537
grad AddEdge W: 2.3420611952043195e-18
grad ChooseDest W: 4.4139885902404785
grad AddEdge W: 6.6063437230134535e-18
grad ChooseDest W: 4.20528507232666
grad AddEdge W: 1.0124909900511385e-18
grad ChooseDest W: 4.3712005615234375
grad AddEdge W: 1.7501297148856072e-18
grad ChooseDest W: 6.462985515594482
grad AddEdge W: 6.559922760627284e-19
grad ChooseDest W: 6.4843316078186035
grad AddEdge W: 1.4221820827253632e-16
grad ChooseDest W: 3.7593417167663574
grad AddEdge W: 6.327458538720281e-17
grad ChooseDest W: 5.522952556610107
grad AddEdge W: 2.273292170131713e-17
grad ChooseDest W: 4.336822032928467
grad AddEdge W: 6.481890677542094e-19
grad ChooseDest W: 5.4145073890686035
grad AddEdge W: 2.953090621505656e-19
grad ChooseDest W: 4.452362060546875
grad AddEdge W: 6.210101462133708e-17
grad ChooseDest W: 5.491802215576172
grad AddEdge W: 7.234735670372278e-19
grad ChooseDest W: 3.8262057304382324
grad AddEdge W: 6.595210285558796e-19
grad ChooseDest W: 5.73710823059082
grad AddEdge W: 7.045401679579556e-17
grad ChooseDest W: 5.327617168426514
grad AddEdge W: 1.786764303444662e-18
grad ChooseDest W: 5.156612396240234
grad AddEdge W: 5.7844942994344974e-15
grad ChooseDest W: 5.575317859649658
grad AddEdge W: 7.358600714505204e-17
grad ChooseDest W: 9.200677871704102
grad AddEdge W: 2.1042953659563116e-18
grad ChooseDest W: 3.170837879180908
grad AddEdge W: 2.157216727186229e-18
grad ChooseDest W: 5.459475040435791
grad AddEdge W: 3.8810732729619857e-19
grad ChooseDest W: 4.467474937438965
grad AddEdge W: 2.040175969635775e-19
grad ChooseDest W: 7.106245994567871
grad AddEdge W: 2.822457347786395e-19
grad ChooseDest W: 2.9984817504882812
grad AddEdge W: 2.4505968455073517e-17
grad ChooseDest W: 7.588444232940674
grad AddEdge W: 1.1343314889553259e-18
grad ChooseDest W: 5.636266708374023
grad AddEdge W: 2.70852046244143e-16
grad ChooseDest W: 4.644443511962891
grad AddEdge W: 4.03366380375138e-16
grad ChooseDest W: 4.116549491882324
grad AddEdge W: 4.1410818751442104e-17
grad ChooseDest W: 3.741269588470459
grad AddEdge W: 6.281186055254065e-17
grad ChooseDest W: 4.659527778625488
grad AddEdge W: 2.8402437989078164e-19
grad ChooseDest W: 9.12048053741455
grad AddEdge W: 9.491163654406536e-17
grad ChooseDest W: 4.417824745178223
grad AddEdge W: 7.803782165068945e-20
grad ChooseDest W: 6.923474311828613
=== Epoch 36: Train Loss: 4.1064, Train Log Prob: 0.0415 ===
Total mismatches: 65946
Predicted valid destination but wrong order: 47376
Epoch 36: Validation Loss: 4.3840, Validation Log Prob: 0.0178
Epoch 36: Edge Precision: 0.3763, Recall: 0.3737, F1: 0.3749, Jaccard: 0.2485
Epoch 36: TP: 2.616607015032212, FP: 4.360057265569076, FN: 4.404867573371511
Epoch 36: Current Learning Rate: 6e-05
[Epoch 36] ‚è±Ô∏è Total: 3301.11s | Current time: 2025-07-15 20:52:35 | üèãÔ∏è Train: 2817.53s | ‚úÖ Val: 483.58s
grad AddEdge W: 7.391981753560904e-17
grad ChooseDest W: 10.772859573364258
grad AddEdge W: 4.729667265128538e-19
grad ChooseDest W: 3.7054085731506348
grad AddEdge W: 9.107788973776634e-19
grad ChooseDest W: 7.146049976348877
grad AddEdge W: 1.8389461652069572e-18
grad ChooseDest W: 3.9736268520355225
grad AddEdge W: 9.58895063031684e-19
grad ChooseDest W: 6.487598419189453
grad AddEdge W: 2.897165353881252e-17
grad ChooseDest W: 5.310032367706299
grad AddEdge W: 3.054282858461102e-15
grad ChooseDest W: 2.2887141704559326
grad AddEdge W: 7.700871508367396e-17
grad ChooseDest W: 2.7400543689727783
grad AddEdge W: 1.038335139868337e-16
grad ChooseDest W: 3.550523519515991
grad AddEdge W: 1.0688303649697018e-18
grad ChooseDest W: 4.546195983886719
grad AddEdge W: 1.1617067211350386e-18
grad ChooseDest W: 5.100227355957031
grad AddEdge W: 4.377265059726221e-19
grad ChooseDest W: 7.826171398162842
grad AddEdge W: 9.951915932141459e-20
grad ChooseDest W: 4.299017429351807
grad AddEdge W: 6.345214590954192e-19
grad ChooseDest W: 5.2834343910217285
grad AddEdge W: 1.6609769897958603e-19
grad ChooseDest W: 4.706217288970947
grad AddEdge W: 3.0267065361929356e-16
grad ChooseDest W: 3.1915881633758545
grad AddEdge W: 8.044051042850539e-18
grad ChooseDest W: 6.8677897453308105
grad AddEdge W: 3.172530677872858e-18
grad ChooseDest W: 5.6503424644470215
grad AddEdge W: 1.7142822416189559e-15
grad ChooseDest W: 2.5744433403015137
grad AddEdge W: 1.5195364721266231e-18
grad ChooseDest W: 3.328824996948242
grad AddEdge W: 4.626110476971009e-17
grad ChooseDest W: 3.470076560974121
grad AddEdge W: 3.610495143028196e-17
grad ChooseDest W: 3.819852352142334
grad AddEdge W: 3.15459421754735e-17
grad ChooseDest W: 8.585411071777344
grad AddEdge W: 3.6950290311641755e-16
grad ChooseDest W: 6.104786396026611
grad AddEdge W: 2.939331247497661e-19
grad ChooseDest W: 5.571020603179932
grad AddEdge W: 1.1355489954194274e-18
grad ChooseDest W: 3.1975748538970947
grad AddEdge W: 3.793124380006839e-17
grad ChooseDest W: 7.068690299987793
grad AddEdge W: 1.1087466064934229e-16
grad ChooseDest W: 3.7733285427093506
grad AddEdge W: 7.509779427900754e-19
grad ChooseDest W: 2.5503122806549072
grad AddEdge W: 5.222882907747596e-17
grad ChooseDest W: 4.587757587432861
grad AddEdge W: 8.529104273980076e-19
grad ChooseDest W: 2.0529086589813232
grad AddEdge W: 7.1450641220927015e-19
grad ChooseDest W: 4.747609615325928
grad AddEdge W: 1.456140777075267e-14
grad ChooseDest W: 3.3205554485321045
grad AddEdge W: 1.0112229221720947e-18
grad ChooseDest W: 5.369919776916504
grad AddEdge W: 3.7454913302814585e-13
grad ChooseDest W: 6.215612888336182
grad AddEdge W: 2.686040578598215e-15
grad ChooseDest W: 4.882393836975098
grad AddEdge W: 2.3900726025696632e-16
grad ChooseDest W: 4.751015663146973
grad AddEdge W: 2.681115704049752e-18
grad ChooseDest W: 8.111335754394531
grad AddEdge W: 8.308261009101254e-19
grad ChooseDest W: 5.286729335784912
grad AddEdge W: 4.369163859602029e-19
grad ChooseDest W: 7.435155391693115
grad AddEdge W: 2.7747364689996467e-16
grad ChooseDest W: 4.853782653808594
grad AddEdge W: 3.2132697779904316e-17
grad ChooseDest W: 6.2345733642578125
grad AddEdge W: 7.865062227747742e-17
grad ChooseDest W: 4.185850143432617
grad AddEdge W: 1.2218125003491537e-14
grad ChooseDest W: 3.710747003555298
grad AddEdge W: 4.6311102874655244e-17
grad ChooseDest W: 4.503101348876953
grad AddEdge W: 1.0715971310902784e-16
grad ChooseDest W: 6.583768367767334
grad AddEdge W: 2.7094871718797083e-17
grad ChooseDest W: 10.70057201385498
grad AddEdge W: 1.0418779875191262e-17
grad ChooseDest W: 6.446626663208008
grad AddEdge W: 1.3632959096906165e-18
grad ChooseDest W: 5.865573406219482
grad AddEdge W: 7.518145325820962e-17
grad ChooseDest W: 6.226834297180176
grad AddEdge W: 1.6531229357290635e-18
grad ChooseDest W: 5.667262554168701
grad AddEdge W: 2.7029592175818047e-19
grad ChooseDest W: 5.6383490562438965
grad AddEdge W: 1.1872673258459631e-18
grad ChooseDest W: 4.750311374664307
grad AddEdge W: 5.614863120521162e-18
grad ChooseDest W: 5.627193450927734
grad AddEdge W: 1.3854968169460803e-18
grad ChooseDest W: 4.69157600402832
grad AddEdge W: 1.4865716703551599e-18
grad ChooseDest W: 3.1112263202667236
grad AddEdge W: 2.8479381812669964e-18
grad ChooseDest W: 5.6703643798828125
grad AddEdge W: 4.056205534234634e-17
grad ChooseDest W: 6.019620418548584
grad AddEdge W: 1.3527815135268784e-18
grad ChooseDest W: 7.360511302947998
grad AddEdge W: 4.197501548620737e-17
grad ChooseDest W: 5.014153003692627
grad AddEdge W: 1.6606676500961596e-16
grad ChooseDest W: 4.119186878204346
grad AddEdge W: 8.277556949846541e-16
grad ChooseDest W: 3.360487937927246
grad AddEdge W: 1.2987728233817598e-18
grad ChooseDest W: 6.621260166168213
grad AddEdge W: 7.858315597915152e-19
grad ChooseDest W: 8.082338333129883
grad AddEdge W: 3.719680667779481e-18
grad ChooseDest W: 5.0428853034973145
grad AddEdge W: 2.535045997261381e-16
grad ChooseDest W: 4.47349739074707
=== Epoch 37: Train Loss: 4.0682, Train Log Prob: 0.0433 ===
Total mismatches: 65301
Predicted valid destination but wrong order: 47449
Epoch 37: Validation Loss: 4.4603, Validation Log Prob: 0.0164
Epoch 37: Edge Precision: 0.3736, Recall: 0.3711, F1: 0.3723, Jaccard: 0.2459
Epoch 37: TP: 2.5988546886184682, FP: 4.378811739441661, FN: 4.422619899785254
Epoch 37: Current Learning Rate: 6e-05
[Epoch 37] ‚è±Ô∏è Total: 3294.53s | Current time: 2025-07-15 21:47:29 | üèãÔ∏è Train: 2812.66s | ‚úÖ Val: 481.88s
grad AddEdge W: 4.2289255445215857e-16
grad ChooseDest W: 13.174277305603027
grad AddEdge W: 6.857271029782946e-17
grad ChooseDest W: 8.836856842041016
grad AddEdge W: 2.0515603013099774e-19
grad ChooseDest W: 5.576694011688232
grad AddEdge W: 4.802931685958126e-19
grad ChooseDest W: 4.698781967163086
grad AddEdge W: 7.953758488726088e-17
grad ChooseDest W: 8.376630783081055
grad AddEdge W: 1.1411965048771841e-16
grad ChooseDest W: 2.336221933364868
grad AddEdge W: 1.9168041267732047e-18
grad ChooseDest W: 3.5496017932891846
grad AddEdge W: 5.971785454380776e-19
grad ChooseDest W: 5.719043731689453
grad AddEdge W: 8.01473907066411e-19
grad ChooseDest W: 2.694448947906494
grad AddEdge W: 9.496898869824641e-19
grad ChooseDest W: 4.14959716796875
grad AddEdge W: 1.1453451920163163e-18
grad ChooseDest W: 3.1712589263916016
grad AddEdge W: 6.790028525171775e-17
grad ChooseDest W: 4.505444526672363
grad AddEdge W: 3.128916907991752e-19
grad ChooseDest W: 2.6427369117736816
grad AddEdge W: 5.776895063938867e-17
grad ChooseDest W: 3.9840147495269775
grad AddEdge W: 1.6903851992766811e-19
grad ChooseDest W: 2.810129404067993
grad AddEdge W: 2.6615229386246985e-17
grad ChooseDest W: 4.229395389556885
grad AddEdge W: 1.4554189938311319e-16
grad ChooseDest W: 7.148410320281982
grad AddEdge W: 9.746465464486484e-19
grad ChooseDest W: 6.229946613311768
grad AddEdge W: 1.7142665756453347e-18
grad ChooseDest W: 9.762293815612793
grad AddEdge W: 3.739288157019438e-18
grad ChooseDest W: 5.022214412689209
grad AddEdge W: 1.238113601111715e-18
grad ChooseDest W: 1.6466275453567505
grad AddEdge W: 6.424519170643425e-15
grad ChooseDest W: 2.3297617435455322
grad AddEdge W: 6.947351412080277e-19
grad ChooseDest W: 7.01185417175293
grad AddEdge W: 4.5565651015344906e-17
grad ChooseDest W: 3.7970101833343506
grad AddEdge W: 1.9685573021830442e-18
grad ChooseDest W: 9.492890357971191
grad AddEdge W: 1.7790586362739753e-16
grad ChooseDest W: 3.853285074234009
grad AddEdge W: 5.763859131754853e-19
grad ChooseDest W: 3.8065342903137207
grad AddEdge W: 1.0967340621632594e-18
grad ChooseDest W: 3.9390389919281006
grad AddEdge W: 4.747523054664807e-17
grad ChooseDest W: 3.6519148349761963
grad AddEdge W: 1.811797783174334e-16
grad ChooseDest W: 7.56232213973999
grad AddEdge W: 3.061265683100161e-19
grad ChooseDest W: 5.125349521636963
grad AddEdge W: 3.7002800253914504e-19
grad ChooseDest W: 5.748489856719971
grad AddEdge W: 6.160959592520122e-19
grad ChooseDest W: 9.884243965148926
grad AddEdge W: 1.5300587265061805e-18
grad ChooseDest W: 6.318867206573486
grad AddEdge W: 8.980948064671987e-17
grad ChooseDest W: 5.16766881942749
grad AddEdge W: 9.38891288103865e-17
grad ChooseDest W: 5.468110084533691
grad AddEdge W: 2.2714106651103997e-18
grad ChooseDest W: 6.074748992919922
grad AddEdge W: 7.948149542428488e-17
grad ChooseDest W: 6.426762104034424
grad AddEdge W: 4.736706510102819e-17
grad ChooseDest W: 3.3790323734283447
grad AddEdge W: 2.816821714574283e-18
grad ChooseDest W: 4.661322593688965
grad AddEdge W: 3.8692878620882716e-18
grad ChooseDest W: 5.405928134918213
grad AddEdge W: 9.663458758556431e-17
grad ChooseDest W: 5.8650922775268555
grad AddEdge W: 7.151623581632186e-17
grad ChooseDest W: 1.802157998085022
grad AddEdge W: 6.749290210875784e-17
grad ChooseDest W: 4.805934906005859
grad AddEdge W: 1.6111496976471973e-19
grad ChooseDest W: 4.176582336425781
grad AddEdge W: 8.135027158353688e-19
grad ChooseDest W: 5.620522499084473
grad AddEdge W: 1.0583822466525445e-18
grad ChooseDest W: 3.8108177185058594
grad AddEdge W: 2.091923093185458e-16
grad ChooseDest W: 8.660888671875
grad AddEdge W: 6.429958727722366e-19
grad ChooseDest W: 3.7119462490081787
grad AddEdge W: 5.4106244517840613e-17
grad ChooseDest W: 3.221876621246338
grad AddEdge W: 3.2112915341964804e-18
grad ChooseDest W: 3.9974477291107178
grad AddEdge W: 5.876557837324257e-19
grad ChooseDest W: 3.6421096324920654
grad AddEdge W: 4.392397708647419e-18
grad ChooseDest W: 4.921915531158447
grad AddEdge W: 1.4055193233788705e-17
grad ChooseDest W: 4.374370098114014
grad AddEdge W: 2.979875249209548e-15
grad ChooseDest W: 5.069766044616699
grad AddEdge W: 7.083489314282137e-19
grad ChooseDest W: 6.479856491088867
grad AddEdge W: 7.005517201791357e-19
grad ChooseDest W: 4.954555988311768
grad AddEdge W: 1.1376190149023413e-17
grad ChooseDest W: 3.8657007217407227
grad AddEdge W: 9.31046890336744e-19
grad ChooseDest W: 5.681575298309326
grad AddEdge W: 1.4473153328446183e-18
grad ChooseDest W: 6.83261251449585
grad AddEdge W: 1.5453350404870392e-14
grad ChooseDest W: 7.730640411376953
grad AddEdge W: 1.6873028658203671e-18
grad ChooseDest W: 4.27667760848999
grad AddEdge W: 9.248906911066106e-15
grad ChooseDest W: 3.5468761920928955
grad AddEdge W: 1.4457978700108898e-18
grad ChooseDest W: 8.078898429870605
grad AddEdge W: 2.8710764392528477e-19
grad ChooseDest W: 4.980484962463379
grad AddEdge W: 2.1170760541915296e-15
grad ChooseDest W: 4.500843048095703
=== Epoch 38: Train Loss: 4.0331, Train Log Prob: 0.0446 ===
Total mismatches: 64642
Predicted valid destination but wrong order: 47695
Epoch 38: Validation Loss: 4.3631, Validation Log Prob: 0.0180
Epoch 38: Edge Precision: 0.3743, Recall: 0.3716, F1: 0.3729, Jaccard: 0.2474
Epoch 38: TP: 2.6021474588403724, FP: 4.373371510379385, FN: 4.41932712956335
Epoch 38: Current Learning Rate: 6e-05
[Epoch 38] ‚è±Ô∏è Total: 3295.56s | Current time: 2025-07-15 22:42:25 | üèãÔ∏è Train: 2814.09s | ‚úÖ Val: 481.46s
grad AddEdge W: 6.0312345734532015e-15
grad ChooseDest W: 11.862895011901855
grad AddEdge W: 1.2003666425642115e-14
grad ChooseDest W: 4.76424503326416
grad AddEdge W: 3.6077777099593524e-19
grad ChooseDest W: 4.145915985107422
grad AddEdge W: 1.0013701671008225e-18
grad ChooseDest W: 5.071896076202393
grad AddEdge W: 7.135995379145765e-20
grad ChooseDest W: 6.739345073699951
grad AddEdge W: 8.216779424205289e-17
grad ChooseDest W: 6.2100677490234375
grad AddEdge W: 6.700428486656154e-18
grad ChooseDest W: 6.291717529296875
grad AddEdge W: 6.584710261658201e-19
grad ChooseDest W: 5.169848442077637
grad AddEdge W: 1.7487357832045574e-16
grad ChooseDest W: 3.3515262603759766
grad AddEdge W: 8.372497787520591e-19
grad ChooseDest W: 4.793738842010498
grad AddEdge W: 9.699761813125974e-19
grad ChooseDest W: 2.129267930984497
grad AddEdge W: 1.565863549513268e-18
grad ChooseDest W: 6.176802158355713
grad AddEdge W: 2.015489751700986e-17
grad ChooseDest W: 5.696653366088867
grad AddEdge W: 5.597886561937491e-17
grad ChooseDest W: 6.954641342163086
grad AddEdge W: 3.0126846998445287e-16
grad ChooseDest W: 8.763691902160645
grad AddEdge W: 9.153110199538415e-19
grad ChooseDest W: 4.753204822540283
grad AddEdge W: 1.2288491947947333e-16
grad ChooseDest W: 3.155320405960083
grad AddEdge W: 2.750466733401351e-18
grad ChooseDest W: 10.58859920501709
grad AddEdge W: 1.7205628056343893e-16
grad ChooseDest W: 4.09229850769043
grad AddEdge W: 7.157050548195024e-17
grad ChooseDest W: 4.36080265045166
grad AddEdge W: 5.646432055008942e-19
grad ChooseDest W: 5.146658897399902
grad AddEdge W: 2.18056415846948e-19
grad ChooseDest W: 5.738309383392334
grad AddEdge W: 5.237001950225687e-19
grad ChooseDest W: 8.700881004333496
grad AddEdge W: 1.1582595234804063e-19
grad ChooseDest W: 4.176147937774658
grad AddEdge W: 6.540228318161335e-16
grad ChooseDest W: 2.1235482692718506
grad AddEdge W: 3.075602894464813e-18
grad ChooseDest W: 3.4775807857513428
grad AddEdge W: 1.8409920971904336e-16
grad ChooseDest W: 3.924337387084961
grad AddEdge W: 1.0675276009998645e-16
grad ChooseDest W: 3.6750829219818115
grad AddEdge W: 1.0370182393818312e-18
grad ChooseDest W: 8.95522689819336
grad AddEdge W: 5.980116982119352e-15
grad ChooseDest W: 2.731363296508789
grad AddEdge W: 5.247594514957311e-19
grad ChooseDest W: 6.076085090637207
grad AddEdge W: 9.627553826015709e-17
grad ChooseDest W: 6.403426647186279
grad AddEdge W: 2.351085264194324e-16
grad ChooseDest W: 6.643630504608154
grad AddEdge W: 2.2861849380312094e-18
grad ChooseDest W: 3.052891969680786
grad AddEdge W: 2.9024502108148535e-18
grad ChooseDest W: 5.40239953994751
grad AddEdge W: 8.575872548825941e-19
grad ChooseDest W: 4.44948673248291
grad AddEdge W: 9.513307031250396e-19
grad ChooseDest W: 6.964189529418945
grad AddEdge W: 2.5128370580821693e-16
grad ChooseDest W: 5.9869065284729
grad AddEdge W: 1.178909596334304e-18
grad ChooseDest W: 3.9608452320098877
grad AddEdge W: 2.587774575777737e-18
grad ChooseDest W: 7.90717077255249
grad AddEdge W: 2.4588671627077794e-17
grad ChooseDest W: 7.947248935699463
grad AddEdge W: 1.5361933047240269e-18
grad ChooseDest W: 3.4275317192077637
grad AddEdge W: 1.0858601526257904e-18
grad ChooseDest W: 5.373190402984619
grad AddEdge W: 3.166078255504638e-18
grad ChooseDest W: 4.638821125030518
grad AddEdge W: 4.565095546358051e-19
grad ChooseDest W: 4.291757106781006
grad AddEdge W: 9.686495904052155e-19
grad ChooseDest W: 6.31014347076416
grad AddEdge W: 3.3779508500378987e-17
grad ChooseDest W: 4.545479774475098
grad AddEdge W: 1.9492176385157955e-19
grad ChooseDest W: 5.971343040466309
grad AddEdge W: 9.790436938283731e-17
grad ChooseDest W: 5.020036220550537
grad AddEdge W: 1.015227510312617e-18
grad ChooseDest W: 5.5587568283081055
grad AddEdge W: 2.1387558095369442e-16
grad ChooseDest W: 5.6574177742004395
grad AddEdge W: 2.1927031890550603e-18
grad ChooseDest W: 7.470052242279053
grad AddEdge W: 5.805306401874348e-17
grad ChooseDest W: 6.357915878295898
grad AddEdge W: 1.3478704388325745e-18
grad ChooseDest W: 6.065501689910889
grad AddEdge W: 1.5290322987635788e-18
grad ChooseDest W: 6.995603561401367
grad AddEdge W: 3.054881916722783e-19
grad ChooseDest W: 5.955479621887207
grad AddEdge W: 6.493508098373156e-17
grad ChooseDest W: 3.8018734455108643
grad AddEdge W: 2.433583394668361e-17
grad ChooseDest W: 7.780735969543457
grad AddEdge W: 5.032183758751432e-19
grad ChooseDest W: 4.851798057556152
grad AddEdge W: 8.265342743018941e-19
grad ChooseDest W: 2.9738008975982666
grad AddEdge W: 1.0718020113202986e-18
grad ChooseDest W: 5.877865314483643
grad AddEdge W: 3.7412800079344655e-17
grad ChooseDest W: 3.8808252811431885
grad AddEdge W: 4.24204274306488e-19
grad ChooseDest W: 4.839570045471191
grad AddEdge W: 4.194424767614285e-17
grad ChooseDest W: 5.60292911529541
grad AddEdge W: 4.102349428040941e-13
grad ChooseDest W: 3.877283811569214
grad AddEdge W: 5.472368438709408e-19
grad ChooseDest W: 4.3361711502075195
=== Epoch 39: Train Loss: 3.9932, Train Log Prob: 0.0462 ===
Total mismatches: 64016
Predicted valid destination but wrong order: 47894
Epoch 39: Validation Loss: 4.3901, Validation Log Prob: 0.0177
Epoch 39: Edge Precision: 0.3731, Recall: 0.3702, F1: 0.3716, Jaccard: 0.2454
Epoch 39: TP: 2.5921259842519686, FP: 4.37680744452398, FN: 4.429348604151754
Epoch 39: Current Learning Rate: 6e-05
[Epoch 39] ‚è±Ô∏è Total: 3298.98s | Current time: 2025-07-15 23:37:24 | üèãÔ∏è Train: 2815.97s | ‚úÖ Val: 483.02s
grad AddEdge W: 4.959095420686019e-15
grad ChooseDest W: 10.294745445251465
grad AddEdge W: 9.068112221719763e-19
grad ChooseDest W: 3.949101448059082
grad AddEdge W: 5.28430582451818e-19
grad ChooseDest W: 4.435014247894287
grad AddEdge W: 2.261879373104681e-19
grad ChooseDest W: 5.108193874359131
grad AddEdge W: 7.127445175045322e-19
grad ChooseDest W: 9.298039436340332
grad AddEdge W: 5.67161401779447e-19
grad ChooseDest W: 5.830681324005127
grad AddEdge W: 1.1627949806284287e-18
grad ChooseDest W: 5.578025817871094
grad AddEdge W: 4.5260109522221906e-18
grad ChooseDest W: 6.041102886199951
grad AddEdge W: 1.2946141728521495e-18
grad ChooseDest W: 5.211240291595459
grad AddEdge W: 8.439805846195307e-17
grad ChooseDest W: 6.232754230499268
grad AddEdge W: 4.84003943423527e-16
grad ChooseDest W: 4.892799377441406
grad AddEdge W: 7.165138389352322e-17
grad ChooseDest W: 4.7491350173950195
grad AddEdge W: 1.530312981146964e-18
grad ChooseDest W: 4.122437953948975
grad AddEdge W: 6.481971297873316e-15
grad ChooseDest W: 8.808916091918945
grad AddEdge W: 2.0215788598005604e-16
grad ChooseDest W: 7.83808708190918
grad AddEdge W: 1.5298091330281487e-16
grad ChooseDest W: 4.283029556274414
grad AddEdge W: 1.2256905559948628e-15
grad ChooseDest W: 6.869053840637207
grad AddEdge W: 9.788768680424334e-17
grad ChooseDest W: 4.544990062713623
grad AddEdge W: 2.842619358229492e-19
grad ChooseDest W: 3.9568583965301514
grad AddEdge W: 3.5735889910740404e-17
grad ChooseDest W: 4.674516201019287
grad AddEdge W: 3.8146193920070654e-19
grad ChooseDest W: 6.8208184242248535
grad AddEdge W: 2.1069208703077794e-16
grad ChooseDest W: 2.9195618629455566
grad AddEdge W: 2.5564459945649765e-13
grad ChooseDest W: 6.682010650634766
grad AddEdge W: 1.279773622084759e-18
grad ChooseDest W: 5.584798336029053
grad AddEdge W: 2.982422704520728e-19
grad ChooseDest W: 3.376339912414551
grad AddEdge W: 9.094813611892974e-19
grad ChooseDest W: 5.3485307693481445
grad AddEdge W: 3.1820341559304798e-18
grad ChooseDest W: 4.296496868133545
grad AddEdge W: 6.865854517563286e-17
grad ChooseDest W: 4.039515972137451
grad AddEdge W: 4.343456575088984e-16
grad ChooseDest W: 7.240841388702393
grad AddEdge W: 2.9674624048840097e-15
grad ChooseDest W: 1.5344252586364746
grad AddEdge W: 1.8435034392436357e-19
grad ChooseDest W: 5.494133472442627
grad AddEdge W: 9.263247436943472e-18
grad ChooseDest W: 4.105318069458008
grad AddEdge W: 1.3133334520939592e-16
grad ChooseDest W: 3.666710138320923
grad AddEdge W: 4.5751124761013996e-17
grad ChooseDest W: 4.00021505355835
grad AddEdge W: 6.087478884639867e-17
grad ChooseDest W: 9.016557693481445
grad AddEdge W: 3.163004245553238e-19
grad ChooseDest W: 6.795915603637695
grad AddEdge W: 4.458781434346637e-16
grad ChooseDest W: 6.5898756980896
grad AddEdge W: 3.617697570057818e-18
grad ChooseDest W: 7.335258960723877
grad AddEdge W: 1.2939529453499899e-18
grad ChooseDest W: 6.703065395355225
grad AddEdge W: 3.6173478380948306e-17
grad ChooseDest W: 4.855540752410889
grad AddEdge W: 8.386087810773827e-15
grad ChooseDest W: 3.0943589210510254
grad AddEdge W: 8.032923085467439e-19
grad ChooseDest W: 6.959229469299316
grad AddEdge W: 1.0939040704925623e-18
grad ChooseDest W: 6.433708667755127
grad AddEdge W: 1.1551738719958602e-16
grad ChooseDest W: 2.9841551780700684
grad AddEdge W: 4.3900741583067574e-17
grad ChooseDest W: 4.313241481781006
grad AddEdge W: 3.116481374515602e-17
grad ChooseDest W: 9.482390403747559
grad AddEdge W: 4.1492318375655116e-18
grad ChooseDest W: 4.627499103546143
grad AddEdge W: 9.826160552834181e-17
grad ChooseDest W: 5.682100296020508
grad AddEdge W: 5.802978053886134e-19
grad ChooseDest W: 6.876865863800049
grad AddEdge W: 3.619627382426904e-18
grad ChooseDest W: 4.768121719360352
grad AddEdge W: 8.489005887285471e-17
grad ChooseDest W: 3.7965786457061768
grad AddEdge W: 1.2267317447754955e-16
grad ChooseDest W: 8.624191284179688
grad AddEdge W: 1.0892462536864038e-16
grad ChooseDest W: 4.464229583740234
grad AddEdge W: 1.934719535352123e-17
grad ChooseDest W: 3.045646905899048
grad AddEdge W: 3.7378448819468976e-19
grad ChooseDest W: 3.1865079402923584
grad AddEdge W: 1.9902959742947686e-15
grad ChooseDest W: 4.874113082885742
grad AddEdge W: 1.1211657721332022e-18
grad ChooseDest W: 9.426980018615723
grad AddEdge W: 8.42141776593486e-19
grad ChooseDest W: 4.342549800872803
grad AddEdge W: 3.2027824513122393e-16
grad ChooseDest W: 9.652335166931152
grad AddEdge W: 3.979049435418138e-17
grad ChooseDest W: 8.717150688171387
grad AddEdge W: 4.105083088623237e-19
grad ChooseDest W: 8.593343734741211
grad AddEdge W: 2.841058933702699e-18
grad ChooseDest W: 10.01105785369873
grad AddEdge W: 5.290761969199157e-19
grad ChooseDest W: 5.329182147979736
grad AddEdge W: 1.8092901686037673e-16
grad ChooseDest W: 6.265519142150879
grad AddEdge W: 1.4048054871897525e-18
grad ChooseDest W: 8.112347602844238
grad AddEdge W: 8.115771944657102e-19
grad ChooseDest W: 6.865012168884277
=== Epoch 40: Train Loss: 3.9627, Train Log Prob: 0.0475 ===
Total mismatches: 63481
Predicted valid destination but wrong order: 48014
Epoch 40: Validation Loss: 4.2509, Validation Log Prob: 0.0199
Epoch 40: Edge Precision: 0.3762, Recall: 0.3735, F1: 0.3747, Jaccard: 0.2483
Epoch 40: TP: 2.6164638511095206, FP: 4.360486757337151, FN: 4.405010737294202
Epoch 40: Current Learning Rate: 6e-05
[Epoch 40] ‚è±Ô∏è Total: 3288.47s | Current time: 2025-07-16 00:32:12 | üèãÔ∏è Train: 2807.42s | ‚úÖ Val: 481.05s
grad AddEdge W: 2.4149542946569318e-17
grad ChooseDest W: 14.085018157958984
grad AddEdge W: 5.9631202204764e-18
grad ChooseDest W: 10.824100494384766
grad AddEdge W: 2.9364392689798105e-18
grad ChooseDest W: 3.7181220054626465
grad AddEdge W: 3.5292722934481444e-17
grad ChooseDest W: 5.600490093231201
grad AddEdge W: 8.06247400945332e-18
grad ChooseDest W: 5.991853713989258
grad AddEdge W: 1.3655507264922536e-19
grad ChooseDest W: 4.200586318969727
grad AddEdge W: 3.1983771768829962e-18
grad ChooseDest W: 6.567744731903076
grad AddEdge W: 4.0949577547926615e-16
grad ChooseDest W: 5.5309672355651855
grad AddEdge W: 7.280894044272993e-17
grad ChooseDest W: 5.783124923706055
grad AddEdge W: 2.1133167011143915e-19
grad ChooseDest W: 4.697379112243652
grad AddEdge W: 5.010226021916543e-17
grad ChooseDest W: 4.331345081329346
grad AddEdge W: 1.0442648998947092e-17
grad ChooseDest W: 5.906284809112549
grad AddEdge W: 5.3007834294107204e-15
grad ChooseDest W: 2.1341257095336914
grad AddEdge W: 9.661048003262174e-13
grad ChooseDest W: 4.946657180786133
grad AddEdge W: 2.3106848387028995e-19
grad ChooseDest W: 4.410825729370117
grad AddEdge W: 8.593294171938196e-17
grad ChooseDest W: 5.68116569519043
grad AddEdge W: 6.032434336893164e-19
grad ChooseDest W: 6.196350574493408
grad AddEdge W: 1.2719011373999387e-18
grad ChooseDest W: 5.629185199737549
grad AddEdge W: 2.985614949600935e-18
grad ChooseDest W: 4.750359535217285
grad AddEdge W: 9.433892522566477e-19
grad ChooseDest W: 7.974800109863281
grad AddEdge W: 1.4999481776127805e-16
grad ChooseDest W: 6.192937850952148
grad AddEdge W: 6.043576459744253e-19
grad ChooseDest W: 4.666778564453125
grad AddEdge W: 1.1119280754781998e-16
grad ChooseDest W: 3.1235435009002686
grad AddEdge W: 1.5445448593324134e-16
grad ChooseDest W: 3.511807918548584
grad AddEdge W: 1.1805583906338875e-16
grad ChooseDest W: 5.208244800567627
grad AddEdge W: 8.089568972161984e-17
grad ChooseDest W: 6.819399833679199
grad AddEdge W: 7.191400712056391e-19
grad ChooseDest W: 5.789455413818359
grad AddEdge W: 3.559061383411894e-17
grad ChooseDest W: 6.556436538696289
grad AddEdge W: 1.1777077277196828e-16
grad ChooseDest W: 3.463057279586792
grad AddEdge W: 3.163447562662778e-19
grad ChooseDest W: 4.4264421463012695
grad AddEdge W: 1.298017090494616e-18
grad ChooseDest W: 4.796734809875488
grad AddEdge W: 2.0291238802244832e-18
grad ChooseDest W: 7.686470985412598
grad AddEdge W: 2.390456149676092e-16
grad ChooseDest W: 1.9800541400909424
grad AddEdge W: 4.486826827579125e-17
grad ChooseDest W: 4.371043682098389
grad AddEdge W: 1.7627144738122135e-16
grad ChooseDest W: 8.239283561706543
grad AddEdge W: 2.4614927890683876e-19
grad ChooseDest W: 4.997638702392578
grad AddEdge W: 1.739652678334235e-16
grad ChooseDest W: 7.162875652313232
grad AddEdge W: 6.2645883387734986e-15
grad ChooseDest W: 1.726494550704956
grad AddEdge W: 1.5088993306317883e-16
grad ChooseDest W: 3.7488319873809814
grad AddEdge W: 7.094563194732691e-19
grad ChooseDest W: 5.614615440368652
grad AddEdge W: 8.22055556574014e-19
grad ChooseDest W: 1.9158114194869995
grad AddEdge W: 8.864033322917977e-19
grad ChooseDest W: 7.1272406578063965
grad AddEdge W: 1.0061051557222292e-18
grad ChooseDest W: 5.768377304077148
grad AddEdge W: 8.164400362584961e-17
grad ChooseDest W: 6.104096412658691
grad AddEdge W: 1.7805714089936306e-19
grad ChooseDest W: 4.789794921875
grad AddEdge W: 2.2222142305277064e-16
grad ChooseDest W: 4.888006687164307
grad AddEdge W: 1.1439785862468021e-18
grad ChooseDest W: 3.1041741371154785
grad AddEdge W: 1.139136891326376e-16
grad ChooseDest W: 4.341733932495117
grad AddEdge W: 6.765739193664768e-17
grad ChooseDest W: 3.429600954055786
grad AddEdge W: 2.5226520523584785e-16
grad ChooseDest W: 8.682756423950195
grad AddEdge W: 7.850048237885414e-18
grad ChooseDest W: 10.302364349365234
grad AddEdge W: 1.1138140472748207e-16
grad ChooseDest W: 5.820271968841553
grad AddEdge W: 4.865355263795231e-17
grad ChooseDest W: 5.944199085235596
grad AddEdge W: 4.784627068221486e-16
grad ChooseDest W: 5.51295280456543
grad AddEdge W: 5.314446706124643e-16
grad ChooseDest W: 4.852644920349121
grad AddEdge W: 4.59417394341901e-18
grad ChooseDest W: 2.3363382816314697
grad AddEdge W: 2.0758277095942607e-14
grad ChooseDest W: 4.8663763999938965
grad AddEdge W: 5.925646410863796e-15
grad ChooseDest W: 4.45990514755249
grad AddEdge W: 5.734087867533303e-19
grad ChooseDest W: 4.593343257904053
grad AddEdge W: 8.48154757689644e-15
grad ChooseDest W: 6.796876430511475
grad AddEdge W: 2.1312700235166863e-16
grad ChooseDest W: 5.20754337310791
grad AddEdge W: 1.5314875776167893e-18
grad ChooseDest W: 5.25000524520874
grad AddEdge W: 1.608402739400364e-17
grad ChooseDest W: 4.471714973449707
grad AddEdge W: 6.0360211566381355e-15
grad ChooseDest W: 5.76124382019043
grad AddEdge W: 1.1080723550325187e-16
grad ChooseDest W: 6.594450950622559
grad AddEdge W: 1.778323959269324e-18
grad ChooseDest W: 6.722570419311523
=== Epoch 41: Train Loss: 3.9182, Train Log Prob: 0.0492 ===
Total mismatches: 62629
Predicted valid destination but wrong order: 48289
Epoch 41: Validation Loss: 4.2368, Validation Log Prob: 0.0201
Epoch 41: Edge Precision: 0.3735, Recall: 0.3710, F1: 0.3721, Jaccard: 0.2463
Epoch 41: TP: 2.5962777380100213, FP: 4.379813886900501, FN: 4.425196850393701
Epoch 41: Current Learning Rate: 6e-05
[Epoch 41] ‚è±Ô∏è Total: 3291.54s | Current time: 2025-07-16 01:27:04 | üèãÔ∏è Train: 2809.64s | ‚úÖ Val: 481.89s
grad AddEdge W: 1.036973987038857e-14
grad ChooseDest W: 10.059375762939453
grad AddEdge W: 2.186529164661753e-19
grad ChooseDest W: 3.349216938018799
grad AddEdge W: 1.22193528836277e-16
grad ChooseDest W: 1.8556543588638306
grad AddEdge W: 4.3249194575616706e-17
grad ChooseDest W: 4.503650665283203
grad AddEdge W: 4.3535053306064006e-18
grad ChooseDest W: 3.8108952045440674
grad AddEdge W: 3.4322003531388468e-19
grad ChooseDest W: 6.434789180755615
grad AddEdge W: 1.263427859028512e-16
grad ChooseDest W: 5.734745979309082
grad AddEdge W: 4.657066966933255e-18
grad ChooseDest W: 7.806664943695068
grad AddEdge W: 1.635310262982224e-16
grad ChooseDest W: 5.812344074249268
grad AddEdge W: 2.6656282360047992e-17
grad ChooseDest W: 9.349449157714844
grad AddEdge W: 1.7358449777910641e-19
grad ChooseDest W: 4.8494648933410645
grad AddEdge W: 1.2800431795668747e-18
grad ChooseDest W: 4.539619445800781
grad AddEdge W: 2.6632707298999924e-18
grad ChooseDest W: 6.5403032302856445
grad AddEdge W: 2.353836318019165e-18
grad ChooseDest W: 4.419713020324707
grad AddEdge W: 5.3654863638099e-19
grad ChooseDest W: 6.3883891105651855
grad AddEdge W: 6.858556137582608e-17
grad ChooseDest W: 4.804957866668701
grad AddEdge W: 2.3566923658791575e-18
grad ChooseDest W: 6.134289264678955
grad AddEdge W: 1.1837091050434897e-18
grad ChooseDest W: 6.737005710601807
grad AddEdge W: 1.3394868913596363e-14
grad ChooseDest W: 2.689506769180298
grad AddEdge W: 1.377350535478505e-18
grad ChooseDest W: 2.55607271194458
grad AddEdge W: 6.746235887822962e-19
grad ChooseDest W: 5.66854190826416
grad AddEdge W: 3.5273921946338726e-18
grad ChooseDest W: 4.306044101715088
grad AddEdge W: 1.1627464871650178e-18
grad ChooseDest W: 7.463728427886963
grad AddEdge W: 1.513325342478988e-16
grad ChooseDest W: 3.0554189682006836
grad AddEdge W: 3.6657532481295456e-19
grad ChooseDest W: 4.674439907073975
grad AddEdge W: 1.77133883258662e-18
grad ChooseDest W: 7.0214948654174805
grad AddEdge W: 8.37859462562299e-19
grad ChooseDest W: 5.7686028480529785
grad AddEdge W: 6.998024082814969e-17
grad ChooseDest W: 4.583485126495361
grad AddEdge W: 1.2321281221992812e-18
grad ChooseDest W: 5.963391304016113
grad AddEdge W: 1.7330084036652066e-14
grad ChooseDest W: 7.046693801879883
grad AddEdge W: 1.3873768950808368e-18
grad ChooseDest W: 6.363319396972656
grad AddEdge W: 2.7647701337584648e-18
grad ChooseDest W: 4.970092296600342
grad AddEdge W: 1.5855882484262264e-14
grad ChooseDest W: 8.948796272277832
grad AddEdge W: 4.2591364304232884e-18
grad ChooseDest W: 6.781431674957275
grad AddEdge W: 3.369473696325302e-18
grad ChooseDest W: 8.29834270477295
grad AddEdge W: 4.500917601159782e-18
grad ChooseDest W: 3.9136743545532227
grad AddEdge W: 2.2342023725334913e-14
grad ChooseDest W: 3.4869747161865234
grad AddEdge W: 6.206144891827744e-17
grad ChooseDest W: 6.103411674499512
grad AddEdge W: 3.108636856807292e-16
grad ChooseDest W: 5.567226409912109
grad AddEdge W: 1.5637213940906751e-15
grad ChooseDest W: 3.9560348987579346
grad AddEdge W: 1.7763028675196426e-16
grad ChooseDest W: 1.2158211469650269
grad AddEdge W: 7.085615230494944e-17
grad ChooseDest W: 8.25953197479248
grad AddEdge W: 4.8539295834301586e-17
grad ChooseDest W: 2.6675453186035156
grad AddEdge W: 1.1173650840136741e-18
grad ChooseDest W: 2.750610113143921
grad AddEdge W: 3.3806941428406425e-19
grad ChooseDest W: 3.5578858852386475
grad AddEdge W: 2.939578884693544e-19
grad ChooseDest W: 7.147169589996338
grad AddEdge W: 3.905827169780518e-19
grad ChooseDest W: 7.534917831420898
grad AddEdge W: 5.136428471665391e-18
grad ChooseDest W: 6.661569595336914
grad AddEdge W: 2.3947077256757164e-16
grad ChooseDest W: 7.523584842681885
grad AddEdge W: 3.2574004426965644e-18
grad ChooseDest W: 6.1406049728393555
grad AddEdge W: 5.486488411765688e-19
grad ChooseDest W: 4.5114545822143555
grad AddEdge W: 6.273707813120994e-16
grad ChooseDest W: 7.714428901672363
grad AddEdge W: 1.863972307844596e-18
grad ChooseDest W: 4.777731418609619
grad AddEdge W: 7.368534822789721e-18
grad ChooseDest W: 2.7707483768463135
grad AddEdge W: 1.3893563796457068e-17
grad ChooseDest W: 5.727376461029053
grad AddEdge W: 1.0197554969857324e-18
grad ChooseDest W: 4.8088812828063965
grad AddEdge W: 1.0768363359258729e-18
grad ChooseDest W: 4.424396514892578
grad AddEdge W: 1.088323806682703e-18
grad ChooseDest W: 4.240818977355957
grad AddEdge W: 8.913757010694917e-18
grad ChooseDest W: 2.391319513320923
grad AddEdge W: 2.9211186432644096e-18
grad ChooseDest W: 6.91641092300415
grad AddEdge W: 3.0711441419487007e-13
grad ChooseDest W: 1.856628656387329
grad AddEdge W: 4.8788376460353553e-17
grad ChooseDest W: 3.457113742828369
grad AddEdge W: 8.058144856319947e-19
grad ChooseDest W: 4.447078704833984
grad AddEdge W: 3.2780144039468452e-18
grad ChooseDest W: 3.560185432434082
grad AddEdge W: 7.432662413368201e-20
grad ChooseDest W: 3.472670555114746
grad AddEdge W: 3.178806124949022e-17
grad ChooseDest W: 7.730895519256592
=== Epoch 42: Train Loss: 3.8845, Train Log Prob: 0.0507 ===
Total mismatches: 62028
Predicted valid destination but wrong order: 48378
Epoch 42: Validation Loss: 4.2107, Validation Log Prob: 0.0206
Epoch 42: Edge Precision: 0.3750, Recall: 0.3719, F1: 0.3733, Jaccard: 0.2473
Epoch 42: TP: 2.603292770221904, FP: 4.364781675017896, FN: 4.418181818181818
Epoch 42: Current Learning Rate: 6e-05
[Epoch 42] ‚è±Ô∏è Total: 3292.73s | Current time: 2025-07-16 02:21:57 | üèãÔ∏è Train: 2812.99s | ‚úÖ Val: 479.75s
grad AddEdge W: 5.345872263742488e-14
grad ChooseDest W: 12.191121101379395
grad AddEdge W: 2.6471471186049557e-18
grad ChooseDest W: 4.238764762878418
grad AddEdge W: 8.987179836611809e-19
grad ChooseDest W: 6.478515148162842
grad AddEdge W: 1.0362434651675634e-12
grad ChooseDest W: 4.879081726074219
grad AddEdge W: 2.7013778240264834e-17
grad ChooseDest W: 3.9165492057800293
grad AddEdge W: 9.56038187991079e-19
grad ChooseDest W: 6.031511306762695
grad AddEdge W: 8.956352883133489e-19
grad ChooseDest W: 4.4810991287231445
grad AddEdge W: 1.812157110432427e-16
grad ChooseDest W: 9.931333541870117
grad AddEdge W: 6.983068036954551e-19
grad ChooseDest W: 3.6409263610839844
grad AddEdge W: 8.39040992873609e-17
grad ChooseDest W: 3.406944751739502
grad AddEdge W: 5.828745147130887e-13
grad ChooseDest W: 2.9888458251953125
grad AddEdge W: 3.0564745495946775e-18
grad ChooseDest W: 7.033431529998779
grad AddEdge W: 3.028618673780283e-19
grad ChooseDest W: 5.6327619552612305
grad AddEdge W: 4.762214754280969e-19
grad ChooseDest W: 5.382931709289551
grad AddEdge W: 1.1958472567496694e-18
grad ChooseDest W: 4.5197954177856445
grad AddEdge W: 3.391044523564572e-16
grad ChooseDest W: 2.9727864265441895
grad AddEdge W: 3.374755021197688e-15
grad ChooseDest W: 3.0083627700805664
grad AddEdge W: 1.213790537179328e-16
grad ChooseDest W: 7.465925216674805
grad AddEdge W: 3.8618883179186785e-18
grad ChooseDest W: 5.1127471923828125
grad AddEdge W: 2.891324135267648e-17
grad ChooseDest W: 5.201648712158203
grad AddEdge W: 1.0539440090758913e-18
grad ChooseDest W: 6.565487861633301
grad AddEdge W: 9.105427166332642e-18
grad ChooseDest W: 5.2770586013793945
grad AddEdge W: 3.0949257301764752e-19
grad ChooseDest W: 6.3575639724731445
grad AddEdge W: 2.1791041329895354e-18
grad ChooseDest W: 3.734353542327881
grad AddEdge W: 5.47322978188126e-17
grad ChooseDest W: 4.546745777130127
grad AddEdge W: 1.2446008684632386e-18
grad ChooseDest W: 6.155916213989258
grad AddEdge W: 5.729069983142404e-19
grad ChooseDest W: 4.17970085144043
grad AddEdge W: 7.730571659871684e-16
grad ChooseDest W: 7.560890197753906
grad AddEdge W: 1.4089765743798726e-16
grad ChooseDest W: 5.444126129150391
grad AddEdge W: 9.141029226692078e-19
grad ChooseDest W: 2.6942334175109863
grad AddEdge W: 7.057605798939587e-19
grad ChooseDest W: 4.217335224151611
grad AddEdge W: 1.2005706028039544e-16
grad ChooseDest W: 7.211990833282471
grad AddEdge W: 5.480281331167152e-17
grad ChooseDest W: 2.6852779388427734
grad AddEdge W: 1.067470169849935e-18
grad ChooseDest W: 6.049651145935059
grad AddEdge W: 3.1779676946801385e-17
grad ChooseDest W: 3.2380599975585938
grad AddEdge W: 5.624750617178161e-19
grad ChooseDest W: 8.472527503967285
grad AddEdge W: 2.3453378809494075e-16
grad ChooseDest W: 7.166289329528809
grad AddEdge W: 2.0804007981248393e-15
grad ChooseDest W: 8.97664737701416
grad AddEdge W: 3.6476408464877476e-17
grad ChooseDest W: 4.5935492515563965
grad AddEdge W: 6.702667002829845e-17
grad ChooseDest W: 5.2108659744262695
grad AddEdge W: 5.197707769202121e-19
grad ChooseDest W: 3.5909230709075928
grad AddEdge W: 1.5411655649342656e-19
grad ChooseDest W: 6.792402267456055
grad AddEdge W: 7.050558220120635e-19
grad ChooseDest W: 4.8307294845581055
grad AddEdge W: 6.951290859747561e-19
grad ChooseDest W: 4.837676048278809
grad AddEdge W: 5.652743960070604e-19
grad ChooseDest W: 5.016144752502441
grad AddEdge W: 1.0393274174593495e-18
grad ChooseDest W: 3.270521640777588
grad AddEdge W: 8.072761137743759e-19
grad ChooseDest W: 5.437685012817383
grad AddEdge W: 1.2922070369032063e-14
grad ChooseDest W: 3.963123083114624
grad AddEdge W: 8.203715484711438e-13
grad ChooseDest W: 3.0657806396484375
grad AddEdge W: 2.429825264991349e-18
grad ChooseDest W: 4.085334300994873
grad AddEdge W: 9.2904744176779e-17
grad ChooseDest W: 4.737620830535889
grad AddEdge W: 4.0132567348479595e-19
grad ChooseDest W: 4.387231826782227
grad AddEdge W: 7.707032535685329e-19
grad ChooseDest W: 3.3649611473083496
grad AddEdge W: 1.2622794759122985e-18
grad ChooseDest W: 8.662817001342773
grad AddEdge W: 1.635524358004268e-18
grad ChooseDest W: 7.5658860206604
grad AddEdge W: 1.4148372482814843e-16
grad ChooseDest W: 2.2964744567871094
grad AddEdge W: 4.116233263663999e-15
grad ChooseDest W: 3.6785926818847656
grad AddEdge W: 3.410403885504133e-19
grad ChooseDest W: 6.021624565124512
grad AddEdge W: 1.690684586959637e-18
grad ChooseDest W: 6.6191301345825195
grad AddEdge W: 4.623682371000921e-18
grad ChooseDest W: 3.718837022781372
grad AddEdge W: 1.2848990399552907e-18
grad ChooseDest W: 4.033778190612793
grad AddEdge W: 1.3127159865821068e-18
grad ChooseDest W: 5.916563987731934
grad AddEdge W: 2.026563516346254e-16
grad ChooseDest W: 6.769225120544434
grad AddEdge W: 1.0668529897153938e-18
grad ChooseDest W: 5.423698902130127
grad AddEdge W: 1.274108992138992e-14
grad ChooseDest W: 3.658160924911499
grad AddEdge W: 6.27799578506757e-17
grad ChooseDest W: 4.084790229797363
=== Epoch 43: Train Loss: 3.8532, Train Log Prob: 0.0521 ===
Total mismatches: 61562
Predicted valid destination but wrong order: 48448
Epoch 43: Validation Loss: 4.1649, Validation Log Prob: 0.0216
Epoch 43: Edge Precision: 0.3742, Recall: 0.3717, F1: 0.3728, Jaccard: 0.2471
Epoch 43: TP: 2.602433786685755, FP: 4.372512526843235, FN: 4.419040801717967
Epoch 43: Current Learning Rate: 6e-05
[Epoch 43] ‚è±Ô∏è Total: 3292.25s | Current time: 2025-07-16 03:16:49 | üèãÔ∏è Train: 2806.88s | ‚úÖ Val: 485.37s
grad AddEdge W: 1.2373054952840874e-16
grad ChooseDest W: 10.145167350769043
grad AddEdge W: 3.2744875126100723e-18
grad ChooseDest W: 3.7414164543151855
grad AddEdge W: 4.3192264035393866e-15
grad ChooseDest W: 3.289133071899414
grad AddEdge W: 8.129031649876327e-19
grad ChooseDest W: 6.9286580085754395
grad AddEdge W: 7.186972710839818e-19
grad ChooseDest W: 8.799115180969238
grad AddEdge W: 1.3610494649907542e-16
grad ChooseDest W: 3.617685079574585
grad AddEdge W: 1.207240590216888e-16
grad ChooseDest W: 5.341027736663818
grad AddEdge W: 1.1951974029809325e-18
grad ChooseDest W: 4.441673755645752
grad AddEdge W: 1.3378943635396798e-16
grad ChooseDest W: 3.425124168395996
grad AddEdge W: 2.339589703611103e-17
grad ChooseDest W: 3.7791991233825684
grad AddEdge W: 2.0989346731041514e-16
grad ChooseDest W: 5.492874622344971
grad AddEdge W: 7.563726596487814e-19
grad ChooseDest W: 4.2963032722473145
grad AddEdge W: 2.4214903780723053e-16
grad ChooseDest W: 7.740395545959473
grad AddEdge W: 9.390852247343812e-19
grad ChooseDest W: 5.274997234344482
grad AddEdge W: 4.291890193935943e-14
grad ChooseDest W: 2.8540663719177246
grad AddEdge W: 2.9654114767296335e-19
grad ChooseDest W: 3.7277958393096924
grad AddEdge W: 4.994674695528301e-17
grad ChooseDest W: 5.087770938873291
grad AddEdge W: 5.136543966758419e-19
grad ChooseDest W: 5.53604793548584
grad AddEdge W: 1.239451669150096e-18
grad ChooseDest W: 5.217863082885742
grad AddEdge W: 5.66416525637843e-19
grad ChooseDest W: 4.83324670791626
grad AddEdge W: 1.4509624755629412e-18
grad ChooseDest W: 6.125246524810791
grad AddEdge W: 6.22900419349177e-17
grad ChooseDest W: 9.999231338500977
grad AddEdge W: 2.6681436508885423e-18
grad ChooseDest W: 4.796522617340088
grad AddEdge W: 1.8570033111838367e-18
grad ChooseDest W: 7.498978137969971
grad AddEdge W: 1.2584659664932759e-17
grad ChooseDest W: 7.033379077911377
grad AddEdge W: 1.8199498982504614e-14
grad ChooseDest W: 6.445317268371582
grad AddEdge W: 4.5942689864713926e-17
grad ChooseDest W: 7.460472583770752
grad AddEdge W: 8.84824910382827e-17
grad ChooseDest W: 4.4825439453125
grad AddEdge W: 5.040769500598941e-17
grad ChooseDest W: 6.862565994262695
grad AddEdge W: 3.251411758459257e-19
grad ChooseDest W: 7.731146335601807
grad AddEdge W: 4.476870013186793e-19
grad ChooseDest W: 5.113490104675293
grad AddEdge W: 1.5943564846802747e-16
grad ChooseDest W: 3.368011713027954
grad AddEdge W: 1.1029689815253115e-16
grad ChooseDest W: 4.677525520324707
grad AddEdge W: 2.4612580455503027e-16
grad ChooseDest W: 3.7276437282562256
grad AddEdge W: 2.4782359586422266e-19
grad ChooseDest W: 1.9519994258880615
grad AddEdge W: 6.823007554449774e-19
grad ChooseDest W: 4.747081279754639
grad AddEdge W: 3.6077755386102445e-18
grad ChooseDest W: 5.9366044998168945
grad AddEdge W: 9.11968072905785e-19
grad ChooseDest W: 3.79929256439209
grad AddEdge W: 1.4500063209492789e-16
grad ChooseDest W: 2.785275936126709
grad AddEdge W: 3.9534402545257413e-17
grad ChooseDest W: 4.10693883895874
grad AddEdge W: 2.64740518241246e-16
grad ChooseDest W: 3.836932420730591
grad AddEdge W: 1.7437762729494854e-16
grad ChooseDest W: 4.466752052307129
grad AddEdge W: 1.8272601711031143e-18
grad ChooseDest W: 6.043858528137207
grad AddEdge W: 1.0090032863958853e-18
grad ChooseDest W: 5.079983711242676
grad AddEdge W: 6.863376036293893e-19
grad ChooseDest W: 6.296558856964111
grad AddEdge W: 1.0010660872358419e-16
grad ChooseDest W: 9.507603645324707
grad AddEdge W: 3.56413094177566e-16
grad ChooseDest W: 5.640925407409668
grad AddEdge W: 2.2193677696910135e-17
grad ChooseDest W: 3.98415207862854
grad AddEdge W: 4.551153553618271e-16
grad ChooseDest W: 5.5717644691467285
grad AddEdge W: 6.800974564858659e-19
grad ChooseDest W: 4.686085224151611
grad AddEdge W: 4.735628859200785e-18
grad ChooseDest W: 4.280239582061768
grad AddEdge W: 3.064728654055099e-17
grad ChooseDest W: 6.04112434387207
grad AddEdge W: 6.333112421604658e-19
grad ChooseDest W: 3.8206863403320312
grad AddEdge W: 4.5816938559271165e-17
grad ChooseDest W: 2.5071566104888916
grad AddEdge W: 5.769624580624347e-19
grad ChooseDest W: 1.7328296899795532
grad AddEdge W: 2.2231473647049214e-18
grad ChooseDest W: 4.179342746734619
grad AddEdge W: 1.2532310381814483e-16
grad ChooseDest W: 2.362950325012207
grad AddEdge W: 4.069428711158009e-16
grad ChooseDest W: 2.1071465015411377
grad AddEdge W: 1.0179896069986358e-16
grad ChooseDest W: 5.083963394165039
grad AddEdge W: 1.0763387288847793e-16
grad ChooseDest W: 3.34149432182312
grad AddEdge W: 1.2374914454857893e-16
grad ChooseDest W: 6.71802282333374
grad AddEdge W: 1.1724774398910916e-18
grad ChooseDest W: 6.509839057922363
grad AddEdge W: 1.5548038345307043e-18
grad ChooseDest W: 9.914779663085938
grad AddEdge W: 1.0250885371900149e-18
grad ChooseDest W: 7.2774481773376465
grad AddEdge W: 1.1750385978627089e-18
grad ChooseDest W: 7.385550498962402
grad AddEdge W: 3.008152808878922e-17
grad ChooseDest W: 3.416184902191162
=== Epoch 44: Train Loss: 3.8237, Train Log Prob: 0.0531 ===
Total mismatches: 61103
Predicted valid destination but wrong order: 48572
Epoch 44: Validation Loss: 4.0971, Validation Log Prob: 0.0231
Epoch 44: Edge Precision: 0.3739, Recall: 0.3713, F1: 0.3725, Jaccard: 0.2462
Epoch 44: TP: 2.6010021474588405, FP: 4.377523264137437, FN: 4.420472440944882
Epoch 44: Current Learning Rate: 6e-05
[Epoch 44] ‚è±Ô∏è Total: 3287.61s | Current time: 2025-07-16 04:11:36 | üèãÔ∏è Train: 2805.20s | ‚úÖ Val: 482.41s
grad AddEdge W: 1.3084071614122874e-16
grad ChooseDest W: 10.457087516784668
grad AddEdge W: 2.162805655713005e-17
grad ChooseDest W: 3.424964427947998
grad AddEdge W: 1.231389760105001e-18
grad ChooseDest W: 7.624129772186279
grad AddEdge W: 1.7578212703550438e-16
grad ChooseDest W: 8.98410415649414
grad AddEdge W: 5.273892137594021e-19
grad ChooseDest W: 5.514605522155762
grad AddEdge W: 2.329561892306796e-16
grad ChooseDest W: 2.6115827560424805
grad AddEdge W: 1.6339665352086558e-14
grad ChooseDest W: 3.3106894493103027
grad AddEdge W: 6.394693764142062e-17
grad ChooseDest W: 8.953923225402832
grad AddEdge W: 8.093276726539692e-17
grad ChooseDest W: 7.137174606323242
grad AddEdge W: 1.4686260903633985e-18
grad ChooseDest W: 5.690594673156738
grad AddEdge W: 3.2468856039911986e-16
grad ChooseDest W: 2.66050124168396
grad AddEdge W: 7.584632035506443e-19
grad ChooseDest W: 4.102978706359863
grad AddEdge W: 3.3363484215150036e-18
grad ChooseDest W: 5.7881245613098145
grad AddEdge W: 4.302901233144489e-17
grad ChooseDest W: 2.3165645599365234
grad AddEdge W: 3.892002551405399e-14
grad ChooseDest W: 6.010983467102051
grad AddEdge W: 3.3246273239339155e-19
grad ChooseDest W: 2.579773426055908
grad AddEdge W: 9.001755958416881e-17
grad ChooseDest W: 4.3395609855651855
grad AddEdge W: 6.002890030352184e-19
grad ChooseDest W: 4.799466609954834
grad AddEdge W: 6.026113729891457e-16
grad ChooseDest W: 4.16357421875
grad AddEdge W: 6.122472946569561e-19
grad ChooseDest W: 5.158369064331055
grad AddEdge W: 1.0431437186129395e-18
grad ChooseDest W: 8.203116416931152
grad AddEdge W: 1.1201727418078323e-18
grad ChooseDest W: 5.175087928771973
grad AddEdge W: 1.0996209225010694e-18
grad ChooseDest W: 5.650251388549805
grad AddEdge W: 7.655385446189049e-19
grad ChooseDest W: 4.265097141265869
grad AddEdge W: 1.1329824607738284e-18
grad ChooseDest W: 3.497899055480957
grad AddEdge W: 2.1622969727235093e-16
grad ChooseDest W: 4.153831481933594
grad AddEdge W: 5.549151396351582e-17
grad ChooseDest W: 5.524481296539307
grad AddEdge W: 7.786522524600143e-19
grad ChooseDest W: 5.105696201324463
grad AddEdge W: 1.013569323377178e-18
grad ChooseDest W: 5.223202705383301
grad AddEdge W: 7.788101343555808e-17
grad ChooseDest W: 8.801661491394043
grad AddEdge W: 5.3393860030697695e-17
grad ChooseDest W: 3.3716769218444824
grad AddEdge W: 2.0408462653121948e-15
grad ChooseDest W: 1.94349205493927
grad AddEdge W: 1.1640665640250758e-18
grad ChooseDest W: 6.3437066078186035
grad AddEdge W: 1.2793463874342785e-16
grad ChooseDest W: 7.875870227813721
grad AddEdge W: 5.962499318029102e-19
grad ChooseDest W: 3.2994768619537354
grad AddEdge W: 1.8214548521280946e-17
grad ChooseDest W: 10.312767028808594
grad AddEdge W: 4.7216598466062955e-18
grad ChooseDest W: 5.822952747344971
grad AddEdge W: 2.8652349414657865e-18
grad ChooseDest W: 4.944155693054199
grad AddEdge W: 5.854502472497572e-17
grad ChooseDest W: 3.306788921356201
grad AddEdge W: 5.618326008758039e-19
grad ChooseDest W: 4.103315353393555
grad AddEdge W: 3.776248902894022e-17
grad ChooseDest W: 2.5785958766937256
grad AddEdge W: 1.4003765760745058e-17
grad ChooseDest W: 4.159139156341553
grad AddEdge W: 9.976225219380744e-19
grad ChooseDest W: 5.203869342803955
grad AddEdge W: 1.1519693319098234e-19
grad ChooseDest W: 4.641165733337402
grad AddEdge W: 5.335241166456389e-17
grad ChooseDest W: 3.791055679321289
grad AddEdge W: 1.0418599343022572e-18
grad ChooseDest W: 3.410051107406616
grad AddEdge W: 7.648444366873964e-19
grad ChooseDest W: 6.546774864196777
grad AddEdge W: 1.5540041576735187e-18
grad ChooseDest W: 5.352797508239746
grad AddEdge W: 2.1997467146890106e-17
grad ChooseDest W: 4.7206315994262695
grad AddEdge W: 2.943274572574066e-19
grad ChooseDest W: 8.765361785888672
grad AddEdge W: 1.3304850751824708e-16
grad ChooseDest W: 7.845676898956299
grad AddEdge W: 1.5118004184761343e-16
grad ChooseDest W: 3.762634754180908
grad AddEdge W: 3.0882126425177246e-17
grad ChooseDest W: 5.111936092376709
grad AddEdge W: 2.781529485053037e-19
grad ChooseDest W: 4.428947448730469
grad AddEdge W: 1.5612725937160221e-18
grad ChooseDest W: 3.2659358978271484
grad AddEdge W: 2.6732208854883928e-18
grad ChooseDest W: 9.762513160705566
grad AddEdge W: 2.3818028809392766e-17
grad ChooseDest W: 3.552039623260498
grad AddEdge W: 7.148280820699767e-19
grad ChooseDest W: 7.24024772644043
grad AddEdge W: 4.281407772107911e-17
grad ChooseDest W: 4.1483612060546875
grad AddEdge W: 5.145823382267615e-19
grad ChooseDest W: 3.947749137878418
grad AddEdge W: 3.1548910513101647e-18
grad ChooseDest W: 4.770031929016113
grad AddEdge W: 3.296608803731578e-18
grad ChooseDest W: 4.68978214263916
grad AddEdge W: 7.634280449847642e-19
grad ChooseDest W: 4.8416948318481445
grad AddEdge W: 5.65247254143211e-19
grad ChooseDest W: 5.26023006439209
grad AddEdge W: 2.631470598431004e-18
grad ChooseDest W: 3.049318552017212
grad AddEdge W: 2.0149345480738405e-16
grad ChooseDest W: 5.962607383728027
=== Epoch 45: Train Loss: 3.7786, Train Log Prob: 0.0557 ===
Total mismatches: 60443
Predicted valid destination but wrong order: 48742
Epoch 45: Validation Loss: 4.0313, Validation Log Prob: 0.0244
Epoch 45: Edge Precision: 0.3743, Recall: 0.3716, F1: 0.3728, Jaccard: 0.2468
Epoch 45: TP: 2.600715819613457, FP: 4.374230493915533, FN: 4.4207587687902645
Epoch 45: Current Learning Rate: 6e-05
[Epoch 45] ‚è±Ô∏è Total: 3290.51s | Current time: 2025-07-16 05:06:27 | üèãÔ∏è Train: 2808.99s | ‚úÖ Val: 481.53s
grad AddEdge W: 5.535682497155442e-18
grad ChooseDest W: 12.925625801086426
grad AddEdge W: 1.7703238819750174e-18
grad ChooseDest W: 4.860034465789795
grad AddEdge W: 1.3363821408951318e-18
grad ChooseDest W: 3.680389165878296
grad AddEdge W: 2.983751363379641e-19
grad ChooseDest W: 4.014530658721924
grad AddEdge W: 1.1558064418256979e-18
grad ChooseDest W: 4.0666704177856445
grad AddEdge W: 2.733982471359241e-18
grad ChooseDest W: 3.1378774642944336
grad AddEdge W: 5.266175059466784e-19
grad ChooseDest W: 5.657499313354492
grad AddEdge W: 5.905331831919797e-19
grad ChooseDest W: 5.282216548919678
grad AddEdge W: 1.535287671200252e-19
grad ChooseDest W: 7.098932266235352
grad AddEdge W: 1.1861047234950199e-18
grad ChooseDest W: 4.887959957122803
grad AddEdge W: 6.48203014019337e-17
grad ChooseDest W: 3.676201343536377
grad AddEdge W: 2.0054318177874925e-14
grad ChooseDest W: 1.300349235534668
grad AddEdge W: 3.3893970444821625e-17
grad ChooseDest W: 2.8357439041137695
grad AddEdge W: 1.3961326018647106e-18
grad ChooseDest W: 3.4720187187194824
grad AddEdge W: 1.9609203158186178e-17
grad ChooseDest W: 4.611673355102539
grad AddEdge W: 8.507047027317497e-17
grad ChooseDest W: 3.285614252090454
grad AddEdge W: 1.5793135062733802e-18
grad ChooseDest W: 2.327815532684326
grad AddEdge W: 3.6266355993600993e-14
grad ChooseDest W: 5.0106048583984375
grad AddEdge W: 8.515658970110909e-17
grad ChooseDest W: 4.083037853240967
grad AddEdge W: 4.32655345980395e-19
grad ChooseDest W: 6.724592208862305
grad AddEdge W: 3.3694672339767666e-19
grad ChooseDest W: 3.2717578411102295
grad AddEdge W: 2.416961134084659e-16
grad ChooseDest W: 6.903885364532471
grad AddEdge W: 2.5030154463609596e-16
grad ChooseDest W: 4.777471542358398
grad AddEdge W: 1.0574001018585997e-14
grad ChooseDest W: 6.3178935050964355
grad AddEdge W: 7.053767241664436e-15
grad ChooseDest W: 5.7030534744262695
grad AddEdge W: 3.166046102994228e-16
grad ChooseDest W: 3.671628475189209
grad AddEdge W: 5.612038629601538e-17
grad ChooseDest W: 1.6829369068145752
grad AddEdge W: 3.6155451944051668e-19
grad ChooseDest W: 4.591519832611084
grad AddEdge W: 4.653494191887903e-16
grad ChooseDest W: 5.35555362701416
grad AddEdge W: 5.282660769074966e-19
grad ChooseDest W: 7.642665386199951
grad AddEdge W: 2.111499488706189e-18
grad ChooseDest W: 7.564169883728027
grad AddEdge W: 8.367505628446742e-17
grad ChooseDest W: 4.989239692687988
grad AddEdge W: 6.254781519523183e-19
grad ChooseDest W: 7.30597448348999
grad AddEdge W: 6.788674304777573e-15
grad ChooseDest W: 4.427544116973877
grad AddEdge W: 2.596945423831536e-19
grad ChooseDest W: 6.837457656860352
grad AddEdge W: 4.006063003554527e-18
grad ChooseDest W: 4.15898323059082
grad AddEdge W: 1.6866710032299532e-18
grad ChooseDest W: 10.952433586120605
grad AddEdge W: 5.855542734835918e-17
grad ChooseDest W: 5.907673358917236
grad AddEdge W: 7.11564718068654e-17
grad ChooseDest W: 6.1755852699279785
grad AddEdge W: 5.893461488208737e-16
grad ChooseDest W: 4.44666051864624
grad AddEdge W: 2.1652823585440806e-18
grad ChooseDest W: 6.186713218688965
grad AddEdge W: 8.82360044506317e-17
grad ChooseDest W: 2.725792169570923
grad AddEdge W: 1.190522591749089e-18
grad ChooseDest W: 4.094274044036865
grad AddEdge W: 1.924838325267779e-18
grad ChooseDest W: 8.275629997253418
grad AddEdge W: 2.5969371065304766e-16
grad ChooseDest W: 7.005764961242676
grad AddEdge W: 5.4182093671289276e-17
grad ChooseDest W: 4.826004981994629
grad AddEdge W: 7.239140084533684e-16
grad ChooseDest W: 8.025075912475586
grad AddEdge W: 6.73830856106353e-17
grad ChooseDest W: 4.362948894500732
grad AddEdge W: 5.839210632667488e-19
grad ChooseDest W: 7.669787406921387
grad AddEdge W: 1.3862879118044107e-18
grad ChooseDest W: 5.9321513175964355
grad AddEdge W: 5.308316292761181e-19
grad ChooseDest W: 4.8727498054504395
grad AddEdge W: 3.263508499436021e-19
grad ChooseDest W: 6.515336513519287
grad AddEdge W: 1.3585787054523797e-18
grad ChooseDest W: 5.288536548614502
grad AddEdge W: 1.5305912240255115e-18
grad ChooseDest W: 4.338958740234375
grad AddEdge W: 2.3494026299358808e-18
grad ChooseDest W: 5.548089504241943
grad AddEdge W: 3.3563456918253576e-13
grad ChooseDest W: 4.233364582061768
grad AddEdge W: 2.6656937884139828e-15
grad ChooseDest W: 10.178699493408203
grad AddEdge W: 5.623834225136544e-17
grad ChooseDest W: 2.977879762649536
grad AddEdge W: 3.333794294578593e-17
grad ChooseDest W: 5.978786945343018
grad AddEdge W: 8.4682832345027e-19
grad ChooseDest W: 6.458922863006592
grad AddEdge W: 1.9876847834766204e-14
grad ChooseDest W: 5.917885780334473
grad AddEdge W: 6.910529984100362e-19
grad ChooseDest W: 5.362486362457275
grad AddEdge W: 1.7034129061834792e-19
grad ChooseDest W: 8.66586971282959
grad AddEdge W: 2.634817412548424e-17
grad ChooseDest W: 4.904705047607422
grad AddEdge W: 4.1090773370061023e-19
grad ChooseDest W: 5.304181098937988
grad AddEdge W: 4.501618802165043e-17
grad ChooseDest W: 4.67128849029541
=== Epoch 46: Train Loss: 3.7585, Train Log Prob: 0.0564 ===
Total mismatches: 60112
Predicted valid destination but wrong order: 48838
Epoch 46: Validation Loss: 4.0293, Validation Log Prob: 0.0244
Epoch 46: Edge Precision: 0.3741, Recall: 0.3709, F1: 0.3723, Jaccard: 0.2473
Epoch 46: TP: 2.5965640658554046, FP: 4.369506084466714, FN: 4.424910522548318
Epoch 46: Current Learning Rate: 6e-05
[Epoch 46] ‚è±Ô∏è Total: 3305.04s | Current time: 2025-07-16 06:01:32 | üèãÔ∏è Train: 2820.76s | ‚úÖ Val: 484.28s
grad AddEdge W: 2.5483612807459635e-18
grad ChooseDest W: 12.837631225585938
grad AddEdge W: 2.4599637974048716e-18
grad ChooseDest W: 5.169632434844971
grad AddEdge W: 8.570150009950723e-19
grad ChooseDest W: 5.3967719078063965
grad AddEdge W: 6.77746265905036e-17
grad ChooseDest W: 7.233656406402588
grad AddEdge W: 6.308125880238845e-19
grad ChooseDest W: 7.936968803405762
grad AddEdge W: 2.3261311441726202e-17
grad ChooseDest W: 3.9851672649383545
grad AddEdge W: 1.0943483698790799e-18
grad ChooseDest W: 5.931278228759766
grad AddEdge W: 9.064074546354738e-19
grad ChooseDest W: 3.772285223007202
grad AddEdge W: 1.0625385061131967e-19
grad ChooseDest W: 3.179576873779297
grad AddEdge W: 8.658014168591871e-19
grad ChooseDest W: 5.5117106437683105
grad AddEdge W: 1.4519317761444885e-19
grad ChooseDest W: 7.884519100189209
grad AddEdge W: 2.055515764227896e-16
grad ChooseDest W: 5.498932361602783
grad AddEdge W: 5.926162204297837e-18
grad ChooseDest W: 2.4172868728637695
grad AddEdge W: 1.9350864726718517e-18
grad ChooseDest W: 5.506048202514648
grad AddEdge W: 3.7694000128582056e-19
grad ChooseDest W: 7.090909004211426
grad AddEdge W: 3.936976051613486e-17
grad ChooseDest W: 2.1047747135162354
grad AddEdge W: 3.788376879451487e-14
grad ChooseDest W: 3.167869806289673
grad AddEdge W: 1.2333035293061068e-16
grad ChooseDest W: 2.5928683280944824
grad AddEdge W: 2.9911531305971306e-18
grad ChooseDest W: 4.6454243659973145
grad AddEdge W: 3.037817697673697e-19
grad ChooseDest W: 4.210410118103027
grad AddEdge W: 1.1656180446614956e-18
grad ChooseDest W: 4.632818698883057
grad AddEdge W: 5.633774640673232e-19
grad ChooseDest W: 8.0567045211792
grad AddEdge W: 4.596164553563119e-17
grad ChooseDest W: 3.9527640342712402
grad AddEdge W: 3.1751165685447907e-17
grad ChooseDest W: 4.035548686981201
grad AddEdge W: 4.446939516697328e-17
grad ChooseDest W: 6.261293411254883
grad AddEdge W: 3.569186890950398e-19
grad ChooseDest W: 5.7460527420043945
grad AddEdge W: 1.0671639062281372e-18
grad ChooseDest W: 7.231630802154541
grad AddEdge W: 1.5057162486448169e-18
grad ChooseDest W: 8.249271392822266
grad AddEdge W: 4.8410013294823835e-18
grad ChooseDest W: 4.467833518981934
grad AddEdge W: 4.461267052530355e-15
grad ChooseDest W: 2.5505855083465576
grad AddEdge W: 2.30375322026607e-18
grad ChooseDest W: 3.9790382385253906
grad AddEdge W: 5.600319320798525e-19
grad ChooseDest W: 3.916951894760132
grad AddEdge W: 5.928592150744788e-19
grad ChooseDest W: 3.933323621749878
grad AddEdge W: 9.622123550730421e-17
grad ChooseDest W: 6.754679203033447
grad AddEdge W: 8.35734454180214e-17
grad ChooseDest W: 4.892895698547363
grad AddEdge W: 7.908566634012111e-17
grad ChooseDest W: 6.26416015625
grad AddEdge W: 4.1835155788236906e-17
grad ChooseDest W: 4.290762424468994
grad AddEdge W: 9.330023021259914e-15
grad ChooseDest W: 5.889321327209473
grad AddEdge W: 1.5816416061074107e-18
grad ChooseDest W: 3.9917187690734863
grad AddEdge W: 1.4069737343704071e-19
grad ChooseDest W: 4.335186958312988
grad AddEdge W: 1.0253486027746015e-16
grad ChooseDest W: 1.6587579250335693
grad AddEdge W: 7.950641361985258e-19
grad ChooseDest W: 6.046146392822266
grad AddEdge W: 4.807647649425444e-19
grad ChooseDest W: 4.498623847961426
grad AddEdge W: 6.854542657250501e-17
grad ChooseDest W: 5.646989822387695
grad AddEdge W: 3.0966960000849153e-18
grad ChooseDest W: 6.16892671585083
grad AddEdge W: 9.214362923897966e-19
grad ChooseDest W: 3.6677961349487305
grad AddEdge W: 2.040893419570194e-18
grad ChooseDest W: 4.171909809112549
grad AddEdge W: 6.918478821691624e-16
grad ChooseDest W: 7.6233673095703125
grad AddEdge W: 7.776248010814046e-18
grad ChooseDest W: 7.228989124298096
grad AddEdge W: 4.275908551318567e-19
grad ChooseDest W: 4.6186323165893555
grad AddEdge W: 7.128350027917394e-17
grad ChooseDest W: 2.5290732383728027
grad AddEdge W: 1.423769210710281e-16
grad ChooseDest W: 3.4326131343841553
grad AddEdge W: 1.8927050468070847e-18
grad ChooseDest W: 4.7912702560424805
grad AddEdge W: 2.3771858962496955e-15
grad ChooseDest W: 3.3746960163116455
grad AddEdge W: 7.860398252581923e-17
grad ChooseDest W: 6.378061771392822
grad AddEdge W: 2.07807278100887e-16
grad ChooseDest W: 6.754807949066162
grad AddEdge W: 1.708904058859924e-16
grad ChooseDest W: 4.047294616699219
grad AddEdge W: 1.1835819260243097e-18
grad ChooseDest W: 3.6645145416259766
grad AddEdge W: 2.9716010624582146e-18
grad ChooseDest W: 4.174468040466309
grad AddEdge W: 1.0733611304526874e-19
grad ChooseDest W: 6.1679582595825195
grad AddEdge W: 5.052932695198166e-17
grad ChooseDest W: 9.694345474243164
grad AddEdge W: 7.100422735396863e-19
grad ChooseDest W: 4.40051794052124
grad AddEdge W: 1.3533647850465691e-15
grad ChooseDest W: 3.3937156200408936
grad AddEdge W: 1.795810350623542e-18
grad ChooseDest W: 7.223846435546875
grad AddEdge W: 1.7717302130934495e-16
grad ChooseDest W: 3.8731260299682617
grad AddEdge W: 5.334867942564005e-17
grad ChooseDest W: 6.80564022064209
=== Epoch 47: Train Loss: 3.7314, Train Log Prob: 0.0578 ===
Total mismatches: 59444
Predicted valid destination but wrong order: 48995
Epoch 47: Validation Loss: 3.9351, Validation Log Prob: 0.0266
Epoch 47: Edge Precision: 0.3726, Recall: 0.3696, F1: 0.3710, Jaccard: 0.2454
Epoch 47: TP: 2.588117394416607, FP: 4.382963493199713, FN: 4.433357193987115
Epoch 47: Current Learning Rate: 6e-05
[Epoch 47] ‚è±Ô∏è Total: 3279.73s | Current time: 2025-07-16 06:56:12 | üèãÔ∏è Train: 2798.81s | ‚úÖ Val: 480.92s
grad AddEdge W: 1.302863992516998e-16
grad ChooseDest W: 11.4391450881958
grad AddEdge W: 4.598303642627181e-17
grad ChooseDest W: 3.52819561958313
grad AddEdge W: 7.509772707058277e-19
grad ChooseDest W: 2.691774368286133
grad AddEdge W: 1.0357733945984848e-16
grad ChooseDest W: 5.4262919425964355
grad AddEdge W: 1.0117582114259931e-18
grad ChooseDest W: 5.08618688583374
grad AddEdge W: 3.714372081102057e-19
grad ChooseDest W: 2.8921761512756348
grad AddEdge W: 9.503112774663232e-17
grad ChooseDest W: 3.929786443710327
grad AddEdge W: 2.7664807432652245e-18
grad ChooseDest W: 6.879140377044678
grad AddEdge W: 1.3486606031127158e-18
grad ChooseDest W: 7.790805339813232
grad AddEdge W: 8.443010281849791e-19
grad ChooseDest W: 3.8159806728363037
grad AddEdge W: 1.233372046744196e-19
grad ChooseDest W: 4.417168140411377
grad AddEdge W: 4.601892309466461e-15
grad ChooseDest W: 4.35733699798584
grad AddEdge W: 6.446562576291337e-13
grad ChooseDest W: 4.073709964752197
grad AddEdge W: 4.251295585396898e-18
grad ChooseDest W: 4.991273403167725
grad AddEdge W: 5.968251718124434e-17
grad ChooseDest W: 4.8136701583862305
grad AddEdge W: 1.2190571175572948e-18
grad ChooseDest W: 3.422837018966675
grad AddEdge W: 1.0690858603814041e-18
grad ChooseDest W: 3.4107487201690674
grad AddEdge W: 5.776604558107738e-17
grad ChooseDest W: 2.7179150581359863
grad AddEdge W: 8.232628266780351e-19
grad ChooseDest W: 5.454155921936035
grad AddEdge W: 8.801120097469417e-13
grad ChooseDest W: 1.2819429636001587
grad AddEdge W: 6.546862095742954e-19
grad ChooseDest W: 8.903021812438965
grad AddEdge W: 2.7329333995473707e-19
grad ChooseDest W: 3.8177425861358643
grad AddEdge W: 1.6392039675616337e-16
grad ChooseDest W: 4.11598014831543
grad AddEdge W: 2.451011064470894e-16
grad ChooseDest W: 3.9663174152374268
grad AddEdge W: 1.826851874752758e-17
grad ChooseDest W: 2.6924424171447754
grad AddEdge W: 3.5310391512365577e-17
grad ChooseDest W: 11.519396781921387
grad AddEdge W: 5.9176972514992755e-18
grad ChooseDest W: 6.5629191398620605
grad AddEdge W: 1.1509855298181622e-18
grad ChooseDest W: 3.6020278930664062
grad AddEdge W: 6.326459304540317e-19
grad ChooseDest W: 4.943617820739746
grad AddEdge W: 1.4020880954799392e-18
grad ChooseDest W: 9.771429061889648
grad AddEdge W: 3.506440785052701e-17
grad ChooseDest W: 5.889009475708008
grad AddEdge W: 1.7158531700987574e-17
grad ChooseDest W: 5.281558513641357
grad AddEdge W: 2.276529258740878e-18
grad ChooseDest W: 7.474950313568115
grad AddEdge W: 4.889598273187935e-16
grad ChooseDest W: 5.136079788208008
grad AddEdge W: 1.0907256496683426e-16
grad ChooseDest W: 6.9036993980407715
grad AddEdge W: 6.665184524364421e-15
grad ChooseDest W: 3.332592487335205
grad AddEdge W: 2.371538189922953e-19
grad ChooseDest W: 4.396612167358398
grad AddEdge W: 6.888106999529943e-17
grad ChooseDest W: 5.157417297363281
grad AddEdge W: 1.714421320458428e-17
grad ChooseDest W: 7.317873954772949
grad AddEdge W: 6.043191634643778e-17
grad ChooseDest W: 5.03877067565918
grad AddEdge W: 4.797842895545856e-13
grad ChooseDest W: 2.905705451965332
grad AddEdge W: 2.422986239084337e-19
grad ChooseDest W: 6.542135715484619
grad AddEdge W: 1.4996292333121923e-18
grad ChooseDest W: 10.990229606628418
grad AddEdge W: 2.242893251601116e-19
grad ChooseDest W: 9.066376686096191
grad AddEdge W: 1.1510258548730242e-18
grad ChooseDest W: 5.157037258148193
grad AddEdge W: 3.876895442181922e-18
grad ChooseDest W: 3.9887819290161133
grad AddEdge W: 2.12180223003069e-18
grad ChooseDest W: 2.9812068939208984
grad AddEdge W: 3.2803759632366535e-17
grad ChooseDest W: 3.0216116905212402
grad AddEdge W: 5.538048130309767e-19
grad ChooseDest W: 8.906905174255371
grad AddEdge W: 5.880524168361449e-19
grad ChooseDest W: 4.563394546508789
grad AddEdge W: 4.849263623030869e-19
grad ChooseDest W: 3.7756130695343018
grad AddEdge W: 1.1626968563282646e-18
grad ChooseDest W: 4.479628562927246
grad AddEdge W: 8.572860303590613e-13
grad ChooseDest W: 2.9081146717071533
grad AddEdge W: 1.2474731497427524e-18
grad ChooseDest W: 4.844280242919922
grad AddEdge W: 5.969766451262141e-17
grad ChooseDest W: 3.4547784328460693
grad AddEdge W: 3.281979926001398e-15
grad ChooseDest W: 8.450915336608887
grad AddEdge W: 1.0462623963174176e-18
grad ChooseDest W: 2.463181495666504
grad AddEdge W: 1.9128894448934607e-16
grad ChooseDest W: 7.54604434967041
grad AddEdge W: 5.753698251905404e-19
grad ChooseDest W: 4.237619876861572
grad AddEdge W: 5.464259483766974e-19
grad ChooseDest W: 5.915399551391602
grad AddEdge W: 2.8459055917068005e-18
grad ChooseDest W: 3.986457347869873
grad AddEdge W: 1.1948138227872794e-16
grad ChooseDest W: 5.491740703582764
grad AddEdge W: 2.494564245440082e-19
grad ChooseDest W: 7.762302875518799
grad AddEdge W: 7.948199628214578e-19
grad ChooseDest W: 3.1385512351989746
grad AddEdge W: 1.2877762181440309e-16
grad ChooseDest W: 4.221165180206299
grad AddEdge W: 1.1788466693963846e-14
grad ChooseDest W: 3.1242713928222656
=== Epoch 48: Train Loss: 3.7072, Train Log Prob: 0.0591 ===
Total mismatches: 59100
Predicted valid destination but wrong order: 49058
Epoch 48: Validation Loss: 3.9673, Validation Log Prob: 0.0259
Epoch 48: Edge Precision: 0.3729, Recall: 0.3697, F1: 0.3712, Jaccard: 0.2452
Epoch 48: TP: 2.589835361488905, FP: 4.379098067287043, FN: 4.431639226914817
Epoch 48: Current Learning Rate: 6e-05
[Epoch 48] ‚è±Ô∏è Total: 3289.36s | Current time: 2025-07-16 07:51:01 | üèãÔ∏è Train: 2807.36s | ‚úÖ Val: 481.99s
grad AddEdge W: 1.8746358488086804e-16
grad ChooseDest W: 11.443031311035156
grad AddEdge W: 5.836785339111482e-18
grad ChooseDest W: 5.032027721405029
grad AddEdge W: 1.1675095999272512e-18
grad ChooseDest W: 6.50283670425415
grad AddEdge W: 3.637759346890189e-17
grad ChooseDest W: 2.39785099029541
grad AddEdge W: 2.706949481022069e-16
grad ChooseDest W: 2.998117446899414
grad AddEdge W: 3.577848957993799e-15
grad ChooseDest W: 6.104626178741455
grad AddEdge W: 5.346183587228092e-19
grad ChooseDest W: 2.7733983993530273
grad AddEdge W: 1.1015441132893522e-16
grad ChooseDest W: 4.660339832305908
grad AddEdge W: 9.553143015575184e-19
grad ChooseDest W: 4.534999370574951
grad AddEdge W: 6.470376819723723e-17
grad ChooseDest W: 3.8694403171539307
grad AddEdge W: 1.781536935563633e-18
grad ChooseDest W: 3.384814739227295
grad AddEdge W: 2.186939940485995e-16
grad ChooseDest W: 9.344027519226074
grad AddEdge W: 4.351879920702734e-18
grad ChooseDest W: 3.400965452194214
grad AddEdge W: 3.5311135974916874e-17
grad ChooseDest W: 8.69136905670166
grad AddEdge W: 2.968482881062104e-17
grad ChooseDest W: 4.705815315246582
grad AddEdge W: 5.482244458235409e-19
grad ChooseDest W: 3.0574698448181152
grad AddEdge W: 1.163902368673484e-18
grad ChooseDest W: 5.715059757232666
grad AddEdge W: 1.537754434422292e-16
grad ChooseDest W: 3.3045191764831543
grad AddEdge W: 5.619109369477642e-17
grad ChooseDest W: 6.145657539367676
grad AddEdge W: 8.751721841272824e-19
grad ChooseDest W: 5.244239807128906
grad AddEdge W: 1.305955646230864e-18
grad ChooseDest W: 3.50826096534729
grad AddEdge W: 4.7147294172415965e-19
grad ChooseDest W: 7.658065319061279
grad AddEdge W: 1.092368306287781e-18
grad ChooseDest W: 5.1287689208984375
grad AddEdge W: 4.544497529014077e-17
grad ChooseDest W: 8.390999794006348
grad AddEdge W: 2.6515142703477176e-19
grad ChooseDest W: 6.435703277587891
grad AddEdge W: 1.0486103774374708e-16
grad ChooseDest W: 7.948394298553467
grad AddEdge W: 3.7774257327515014e-19
grad ChooseDest W: 6.577795505523682
grad AddEdge W: 2.8167720320387414e-19
grad ChooseDest W: 2.7834904193878174
grad AddEdge W: 2.3458066690179596e-18
grad ChooseDest W: 2.0202345848083496
grad AddEdge W: 4.234996322298786e-17
grad ChooseDest W: 6.347968101501465
grad AddEdge W: 2.7719925785589103e-17
grad ChooseDest W: 2.1333653926849365
grad AddEdge W: 5.48251372620431e-17
grad ChooseDest W: 4.294065475463867
grad AddEdge W: 1.3085692267738019e-18
grad ChooseDest W: 4.446436882019043
grad AddEdge W: 3.732942440950237e-18
grad ChooseDest W: 3.4635159969329834
grad AddEdge W: 1.1063965532859352e-16
grad ChooseDest W: 4.864193439483643
grad AddEdge W: 6.070133610298151e-19
grad ChooseDest W: 2.8067121505737305
grad AddEdge W: 5.178049718547222e-18
grad ChooseDest W: 3.9909396171569824
grad AddEdge W: 8.27890850506481e-19
grad ChooseDest W: 5.987112045288086
grad AddEdge W: 8.571520027840264e-19
grad ChooseDest W: 5.435366153717041
grad AddEdge W: 3.4249942658350145e-17
grad ChooseDest W: 7.1500749588012695
grad AddEdge W: 1.845508238637177e-16
grad ChooseDest W: 2.8699228763580322
grad AddEdge W: 2.0408074703346709e-19
grad ChooseDest W: 4.228853225708008
grad AddEdge W: 1.961416251954874e-18
grad ChooseDest W: 4.276394844055176
grad AddEdge W: 2.0121922789071047e-17
grad ChooseDest W: 3.044755220413208
grad AddEdge W: 2.638038288417583e-17
grad ChooseDest W: 3.136155128479004
grad AddEdge W: 9.109455742710929e-19
grad ChooseDest W: 4.795499801635742
grad AddEdge W: 6.856772033078423e-19
grad ChooseDest W: 3.946347951889038
grad AddEdge W: 3.622149197620637e-19
grad ChooseDest W: 4.55076265335083
grad AddEdge W: 1.1849758287540381e-18
grad ChooseDest W: 6.772854328155518
grad AddEdge W: 8.242408643548025e-19
grad ChooseDest W: 7.385394096374512
grad AddEdge W: 7.279045791912304e-17
grad ChooseDest W: 5.489809989929199
grad AddEdge W: 3.2020055632809295e-16
grad ChooseDest W: 1.7048609256744385
grad AddEdge W: 3.673292048155262e-16
grad ChooseDest W: 3.963878631591797
grad AddEdge W: 1.3951117575912436e-18
grad ChooseDest W: 7.253866672515869
grad AddEdge W: 1.3750229526323573e-18
grad ChooseDest W: 3.879746675491333
grad AddEdge W: 6.313479848112625e-17
grad ChooseDest W: 5.131870269775391
grad AddEdge W: 6.668256712864112e-14
grad ChooseDest W: 7.560214042663574
grad AddEdge W: 7.928137065560625e-17
grad ChooseDest W: 4.455117702484131
grad AddEdge W: 2.3194012027088888e-16
grad ChooseDest W: 3.771479845046997
grad AddEdge W: 3.566617978160538e-19
grad ChooseDest W: 7.0944905281066895
grad AddEdge W: 7.97427055166466e-20
grad ChooseDest W: 7.338363170623779
grad AddEdge W: 1.2523138603182495e-16
grad ChooseDest W: 3.901669502258301
grad AddEdge W: 2.7581448306422213e-19
grad ChooseDest W: 3.925457000732422
grad AddEdge W: 4.319934619036242e-18
grad ChooseDest W: 3.948322296142578
grad AddEdge W: 8.761436043591493e-19
grad ChooseDest W: 7.150921821594238
grad AddEdge W: 1.8818109488927907e-19
grad ChooseDest W: 7.502513885498047
=== Epoch 49: Train Loss: 3.6613, Train Log Prob: 0.0610 ===
Total mismatches: 58416
Predicted valid destination but wrong order: 49255
Epoch 49: Validation Loss: 3.9408, Validation Log Prob: 0.0264
Epoch 49: Edge Precision: 0.3712, Recall: 0.3684, F1: 0.3697, Jaccard: 0.2442
Epoch 49: TP: 2.5799570508231926, FP: 4.394130279169649, FN: 4.44151753758053
Epoch 49: Current Learning Rate: 6e-05
[Epoch 49] ‚è±Ô∏è Total: 3293.78s | Current time: 2025-07-16 08:45:55 | üèãÔ∏è Train: 2812.13s | ‚úÖ Val: 481.65s
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:3638: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
grad AddEdge W: 5.964262722338458e-17
grad ChooseDest W: 11.290111541748047
grad AddEdge W: 6.118489037944352e-19
grad ChooseDest W: 3.622199773788452
grad AddEdge W: 5.814095692191091e-17
grad ChooseDest W: 5.578514099121094
grad AddEdge W: 1.9383477570418528e-13
grad ChooseDest W: 0.5525214076042175
grad AddEdge W: 8.985315578306267e-17
grad ChooseDest W: 6.313733100891113
grad AddEdge W: 2.112454840954657e-17
grad ChooseDest W: 5.072593688964844
grad AddEdge W: 4.431001731526901e-17
grad ChooseDest W: 8.965222358703613
grad AddEdge W: 2.7025068014855264e-18
grad ChooseDest W: 5.719194412231445
grad AddEdge W: 1.3212507327448524e-18
grad ChooseDest W: 4.377755641937256
grad AddEdge W: 4.818178175611127e-19
grad ChooseDest W: 7.4227142333984375
grad AddEdge W: 8.81364768385584e-19
grad ChooseDest W: 4.2266082763671875
grad AddEdge W: 9.257527276212515e-19
grad ChooseDest W: 5.808146953582764
grad AddEdge W: 5.687449046453303e-17
grad ChooseDest W: 2.890023708343506
grad AddEdge W: 6.460459461846611e-19
grad ChooseDest W: 4.30903959274292
grad AddEdge W: 5.718977995030272e-16
grad ChooseDest W: 3.311306953430176
grad AddEdge W: 5.065734721384465e-19
grad ChooseDest W: 5.743450164794922
grad AddEdge W: 8.522558173407484e-19
grad ChooseDest W: 5.248363018035889
grad AddEdge W: 1.3469825638425754e-18
grad ChooseDest W: 3.4852964878082275
grad AddEdge W: 4.0488619488334803e-19
grad ChooseDest W: 6.199133396148682
grad AddEdge W: 7.594570858720452e-17
grad ChooseDest W: 5.264050006866455
grad AddEdge W: 4.192673295385265e-18
grad ChooseDest W: 7.093903064727783
grad AddEdge W: 1.8520373323763746e-18
grad ChooseDest W: 6.156462669372559
grad AddEdge W: 8.138595636195757e-17
grad ChooseDest W: 4.4528889656066895
grad AddEdge W: 5.5199773978913266e-17
grad ChooseDest W: 3.826289176940918
grad AddEdge W: 1.3169194289418869e-17
grad ChooseDest W: 2.4800045490264893
grad AddEdge W: 3.582655717768234e-18
grad ChooseDest W: 7.67242431640625
grad AddEdge W: 1.1335607634195796e-18
grad ChooseDest W: 4.719645023345947
grad AddEdge W: 3.1581580424984735e-17
grad ChooseDest W: 5.220656394958496
grad AddEdge W: 4.4038134250357075e-11
grad ChooseDest W: 1.0170639753341675
grad AddEdge W: 4.944847982406067e-17
grad ChooseDest W: 7.262277126312256
grad AddEdge W: 8.971113218060977e-17
grad ChooseDest W: 3.49395751953125
grad AddEdge W: 1.0465410362426591e-16
grad ChooseDest W: 4.549532890319824
grad AddEdge W: 5.054462317586899e-17
grad ChooseDest W: 8.815702438354492
grad AddEdge W: 4.509306949650356e-14
grad ChooseDest W: 2.0346059799194336
grad AddEdge W: 2.6852040442272954e-19
grad ChooseDest W: 9.110804557800293
grad AddEdge W: 4.894512987476934e-19
grad ChooseDest W: 3.3575501441955566
grad AddEdge W: 1.4088467814699569e-18
grad ChooseDest W: 5.312265396118164
grad AddEdge W: 5.3714697747708025e-18
grad ChooseDest W: 2.8280856609344482
grad AddEdge W: 1.2141077857596604e-18
grad ChooseDest W: 5.715503215789795
grad AddEdge W: 1.176674967909492e-18
grad ChooseDest W: 7.153961181640625
grad AddEdge W: 3.8474097620668566e-18
grad ChooseDest W: 4.6711907386779785
grad AddEdge W: 4.735064308432717e-18
grad ChooseDest W: 4.899677753448486
grad AddEdge W: 4.556459884160574e-18
grad ChooseDest W: 6.239245891571045
grad AddEdge W: 5.217081373123879e-19
grad ChooseDest W: 3.0435938835144043
grad AddEdge W: 1.2527642105309479e-18
grad ChooseDest W: 5.70303201675415
grad AddEdge W: 5.992705824009462e-17
grad ChooseDest W: 9.685216903686523
grad AddEdge W: 8.298691563389782e-19
grad ChooseDest W: 3.4745888710021973
grad AddEdge W: 4.6081666133790185e-17
grad ChooseDest W: 7.6228928565979
grad AddEdge W: 2.9403789234422475e-17
grad ChooseDest W: 3.162296772003174
grad AddEdge W: 5.560809018160414e-17
grad ChooseDest W: 8.317431449890137
grad AddEdge W: 3.949079358336362e-15
grad ChooseDest W: 6.233713626861572
grad AddEdge W: 3.153464009657146e-19
grad ChooseDest W: 5.4735894203186035
grad AddEdge W: 1.3603271047054238e-16
grad ChooseDest W: 2.082608938217163
grad AddEdge W: 1.865035855317186e-19
grad ChooseDest W: 3.5405306816101074
grad AddEdge W: 8.235999131174081e-17
grad ChooseDest W: 4.112972736358643
grad AddEdge W: 5.632641382554519e-17
grad ChooseDest W: 6.2652482986450195
grad AddEdge W: 3.3751631479760184e-19
grad ChooseDest W: 8.546939849853516
grad AddEdge W: 2.349488398225645e-19
grad ChooseDest W: 5.442385673522949
grad AddEdge W: 5.2120916090026e-13
grad ChooseDest W: 4.092832088470459
grad AddEdge W: 6.272797513264588e-19
grad ChooseDest W: 5.725215435028076
grad AddEdge W: 1.6921629502892992e-14
grad ChooseDest W: 5.358055114746094
grad AddEdge W: 2.1471209878100195e-18
grad ChooseDest W: 5.153951168060303
grad AddEdge W: 2.4323897316854164e-18
grad ChooseDest W: 4.597339630126953
grad AddEdge W: 6.644759727739702e-17
grad ChooseDest W: 4.367719650268555
grad AddEdge W: 1.3142780137713366e-18
grad ChooseDest W: 4.294567584991455
grad AddEdge W: 1.1133761013831789e-12
grad ChooseDest W: 2.4446024894714355
=== Epoch 50: Train Loss: 3.6388, Train Log Prob: 0.0620 ===
Total mismatches: 57966
Predicted valid destination but wrong order: 49416
Epoch 50: Validation Loss: 3.8778, Validation Log Prob: 0.0282
Epoch 50: Edge Precision: 0.3770, Recall: 0.3742, F1: 0.3755, Jaccard: 0.2489
Epoch 50: TP: 2.619899785254116, FP: 4.353758052970651, FN: 4.4015748031496065
Epoch 50: Current Learning Rate: 6e-05
üíæ Checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/checkpoint_epoch_50.pth
[Epoch 50] ‚è±Ô∏è Total: 3301.40s | Current time: 2025-07-16 09:40:56 | üèãÔ∏è Train: 2814.25s | ‚úÖ Val: 487.15s
Training finished at: 2025-07-16 09:40:56
Training time: 166049.15249538422
‚úÖ Model saved to: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/model.pth
üìà Metrics saved to: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC
Device for model: cuda:3

Epoch-wise Validation Metrics:

Epoch 1:
  Validation Loss: 7.1203, Validation Log Prob: 0.0014
  Edge Precision: 0.3770, Recall: 0.3746, F1: 0.3757, Jaccard: 0.2487
  TP: 2.6231925554760203, FP: 4.359198282032928, FN: 4.398282032927702

Epoch 2:
  Validation Loss: 6.8909, Validation Log Prob: 0.0018
  Edge Precision: 0.3796, Recall: 0.3774, F1: 0.3784, Jaccard: 0.2510
  TP: 2.643092340730136, FP: 4.3440229062276305, FN: 4.378382247673586

Epoch 3:
  Validation Loss: 6.6188, Validation Log Prob: 0.0022
  Edge Precision: 0.3765, Recall: 0.3751, F1: 0.3757, Jaccard: 0.2486
  TP: 2.62705798138869, FP: 4.37294201861131, FN: 4.394416607015033

Epoch 4:
  Validation Loss: 6.7102, Validation Log Prob: 0.0021
  Edge Precision: 0.3763, Recall: 0.3740, F1: 0.3751, Jaccard: 0.2483
  TP: 2.6197566213314243, FP: 4.363636363636363, FN: 4.4017179670722975

Epoch 5:
  Validation Loss: 6.3180, Validation Log Prob: 0.0030
  Edge Precision: 0.3789, Recall: 0.3770, F1: 0.3779, Jaccard: 0.2507
  TP: 2.641088045812455, FP: 4.350894774516822, FN: 4.380386542591267

Epoch 6:
  Validation Loss: 6.4447, Validation Log Prob: 0.0027
  Edge Precision: 0.3790, Recall: 0.3768, F1: 0.3778, Jaccard: 0.2506
  TP: 2.6402290622763065, FP: 4.348174659985683, FN: 4.381245526127416

Epoch 7:
  Validation Loss: 6.2905, Validation Log Prob: 0.0031
  Edge Precision: 0.3772, Recall: 0.3751, F1: 0.3760, Jaccard: 0.2492
  TP: 2.6277738010021476, FP: 4.358911954187545, FN: 4.393700787401575

Epoch 8:
  Validation Loss: 6.3083, Validation Log Prob: 0.0031
  Edge Precision: 0.3791, Recall: 0.3770, F1: 0.3779, Jaccard: 0.2511
  TP: 2.640801717967072, FP: 4.345454545454546, FN: 4.3806728704366495

Epoch 9:
  Validation Loss: 6.0553, Validation Log Prob: 0.0039
  Edge Precision: 0.3810, Recall: 0.3791, F1: 0.3800, Jaccard: 0.2525
  TP: 2.655261274158912, FP: 4.336005726556908, FN: 4.366213314244811

Epoch 10:
  Validation Loss: 6.0239, Validation Log Prob: 0.0039
  Edge Precision: 0.3784, Recall: 0.3757, F1: 0.3769, Jaccard: 0.2502
  TP: 2.6319255547602003, FP: 4.343450250536865, FN: 4.3895490336435214

Epoch 11:
  Validation Loss: 6.0367, Validation Log Prob: 0.0038
  Edge Precision: 0.3793, Recall: 0.3772, F1: 0.3782, Jaccard: 0.2509
  TP: 2.6418038654259126, FP: 4.345168217609163, FN: 4.37967072297781

Epoch 12:
  Validation Loss: 5.9138, Validation Log Prob: 0.0043
  Edge Precision: 0.3793, Recall: 0.3767, F1: 0.3779, Jaccard: 0.2510
  TP: 2.6382247673586257, FP: 4.340730136005726, FN: 4.383249821045097

Epoch 13:
  Validation Loss: 5.8936, Validation Log Prob: 0.0045
  Edge Precision: 0.3810, Recall: 0.3794, F1: 0.3801, Jaccard: 0.2522
  TP: 2.657408732999284, FP: 4.337294201861131, FN: 4.364065855404438

Epoch 14:
  Validation Loss: 5.7643, Validation Log Prob: 0.0050
  Edge Precision: 0.3817, Recall: 0.3796, F1: 0.3806, Jaccard: 0.2530
  TP: 2.659699355762348, FP: 4.32655690765927, FN: 4.361775232641374

Epoch 15:
  Validation Loss: 5.7658, Validation Log Prob: 0.0050
  Edge Precision: 0.3776, Recall: 0.3753, F1: 0.3764, Jaccard: 0.2493
  TP: 2.6284896206156048, FP: 4.3546170365068, FN: 4.392984967788117

Epoch 16:
  Validation Loss: 5.7518, Validation Log Prob: 0.0052
  Edge Precision: 0.3767, Recall: 0.3741, F1: 0.3753, Jaccard: 0.2486
  TP: 2.619899785254116, FP: 4.358339298496778, FN: 4.4015748031496065

Epoch 17:
  Validation Loss: 5.5605, Validation Log Prob: 0.0060
  Edge Precision: 0.3764, Recall: 0.3743, F1: 0.3753, Jaccard: 0.2486
  TP: 2.621760916249105, FP: 4.365640658554044, FN: 4.399713672154617

Epoch 18:
  Validation Loss: 5.6465, Validation Log Prob: 0.0056
  Edge Precision: 0.3773, Recall: 0.3754, F1: 0.3763, Jaccard: 0.2494
  TP: 2.6307802433786684, FP: 4.359484609878311, FN: 4.390694345025054

Epoch 19:
  Validation Loss: 5.4784, Validation Log Prob: 0.0066
  Edge Precision: 0.3778, Recall: 0.3756, F1: 0.3767, Jaccard: 0.2502
  TP: 2.6307802433786684, FP: 4.354044380816035, FN: 4.390694345025054

Epoch 20:
  Validation Loss: 5.4806, Validation Log Prob: 0.0065
  Edge Precision: 0.3805, Recall: 0.3787, F1: 0.3795, Jaccard: 0.2521
  TP: 2.651682176091625, FP: 4.340300644237652, FN: 4.369792412312098

Epoch 21:
  Validation Loss: 5.4035, Validation Log Prob: 0.0069
  Edge Precision: 0.3783, Recall: 0.3761, F1: 0.3771, Jaccard: 0.2507
  TP: 2.6346456692913387, FP: 4.348890479599141, FN: 4.386828919112384

Epoch 22:
  Validation Loss: 5.3037, Validation Log Prob: 0.0075
  Edge Precision: 0.3776, Recall: 0.3752, F1: 0.3763, Jaccard: 0.2499
  TP: 2.6272011453113815, FP: 4.354187544738726, FN: 4.394273443092341

Epoch 23:
  Validation Loss: 5.2827, Validation Log Prob: 0.0077
  Edge Precision: 0.3776, Recall: 0.3755, F1: 0.3765, Jaccard: 0.2498
  TP: 2.6283464566929133, FP: 4.356335003579098, FN: 4.393128131710809

Epoch 24:
  Validation Loss: 5.1643, Validation Log Prob: 0.0086
  Edge Precision: 0.3780, Recall: 0.3758, F1: 0.3768, Jaccard: 0.2499
  TP: 2.631782390837509, FP: 4.355476020042949, FN: 4.389692197566213

Epoch 25:
  Validation Loss: 5.1714, Validation Log Prob: 0.0086
  Edge Precision: 0.3777, Recall: 0.3752, F1: 0.3763, Jaccard: 0.2502
  TP: 2.6260558339298496, FP: 4.351753758052971, FN: 4.395418754473873

Epoch 26:
  Validation Loss: 4.9784, Validation Log Prob: 0.0103
  Edge Precision: 0.3782, Recall: 0.3762, F1: 0.3771, Jaccard: 0.2502
  TP: 2.635361488904796, FP: 4.352469577666428, FN: 4.386113099498926

Epoch 27:
  Validation Loss: 5.0627, Validation Log Prob: 0.0096
  Edge Precision: 0.3774, Recall: 0.3750, F1: 0.3761, Jaccard: 0.2495
  TP: 2.626485325697924, FP: 4.353185397279885, FN: 4.394989262705798

Epoch 28:
  Validation Loss: 4.9516, Validation Log Prob: 0.0104
  Edge Precision: 0.3782, Recall: 0.3756, F1: 0.3768, Jaccard: 0.2503
  TP: 2.62992125984252, FP: 4.347745168217609, FN: 4.391553328561202

Epoch 29:
  Validation Loss: 4.8405, Validation Log Prob: 0.0117
  Edge Precision: 0.3755, Recall: 0.3727, F1: 0.3740, Jaccard: 0.2479
  TP: 2.6103078024337867, FP: 4.362061560486757, FN: 4.411166785969936

Epoch 30:
  Validation Loss: 4.8249, Validation Log Prob: 0.0117
  Edge Precision: 0.3760, Recall: 0.3733, F1: 0.3746, Jaccard: 0.2484
  TP: 2.6141732283464565, FP: 4.359055118110236, FN: 4.407301360057265

Epoch 31:
  Validation Loss: 4.7704, Validation Log Prob: 0.0123
  Edge Precision: 0.3757, Recall: 0.3735, F1: 0.3745, Jaccard: 0.2483
  TP: 2.6158911954187545, FP: 4.368790264853257, FN: 4.405583392984968

Epoch 32:
  Validation Loss: 4.7355, Validation Log Prob: 0.0126
  Edge Precision: 0.3753, Recall: 0.3727, F1: 0.3739, Jaccard: 0.2478
  TP: 2.610021474588404, FP: 4.366785969935576, FN: 4.411453113815319

Epoch 33:
  Validation Loss: 4.6315, Validation Log Prob: 0.0141
  Edge Precision: 0.3749, Recall: 0.3723, F1: 0.3735, Jaccard: 0.2478
  TP: 2.606871868289191, FP: 4.372083035075161, FN: 4.414602720114531

Epoch 34:
  Validation Loss: 4.5300, Validation Log Prob: 0.0153
  Edge Precision: 0.3763, Recall: 0.3736, F1: 0.3748, Jaccard: 0.2485
  TP: 2.615748031496063, FP: 4.3596277738010025, FN: 4.405726556907659

Epoch 35:
  Validation Loss: 4.4638, Validation Log Prob: 0.0163
  Edge Precision: 0.3744, Recall: 0.3715, F1: 0.3728, Jaccard: 0.2470
  TP: 2.6001431639226915, FP: 4.371653543307087, FN: 4.421331424481031

Epoch 36:
  Validation Loss: 4.3840, Validation Log Prob: 0.0178
  Edge Precision: 0.3763, Recall: 0.3737, F1: 0.3749, Jaccard: 0.2485
  TP: 2.616607015032212, FP: 4.360057265569076, FN: 4.404867573371511

Epoch 37:
  Validation Loss: 4.4603, Validation Log Prob: 0.0164
  Edge Precision: 0.3736, Recall: 0.3711, F1: 0.3723, Jaccard: 0.2459
  TP: 2.5988546886184682, FP: 4.378811739441661, FN: 4.422619899785254

Epoch 38:
  Validation Loss: 4.3631, Validation Log Prob: 0.0180
  Edge Precision: 0.3743, Recall: 0.3716, F1: 0.3729, Jaccard: 0.2474
  TP: 2.6021474588403724, FP: 4.373371510379385, FN: 4.41932712956335

Epoch 39:
  Validation Loss: 4.3901, Validation Log Prob: 0.0177
  Edge Precision: 0.3731, Recall: 0.3702, F1: 0.3716, Jaccard: 0.2454
  TP: 2.5921259842519686, FP: 4.37680744452398, FN: 4.429348604151754

Epoch 40:
  Validation Loss: 4.2509, Validation Log Prob: 0.0199
  Edge Precision: 0.3762, Recall: 0.3735, F1: 0.3747, Jaccard: 0.2483
  TP: 2.6164638511095206, FP: 4.360486757337151, FN: 4.405010737294202

Epoch 41:
  Validation Loss: 4.2368, Validation Log Prob: 0.0201
  Edge Precision: 0.3735, Recall: 0.3710, F1: 0.3721, Jaccard: 0.2463
  TP: 2.5962777380100213, FP: 4.379813886900501, FN: 4.425196850393701

Epoch 42:
  Validation Loss: 4.2107, Validation Log Prob: 0.0206
  Edge Precision: 0.3750, Recall: 0.3719, F1: 0.3733, Jaccard: 0.2473
  TP: 2.603292770221904, FP: 4.364781675017896, FN: 4.418181818181818

Epoch 43:
  Validation Loss: 4.1649, Validation Log Prob: 0.0216
  Edge Precision: 0.3742, Recall: 0.3717, F1: 0.3728, Jaccard: 0.2471
  TP: 2.602433786685755, FP: 4.372512526843235, FN: 4.419040801717967

Epoch 44:
  Validation Loss: 4.0971, Validation Log Prob: 0.0231
  Edge Precision: 0.3739, Recall: 0.3713, F1: 0.3725, Jaccard: 0.2462
  TP: 2.6010021474588405, FP: 4.377523264137437, FN: 4.420472440944882

Epoch 45:
  Validation Loss: 4.0313, Validation Log Prob: 0.0244
  Edge Precision: 0.3743, Recall: 0.3716, F1: 0.3728, Jaccard: 0.2468
  TP: 2.600715819613457, FP: 4.374230493915533, FN: 4.4207587687902645

Epoch 46:
  Validation Loss: 4.0293, Validation Log Prob: 0.0244
  Edge Precision: 0.3741, Recall: 0.3709, F1: 0.3723, Jaccard: 0.2473
  TP: 2.5965640658554046, FP: 4.369506084466714, FN: 4.424910522548318

Epoch 47:
  Validation Loss: 3.9351, Validation Log Prob: 0.0266
  Edge Precision: 0.3726, Recall: 0.3696, F1: 0.3710, Jaccard: 0.2454
  TP: 2.588117394416607, FP: 4.382963493199713, FN: 4.433357193987115

Epoch 48:
  Validation Loss: 3.9673, Validation Log Prob: 0.0259
  Edge Precision: 0.3729, Recall: 0.3697, F1: 0.3712, Jaccard: 0.2452
  TP: 2.589835361488905, FP: 4.379098067287043, FN: 4.431639226914817

Epoch 49:
  Validation Loss: 3.9408, Validation Log Prob: 0.0264
  Edge Precision: 0.3712, Recall: 0.3684, F1: 0.3697, Jaccard: 0.2442
  TP: 2.5799570508231926, FP: 4.394130279169649, FN: 4.44151753758053/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4434: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4449: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))


Epoch 50:
  Validation Loss: 3.8778, Validation Log Prob: 0.0282
  Edge Precision: 0.3770, Recall: 0.3742, F1: 0.3755, Jaccard: 0.2489
  TP: 2.619899785254116, FP: 4.353758052970651, FN: 4.4015748031496065
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 73.33%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 26.67%
  ‚ùå False Discovery rate (FP/TP+FP): 21.50%
  üéØ Precision (TP/TP+FP): 78.50%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 23.64%
  ‚ö†Ô∏è Std. False Negative rate: 23.64%
  ‚ùå Std. False Discovery rate: 19.92%
  üéØ Std. Precision: 19.92%
üìâ  Average detailed edge-metrics
  F1: 0.76
  Jaccard: 0.66
  TP: 4.25
  FP: 1.05
  FN: 1.50

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 53.11%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 46.89%
  ‚ùå False Discovery rate (FP/TP+FP): 43.77%
  üéØ Precision (TP/TP+FP): 56.23%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 21.96%
  ‚ö†Ô∏è Std. False Negative rate: 21.96%
  ‚ùå Std. False Discovery rate: 22.17%
  üéØ Std. Precision: 22.17%
üìâ  Average detailed edge-metrics
  F1: 0.55
  Jaccard: 0.41
  TP: 3.24
  FP: 2.45
  FN: 2.81

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 44.51%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 55.49%
  ‚ùå False Discovery rate (FP/TP+FP): 54.00%
  üéØ Precision (TP/TP+FP): 46.00%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.43%
  ‚ö†Ô∏è Std. False Negative rate: 19.43%
  ‚ùå Std. False Discovery rate: 19.58%
  üéØ Std. Precision: 19.58%
üìâ  Average detailed edge-metrics
  F1: 0.45
  Jaccard: 0.31
  TP: 2.92
  FP: 3.38
  FN: 3.61

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 37.16%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 62.84%
  ‚ùå False Discovery rate (FP/TP+FP): 61.96%
  üéØ Precision (TP/TP+FP): 38.04%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.95%
  ‚ö†Ô∏è Std. False Negative rate: 17.95%
  ‚ùå Std. False Discovery rate: 18.25%
  üéØ Std. Precision: 18.25%
üìâ  Average detailed edge-metrics
  F1: 0.38
  Jaccard: 0.25
  TP: 2.61
  FP: 4.24
  FN: 4.41

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 32.03%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 67.97%
  ‚ùå False Discovery rate (FP/TP+FP): 67.57%
  üéØ Precision (TP/TP+FP): 32.43%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.49%
  ‚ö†Ô∏è Std. False Negative rate: 17.49%
  ‚ùå Std. False Discovery rate: 17.69%
  üéØ Std. Precision: 17.69%
üìâ  Average detailed edge-metrics
  F1: 0.32
  Jaccard: 0.21
  TP: 2.28
  FP: 4.75
  FN: 4.84

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 35.08%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 64.92%
  ‚ùå False Discovery rate (FP/TP+FP): 64.27%
  üéØ Precision (TP/TP+FP): 35.73%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.25%
  ‚ö†Ô∏è Std. False Negative rate: 18.25%
  ‚ùå Std. False Discovery rate: 18.56%
  üéØ Std. Precision: 18.56%
üìâ  Average detailed edge-metrics
  F1: 0.35
  Jaccard: 0.23
  TP: 2.46
  FP: 4.44
  FN: 4.56
[6, 7, 8, 9, 10, 999]
[0.7333333333333333, 0.5311324529811925, 0.44514518002322884, 0.3715608465608466, 0.32027376954398395, 0.3507574053243344]
[np.float64(0.19922767545365444), np.float64(0.22173138158161912), np.float64(0.19575238601097736), np.float64(0.1825272285770254), np.float64(0.17685645277363277), np.float64(0.18563439630674153)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.FIXED

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 76.17%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 23.83%
  ‚ùå False Discovery rate (FP/TP+FP): 20.00%
  üéØ Precision (TP/TP+FP): 80.00%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 25.22%
  ‚ö†Ô∏è Std. False Negative rate: 25.22%
  ‚ùå Std. False Discovery rate: 22.58%
  üéØ Std. Precision: 22.58%
üìâ  Average detailed edge-metrics
  F1: 0.78
  Jaccard: 0.70
  TP: 4.40
  FP: 1.00
  FN: 1.35

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 57.01%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 42.99%
  ‚ùå False Discovery rate (FP/TP+FP): 40.85%
  üéØ Precision (TP/TP+FP): 59.15%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 22.36%
  ‚ö†Ô∏è Std. False Negative rate: 22.36%
  ‚ùå Std. False Discovery rate: 22.07%
  üéØ Std. Precision: 22.07%
üìâ  Average detailed edge-metrics
  F1: 0.58
  Jaccard: 0.45
  TP: 3.50
  FP: 2.30
  FN: 2.55

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 46.16%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 53.84%
  ‚ùå False Discovery rate (FP/TP+FP): 53.26%
  üéØ Precision (TP/TP+FP): 46.74%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.76%
  ‚ö†Ô∏è Std. False Negative rate: 17.76%
  ‚ùå Std. False Discovery rate: 17.91%
  üéØ Std. Precision: 17.91%
üìâ  Average detailed edge-metrics
  F1: 0.46
  Jaccard: 0.32
  TP: 3.02
  FP: 3.44
  FN: 3.51

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 39.26%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 60.74%
  ‚ùå False Discovery rate (FP/TP+FP): 60.52%
  üéØ Precision (TP/TP+FP): 39.48%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.21%
  ‚ö†Ô∏è Std. False Negative rate: 18.21%
  ‚ùå Std. False Discovery rate: 18.23%
  üéØ Std. Precision: 18.23%
üìâ  Average detailed edge-metrics
  F1: 0.39
  Jaccard: 0.26
  TP: 2.76
  FP: 4.21
  FN: 4.26

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 33.09%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 66.91%
  ‚ùå False Discovery rate (FP/TP+FP): 66.77%
  üéØ Precision (TP/TP+FP): 33.23%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.64%
  ‚ö†Ô∏è Std. False Negative rate: 17.64%
  ‚ùå Std. False Discovery rate: 17.68%
  üéØ Std. Precision: 17.68%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.36
  FP: 4.73
  FN: 4.76

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.65%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.35%
  ‚ùå False Discovery rate (FP/TP+FP): 63.10%
  üéØ Precision (TP/TP+FP): 36.90%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.70%
  ‚ö†Ô∏è Std. False Negative rate: 18.70%
  ‚ùå Std. False Discovery rate: 18.80%
  üéØ Std. Precision: 18.80%
üìâ  Average detailed edge-metrics
  F1: 0.37
  Jaccard: 0.24
  TP: 2.57
  FP: 4.41
  FN: 4.45
[6, 7, 8, 9, 10, 999]
[0.7616666666666666, 0.5700680272108843, 0.46161440185830427, 0.3926088369070825, 0.3308822624890824, 0.36653117223983367]
[np.float64(0.2258317958127243), np.float64(0.22069050284447886), np.float64(0.17913373634999938), np.float64(0.18231518275689512), np.float64(0.1768156364068546), np.float64(0.18795072495752763)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_ASC

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 77.17%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 22.83%
  ‚ùå False Discovery rate (FP/TP+FP): 16.08%
  üéØ Precision (TP/TP+FP): 83.92%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 20.15%
  ‚ö†Ô∏è Std. False Negative rate: 20.15%
  ‚ùå Std. False Discovery rate: 15.12%
  üéØ Std. Precision: 15.12%
üìâ  Average detailed edge-metrics
  F1: 0.80
  Jaccard: 0.71
  TP: 4.40
  FP: 0.80
  FN: 1.35

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4477: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4491: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))

  ‚úÖ Recall (TP/TP+FN): 57.76%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 42.24%
  ‚ùå False Discovery rate (FP/TP+FP): 39.54%
  üéØ Precision (TP/TP+FP): 60.46%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 21.56%
  ‚ö†Ô∏è Std. False Negative rate: 21.56%
  ‚ùå Std. False Discovery rate: 21.57%
  üéØ Std. Precision: 21.57%
üìâ  Average detailed edge-metrics
  F1: 0.59
  Jaccard: 0.45
  TP: 3.51
  FP: 2.25
  FN: 2.53

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 44.57%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 55.43%
  ‚ùå False Discovery rate (FP/TP+FP): 53.77%
  üéØ Precision (TP/TP+FP): 46.23%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.19%
  ‚ö†Ô∏è Std. False Negative rate: 19.19%
  ‚ùå Std. False Discovery rate: 19.43%
  üéØ Std. Precision: 19.43%
üìâ  Average detailed edge-metrics
  F1: 0.45
  Jaccard: 0.32
  TP: 2.92
  FP: 3.36
  FN: 3.61

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.97%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.03%
  ‚ùå False Discovery rate (FP/TP+FP): 61.77%
  üéØ Precision (TP/TP+FP): 38.23%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.05%
  ‚ö†Ô∏è Std. False Negative rate: 18.05%
  ‚ùå Std. False Discovery rate: 18.42%
  üéØ Std. Precision: 18.42%
üìâ  Average detailed edge-metrics
  F1: 0.38
  Jaccard: 0.25
  TP: 2.60
  FP: 4.17
  FN: 4.42

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 31.57%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 68.43%
  ‚ùå False Discovery rate (FP/TP+FP): 67.64%
  üéØ Precision (TP/TP+FP): 32.36%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.40%
  ‚ö†Ô∏è Std. False Negative rate: 17.40%
  ‚ùå Std. False Discovery rate: 17.67%
  üéØ Std. Precision: 17.67%
üìâ  Average detailed edge-metrics
  F1: 0.32
  Jaccard: 0.20
  TP: 2.25
  FP: 4.68
  FN: 4.87

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 34.89%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 65.11%
  ‚ùå False Discovery rate (FP/TP+FP): 64.04%
  üéØ Precision (TP/TP+FP): 35.96%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.58%
  ‚ö†Ô∏è Std. False Negative rate: 18.58%
  ‚ùå Std. False Discovery rate: 18.99%
  üéØ Std. Precision: 18.99%
üìâ  Average detailed edge-metrics
  F1: 0.35
  Jaccard: 0.23
  TP: 2.44
  FP: 4.36
  FN: 4.58
[6, 7, 8, 9, 10, 999]
[0.7716666666666667, 0.5775910364145659, 0.4456794425087108, 0.36968346792908197, 0.315663788119226, 0.34890019429389507]
[np.float64(0.15122417134836613), np.float64(0.21566532262573318), np.float64(0.19425602197302994), np.float64(0.18420119975350513), np.float64(0.1767289666494426), np.float64(0.18986559774127268)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 82.83%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 17.17%
  ‚ùå False Discovery rate (FP/TP+FP): 14.17%
  üéØ Precision (TP/TP+FP): 85.83%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 21.97%
  ‚ö†Ô∏è Std. False Negative rate: 21.97%
  ‚ùå Std. False Discovery rate: 19.49%
  üéØ Std. Precision: 19.49%
üìâ  Average detailed edge-metrics
  F1: 0.84
  Jaccard: 0.78
  TP: 4.75
  FP: 0.75
  FN: 1.00

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 60.97%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 39.03%
  ‚ùå False Discovery rate (FP/TP+FP): 36.93%
  üéØ Precision (TP/TP+FP): 63.07%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.68%
  ‚ö†Ô∏è Std. False Negative rate: 19.68%
  ‚ùå Std. False Discovery rate: 19.86%
  üéØ Std. Precision: 19.86%
üìâ  Average detailed edge-metrics
  F1: 0.62
  Jaccard: 0.48
  TP: 3.72
  FP: 2.12
  FN: 2.32

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 46.61%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 53.39%
  ‚ùå False Discovery rate (FP/TP+FP): 52.60%
  üéØ Precision (TP/TP+FP): 47.40%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 16.93%
  ‚ö†Ô∏è Std. False Negative rate: 16.93%
  ‚ùå Std. False Discovery rate: 17.25%
  üéØ Std. Precision: 17.25%
üìâ  Average detailed edge-metrics
  F1: 0.47
  Jaccard: 0.32
  TP: 3.06
  FP: 3.37
  FN: 3.48

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 40.15%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 59.85%
  ‚ùå False Discovery rate (FP/TP+FP): 59.53%
  üéØ Precision (TP/TP+FP): 40.47%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.48%
  ‚ö†Ô∏è Std. False Negative rate: 18.48%
  ‚ùå Std. False Discovery rate: 18.56%
  üéØ Std. Precision: 18.56%
üìâ  Average detailed edge-metrics
  F1: 0.40
  Jaccard: 0.27
  TP: 2.82
  FP: 4.14
  FN: 4.20

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 33.68%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 66.32%
  ‚ùå False Discovery rate (FP/TP+FP): 66.19%
  üéØ Precision (TP/TP+FP): 33.81%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.12%
  ‚ö†Ô∏è Std. False Negative rate: 17.12%
  ‚ùå Std. False Discovery rate: 17.14%
  üéØ Std. Precision: 17.14%
üìâ  Average detailed edge-metrics
  F1: 0.34
  Jaccard: 0.22
  TP: 2.40
  FP: 4.69
  FN: 4.72

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 37.42%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 62.58%
  ‚ùå False Discovery rate (FP/TP+FP): 62.30%
  üéØ Precision (TP/TP+FP): 37.70%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.53%
  ‚ö†Ô∏è Std. False Negative rate: 18.53%
  ‚ùå Std. False Discovery rate: 18.68%
  üéØ Std. Precision: 18.68%
üìâ  Average detailed edge-metrics
  F1: 0.38
  Jaccard: 0.25
  TP: 2.62
  FP: 4.35
  FN: 4.40
[6, 7, 8, 9, 10, 999]
[0.8283333333333334, 0.6096838735494197, 0.466144018583043, 0.401450385222315, 0.3367977389870957, 0.3741878856052084]
[np.float64(0.19490025939210834), np.float64(0.19857962630886658), np.float64(0.17245687766559112), np.float64(0.18561034400696588), np.float64(0.17141517585634067), np.float64(0.1868054344541122)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 81.83%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 18.17%
  ‚ùå False Discovery rate (FP/TP+FP): 15.42%
  üéØ Precision (TP/TP+FP): 84.58%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.54%
  ‚ö†Ô∏è Std. False Negative rate: 18.54%
  ‚ùå Std. False Discovery rate: 16.76%
  üéØ Std. Precision: 16.76%
üìâ  Average detailed edge-metrics
  F1: 0.83
  Jaccard: 0.75
  TP: 4.70
  FP: 0.85
  FN: 1.05

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 54.10%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 45.90%
  ‚ùå False Discovery rate (FP/TP+FP): 42.79%
  üéØ Precision (TP/TP+FP): 57.21%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 20.98%
  ‚ö†Ô∏è Std. False Negative rate: 20.98%
  ‚ùå Std. False Discovery rate: 21.29%
  üéØ Std. Precision: 21.29%
üìâ  Average detailed edge-metrics
  F1: 0.56
  Jaccard: 0.42
  TP: 3.31
  FP: 2.39
  FN: 2.73

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 46.14%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 53.86%
  ‚ùå False Discovery rate (FP/TP+FP): 52.67%
  üéØ Precision (TP/TP+FP): 47.33%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.73%
  ‚ö†Ô∏è Std. False Negative rate: 18.73%
  ‚ùå Std. False Discovery rate: 18.86%
  üéØ Std. Precision: 18.86%
üìâ  Average detailed edge-metrics/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4505: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4525: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5344: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5361: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5377: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5409: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5663: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))

  F1: 0.47
  Jaccard: 0.33
  TP: 3.03
  FP: 3.32
  FN: 3.50

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 37.10%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 62.90%
  ‚ùå False Discovery rate (FP/TP+FP): 61.92%
  üéØ Precision (TP/TP+FP): 38.08%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.15%
  ‚ö†Ô∏è Std. False Negative rate: 18.15%
  ‚ùå Std. False Discovery rate: 18.50%
  üéØ Std. Precision: 18.50%
üìâ  Average detailed edge-metrics
  F1: 0.38
  Jaccard: 0.25
  TP: 2.61
  FP: 4.22
  FN: 4.41

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 31.13%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 68.87%
  ‚ùå False Discovery rate (FP/TP+FP): 68.12%
  üéØ Precision (TP/TP+FP): 31.88%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.51%
  ‚ö†Ô∏è Std. False Negative rate: 17.51%
  ‚ùå Std. False Discovery rate: 17.77%
  üéØ Std. Precision: 17.77%
üìâ  Average detailed edge-metrics
  F1: 0.31
  Jaccard: 0.20
  TP: 2.22
  FP: 4.72
  FN: 4.90

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 34.74%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 65.26%
  ‚ùå False Discovery rate (FP/TP+FP): 64.35%
  üéØ Precision (TP/TP+FP): 35.65%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.70%
  ‚ö†Ô∏è Std. False Negative rate: 18.70%
  ‚ùå Std. False Discovery rate: 19.05%
  üéØ Std. Precision: 19.05%
üìâ  Average detailed edge-metrics
  F1: 0.35
  Jaccard: 0.23
  TP: 2.43
  FP: 4.40
  FN: 4.59
[6, 7, 8, 9, 10, 999]
[0.8183333333333334, 0.5409763905562225, 0.46135888501742156, 0.3709621275410749, 0.31132597704542936, 0.3473959505061867]
[np.float64(0.16757046610638496), np.float64(0.2128720751201752), np.float64(0.18862573512344677), np.float64(0.18496399594951285), np.float64(0.17771550987267568), np.float64(0.19049962167001555)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 85.50%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 14.50%
  ‚ùå False Discovery rate (FP/TP+FP): 12.83%
  üéØ Precision (TP/TP+FP): 87.17%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 20.28%
  ‚ö†Ô∏è Std. False Negative rate: 20.28%
  ‚ùå Std. False Discovery rate: 18.20%
  üéØ Std. Precision: 18.20%
üìâ  Average detailed edge-metrics
  F1: 0.86
  Jaccard: 0.80
  TP: 4.90
  FP: 0.70
  FN: 0.85

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 57.75%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 42.25%
  ‚ùå False Discovery rate (FP/TP+FP): 39.98%
  üéØ Precision (TP/TP+FP): 60.02%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 22.37%
  ‚ö†Ô∏è Std. False Negative rate: 22.37%
  ‚ùå Std. False Discovery rate: 22.13%
  üéØ Std. Precision: 22.13%
üìâ  Average detailed edge-metrics
  F1: 0.59
  Jaccard: 0.45
  TP: 3.54
  FP: 2.25
  FN: 2.50

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 45.64%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 54.36%
  ‚ùå False Discovery rate (FP/TP+FP): 53.41%
  üéØ Precision (TP/TP+FP): 46.59%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 16.57%
  ‚ö†Ô∏è Std. False Negative rate: 16.57%
  ‚ùå Std. False Discovery rate: 16.68%
  üéØ Std. Precision: 16.68%
üìâ  Average detailed edge-metrics
  F1: 0.46
  Jaccard: 0.31
  TP: 2.99
  FP: 3.41
  FN: 3.55

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 38.90%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 61.10%
  ‚ùå False Discovery rate (FP/TP+FP): 60.75%
  üéØ Precision (TP/TP+FP): 39.25%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.89%
  ‚ö†Ô∏è Std. False Negative rate: 17.89%
  ‚ùå Std. False Discovery rate: 18.00%
  üéØ Std. Precision: 18.00%
üìâ  Average detailed edge-metrics
  F1: 0.39
  Jaccard: 0.26
  TP: 2.73
  FP: 4.22
  FN: 4.28

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 33.04%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 66.96%
  ‚ùå False Discovery rate (FP/TP+FP): 66.75%
  üéØ Precision (TP/TP+FP): 33.25%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.04%
  ‚ö†Ô∏è Std. False Negative rate: 17.04%
  ‚ùå Std. False Discovery rate: 17.11%
  üéØ Std. Precision: 17.11%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.35
  FP: 4.72
  FN: 4.76

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.50%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.50%
  ‚ùå False Discovery rate (FP/TP+FP): 63.15%
  üéØ Precision (TP/TP+FP): 36.85%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.23%
  ‚ö†Ô∏è Std. False Negative rate: 18.23%
  ‚ùå Std. False Discovery rate: 18.39%
  üéØ Std. Precision: 18.39%
üìâ  Average detailed edge-metrics
  F1: 0.37
  Jaccard: 0.24
  TP: 2.56
  FP: 4.40
  FN: 4.47
[6, 7, 8, 9, 10, 999]
[0.8550000000000001, 0.5774709883953582, 0.4563530778164924, 0.38895386614684857, 0.33039942921110577, 0.36500596516344547]
[np.float64(0.18204852832875817), np.float64(0.22127775677256523), np.float64(0.16681898054975935), np.float64(0.18002307212131585), np.float64(0.1710843510948219), np.float64(0.18387992189972305)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC
Device for model: cuda:3
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/random
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.FIXED
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/fixed
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_ASC
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/deg_asc
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/deg_desc
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/neigh_min_rem
56474
6067
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_deg_desc__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/neigh_max_rem
56459
6911
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC
Device for model: cuda:3
‚úÖ Code finished successfully at: 2025-07-16 12:45:45
