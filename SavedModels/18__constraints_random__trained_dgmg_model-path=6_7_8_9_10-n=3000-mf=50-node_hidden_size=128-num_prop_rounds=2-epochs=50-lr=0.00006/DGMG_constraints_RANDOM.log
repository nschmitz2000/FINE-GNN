nohup: ignoring input
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:2859: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  metadata = torch.load(load_path_metadata)
/home/nschmitz/.pyenv/versions/3.12.9/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
‚úÖ Using device: cuda:5
/home/nschmitz/GNNs/kirigami-database/data/cleaned
Model without constraints:  False
üíø Folder that model data gets saved into:  18__constraints_random__
Metrics---
Amount unfoldings:  3000
Max files per dir:  50
Batch size:  1
Num prop rounds:  2
Learning rate:  6e-05
Num epochs:  50
Accum steps:  4
Training node ordering strategy:  NodeOrder.RANDOM
Add edge is deterministic:  True
Add edge is deterministic threshold:  0.5
Calc PR-Curve:  False
---------------
6_7_8_9_10
Found existing dataset files; skipping save.
Loading dataset from disk‚Ä¶
6_7_8_9_10
Max amount of features per node in a graph:  128
Loaded 46571 graphs with actions!
Train size: 32599, Validation size: 6985, Test size: 6987
Graph(num_nodes=9, num_edges=16,
      ndata_schemes={'a': Scheme(shape=(256,), dtype=torch.float32), 'hv': Scheme(shape=(128,), dtype=torch.float32), 'deg_rem': Scheme(shape=(1,), dtype=torch.int16), 'deg_target': Scheme(shape=(1,), dtype=torch.int16)}
      edata_schemes={'he': Scheme(shape=(1,), dtype=torch.float32)})
[[0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 1. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]]
--------
Graph(num_nodes=9, num_edges=30,
      ndata_schemes={'deg_target': Scheme(shape=(1,), dtype=torch.int16), 'hv': Scheme(shape=(128,), dtype=torch.float32)}
      edata_schemes={})
[[0. 0. 0. 1. 1. 0. 1. 1. 1.]
 [0. 0. 1. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 1. 0. 1. 0. 0. 0.]
 [1. 1. 1. 0. 0. 0. 1. 0. 0.]
 [1. 1. 0. 0. 0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0. 0. 1. 1. 0.]
 [1. 0. 0. 1. 0. 1. 0. 0. 0.]
 [1. 0. 0. 0. 0. 1. 0. 0. 1.]
 [1. 0. 0. 0. 1. 0. 0. 1. 0.]]
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM
Device for model: cuda:5
Dir_name to save to:  6_7_8_9_10
Training started at: 2025-07-14 11:32:41
grad AddEdge W: 621.9080810546875
grad ChooseDest W: 6.77458381652832
grad AddEdge W: 40.54648208618164
grad ChooseDest W: 3.8223156929016113
grad AddEdge W: 9.956860542297363
grad ChooseDest W: 2.4215147495269775
grad AddEdge W: 6.736915111541748
grad ChooseDest W: 2.676081657409668
grad AddEdge W: 2.5238099098205566
grad ChooseDest W: 4.09407901763916
grad AddEdge W: 2.590759754180908
grad ChooseDest W: 3.5287652015686035
grad AddEdge W: 1.4666476249694824
grad ChooseDest W: 4.572969913482666
grad AddEdge W: 0.8598754405975342
grad ChooseDest W: 5.025073528289795
grad AddEdge W: 1.2829852104187012
grad ChooseDest W: 2.0671963691711426
grad AddEdge W: 0.6222363114356995
grad ChooseDest W: 1.6966173648834229
grad AddEdge W: 0.4342111647129059
grad ChooseDest W: 4.875606536865234
grad AddEdge W: 0.33385249972343445
grad ChooseDest W: 3.0015454292297363
grad AddEdge W: 0.46410465240478516
grad ChooseDest W: 2.538686990737915
grad AddEdge W: 0.2638091742992401
grad ChooseDest W: 3.969625949859619
grad AddEdge W: 0.4055202305316925
grad ChooseDest W: 3.130643129348755
grad AddEdge W: 0.31472185254096985
grad ChooseDest W: 2.489906072616577
grad AddEdge W: 0.16203320026397705
grad ChooseDest W: 3.3494110107421875
grad AddEdge W: 0.11317592859268188
grad ChooseDest W: 4.392729759216309
grad AddEdge W: 0.12053835391998291
grad ChooseDest W: 3.1767826080322266
grad AddEdge W: 0.14647018909454346
grad ChooseDest W: 2.535785675048828
grad AddEdge W: 0.15900050103664398
grad ChooseDest W: 4.156505584716797
grad AddEdge W: 0.14034099876880646
grad ChooseDest W: 4.6926116943359375
grad AddEdge W: 0.11991464346647263
grad ChooseDest W: 3.2997329235076904
grad AddEdge W: 0.0701310932636261
grad ChooseDest W: 3.24839448928833
grad AddEdge W: 0.1461668461561203
grad ChooseDest W: 2.971909999847412
grad AddEdge W: 0.07606910914182663
grad ChooseDest W: 3.6761934757232666
grad AddEdge W: 0.10156720131635666
grad ChooseDest W: 2.876753568649292
grad AddEdge W: 0.15703079104423523
grad ChooseDest W: 2.970709800720215
grad AddEdge W: 0.24874836206436157
grad ChooseDest W: 3.1413791179656982
grad AddEdge W: 0.12990599870681763
grad ChooseDest W: 2.12438702583313
grad AddEdge W: 0.09108404815196991
grad ChooseDest W: 4.445516109466553
grad AddEdge W: 0.08518991619348526
grad ChooseDest W: 5.127828598022461
grad AddEdge W: 0.032044343650341034
grad ChooseDest W: 4.420119762420654
grad AddEdge W: 0.048472970724105835
grad ChooseDest W: 2.933337688446045
grad AddEdge W: 0.05407930165529251
grad ChooseDest W: 2.123940944671631
grad AddEdge W: 0.05492105707526207
grad ChooseDest W: 2.5229947566986084
grad AddEdge W: 0.02443193644285202
grad ChooseDest W: 2.799471855163574
grad AddEdge W: 0.14169342815876007
grad ChooseDest W: 2.4052884578704834
grad AddEdge W: 0.34493908286094666
grad ChooseDest W: 1.42710280418396
grad AddEdge W: 0.021004006266593933
grad ChooseDest W: 2.7834157943725586
grad AddEdge W: 0.06731941550970078
grad ChooseDest W: 3.170961856842041
grad AddEdge W: 0.01868974044919014
grad ChooseDest W: 4.4718918800354
grad AddEdge W: 0.03574131801724434
grad ChooseDest W: 2.308363437652588
grad AddEdge W: 0.015390557236969471
grad ChooseDest W: 2.4514122009277344
grad AddEdge W: 0.010820334777235985
grad ChooseDest W: 4.703817844390869
grad AddEdge W: 0.010087287984788418
grad ChooseDest W: 2.8586525917053223
grad AddEdge W: 0.10116282105445862
grad ChooseDest W: 1.5918773412704468
grad AddEdge W: 0.019277360290288925
grad ChooseDest W: 2.1440789699554443
grad AddEdge W: 0.022674839943647385
grad ChooseDest W: 2.417602062225342
grad AddEdge W: 0.01708497479557991
grad ChooseDest W: 3.244642496109009
grad AddEdge W: 0.01269604079425335
grad ChooseDest W: 4.610450744628906
grad AddEdge W: 0.011959332041442394
grad ChooseDest W: 2.331130027770996
grad AddEdge W: 0.00635926891118288
grad ChooseDest W: 5.1800408363342285
grad AddEdge W: 0.013615273870527744
grad ChooseDest W: 3.0345993041992188
grad AddEdge W: 0.004915787372738123
grad ChooseDest W: 2.673752784729004
grad AddEdge W: 0.03870287537574768
grad ChooseDest W: 3.3049814701080322
grad AddEdge W: 0.0064365314319729805
grad ChooseDest W: 2.7503654956817627
grad AddEdge W: 0.003836887190118432
grad ChooseDest W: 3.9359917640686035
grad AddEdge W: 0.004435907118022442
grad ChooseDest W: 2.723409414291382
grad AddEdge W: 0.008331285789608955
grad ChooseDest W: 3.048476219177246
grad AddEdge W: 0.002839194843545556
grad ChooseDest W: 4.715783596038818
grad AddEdge W: 0.0034881276078522205
grad ChooseDest W: 2.682173490524292
grad AddEdge W: 0.002735777059569955
grad ChooseDest W: 3.9296212196350098
grad AddEdge W: 0.006848087999969721
grad ChooseDest W: 2.718111991882324
grad AddEdge W: 0.0019468542886897922
grad ChooseDest W: 4.238114356994629
grad AddEdge W: 0.0018685499671846628
grad ChooseDest W: 3.2894580364227295
=== Epoch 1: Train Loss: 6.3538, Train Log Prob: 0.0045 ===
Total mismatches: 97119
Predicted valid destination but wrong order: 22794
Epoch 1: Validation Loss: 7.1535, Validation Log Prob: 0.0015
Epoch 1: Edge Precision: 0.3599, Recall: 0.3566, F1: 0.3581, Jaccard: 0.2349
Epoch 1: TP: 2.497208303507516, FP: 4.465282748747316, FN: 4.524266284896206
Epoch 1: warmup, skipping learning rate scheduler
Epoch 1: Current Learning Rate: 6e-05
[Epoch 1] ‚è±Ô∏è Total: 1894.56s | Current time: 2025-07-14 12:04:16 | üèãÔ∏è Train: 1661.93s | ‚úÖ Val: 232.63s
grad AddEdge W: 0.011680837720632553
grad ChooseDest W: 7.605839252471924
grad AddEdge W: 0.0012928812066093087
grad ChooseDest W: 2.225369691848755
grad AddEdge W: 0.0018655962776392698
grad ChooseDest W: 2.62730073928833
grad AddEdge W: 0.0015561438631266356
grad ChooseDest W: 3.127112865447998
grad AddEdge W: 0.0010205807629972696
grad ChooseDest W: 2.2748911380767822
grad AddEdge W: 0.0022522159852087498
grad ChooseDest W: 2.1769940853118896
grad AddEdge W: 0.0008983322768472135
grad ChooseDest W: 2.445435047149658
grad AddEdge W: 0.0029886076226830482
grad ChooseDest W: 1.9099048376083374
grad AddEdge W: 0.0013000238686800003
grad ChooseDest W: 2.0365147590637207
grad AddEdge W: 0.003606025595217943
grad ChooseDest W: 3.3171193599700928
grad AddEdge W: 0.00739462161436677
grad ChooseDest W: 1.986035704612732
grad AddEdge W: 0.0006996576557867229
grad ChooseDest W: 2.56538462638855
grad AddEdge W: 0.002671880181878805
grad ChooseDest W: 3.179746627807617
grad AddEdge W: 0.002109730616211891
grad ChooseDest W: 3.247628688812256
grad AddEdge W: 0.0013540757354348898
grad ChooseDest W: 3.386603355407715
grad AddEdge W: 0.0009613350266590714
grad ChooseDest W: 3.768192768096924
grad AddEdge W: 0.0030471286736428738
grad ChooseDest W: 2.6001639366149902
grad AddEdge W: 0.006643250584602356
grad ChooseDest W: 5.064055442810059
grad AddEdge W: 0.0019737626425921917
grad ChooseDest W: 2.1047005653381348
grad AddEdge W: 0.0005917766829952598
grad ChooseDest W: 2.699115037918091
grad AddEdge W: 0.0005646446952596307
grad ChooseDest W: 2.3854868412017822
grad AddEdge W: 0.0002925457083620131
grad ChooseDest W: 3.8963544368743896
grad AddEdge W: 0.00041000996134243906
grad ChooseDest W: 4.069774150848389
grad AddEdge W: 0.0006167867104522884
grad ChooseDest W: 2.6280415058135986
grad AddEdge W: 0.0011875058989971876
grad ChooseDest W: 2.1313552856445312
grad AddEdge W: 0.0007559264195151627
grad ChooseDest W: 3.4064688682556152
grad AddEdge W: 0.00023865955881774426
grad ChooseDest W: 3.9558522701263428
grad AddEdge W: 0.0009207053808495402
grad ChooseDest W: 2.2467164993286133
grad AddEdge W: 0.00011178110435139388
grad ChooseDest W: 2.538760185241699
grad AddEdge W: 0.000177808542503044
grad ChooseDest W: 2.114872694015503
grad AddEdge W: 0.0009423322044312954
grad ChooseDest W: 1.9517625570297241
grad AddEdge W: 0.00021514890249818563
grad ChooseDest W: 2.832599401473999
grad AddEdge W: 0.0025931466370821
grad ChooseDest W: 1.7466555833816528
grad AddEdge W: 0.0001578064839122817
grad ChooseDest W: 3.2834689617156982
grad AddEdge W: 0.021343963220715523
grad ChooseDest W: 1.8617007732391357
grad AddEdge W: 0.0005607664934359491
grad ChooseDest W: 2.730215311050415
grad AddEdge W: 0.00012270537263248116
grad ChooseDest W: 2.168914794921875
grad AddEdge W: 0.0001835669536376372
grad ChooseDest W: 2.182431697845459
grad AddEdge W: 0.00014662803732790053
grad ChooseDest W: 2.122995615005493
grad AddEdge W: 0.00012006222823401913
grad ChooseDest W: 2.2498579025268555
grad AddEdge W: 8.9042259787675e-05
grad ChooseDest W: 1.4565229415893555
grad AddEdge W: 0.00015446063480339944
grad ChooseDest W: 2.1588375568389893
grad AddEdge W: 0.0008659576997160912
grad ChooseDest W: 2.4657645225524902
grad AddEdge W: 0.00031182955717667937
grad ChooseDest W: 1.898701548576355
grad AddEdge W: 7.09819869371131e-05
grad ChooseDest W: 3.5825870037078857
grad AddEdge W: 0.0002314880257472396
grad ChooseDest W: 3.8870747089385986
grad AddEdge W: 0.0009441936272196472
grad ChooseDest W: 2.7241804599761963
grad AddEdge W: 5.8643363445298746e-05
grad ChooseDest W: 2.812823534011841
grad AddEdge W: 0.00026626847102306783
grad ChooseDest W: 3.18780255317688
grad AddEdge W: 3.373942308826372e-05
grad ChooseDest W: 2.7711877822875977
grad AddEdge W: 3.961231413995847e-05
grad ChooseDest W: 3.8935000896453857
grad AddEdge W: 0.00012372560740914196
grad ChooseDest W: 2.254108428955078
grad AddEdge W: 2.710157241381239e-05
grad ChooseDest W: 2.6095736026763916
grad AddEdge W: 5.0676138926064596e-05
grad ChooseDest W: 2.3177783489227295
grad AddEdge W: 3.1576746550854295e-05
grad ChooseDest W: 1.7409015893936157
grad AddEdge W: 0.0024496775586158037
grad ChooseDest W: 2.05911922454834
grad AddEdge W: 0.00020019775547552854
grad ChooseDest W: 1.7052899599075317
grad AddEdge W: 2.1632657080772333e-05
grad ChooseDest W: 2.7865984439849854
grad AddEdge W: 0.00015227818221319467
grad ChooseDest W: 2.2602038383483887
grad AddEdge W: 0.00019435423018876463
grad ChooseDest W: 1.664272665977478
grad AddEdge W: 1.4314809050119948e-05
grad ChooseDest W: 1.9115244150161743
grad AddEdge W: 2.7279787900624797e-05
grad ChooseDest W: 2.7547285556793213
grad AddEdge W: 2.9899272703914903e-05
grad ChooseDest W: 2.3276455402374268
grad AddEdge W: 5.7741752243600786e-05
grad ChooseDest W: 2.02960205078125
grad AddEdge W: 2.147676786989905e-05
grad ChooseDest W: 3.222102403640747
grad AddEdge W: 0.0004997718497179449
grad ChooseDest W: 2.175466775894165
=== Epoch 2: Train Loss: 6.1238, Train Log Prob: 0.0055 ===
Total mismatches: 93664
Predicted valid destination but wrong order: 22279
Epoch 2: Validation Loss: 6.8048, Validation Log Prob: 0.0022
Epoch 2: Edge Precision: 0.3608, Recall: 0.3587, F1: 0.3596, Jaccard: 0.2367
Epoch 2: TP: 2.5110952040085897, FP: 4.4747315676449535, FN: 4.510379384395132
Epoch 2: warmup, skipping learning rate scheduler
Epoch 2: Current Learning Rate: 6e-05
[Epoch 2] ‚è±Ô∏è Total: 1916.57s | Current time: 2025-07-14 12:36:12 | üèãÔ∏è Train: 1682.04s | ‚úÖ Val: 234.54s
grad AddEdge W: 0.00042196709546260536
grad ChooseDest W: 3.921543598175049
grad AddEdge W: 1.7422620658180676e-05
grad ChooseDest W: 3.089766263961792
grad AddEdge W: 8.402374078286812e-06
grad ChooseDest W: 1.8290140628814697
grad AddEdge W: 5.4330022976500914e-05
grad ChooseDest W: 1.5298409461975098
grad AddEdge W: 1.228347719006706e-05
grad ChooseDest W: 2.100675582885742
grad AddEdge W: 1.2332679943938274e-05
grad ChooseDest W: 1.3942091464996338
grad AddEdge W: 3.7748290196759626e-05
grad ChooseDest W: 3.337956666946411
grad AddEdge W: 7.352567536145216e-06
grad ChooseDest W: 4.159029006958008
grad AddEdge W: 1.0827060577867087e-05
grad ChooseDest W: 2.4981205463409424
grad AddEdge W: 1.1130702660011593e-05
grad ChooseDest W: 2.5680081844329834
grad AddEdge W: 7.721829206275288e-06
grad ChooseDest W: 2.5501372814178467
grad AddEdge W: 3.139293403364718e-05
grad ChooseDest W: 1.6747947931289673
grad AddEdge W: 0.00013230294280219823
grad ChooseDest W: 2.3534038066864014
grad AddEdge W: 1.5758232621010393e-05
grad ChooseDest W: 2.3437201976776123
grad AddEdge W: 4.998557869839715e-06
grad ChooseDest W: 3.431889295578003
grad AddEdge W: 2.4511368792445865e-06
grad ChooseDest W: 2.80902099609375
grad AddEdge W: 1.6327610865118913e-05
grad ChooseDest W: 2.709625005722046
grad AddEdge W: 3.4421241252857726e-06
grad ChooseDest W: 3.690249443054199
grad AddEdge W: 1.4853551874693949e-05
grad ChooseDest W: 2.376227855682373
grad AddEdge W: 3.2800126064103097e-06
grad ChooseDest W: 3.0449585914611816
grad AddEdge W: 1.510470247012563e-05
grad ChooseDest W: 1.554455280303955
grad AddEdge W: 5.1208457989559975e-06
grad ChooseDest W: 1.5106079578399658
grad AddEdge W: 2.1726520571974106e-05
grad ChooseDest W: 2.864548683166504
grad AddEdge W: 2.200213657488348e-06
grad ChooseDest W: 3.2103793621063232
grad AddEdge W: 9.614873306418303e-07
grad ChooseDest W: 1.7213083505630493
grad AddEdge W: 2.782492401820491e-06
grad ChooseDest W: 3.3324997425079346
grad AddEdge W: 3.7053391679364722e-06
grad ChooseDest W: 2.2771902084350586
grad AddEdge W: 1.210700884257676e-05
grad ChooseDest W: 3.4703328609466553
grad AddEdge W: 9.288836736232042e-06
grad ChooseDest W: 1.884803295135498
grad AddEdge W: 1.2006616998405661e-06
grad ChooseDest W: 2.2719004154205322
grad AddEdge W: 1.2381905207803356e-06
grad ChooseDest W: 3.589583158493042
grad AddEdge W: 9.026456950778083e-07
grad ChooseDest W: 1.8836357593536377
grad AddEdge W: 1.4792567526455969e-06
grad ChooseDest W: 2.322127342224121
grad AddEdge W: 9.597025609764387e-07
grad ChooseDest W: 4.499275207519531
grad AddEdge W: 4.699591954704374e-06
grad ChooseDest W: 2.0714735984802246
grad AddEdge W: 1.139774099101487e-06
grad ChooseDest W: 2.2655344009399414
grad AddEdge W: 6.956588094908511e-06
grad ChooseDest W: 2.6105728149414062
grad AddEdge W: 5.9512003645068035e-06
grad ChooseDest W: 2.1506314277648926
grad AddEdge W: 6.614746439481678e-07
grad ChooseDest W: 2.2079923152923584
grad AddEdge W: 2.637726083776215e-06
grad ChooseDest W: 2.8689520359039307
grad AddEdge W: 3.417758762225276e-06
grad ChooseDest W: 1.7588757276535034
grad AddEdge W: 2.2114581952337176e-06
grad ChooseDest W: 3.5061726570129395
grad AddEdge W: 1.9126557049276016e-07
grad ChooseDest W: 3.4864730834960938
grad AddEdge W: 2.44717080022383e-06
grad ChooseDest W: 2.4331815242767334
grad AddEdge W: 2.6720611003838712e-06
grad ChooseDest W: 2.027066946029663
grad AddEdge W: 2.351298917346867e-06
grad ChooseDest W: 2.552532911300659
grad AddEdge W: 2.1735625068686204e-06
grad ChooseDest W: 2.642972707748413
grad AddEdge W: 4.892252491117688e-06
grad ChooseDest W: 1.3402460813522339
grad AddEdge W: 1.8700171722230152e-07
grad ChooseDest W: 2.7641258239746094
grad AddEdge W: 2.460195389630826e-07
grad ChooseDest W: 3.213975191116333
grad AddEdge W: 7.674786502320785e-06
grad ChooseDest W: 1.1805752515792847
grad AddEdge W: 2.0665816009568516e-06
grad ChooseDest W: 3.0343992710113525
grad AddEdge W: 1.5404325495183002e-06
grad ChooseDest W: 2.5191447734832764
grad AddEdge W: 2.570095603005029e-07
grad ChooseDest W: 1.8855178356170654
grad AddEdge W: 1.1024225159417256e-06
grad ChooseDest W: 3.2829318046569824
grad AddEdge W: 1.447285740141524e-07
grad ChooseDest W: 2.0658493041992188
grad AddEdge W: 6.502895075755077e-07
grad ChooseDest W: 2.211665630340576
grad AddEdge W: 1.3676989851774124e-07
grad ChooseDest W: 3.2603747844696045
grad AddEdge W: 7.200871721124713e-08
grad ChooseDest W: 3.018361806869507
grad AddEdge W: 1.1380378737158026e-06
grad ChooseDest W: 1.8051291704177856
grad AddEdge W: 8.133580422509112e-07
grad ChooseDest W: 3.3295185565948486
grad AddEdge W: 6.824500786706267e-08
grad ChooseDest W: 2.985842704772949
grad AddEdge W: 8.335355801136757e-07
grad ChooseDest W: 1.3103326559066772
grad AddEdge W: 4.440851625986397e-06
grad ChooseDest W: 1.859778881072998
grad AddEdge W: 5.355019894182078e-08
grad ChooseDest W: 1.7894021272659302
grad AddEdge W: 4.28398561780341e-06
grad ChooseDest W: 2.210700511932373
=== Epoch 3: Train Loss: 6.0425, Train Log Prob: 0.0063 ===
Total mismatches: 92443
Predicted valid destination but wrong order: 21628
Epoch 3: Validation Loss: 6.3654, Validation Log Prob: 0.0033
Epoch 3: Edge Precision: 0.3615, Recall: 0.3591, F1: 0.3602, Jaccard: 0.2369
Epoch 3: TP: 2.5139584824624195, FP: 4.467430207587688, FN: 4.507516105941303
Epoch 3: warmup, skipping learning rate scheduler
Epoch 3: Current Learning Rate: 6e-05
[Epoch 3] ‚è±Ô∏è Total: 1921.26s | Current time: 2025-07-14 13:08:14 | üèãÔ∏è Train: 1683.95s | ‚úÖ Val: 237.31s
grad AddEdge W: 3.321722488180967e-07
grad ChooseDest W: 4.012953281402588
grad AddEdge W: 8.829525910414304e-08
grad ChooseDest W: 2.569596290588379
grad AddEdge W: 5.79479319640086e-07
grad ChooseDest W: 2.6482837200164795
grad AddEdge W: 3.209255794445198e-07
grad ChooseDest W: 2.478525161743164
grad AddEdge W: 4.0266282752554616e-08
grad ChooseDest W: 5.747061729431152
grad AddEdge W: 4.699987243839132e-07
grad ChooseDest W: 2.3629724979400635
grad AddEdge W: 3.129795800305146e-07
grad ChooseDest W: 3.258117437362671
grad AddEdge W: 2.9125666856089083e-07
grad ChooseDest W: 3.5427908897399902
grad AddEdge W: 1.1958969707848155e-06
grad ChooseDest W: 1.6411327123641968
grad AddEdge W: 1.8908606236323067e-08
grad ChooseDest W: 3.185269594192505
grad AddEdge W: 4.5598053333151256e-08
grad ChooseDest W: 3.437412977218628
grad AddEdge W: 2.9777709187328583e-08
grad ChooseDest W: 2.291691780090332
grad AddEdge W: 1.7064205337646854e-07
grad ChooseDest W: 2.361304998397827
grad AddEdge W: 1.5163347910629454e-08
grad ChooseDest W: 2.493621587753296
grad AddEdge W: 1.2932220627703828e-08
grad ChooseDest W: 3.3366315364837646
grad AddEdge W: 1.2009560634851368e-07
grad ChooseDest W: 2.1510820388793945
grad AddEdge W: 1.5853077073302302e-08
grad ChooseDest W: 2.189700126647949
grad AddEdge W: 8.917055538404384e-09
grad ChooseDest W: 2.5486483573913574
grad AddEdge W: 1.8183959582529496e-07
grad ChooseDest W: 1.6615015268325806
grad AddEdge W: 2.3795352888100751e-07
grad ChooseDest W: 2.174309492111206
grad AddEdge W: 2.110674017785641e-07
grad ChooseDest W: 1.8570772409439087
grad AddEdge W: 1.3755310135366017e-07
grad ChooseDest W: 1.527127742767334
grad AddEdge W: 8.200910173172815e-09
grad ChooseDest W: 1.7141107320785522
grad AddEdge W: 9.368879894111615e-09
grad ChooseDest W: 2.9943974018096924
grad AddEdge W: 4.481072224393756e-08
grad ChooseDest W: 1.8461658954620361
grad AddEdge W: 5.1836530623461385e-09
grad ChooseDest W: 2.682107925415039
grad AddEdge W: 5.413195669490278e-09
grad ChooseDest W: 2.1354761123657227
grad AddEdge W: 7.497170884107618e-09
grad ChooseDest W: 1.8600890636444092
grad AddEdge W: 1.6579538852923292e-09
grad ChooseDest W: 1.8323867321014404
grad AddEdge W: 5.4401539273385424e-08
grad ChooseDest W: 1.7148473262786865
grad AddEdge W: 3.2656599646685436e-09
grad ChooseDest W: 5.206107139587402
grad AddEdge W: 4.299153921749621e-09
grad ChooseDest W: 1.9273258447647095
grad AddEdge W: 5.7593618763007726e-09
grad ChooseDest W: 2.8500871658325195
grad AddEdge W: 3.5406337861587645e-09
grad ChooseDest W: 2.7378180027008057
grad AddEdge W: 3.0636929437832805e-08
grad ChooseDest W: 2.0834548473358154
grad AddEdge W: 1.268057880920992e-09
grad ChooseDest W: 2.1856284141540527
grad AddEdge W: 1.4373083834229305e-09
grad ChooseDest W: 2.5414464473724365
grad AddEdge W: 1.6950444381436114e-09
grad ChooseDest W: 2.3082339763641357
grad AddEdge W: 2.0036244663401703e-08
grad ChooseDest W: 2.45127010345459
grad AddEdge W: 1.4621151400717736e-08
grad ChooseDest W: 2.426072835922241
grad AddEdge W: 1.4429371731239371e-05
grad ChooseDest W: 1.660335898399353
grad AddEdge W: 1.7462649992694423e-08
grad ChooseDest W: 1.3423441648483276
grad AddEdge W: 1.4215380588211701e-06
grad ChooseDest W: 1.5327246189117432
grad AddEdge W: 2.4150629940322688e-08
grad ChooseDest W: 1.6172124147415161
grad AddEdge W: 2.7924110668209323e-07
grad ChooseDest W: 2.555809497833252
grad AddEdge W: 3.564219497320664e-08
grad ChooseDest W: 1.4505064487457275
grad AddEdge W: 1.1839953906189749e-08
grad ChooseDest W: 2.851092576980591
grad AddEdge W: 2.141785770959359e-08
grad ChooseDest W: 2.373873233795166
grad AddEdge W: 5.579441353376069e-09
grad ChooseDest W: 2.3000762462615967
grad AddEdge W: 1.9978534382403268e-08
grad ChooseDest W: 1.528780460357666
grad AddEdge W: 1.0099822089770782e-09
grad ChooseDest W: 2.521784543991089
grad AddEdge W: 5.318998130832142e-09
grad ChooseDest W: 2.707824230194092
grad AddEdge W: 6.936129448575912e-10
grad ChooseDest W: 2.1155123710632324
grad AddEdge W: 1.7080546976799837e-09
grad ChooseDest W: 2.2453885078430176
grad AddEdge W: 5.854248197323386e-10
grad ChooseDest W: 2.1803245544433594
grad AddEdge W: 4.104780959579557e-10
grad ChooseDest W: 3.833538293838501
grad AddEdge W: 5.982753492617121e-08
grad ChooseDest W: 2.184384822845459
grad AddEdge W: 2.558313560996339e-10
grad ChooseDest W: 1.4922258853912354
grad AddEdge W: 4.197542313733038e-09
grad ChooseDest W: 4.161923408508301
grad AddEdge W: 3.835293194143219e-10
grad ChooseDest W: 1.407245397567749
grad AddEdge W: 3.69438757363838e-10
grad ChooseDest W: 2.945951223373413
grad AddEdge W: 3.116599478492077e-10
grad ChooseDest W: 2.0500640869140625
grad AddEdge W: 4.349297810790631e-09
grad ChooseDest W: 2.1464014053344727
grad AddEdge W: 1.799091431387012e-10
grad ChooseDest W: 2.1223812103271484
grad AddEdge W: 2.589089498350461e-10
grad ChooseDest W: 1.715118408203125
grad AddEdge W: 1.3188962422194805e-10
grad ChooseDest W: 2.4049487113952637
=== Epoch 4: Train Loss: 6.0150, Train Log Prob: 0.0066 ===
Total mismatches: 91904
Predicted valid destination but wrong order: 20871
Epoch 4: Validation Loss: 6.3021, Validation Log Prob: 0.0035
Epoch 4: Edge Precision: 0.3617, Recall: 0.3583, F1: 0.3599, Jaccard: 0.2367
Epoch 4: TP: 2.509663564781675, FP: 4.45125268432355, FN: 4.511811023622047
Epoch 4: warmup, skipping learning rate scheduler
Epoch 4: Current Learning Rate: 6e-05
[Epoch 4] ‚è±Ô∏è Total: 1925.05s | Current time: 2025-07-14 13:40:19 | üèãÔ∏è Train: 1685.72s | ‚úÖ Val: 239.34s
grad AddEdge W: 7.349277098001039e-08
grad ChooseDest W: 4.89448881149292
grad AddEdge W: 2.614721772431494e-10
grad ChooseDest W: 1.995784044265747
grad AddEdge W: 1.1611600569949587e-10
grad ChooseDest W: 1.7701822519302368
grad AddEdge W: 1.974389818304445e-10
grad ChooseDest W: 2.410552501678467
grad AddEdge W: 1.0544740081108728e-10
grad ChooseDest W: 3.1120948791503906
grad AddEdge W: 1.116066614126332e-10
grad ChooseDest W: 2.3291609287261963
grad AddEdge W: 1.023468809702166e-10
grad ChooseDest W: 2.228766918182373
grad AddEdge W: 7.883565467103892e-10
grad ChooseDest W: 3.213829278945923
grad AddEdge W: 1.0204501688093615e-09
grad ChooseDest W: 1.8062913417816162
grad AddEdge W: 1.7563324059000074e-11
grad ChooseDest W: 2.156534194946289
grad AddEdge W: 5.244227968548287e-11
grad ChooseDest W: 3.593109130859375
grad AddEdge W: 4.79611628190213e-10
grad ChooseDest W: 2.1419765949249268
grad AddEdge W: 5.022940396948172e-11
grad ChooseDest W: 1.942397117614746
grad AddEdge W: 4.789498797563851e-10
grad ChooseDest W: 1.8943982124328613
grad AddEdge W: 3.696503866890133e-11
grad ChooseDest W: 2.109327793121338
grad AddEdge W: 3.056390557309996e-11
grad ChooseDest W: 2.0017504692077637
grad AddEdge W: 2.3964568177103907e-11
grad ChooseDest W: 1.8816778659820557
grad AddEdge W: 3.337016496463008e-10
grad ChooseDest W: 3.46201753616333
grad AddEdge W: 1.880317256064412e-11
grad ChooseDest W: 2.2606594562530518
grad AddEdge W: 2.20535554262824e-08
grad ChooseDest W: 1.4964150190353394
grad AddEdge W: 2.6208174519481986e-11
grad ChooseDest W: 2.0913562774658203
grad AddEdge W: 1.2964363957368441e-11
grad ChooseDest W: 2.5572469234466553
grad AddEdge W: 4.039268919342476e-10
grad ChooseDest W: 4.2021026611328125
grad AddEdge W: 3.5648550084088626e-11
grad ChooseDest W: 2.673603057861328
grad AddEdge W: 1.0768817261530561e-11
grad ChooseDest W: 2.202493906021118
grad AddEdge W: 6.2699001546207e-08
grad ChooseDest W: 1.8579249382019043
grad AddEdge W: 6.961689447138042e-09
grad ChooseDest W: 1.9426982402801514
grad AddEdge W: 1.802891516633487e-11
grad ChooseDest W: 2.2197158336639404
grad AddEdge W: 1.6225394638968993e-10
grad ChooseDest W: 2.8195769786834717
grad AddEdge W: 6.4037958963369945e-12
grad ChooseDest W: 2.8527109622955322
grad AddEdge W: 1.1795274479364792e-10
grad ChooseDest W: 2.7434072494506836
grad AddEdge W: 4.61864990697336e-09
grad ChooseDest W: 2.27026629447937
grad AddEdge W: 4.0049617268389515e-12
grad ChooseDest W: 2.138988971710205
grad AddEdge W: 3.2894422862667083e-12
grad ChooseDest W: 3.577366828918457
grad AddEdge W: 3.909130352752754e-09
grad ChooseDest W: 1.7329695224761963
grad AddEdge W: 6.2878669285226785e-12
grad ChooseDest W: 1.4933538436889648
grad AddEdge W: 3.81118964512539e-12
grad ChooseDest W: 1.9056777954101562
grad AddEdge W: 1.4794508673698914e-10
grad ChooseDest W: 2.066197633743286
grad AddEdge W: 2.0850213916512317e-12
grad ChooseDest W: 2.1782662868499756
grad AddEdge W: 1.9037842483027312e-11
grad ChooseDest W: 2.6558444499969482
grad AddEdge W: 2.727853967016136e-12
grad ChooseDest W: 2.600383758544922
grad AddEdge W: 1.368155155140327e-12
grad ChooseDest W: 1.7936760187149048
grad AddEdge W: 2.6381113960027847e-10
grad ChooseDest W: 2.1274046897888184
grad AddEdge W: 5.076716824703453e-11
grad ChooseDest W: 3.0490925312042236
grad AddEdge W: 6.014914954999284e-11
grad ChooseDest W: 2.3439812660217285
grad AddEdge W: 1.1568540179626718e-12
grad ChooseDest W: 2.1031618118286133
grad AddEdge W: 3.482753321848442e-11
grad ChooseDest W: 2.3184163570404053
grad AddEdge W: 4.599248933090383e-12
grad ChooseDest W: 2.7483601570129395
grad AddEdge W: 2.0251415561861608e-12
grad ChooseDest W: 2.3456318378448486
grad AddEdge W: 1.476659683419257e-09
grad ChooseDest W: 1.8171367645263672
grad AddEdge W: 2.3102607066977088e-12
grad ChooseDest W: 2.1780097484588623
grad AddEdge W: 3.1326744583809463e-12
grad ChooseDest W: 2.267056465148926
grad AddEdge W: 2.8168502686637398e-12
grad ChooseDest W: 2.3084664344787598
grad AddEdge W: 3.8102038885101663e-11
grad ChooseDest W: 1.610106110572815
grad AddEdge W: 1.3226491013540453e-09
grad ChooseDest W: 2.11836314201355
grad AddEdge W: 2.5466493063647233e-12
grad ChooseDest W: 1.7101622819900513
grad AddEdge W: 2.3453374659032633e-12
grad ChooseDest W: 1.3868736028671265
grad AddEdge W: 1.0470410346033465e-12
grad ChooseDest W: 3.1022207736968994
grad AddEdge W: 7.273931890844354e-13
grad ChooseDest W: 1.6034421920776367
grad AddEdge W: 7.56670984550234e-13
grad ChooseDest W: 2.1606132984161377
grad AddEdge W: 6.618750144314622e-13
grad ChooseDest W: 2.3154492378234863
grad AddEdge W: 8.376618626168564e-13
grad ChooseDest W: 1.84238600730896
grad AddEdge W: 8.237482744533065e-12
grad ChooseDest W: 2.039433717727661
grad AddEdge W: 1.6249413923793976e-12
grad ChooseDest W: 1.4016835689544678
grad AddEdge W: 2.111998076426147e-11
grad ChooseDest W: 2.030806541442871
grad AddEdge W: 1.9281479895789239e-13
grad ChooseDest W: 1.6626108884811401
=== Epoch 5: Train Loss: 6.0126, Train Log Prob: 0.0069 ===
Total mismatches: 91751
Predicted valid destination but wrong order: 20322
Epoch 5: Validation Loss: 6.2796, Validation Log Prob: 0.0037
Epoch 5: Edge Precision: 0.3604, Recall: 0.3567, F1: 0.3584, Jaccard: 0.2354
Epoch 5: TP: 2.4979241231209737, FP: 4.454831782390838, FN: 4.5235504652827485
Epoch 5: warmup, skipping learning rate scheduler
Epoch 5: Current Learning Rate: 6e-05
[Epoch 5] ‚è±Ô∏è Total: 1920.23s | Current time: 2025-07-14 14:12:19 | üèãÔ∏è Train: 1680.57s | ‚úÖ Val: 239.66s
grad AddEdge W: 5.949930770299261e-09
grad ChooseDest W: 3.9665279388427734
grad AddEdge W: 3.396998665434331e-13
grad ChooseDest W: 2.289854049682617
grad AddEdge W: 1.5399691884101369e-13
grad ChooseDest W: 3.9687445163726807
grad AddEdge W: 8.698551996972129e-14
grad ChooseDest W: 1.5531322956085205
grad AddEdge W: 1.450487820131574e-11
grad ChooseDest W: 2.00559663772583
grad AddEdge W: 2.802941797934444e-13
grad ChooseDest W: 1.5323127508163452
grad AddEdge W: 1.4753698263092474e-11
grad ChooseDest W: 1.9451130628585815
grad AddEdge W: 5.2948014896136986e-12
grad ChooseDest W: 2.2830941677093506
grad AddEdge W: 1.3711784257932486e-13
grad ChooseDest W: 1.6072142124176025
grad AddEdge W: 2.1922544888360734e-13
grad ChooseDest W: 2.698209762573242
grad AddEdge W: 4.266384608878765e-13
grad ChooseDest W: 2.4955430030822754
grad AddEdge W: 1.1646248201935272e-12
grad ChooseDest W: 2.983712673187256
grad AddEdge W: 5.025274467385099e-12
grad ChooseDest W: 2.7353312969207764
grad AddEdge W: 4.3909771652028695e-12
grad ChooseDest W: 2.1948156356811523
grad AddEdge W: 2.982505084916931e-12
grad ChooseDest W: 2.0652246475219727
grad AddEdge W: 1.882885937009443e-13
grad ChooseDest W: 2.4505248069763184
grad AddEdge W: 7.742146089810206e-14
grad ChooseDest W: 1.6093937158584595
grad AddEdge W: 3.4428354806805006e-13
grad ChooseDest W: 2.454254388809204
grad AddEdge W: 1.1112111664851598e-12
grad ChooseDest W: 1.6702748537063599
grad AddEdge W: 3.809455789011151e-12
grad ChooseDest W: 2.1477513313293457
grad AddEdge W: 4.726929568252269e-13
grad ChooseDest W: 2.793544054031372
grad AddEdge W: 8.523029829642093e-14
grad ChooseDest W: 2.2772438526153564
grad AddEdge W: 2.6797542049955547e-12
grad ChooseDest W: 1.862808108329773
grad AddEdge W: 7.047545273058953e-12
grad ChooseDest W: 1.9380269050598145
grad AddEdge W: 5.238487120755482e-14
grad ChooseDest W: 2.220926523208618
grad AddEdge W: 2.4783293906677217e-12
grad ChooseDest W: 1.9604026079177856
grad AddEdge W: 7.037580018525932e-14
grad ChooseDest W: 2.770129680633545
grad AddEdge W: 2.6833540944792844e-10
grad ChooseDest W: 2.47737979888916
grad AddEdge W: 1.2380231388664509e-12
grad ChooseDest W: 2.931396007537842
grad AddEdge W: 1.9084893665421987e-09
grad ChooseDest W: 1.6017166376113892
grad AddEdge W: 2.3493828410492412e-12
grad ChooseDest W: 2.010006904602051
grad AddEdge W: 1.0886854162313678e-13
grad ChooseDest W: 1.984539270401001
grad AddEdge W: 5.898284790271935e-14
grad ChooseDest W: 1.5948079824447632
grad AddEdge W: 6.745875424018707e-14
grad ChooseDest W: 2.420431137084961
grad AddEdge W: 2.1453047400598474e-14
grad ChooseDest W: 3.0811030864715576
grad AddEdge W: 5.2391197158385694e-08
grad ChooseDest W: 1.0561137199401855
grad AddEdge W: 5.268343053477231e-12
grad ChooseDest W: 1.7774602174758911
grad AddEdge W: 2.687836498510565e-12
grad ChooseDest W: 2.793534994125366
grad AddEdge W: 5.714112722484538e-14
grad ChooseDest W: 3.425020456314087
grad AddEdge W: 1.729614064818752e-12
grad ChooseDest W: 2.5446858406066895
grad AddEdge W: 3.3680536658253768e-12
grad ChooseDest W: 2.192822217941284
grad AddEdge W: 2.0052121313222926e-13
grad ChooseDest W: 2.931168556213379
grad AddEdge W: 9.015139437913711e-13
grad ChooseDest W: 2.244016647338867
grad AddEdge W: 1.4066400022311361e-14
grad ChooseDest W: 1.8800495862960815
grad AddEdge W: 8.447001236498719e-13
grad ChooseDest W: 1.9620225429534912
grad AddEdge W: 1.3694753881257626e-12
grad ChooseDest W: 2.601551055908203
grad AddEdge W: 3.3470223662936116e-13
grad ChooseDest W: 2.890986680984497
grad AddEdge W: 4.9785665905610274e-14
grad ChooseDest W: 1.6903903484344482
grad AddEdge W: 1.0900152308535027e-12
grad ChooseDest W: 2.3051347732543945
grad AddEdge W: 1.5082368687302505e-10
grad ChooseDest W: 1.7870442867279053
grad AddEdge W: 5.63361037236431e-14
grad ChooseDest W: 1.2475892305374146
grad AddEdge W: 1.9437398739163836e-12
grad ChooseDest W: 2.0331432819366455
grad AddEdge W: 6.303008652118122e-14
grad ChooseDest W: 2.792288303375244
grad AddEdge W: 6.136864155953728e-14
grad ChooseDest W: 1.5586273670196533
grad AddEdge W: 5.004293962725115e-13
grad ChooseDest W: 3.4016401767730713
grad AddEdge W: 2.6914199154396377e-14
grad ChooseDest W: 2.1031653881073
grad AddEdge W: 3.364765538134244e-14
grad ChooseDest W: 2.3085923194885254
grad AddEdge W: 4.295872942853933e-14
grad ChooseDest W: 3.7941412925720215
grad AddEdge W: 4.6614025573515905e-14
grad ChooseDest W: 1.9031422138214111
grad AddEdge W: 1.169771558399502e-14
grad ChooseDest W: 2.5166194438934326
grad AddEdge W: 3.6537348938481956e-14
grad ChooseDest W: 2.8539066314697266
grad AddEdge W: 4.092009391921947e-13
grad ChooseDest W: 2.2140846252441406
grad AddEdge W: 2.7790456432396846e-14
grad ChooseDest W: 3.1400578022003174
grad AddEdge W: 5.895795657240299e-11
grad ChooseDest W: 1.5749207735061646
grad AddEdge W: 5.895921771637003e-13
grad ChooseDest W: 2.2008466720581055
grad AddEdge W: 1.2719051818168257e-14
grad ChooseDest W: 2.1136040687561035
=== Epoch 6: Train Loss: 6.0017, Train Log Prob: 0.0070 ===
Total mismatches: 91370
Predicted valid destination but wrong order: 20323
Epoch 6: Validation Loss: 6.3617, Validation Log Prob: 0.0036
Epoch 6: Edge Precision: 0.3589, Recall: 0.3558, F1: 0.3572, Jaccard: 0.2342
Epoch 6: TP: 2.490622763063708, FP: 4.473586256263421, FN: 4.530851825340014
Epoch 6: Current Learning Rate: 6e-05
[Epoch 6] ‚è±Ô∏è Total: 1919.95s | Current time: 2025-07-14 14:44:19 | üèãÔ∏è Train: 1680.90s | ‚úÖ Val: 239.05s
grad AddEdge W: 7.874361156613024e-14
grad ChooseDest W: 2.6175742149353027
grad AddEdge W: 2.0457485531977238e-14
grad ChooseDest W: 2.781031847000122
grad AddEdge W: 1.5256541689712133e-12
grad ChooseDest W: 3.5407345294952393
grad AddEdge W: 1.8956888738885097e-14
grad ChooseDest W: 2.643007755279541
grad AddEdge W: 1.9381887520169942e-14
grad ChooseDest W: 2.7484548091888428
grad AddEdge W: 6.200817853976859e-14
grad ChooseDest W: 1.9769537448883057
grad AddEdge W: 9.531746052171552e-13
grad ChooseDest W: 1.782949447631836
grad AddEdge W: 1.3896420838588157e-14
grad ChooseDest W: 2.995441198348999
grad AddEdge W: 7.258314609071004e-12
grad ChooseDest W: 3.6734211444854736
grad AddEdge W: 1.335988645287476e-14
grad ChooseDest W: 3.192518472671509
grad AddEdge W: 1.6875078808278876e-12
grad ChooseDest W: 1.5699644088745117
grad AddEdge W: 9.43168286526852e-13
grad ChooseDest W: 1.2573318481445312
grad AddEdge W: 7.814704287324725e-13
grad ChooseDest W: 1.9688891172409058
grad AddEdge W: 1.1711108699550415e-13
grad ChooseDest W: 2.328584671020508
grad AddEdge W: 4.0621642876693953e-13
grad ChooseDest W: 2.436666250228882
grad AddEdge W: 7.279667862437889e-13
grad ChooseDest W: 2.7290401458740234
grad AddEdge W: 4.144100156349556e-13
grad ChooseDest W: 1.8123306035995483
grad AddEdge W: 1.2892065073908526e-14
grad ChooseDest W: 2.1378567218780518
grad AddEdge W: 1.3710297003602379e-12
grad ChooseDest W: 1.4578677415847778
grad AddEdge W: 4.309195839378001e-15
grad ChooseDest W: 2.3843531608581543
grad AddEdge W: 1.21024507960827e-14
grad ChooseDest W: 2.885768175125122
grad AddEdge W: 2.8136776017277133e-14
grad ChooseDest W: 1.986768364906311
grad AddEdge W: 1.0210084324226501e-14
grad ChooseDest W: 2.5751965045928955
grad AddEdge W: 9.510994998572584e-15
grad ChooseDest W: 1.394380807876587
grad AddEdge W: 2.3991998166807138e-14
grad ChooseDest W: 2.5880143642425537
grad AddEdge W: 8.312373921506074e-15
grad ChooseDest W: 1.955365538597107
grad AddEdge W: 1.238732406987032e-14
grad ChooseDest W: 2.4587531089782715
grad AddEdge W: 1.3681739075011023e-09
grad ChooseDest W: 2.006892681121826
grad AddEdge W: 6.957403403395901e-13
grad ChooseDest W: 1.513035535812378
grad AddEdge W: 3.4880880295403132e-15
grad ChooseDest W: 1.5105640888214111
grad AddEdge W: 3.9148301570965383e-13
grad ChooseDest W: 1.5453295707702637
grad AddEdge W: 1.481637146413979e-14
grad ChooseDest W: 2.3555548191070557
grad AddEdge W: 5.550934603464064e-13
grad ChooseDest W: 1.593925952911377
grad AddEdge W: 3.895915758918419e-15
grad ChooseDest W: 1.857981562614441
grad AddEdge W: 2.9166668131250587e-14
grad ChooseDest W: 1.6406742334365845
grad AddEdge W: 1.1023156332493587e-14
grad ChooseDest W: 1.0963486433029175
grad AddEdge W: 1.3448714798053318e-14
grad ChooseDest W: 2.1122055053710938
grad AddEdge W: 5.0778375814298105e-15
grad ChooseDest W: 3.1327617168426514
grad AddEdge W: 1.0712011139224487e-11
grad ChooseDest W: 1.8596352338790894
grad AddEdge W: 1.163159619213235e-14
grad ChooseDest W: 1.3035838603973389
grad AddEdge W: 6.9003267997528456e-15
grad ChooseDest W: 3.2853899002075195
grad AddEdge W: 1.8308103297282052e-11
grad ChooseDest W: 1.32924485206604
grad AddEdge W: 4.616600070977972e-12
grad ChooseDest W: 2.8142406940460205
grad AddEdge W: 4.994490064580415e-13
grad ChooseDest W: 2.9345269203186035
grad AddEdge W: 8.687819581310648e-15
grad ChooseDest W: 2.88501238822937
grad AddEdge W: 4.946199767580546e-15
grad ChooseDest W: 3.0616824626922607
grad AddEdge W: 3.0830196793944775e-13
grad ChooseDest W: 2.7182071208953857
grad AddEdge W: 6.73894262875202e-13
grad ChooseDest W: 1.8794504404067993
grad AddEdge W: 2.0666643377642742e-14
grad ChooseDest W: 2.625683069229126
grad AddEdge W: 7.372495151804388e-15
grad ChooseDest W: 1.8694913387298584
grad AddEdge W: 2.611471423222961e-13
grad ChooseDest W: 2.28762149810791
grad AddEdge W: 1.9281091446479628e-15
grad ChooseDest W: 2.0186803340911865
grad AddEdge W: 7.998383043123641e-15
grad ChooseDest W: 1.8644918203353882
grad AddEdge W: 5.706243701701251e-15
grad ChooseDest W: 1.8877876996994019
grad AddEdge W: 5.074326206346968e-15
grad ChooseDest W: 2.6374385356903076
grad AddEdge W: 7.31500956677308e-15
grad ChooseDest W: 2.4209365844726562
grad AddEdge W: 8.003025630707542e-15
grad ChooseDest W: 1.897140383720398
grad AddEdge W: 1.7617927431469232e-15
grad ChooseDest W: 2.1855623722076416
grad AddEdge W: 2.4042638539778505e-12
grad ChooseDest W: 1.5261194705963135
grad AddEdge W: 2.0141471769510172e-13
grad ChooseDest W: 1.6765637397766113
grad AddEdge W: 2.1666409111276153e-08
grad ChooseDest W: 0.979040265083313
grad AddEdge W: 4.434715615564411e-12
grad ChooseDest W: 1.236864447593689
grad AddEdge W: 2.349119976232522e-13
grad ChooseDest W: 2.4597904682159424
grad AddEdge W: 3.8762040994490266e-13
grad ChooseDest W: 1.609570860862732
grad AddEdge W: 2.033491105536689e-13
grad ChooseDest W: 1.768354058265686
grad AddEdge W: 1.776881419779386e-11
grad ChooseDest W: 2.0254220962524414
=== Epoch 7: Train Loss: 6.0014, Train Log Prob: 0.0070 ===
Total mismatches: 91203
Predicted valid destination but wrong order: 20213
Epoch 7: Validation Loss: 6.2792, Validation Log Prob: 0.0037
Epoch 7: Edge Precision: 0.3592, Recall: 0.3561, F1: 0.3575, Jaccard: 0.2347
Epoch 7: TP: 2.4937723693629206, FP: 4.4728704366499645, FN: 4.527702219040802
Epoch 7: Current Learning Rate: 6e-05
[Epoch 7] ‚è±Ô∏è Total: 1916.24s | Current time: 2025-07-14 15:16:15 | üèãÔ∏è Train: 1681.57s | ‚úÖ Val: 234.67s
grad AddEdge W: 9.527742093548563e-13
grad ChooseDest W: 3.9202871322631836
grad AddEdge W: 7.547897040455914e-15
grad ChooseDest W: 2.2275187969207764
grad AddEdge W: 1.41184251247449e-13
grad ChooseDest W: 2.7249984741210938
grad AddEdge W: 1.8940895486291515e-15
grad ChooseDest W: 1.4730024337768555
grad AddEdge W: 1.323479832129014e-14
grad ChooseDest W: 3.2168993949890137
grad AddEdge W: 2.009583634481754e-14
grad ChooseDest W: 2.1145050525665283
grad AddEdge W: 4.51805395396479e-13
grad ChooseDest W: 2.3110783100128174
grad AddEdge W: 7.939478677905683e-15
grad ChooseDest W: 2.106841802597046
grad AddEdge W: 4.236708640165121e-13
grad ChooseDest W: 2.265667200088501
grad AddEdge W: 6.730254442405444e-15
grad ChooseDest W: 2.149338960647583
grad AddEdge W: 5.985878923743049e-15
grad ChooseDest W: 1.987257957458496
grad AddEdge W: 1.2630563641421316e-13
grad ChooseDest W: 1.9490966796875
grad AddEdge W: 1.2904654016383799e-13
grad ChooseDest W: 2.8577868938446045
grad AddEdge W: 9.804241193043812e-15
grad ChooseDest W: 1.7773489952087402
grad AddEdge W: 7.841292177200587e-13
grad ChooseDest W: 2.047377586364746
grad AddEdge W: 1.8638200791759596e-12
grad ChooseDest W: 2.321822166442871
grad AddEdge W: 1.9459770166790902e-13
grad ChooseDest W: 1.988336443901062
grad AddEdge W: 2.0922705842169043e-13
grad ChooseDest W: 2.2669918537139893
grad AddEdge W: 3.638457129985159e-15
grad ChooseDest W: 2.2531304359436035
grad AddEdge W: 4.61914975873382e-15
grad ChooseDest W: 2.8660099506378174
grad AddEdge W: 7.008026191963283e-15
grad ChooseDest W: 1.3699288368225098
grad AddEdge W: 8.743907561978986e-15
grad ChooseDest W: 2.6170544624328613
grad AddEdge W: 2.100627750087694e-13
grad ChooseDest W: 1.6317046880722046
grad AddEdge W: 3.4200342261843505e-15
grad ChooseDest W: 1.8117115497589111
grad AddEdge W: 7.753835008957044e-13
grad ChooseDest W: 2.480280876159668
grad AddEdge W: 2.2777493866726894e-13
grad ChooseDest W: 1.6215499639511108
grad AddEdge W: 3.437924112601777e-14
grad ChooseDest W: 2.754237413406372
grad AddEdge W: 5.2836699757443945e-15
grad ChooseDest W: 2.490931987762451
grad AddEdge W: 1.4354969578474208e-13
grad ChooseDest W: 2.332873821258545
grad AddEdge W: 9.895813552276977e-13
grad ChooseDest W: 2.113016366958618
grad AddEdge W: 6.277466610762894e-13
grad ChooseDest W: 2.710383415222168
grad AddEdge W: 1.8813949009821584e-10
grad ChooseDest W: 3.1689610481262207
grad AddEdge W: 4.05150999385664e-16
grad ChooseDest W: 1.4710075855255127
grad AddEdge W: 1.9978227620036295e-16
grad ChooseDest W: 2.5468924045562744
grad AddEdge W: 6.029781382553068e-17
grad ChooseDest W: 2.1396725177764893
grad AddEdge W: 2.11238446575163e-14
grad ChooseDest W: 2.4352450370788574
grad AddEdge W: 1.2932248899261422e-16
grad ChooseDest W: 2.9512412548065186
grad AddEdge W: 1.603969382189325e-16
grad ChooseDest W: 1.493240237236023
grad AddEdge W: 2.5935205589036614e-13
grad ChooseDest W: 1.419232726097107
grad AddEdge W: 6.011635501603158e-15
grad ChooseDest W: 1.841085433959961
grad AddEdge W: 6.088738369692875e-15
grad ChooseDest W: 1.1020783185958862
grad AddEdge W: 3.818371163146709e-15
grad ChooseDest W: 1.6777610778808594
grad AddEdge W: 6.190240445344726e-15
grad ChooseDest W: 1.949294090270996
grad AddEdge W: 3.386081373714805e-17
grad ChooseDest W: 1.6999233961105347
grad AddEdge W: 8.411892801860827e-17
grad ChooseDest W: 2.140795946121216
grad AddEdge W: 1.0264742513279874e-14
grad ChooseDest W: 1.9267241954803467
grad AddEdge W: 5.948026715396149e-15
grad ChooseDest W: 3.8218774795532227
grad AddEdge W: 4.911495067491194e-17
grad ChooseDest W: 3.3481786251068115
grad AddEdge W: 2.8136062815535545e-15
grad ChooseDest W: 1.719468116760254
grad AddEdge W: 2.0907355596108523e-17
grad ChooseDest W: 2.1345772743225098
grad AddEdge W: 1.3198368122443002e-16
grad ChooseDest W: 1.6149506568908691
grad AddEdge W: 7.534557250918504e-17
grad ChooseDest W: 2.7292017936706543
grad AddEdge W: 1.0790833803316793e-16
grad ChooseDest W: 2.8221070766448975
grad AddEdge W: 2.1531441993932754e-15
grad ChooseDest W: 2.4563496112823486
grad AddEdge W: 6.653334612841672e-17
grad ChooseDest W: 2.478785276412964
grad AddEdge W: 4.744756168603042e-16
grad ChooseDest W: 1.8545851707458496
grad AddEdge W: 1.2867543523025074e-16
grad ChooseDest W: 2.446180582046509
grad AddEdge W: 4.9241687979644864e-17
grad ChooseDest W: 3.864056348800659
grad AddEdge W: 1.2206475335851475e-16
grad ChooseDest W: 2.882473945617676
grad AddEdge W: 4.025024332387182e-15
grad ChooseDest W: 3.1152422428131104
grad AddEdge W: 3.5432275450659632e-15
grad ChooseDest W: 2.182793140411377
grad AddEdge W: 1.65172126163748e-16
grad ChooseDest W: 1.7442344427108765
grad AddEdge W: 7.452896657358289e-17
grad ChooseDest W: 1.2033004760742188
grad AddEdge W: 9.349590329374435e-15
grad ChooseDest W: 1.8113235235214233
grad AddEdge W: 2.827157114643729e-15
grad ChooseDest W: 1.9412814378738403
grad AddEdge W: 9.058315921725297e-17
grad ChooseDest W: 1.8811981678009033
=== Epoch 8: Train Loss: 6.0071, Train Log Prob: 0.0071 ===
Total mismatches: 91488
Predicted valid destination but wrong order: 19793
Epoch 8: Validation Loss: 6.2557, Validation Log Prob: 0.0039
Epoch 8: Edge Precision: 0.3596, Recall: 0.3572, F1: 0.3583, Jaccard: 0.2353
Epoch 8: TP: 2.501646385110952, FP: 4.477451682176092, FN: 4.519828203292771
Epoch 8: Current Learning Rate: 6e-05
[Epoch 8] ‚è±Ô∏è Total: 1910.94s | Current time: 2025-07-14 15:48:06 | üèãÔ∏è Train: 1672.08s | ‚úÖ Val: 238.86s
grad AddEdge W: 2.7396664462471217e-15
grad ChooseDest W: 5.24233341217041
grad AddEdge W: 6.068583432471196e-17
grad ChooseDest W: 2.02152156829834
grad AddEdge W: 6.819326733072811e-16
grad ChooseDest W: 1.8230018615722656
grad AddEdge W: 7.194688589555166e-17
grad ChooseDest W: 0.7975207567214966
grad AddEdge W: 8.914753763333044e-18
grad ChooseDest W: 1.5912847518920898
grad AddEdge W: 8.914271298660244e-15
grad ChooseDest W: 2.7241299152374268
grad AddEdge W: 2.6753433995073406e-15
grad ChooseDest W: 1.9301162958145142
grad AddEdge W: 1.9857105614348074e-15
grad ChooseDest W: 1.4628328084945679
grad AddEdge W: 1.6892699674914672e-16
grad ChooseDest W: 2.3286521434783936
grad AddEdge W: 1.722684384939245e-15
grad ChooseDest W: 2.4510552883148193
grad AddEdge W: 1.1285220703077543e-14
grad ChooseDest W: 1.6913365125656128
grad AddEdge W: 9.292386859254123e-17
grad ChooseDest W: 1.593995213508606
grad AddEdge W: 2.7241418675711673e-16
grad ChooseDest W: 2.2043087482452393
grad AddEdge W: 6.597510509406183e-17
grad ChooseDest W: 3.3785886764526367
grad AddEdge W: 1.6681195547985273e-17
grad ChooseDest W: 2.6968472003936768
grad AddEdge W: 3.357603637395984e-13
grad ChooseDest W: 1.1987296342849731
grad AddEdge W: 2.6849576469751505e-15
grad ChooseDest W: 2.240886926651001
grad AddEdge W: 4.1764928154231154e-17
grad ChooseDest W: 2.4645755290985107
grad AddEdge W: 3.8812654373580394e-17
grad ChooseDest W: 2.0689690113067627
grad AddEdge W: 3.510457574107258e-17
grad ChooseDest W: 2.0376529693603516
grad AddEdge W: 2.498265444411435e-16
grad ChooseDest W: 2.2335362434387207
grad AddEdge W: 1.570594464270158e-17
grad ChooseDest W: 3.1356050968170166
grad AddEdge W: 2.08477823792647e-16
grad ChooseDest W: 7.574844837188721
grad AddEdge W: 5.203108248614564e-15
grad ChooseDest W: 2.27329421043396
grad AddEdge W: 6.93267085872287e-17
grad ChooseDest W: 2.184487819671631
grad AddEdge W: 3.7560051463268894e-17
grad ChooseDest W: 1.3227812051773071
grad AddEdge W: 8.350954075261801e-17
grad ChooseDest W: 2.8116793632507324
grad AddEdge W: 1.878763413890871e-17
grad ChooseDest W: 3.200899600982666
grad AddEdge W: 2.3234401602038627e-17
grad ChooseDest W: 1.8445656299591064
grad AddEdge W: 3.4757206205914614e-17
grad ChooseDest W: 2.4451770782470703
grad AddEdge W: 5.876789550466168e-15
grad ChooseDest W: 3.3884642124176025
grad AddEdge W: 1.1079355062719779e-16
grad ChooseDest W: 1.8167189359664917
grad AddEdge W: 3.697006323700422e-17
grad ChooseDest W: 2.3227827548980713
grad AddEdge W: 3.700996113579786e-15
grad ChooseDest W: 2.2413992881774902
grad AddEdge W: 8.282465188138525e-15
grad ChooseDest W: 1.6827113628387451
grad AddEdge W: 3.8548386711481953e-17
grad ChooseDest W: 1.781772255897522
grad AddEdge W: 1.0257141239611214e-15
grad ChooseDest W: 1.8199251890182495
grad AddEdge W: 2.109049358225111e-15
grad ChooseDest W: 2.1383659839630127
grad AddEdge W: 1.8679419380089428e-14
grad ChooseDest W: 1.375962734222412
grad AddEdge W: 1.4865921197488273e-14
grad ChooseDest W: 2.6329054832458496
grad AddEdge W: 1.557370764568636e-15
grad ChooseDest W: 1.5866974592208862
grad AddEdge W: 1.3414182323584945e-15
grad ChooseDest W: 2.5546517372131348
grad AddEdge W: 1.665850432942172e-17
grad ChooseDest W: 2.508197546005249
grad AddEdge W: 2.8007677105277925e-17
grad ChooseDest W: 2.427192211151123
grad AddEdge W: 1.1340157830668358e-16
grad ChooseDest W: 1.2870646715164185
grad AddEdge W: 2.1330703316949778e-13
grad ChooseDest W: 2.361069679260254
grad AddEdge W: 6.190699113685664e-17
grad ChooseDest W: 2.633774518966675
grad AddEdge W: 3.7360710861810965e-17
grad ChooseDest W: 2.7380566596984863
grad AddEdge W: 1.6583797188144192e-14
grad ChooseDest W: 2.199718952178955
grad AddEdge W: 4.217275129374948e-13
grad ChooseDest W: 1.6946288347244263
grad AddEdge W: 2.780342450681931e-15
grad ChooseDest W: 1.2113536596298218
grad AddEdge W: 9.306266288188272e-15
grad ChooseDest W: 3.7924246788024902
grad AddEdge W: 1.985737242972646e-15
grad ChooseDest W: 1.7432324886322021
grad AddEdge W: 1.5388795324042622e-17
grad ChooseDest W: 3.1542460918426514
grad AddEdge W: 1.9709402126588237e-15
grad ChooseDest W: 1.1676013469696045
grad AddEdge W: 5.4031316909768784e-15
grad ChooseDest W: 2.037947654724121
grad AddEdge W: 2.4807002641038716e-17
grad ChooseDest W: 2.1417930126190186
grad AddEdge W: 8.259763858119162e-14
grad ChooseDest W: 1.8289220333099365
grad AddEdge W: 1.048712071021978e-17
grad ChooseDest W: 2.2529444694519043
grad AddEdge W: 9.194181329953867e-17
grad ChooseDest W: 2.215891122817993
grad AddEdge W: 6.927699834113671e-17
grad ChooseDest W: 1.7925902605056763
grad AddEdge W: 1.9305241414597032e-16
grad ChooseDest W: 2.564539670944214
grad AddEdge W: 1.7085294055994415e-15
grad ChooseDest W: 1.8731796741485596
grad AddEdge W: 3.447214984066149e-16
grad ChooseDest W: 1.6799510717391968
grad AddEdge W: 4.326644294574966e-17
grad ChooseDest W: 2.092379093170166
grad AddEdge W: 2.3814256865799524e-17
grad ChooseDest W: 2.1912858486175537
=== Epoch 9: Train Loss: 5.9985, Train Log Prob: 0.0070 ===
Total mismatches: 91402
Predicted valid destination but wrong order: 20141
Epoch 9: Validation Loss: 6.1673, Validation Log Prob: 0.0042
Epoch 9: Edge Precision: 0.3592, Recall: 0.3570, F1: 0.3580, Jaccard: 0.2346
Epoch 9: TP: 2.4997852541159626, FP: 4.480458124552613, FN: 4.5216893342877595
Epoch 9: Current Learning Rate: 6e-05
[Epoch 9] ‚è±Ô∏è Total: 1912.96s | Current time: 2025-07-14 16:19:59 | üèãÔ∏è Train: 1679.14s | ‚úÖ Val: 233.82s
grad AddEdge W: 1.2690907454429839e-14
grad ChooseDest W: 4.054193496704102
grad AddEdge W: 1.5847069529962557e-15
grad ChooseDest W: 2.6358730792999268
grad AddEdge W: 6.026706255907841e-17
grad ChooseDest W: 2.047473669052124
grad AddEdge W: 4.443869022263531e-17
grad ChooseDest W: 2.4128472805023193
grad AddEdge W: 1.3028813037528576e-15
grad ChooseDest W: 1.8055462837219238
grad AddEdge W: 5.336490950335173e-15
grad ChooseDest W: 1.630864143371582
grad AddEdge W: 1.5431006681573448e-16
grad ChooseDest W: 1.6077003479003906
grad AddEdge W: 4.291416657519803e-17
grad ChooseDest W: 1.7572569847106934
grad AddEdge W: 1.7475574481430611e-13
grad ChooseDest W: 1.9761600494384766
grad AddEdge W: 1.6507478354926277e-16
grad ChooseDest W: 1.551076889038086
grad AddEdge W: 2.272359110400753e-17
grad ChooseDest W: 1.5889647006988525
grad AddEdge W: 4.01940691576011e-17
grad ChooseDest W: 1.0507107973098755
grad AddEdge W: 2.685368239578887e-17
grad ChooseDest W: 2.3395562171936035
grad AddEdge W: 2.122250644640755e-17
grad ChooseDest W: 3.1667675971984863
grad AddEdge W: 1.0278495999002679e-15
grad ChooseDest W: 1.934648871421814
grad AddEdge W: 1.7130427147797676e-15
grad ChooseDest W: 1.5400493144989014
grad AddEdge W: 1.9929270703871772e-15
grad ChooseDest W: 1.7312835454940796
grad AddEdge W: 1.3144811817374888e-16
grad ChooseDest W: 1.7541613578796387
grad AddEdge W: 1.0913198300459517e-16
grad ChooseDest W: 1.822872519493103
grad AddEdge W: 2.8918680892384626e-17
grad ChooseDest W: 2.226715564727783
grad AddEdge W: 5.848325749427516e-17
grad ChooseDest W: 3.3603811264038086
grad AddEdge W: 7.559810082403013e-17
grad ChooseDest W: 2.193127393722534
grad AddEdge W: 2.7992586022182507e-15
grad ChooseDest W: 1.6364679336547852
grad AddEdge W: 5.851691381903872e-17
grad ChooseDest W: 1.887024164199829
grad AddEdge W: 6.831693943498316e-16
grad ChooseDest W: 1.8799561262130737
grad AddEdge W: 1.175622438482661e-15
grad ChooseDest W: 1.714192509651184
grad AddEdge W: 6.120883746497209e-17
grad ChooseDest W: 2.3608405590057373
grad AddEdge W: 3.0024608136478223e-17
grad ChooseDest W: 2.281813144683838
grad AddEdge W: 3.785417703675805e-17
grad ChooseDest W: 1.6060930490493774
grad AddEdge W: 6.826503881462913e-17
grad ChooseDest W: 1.8490734100341797
grad AddEdge W: 4.8567843491602016e-17
grad ChooseDest W: 2.9951820373535156
grad AddEdge W: 1.4272859194537133e-17
grad ChooseDest W: 2.0340988636016846
grad AddEdge W: 4.576530872476246e-15
grad ChooseDest W: 1.494107723236084
grad AddEdge W: 7.047730014332881e-15
grad ChooseDest W: 2.2687387466430664
grad AddEdge W: 3.094455725325242e-15
grad ChooseDest W: 2.47389817237854
grad AddEdge W: 6.719620234920242e-17
grad ChooseDest W: 1.9830355644226074
grad AddEdge W: 8.959193214561843e-17
grad ChooseDest W: 1.7294985055923462
grad AddEdge W: 1.8274047622741886e-15
grad ChooseDest W: 2.460926055908203
grad AddEdge W: 3.500817346550769e-16
grad ChooseDest W: 2.688135862350464
grad AddEdge W: 1.007401562809059e-16
grad ChooseDest W: 2.4383673667907715
grad AddEdge W: 2.2589630858165793e-17
grad ChooseDest W: 2.134305000305176
grad AddEdge W: 1.222204909537793e-15
grad ChooseDest W: 2.183725595474243
grad AddEdge W: 1.324233839682836e-15
grad ChooseDest W: 2.130633592605591
grad AddEdge W: 5.052613999051762e-16
grad ChooseDest W: 6.098621845245361
grad AddEdge W: 6.086956106492734e-16
grad ChooseDest W: 2.367586612701416
grad AddEdge W: 4.467521093826627e-17
grad ChooseDest W: 2.3390212059020996
grad AddEdge W: 2.9253563970759984e-15
grad ChooseDest W: 3.1435484886169434
grad AddEdge W: 9.725832411907665e-16
grad ChooseDest W: 1.6684173345565796
grad AddEdge W: 2.4687046298269705e-15
grad ChooseDest W: 1.844213604927063
grad AddEdge W: 5.413854704572644e-15
grad ChooseDest W: 2.7268240451812744
grad AddEdge W: 3.4310652421355617e-16
grad ChooseDest W: 1.4961923360824585
grad AddEdge W: 2.4327252113264496e-16
grad ChooseDest W: 3.6590383052825928
grad AddEdge W: 3.3814415522228725e-17
grad ChooseDest W: 3.0241096019744873
grad AddEdge W: 9.187384367008184e-15
grad ChooseDest W: 4.094826698303223
grad AddEdge W: 3.96168691436065e-17
grad ChooseDest W: 2.102384090423584
grad AddEdge W: 8.752112849548378e-17
grad ChooseDest W: 3.946523904800415
grad AddEdge W: 2.9935940065815153e-15
grad ChooseDest W: 2.2433180809020996
grad AddEdge W: 2.7047892823087746e-16
grad ChooseDest W: 4.368594646453857
grad AddEdge W: 1.5296930299573708e-17
grad ChooseDest W: 2.7252464294433594
grad AddEdge W: 1.0053408242926179e-16
grad ChooseDest W: 3.492780923843384
grad AddEdge W: 1.1656443076459442e-17
grad ChooseDest W: 3.2821717262268066
grad AddEdge W: 5.736127447596856e-15
grad ChooseDest W: 4.038182258605957
grad AddEdge W: 1.0749692486626365e-16
grad ChooseDest W: 2.097820520401001
grad AddEdge W: 6.3594136772186175e-15
grad ChooseDest W: 1.9926731586456299
grad AddEdge W: 7.392439019003524e-17
grad ChooseDest W: 2.372481346130371
grad AddEdge W: 2.5442216140114112e-17
grad ChooseDest W: 2.5557987689971924
=== Epoch 10: Train Loss: 6.0080, Train Log Prob: 0.0070 ===
Total mismatches: 91627
Predicted valid destination but wrong order: 19965
Epoch 10: Validation Loss: 6.3129, Validation Log Prob: 0.0039
Epoch 10: Edge Precision: 0.3625, Recall: 0.3592, F1: 0.3607, Jaccard: 0.2370
Epoch 10: TP: 2.517108088761632, FP: 4.448389405869721, FN: 4.5043664996420905
Epoch 10: Current Learning Rate: 6e-05
[Epoch 10] ‚è±Ô∏è Total: 1903.98s | Current time: 2025-07-14 16:51:43 | üèãÔ∏è Train: 1669.53s | ‚úÖ Val: 234.45s
grad AddEdge W: 8.041679132222545e-15
grad ChooseDest W: 6.827363967895508
grad AddEdge W: 3.530468476023235e-15
grad ChooseDest W: 2.7340903282165527
grad AddEdge W: 1.1957932575721014e-14
grad ChooseDest W: 1.9143128395080566
grad AddEdge W: 1.234133224547722e-16
grad ChooseDest W: 1.7223172187805176
grad AddEdge W: 4.4925757328363684e-17
grad ChooseDest W: 1.9945629835128784
grad AddEdge W: 3.4828967763648284e-15
grad ChooseDest W: 2.2029595375061035
grad AddEdge W: 5.0893123367662645e-15
grad ChooseDest W: 0.7379196286201477
grad AddEdge W: 3.0797840558876135e-15
grad ChooseDest W: 2.2370030879974365
grad AddEdge W: 1.056537745555934e-16
grad ChooseDest W: 2.445964813232422
grad AddEdge W: 3.309134717029407e-17
grad ChooseDest W: 2.5603137016296387
grad AddEdge W: 4.491168202306048e-17
grad ChooseDest W: 2.782397508621216
grad AddEdge W: 2.037594082311212e-15
grad ChooseDest W: 2.083165168762207
grad AddEdge W: 2.2824808364830867e-15
grad ChooseDest W: 2.11763596534729
grad AddEdge W: 5.674119527190378e-17
grad ChooseDest W: 1.9709694385528564
grad AddEdge W: 2.5226543684641936e-17
grad ChooseDest W: 2.737293243408203
grad AddEdge W: 1.129013259419911e-16
grad ChooseDest W: 1.8379361629486084
grad AddEdge W: 2.424072769770247e-15
grad ChooseDest W: 3.025942325592041
grad AddEdge W: 7.556856716743954e-17
grad ChooseDest W: 1.495071291923523
grad AddEdge W: 5.942469491047891e-17
grad ChooseDest W: 2.00813889503479
grad AddEdge W: 9.054508905674083e-17
grad ChooseDest W: 1.5504770278930664
grad AddEdge W: 3.7092413175768166e-17
grad ChooseDest W: 2.2204294204711914
grad AddEdge W: 4.9790022698983815e-17
grad ChooseDest W: 1.7713751792907715
grad AddEdge W: 5.1606663525382974e-17
grad ChooseDest W: 2.62023663520813
grad AddEdge W: 5.433143338847519e-15
grad ChooseDest W: 2.1726431846618652
grad AddEdge W: 5.568288716131364e-17
grad ChooseDest W: 1.339228868484497
grad AddEdge W: 2.854872932650396e-17
grad ChooseDest W: 1.65252685546875
grad AddEdge W: 6.094168697917722e-15
grad ChooseDest W: 2.0457041263580322
grad AddEdge W: 2.9683306798293944e-17
grad ChooseDest W: 2.1606483459472656
grad AddEdge W: 2.722072089624992e-15
grad ChooseDest W: 1.3861421346664429
grad AddEdge W: 4.847304197595854e-17
grad ChooseDest W: 2.284838914871216
grad AddEdge W: 3.721272825022523e-17
grad ChooseDest W: 2.170559883117676
grad AddEdge W: 3.0868201997617773e-16
grad ChooseDest W: 4.267051696777344
grad AddEdge W: 3.488673607239552e-17
grad ChooseDest W: 2.5223190784454346
grad AddEdge W: 8.058196989378053e-17
grad ChooseDest W: 1.8079054355621338
grad AddEdge W: 2.796591441051135e-17
grad ChooseDest W: 2.4720847606658936
grad AddEdge W: 2.69882878425084e-17
grad ChooseDest W: 2.879916191101074
grad AddEdge W: 1.2643586688282056e-15
grad ChooseDest W: 2.6226840019226074
grad AddEdge W: 2.3073376535949746e-15
grad ChooseDest W: 2.0802602767944336
grad AddEdge W: 1.7755330932416008e-17
grad ChooseDest W: 3.1281027793884277
grad AddEdge W: 2.5386806619601635e-17
grad ChooseDest W: 2.543790578842163
grad AddEdge W: 4.898534801653713e-17
grad ChooseDest W: 1.906916856765747
grad AddEdge W: 3.595624156150167e-16
grad ChooseDest W: 2.1024749279022217
grad AddEdge W: 2.1005169703542918e-17
grad ChooseDest W: 2.0530500411987305
grad AddEdge W: 2.551379915918437e-15
grad ChooseDest W: 3.544936418533325
grad AddEdge W: 4.307460692385551e-15
grad ChooseDest W: 1.1589754819869995
grad AddEdge W: 2.0671128364158894e-16
grad ChooseDest W: 3.149296998977661
grad AddEdge W: 1.2599594410892404e-17
grad ChooseDest W: 2.868248701095581
grad AddEdge W: 1.0719105332807625e-16
grad ChooseDest W: 2.584343910217285
grad AddEdge W: 7.507390654368793e-17
grad ChooseDest W: 1.1243669986724854
grad AddEdge W: 7.412308241424182e-15
grad ChooseDest W: 2.1840476989746094
grad AddEdge W: 1.7958882333178085e-14
grad ChooseDest W: 1.473164677619934
grad AddEdge W: 8.334985518972587e-17
grad ChooseDest W: 2.2304160594940186
grad AddEdge W: 9.246154080457309e-17
grad ChooseDest W: 2.414891481399536
grad AddEdge W: 7.175762940661926e-14
grad ChooseDest W: 1.4923838376998901
grad AddEdge W: 1.8038584043933766e-17
grad ChooseDest W: 2.6684725284576416
grad AddEdge W: 2.5690099008639103e-17
grad ChooseDest W: 1.7847249507904053
grad AddEdge W: 1.468198471073933e-16
grad ChooseDest W: 2.312782049179077
grad AddEdge W: 5.3786026359109085e-17
grad ChooseDest W: 4.231296539306641
grad AddEdge W: 3.101772779735429e-17
grad ChooseDest W: 2.4665422439575195
grad AddEdge W: 5.427041339965279e-17
grad ChooseDest W: 2.0689852237701416
grad AddEdge W: 2.8186656093475045e-15
grad ChooseDest W: 2.281761884689331
grad AddEdge W: 1.2716213410762008e-15
grad ChooseDest W: 3.125913143157959
grad AddEdge W: 1.8598768290983662e-15
grad ChooseDest W: 2.123767852783203
grad AddEdge W: 2.6054492582547903e-16
grad ChooseDest W: 3.133821487426758
grad AddEdge W: 1.5263900977714466e-17
grad ChooseDest W: 1.980347752571106
grad AddEdge W: 4.4253981718294505e-15
grad ChooseDest W: 2.506251811981201
=== Epoch 11: Train Loss: 6.0033, Train Log Prob: 0.0070 ===
Total mismatches: 91527
Predicted valid destination but wrong order: 20271
Epoch 11: Validation Loss: 6.3024, Validation Log Prob: 0.0038
Epoch 11: Edge Precision: 0.3608, Recall: 0.3591, F1: 0.3599, Jaccard: 0.2367
Epoch 11: TP: 2.514674302075877, FP: 4.4773085182534, FN: 4.506800286327845
Epoch 11: Current Learning Rate: 6e-05
[Epoch 11] ‚è±Ô∏è Total: 1913.84s | Current time: 2025-07-14 17:23:37 | üèãÔ∏è Train: 1676.99s | ‚úÖ Val: 236.85s
grad AddEdge W: 1.3774718721210185e-15
grad ChooseDest W: 4.743810653686523
grad AddEdge W: 3.719076031838929e-14
grad ChooseDest W: 4.113101482391357
grad AddEdge W: 3.0946560486172676e-15
grad ChooseDest W: 2.373059034347534
grad AddEdge W: 7.803996435618715e-14
grad ChooseDest W: 1.771941900253296
grad AddEdge W: 9.90460506817279e-16
grad ChooseDest W: 2.2906665802001953
grad AddEdge W: 6.80165339062835e-15
grad ChooseDest W: 2.071021556854248
grad AddEdge W: 3.870007178348948e-17
grad ChooseDest W: 1.9787743091583252
grad AddEdge W: 5.291740929182307e-15
grad ChooseDest W: 1.404435157775879
grad AddEdge W: 1.5661943597197433e-17
grad ChooseDest W: 2.2642147541046143
grad AddEdge W: 4.6403131680885444e-17
grad ChooseDest W: 1.7873551845550537
grad AddEdge W: 8.536497568800141e-16
grad ChooseDest W: 1.9461997747421265
grad AddEdge W: 1.5559132326246482e-16
grad ChooseDest W: 2.207174777984619
grad AddEdge W: 6.783679880883206e-16
grad ChooseDest W: 2.422691822052002
grad AddEdge W: 8.11992848587615e-17
grad ChooseDest W: 1.8989571332931519
grad AddEdge W: 1.5269054670272696e-14
grad ChooseDest W: 2.6490368843078613
grad AddEdge W: 1.6296052313846703e-16
grad ChooseDest W: 2.098731756210327
grad AddEdge W: 3.224011955706629e-15
grad ChooseDest W: 2.425837755203247
grad AddEdge W: 1.980327958222582e-17
grad ChooseDest W: 3.077672243118286
grad AddEdge W: 3.1197635620705544e-13
grad ChooseDest W: 1.430777907371521
grad AddEdge W: 4.014097408844255e-17
grad ChooseDest W: 3.112199544906616
grad AddEdge W: 1.175373093158813e-15
grad ChooseDest W: 1.3642834424972534
grad AddEdge W: 1.0770933161267747e-16
grad ChooseDest W: 1.9706753492355347
grad AddEdge W: 5.641598094227243e-17
grad ChooseDest W: 2.6732211112976074
grad AddEdge W: 2.3573499331071065e-17
grad ChooseDest W: 2.3912975788116455
grad AddEdge W: 5.327471624398801e-17
grad ChooseDest W: 1.7903167009353638
grad AddEdge W: 4.906133944505115e-17
grad ChooseDest W: 2.350536346435547
grad AddEdge W: 9.602869598484269e-18
grad ChooseDest W: 2.3255209922790527
grad AddEdge W: 7.721100031576368e-15
grad ChooseDest W: 3.1638057231903076
grad AddEdge W: 4.3018788379073735e-17
grad ChooseDest W: 2.7308475971221924
grad AddEdge W: 5.558631878788175e-17
grad ChooseDest W: 1.537522792816162
grad AddEdge W: 5.699502722339425e-17
grad ChooseDest W: 2.627786874771118
grad AddEdge W: 1.7294468908493313e-17
grad ChooseDest W: 3.336300849914551
grad AddEdge W: 3.217668102448168e-15
grad ChooseDest W: 2.5024771690368652
grad AddEdge W: 1.6006326038903174e-13
grad ChooseDest W: 1.5858781337738037
grad AddEdge W: 6.6049613288475875e-15
grad ChooseDest W: 2.143026113510132
grad AddEdge W: 2.188563458065118e-13
grad ChooseDest W: 1.5160794258117676
grad AddEdge W: 3.8708925924766245e-17
grad ChooseDest W: 2.6759519577026367
grad AddEdge W: 1.5167468261903034e-16
grad ChooseDest W: 2.3400180339813232
grad AddEdge W: 3.0061239532120114e-15
grad ChooseDest W: 2.295177936553955
grad AddEdge W: 5.107833559190927e-15
grad ChooseDest W: 4.167725086212158
grad AddEdge W: 2.442303532685895e-17
grad ChooseDest W: 4.2748613357543945
grad AddEdge W: 2.5437787415114503e-17
grad ChooseDest W: 2.5447354316711426
grad AddEdge W: 8.325866309322888e-15
grad ChooseDest W: 2.6120522022247314
grad AddEdge W: 7.61155188407943e-17
grad ChooseDest W: 2.29731822013855
grad AddEdge W: 6.411222818014341e-17
grad ChooseDest W: 2.0400962829589844
grad AddEdge W: 6.197921393049987e-17
grad ChooseDest W: 3.3984036445617676
grad AddEdge W: 1.369204539388241e-16
grad ChooseDest W: 1.8012454509735107
grad AddEdge W: 1.6240907160257998e-17
grad ChooseDest W: 2.5184834003448486
grad AddEdge W: 6.932486893754638e-17
grad ChooseDest W: 1.943550705909729
grad AddEdge W: 3.070127827349355e-17
grad ChooseDest W: 1.7756001949310303
grad AddEdge W: 1.38271119441626e-15
grad ChooseDest W: 1.8107094764709473
grad AddEdge W: 2.1781632233145625e-15
grad ChooseDest W: 1.5211318731307983
grad AddEdge W: 6.871709467375059e-18
grad ChooseDest W: 2.5304348468780518
grad AddEdge W: 1.788046350676058e-17
grad ChooseDest W: 2.151137113571167
grad AddEdge W: 1.0359513376918572e-16
grad ChooseDest W: 2.234574794769287
grad AddEdge W: 6.560558591203096e-15
grad ChooseDest W: 1.744280457496643
grad AddEdge W: 3.1231229625020007e-13
grad ChooseDest W: 1.4618840217590332
grad AddEdge W: 3.883440922369054e-17
grad ChooseDest W: 2.5638482570648193
grad AddEdge W: 4.702968327545843e-15
grad ChooseDest W: 1.8900913000106812
grad AddEdge W: 6.444414280508534e-15
grad ChooseDest W: 1.489917516708374
grad AddEdge W: 3.057548382231004e-15
grad ChooseDest W: 2.7169644832611084
grad AddEdge W: 2.703967395652142e-15
grad ChooseDest W: 3.602431297302246
grad AddEdge W: 5.881187319058532e-17
grad ChooseDest W: 2.390618085861206
grad AddEdge W: 1.3997014047887604e-15
grad ChooseDest W: 2.0007128715515137
grad AddEdge W: 7.309744780549862e-17
grad ChooseDest W: 2.118560314178467
grad AddEdge W: 1.1796103754774527e-16
grad ChooseDest W: 3.5900063514709473
=== Epoch 12: Train Loss: 5.9968, Train Log Prob: 0.0070 ===
Total mismatches: 91109
Predicted valid destination but wrong order: 19984
Epoch 12: Validation Loss: 6.2412, Validation Log Prob: 0.0040
Epoch 12: Edge Precision: 0.3599, Recall: 0.3567, F1: 0.3582, Jaccard: 0.2356
Epoch 12: TP: 2.498783106657122, FP: 4.46571224051539, FN: 4.5226914817466
Epoch 12: Current Learning Rate: 6e-05
[Epoch 12] ‚è±Ô∏è Total: 1919.47s | Current time: 2025-07-14 17:55:36 | üèãÔ∏è Train: 1684.89s | ‚úÖ Val: 234.58s
grad AddEdge W: 5.184819960250397e-15
grad ChooseDest W: 5.532216548919678
grad AddEdge W: 7.120061347307367e-18
grad ChooseDest W: 2.3697028160095215
grad AddEdge W: 1.9993755322146242e-14
grad ChooseDest W: 1.7271437644958496
grad AddEdge W: 4.47926242631345e-17
grad ChooseDest W: 2.5617735385894775
grad AddEdge W: 4.079791762220971e-17
grad ChooseDest W: 1.6059736013412476
grad AddEdge W: 9.244036895135867e-16
grad ChooseDest W: 2.9247751235961914
grad AddEdge W: 5.14940048346757e-17
grad ChooseDest W: 1.9714899063110352
grad AddEdge W: 1.4044199288763826e-13
grad ChooseDest W: 1.9050599336624146
grad AddEdge W: 3.6415272009024825e-15
grad ChooseDest W: 2.1700689792633057
grad AddEdge W: 1.0727107808925708e-16
grad ChooseDest W: 1.9250226020812988
grad AddEdge W: 6.2395000699087735e-15
grad ChooseDest W: 3.0698866844177246
grad AddEdge W: 1.4657549133700025e-16
grad ChooseDest W: 3.0768985748291016
grad AddEdge W: 3.749785634858107e-15
grad ChooseDest W: 2.4892125129699707
grad AddEdge W: 3.196364853247808e-17
grad ChooseDest W: 2.895372152328491
grad AddEdge W: 2.3693602647291315e-17
grad ChooseDest W: 2.098418951034546
grad AddEdge W: 1.9059549912950817e-17
grad ChooseDest W: 1.760363221168518
grad AddEdge W: 2.403044965096422e-15
grad ChooseDest W: 2.555753707885742
grad AddEdge W: 2.502936195840832e-15
grad ChooseDest W: 3.134840965270996
grad AddEdge W: 7.343902045892382e-17
grad ChooseDest W: 2.2304153442382812
grad AddEdge W: 4.275622417964156e-15
grad ChooseDest W: 1.9158375263214111
grad AddEdge W: 5.740493161286123e-17
grad ChooseDest W: 1.9372435808181763
grad AddEdge W: 2.4614706309677288e-17
grad ChooseDest W: 2.1024398803710938
grad AddEdge W: 2.509007516248514e-15
grad ChooseDest W: 2.1412391662597656
grad AddEdge W: 1.8944828344968911e-13
grad ChooseDest W: 2.2022931575775146
grad AddEdge W: 8.535896704803183e-16
grad ChooseDest W: 2.0903635025024414
grad AddEdge W: 7.870112926393542e-16
grad ChooseDest W: 1.623957872390747
grad AddEdge W: 2.414083769780281e-17
grad ChooseDest W: 2.9383087158203125
grad AddEdge W: 9.236372338841624e-18
grad ChooseDest W: 2.269724130630493
grad AddEdge W: 2.2976463938916003e-13
grad ChooseDest W: 2.1433048248291016
grad AddEdge W: 2.2596194701762524e-16
grad ChooseDest W: 2.1528983116149902
grad AddEdge W: 1.5038153462381394e-17
grad ChooseDest W: 2.1356658935546875
grad AddEdge W: 3.001734670830987e-14
grad ChooseDest W: 1.557712197303772
grad AddEdge W: 3.1099841685890183e-15
grad ChooseDest W: 3.599564790725708
grad AddEdge W: 2.3647905219786945e-16
grad ChooseDest W: 1.4345495700836182
grad AddEdge W: 9.495354482261974e-17
grad ChooseDest W: 1.6106045246124268
grad AddEdge W: 4.054072056763848e-15
grad ChooseDest W: 2.050682544708252
grad AddEdge W: 7.119784407238285e-17
grad ChooseDest W: 1.8596000671386719
grad AddEdge W: 3.164242769849524e-15
grad ChooseDest W: 1.4433590173721313
grad AddEdge W: 1.8931358424701024e-17
grad ChooseDest W: 1.1411523818969727
grad AddEdge W: 8.778489323176978e-17
grad ChooseDest W: 1.335728645324707
grad AddEdge W: 3.3746698943864888e-15
grad ChooseDest W: 1.6442344188690186
grad AddEdge W: 1.3153361185609322e-14
grad ChooseDest W: 1.5668058395385742
grad AddEdge W: 2.0026710991843743e-17
grad ChooseDest W: 1.848758339881897
grad AddEdge W: 6.004347315796349e-18
grad ChooseDest W: 1.5284096002578735
grad AddEdge W: 2.9583983053154414e-15
grad ChooseDest W: 1.6930105686187744
grad AddEdge W: 1.3441897876893816e-13
grad ChooseDest W: 1.415985107421875
grad AddEdge W: 2.9632319385336175e-17
grad ChooseDest W: 2.200836181640625
grad AddEdge W: 2.852535915809362e-15
grad ChooseDest W: 2.4544410705566406
grad AddEdge W: 3.4394071402374772e-15
grad ChooseDest W: 1.6126952171325684
grad AddEdge W: 7.521219790721699e-17
grad ChooseDest W: 1.5777270793914795
grad AddEdge W: 2.051386955066064e-15
grad ChooseDest W: 1.2055476903915405
grad AddEdge W: 9.54677269088276e-17
grad ChooseDest W: 2.9613678455352783
grad AddEdge W: 2.831305564382025e-17
grad ChooseDest W: 2.477303981781006
grad AddEdge W: 1.5291900842926039e-15
grad ChooseDest W: 1.7666369676589966
grad AddEdge W: 2.2302697256143316e-15
grad ChooseDest W: 2.357088088989258
grad AddEdge W: 2.7116126951817246e-17
grad ChooseDest W: 2.996080160140991
grad AddEdge W: 1.1475281555672488e-15
grad ChooseDest W: 3.7089133262634277
grad AddEdge W: 2.2064026400315308e-16
grad ChooseDest W: 3.018122673034668
grad AddEdge W: 1.8652208129021528e-17
grad ChooseDest W: 2.887651205062866
grad AddEdge W: 3.9551885834684334e-17
grad ChooseDest W: 1.7418358325958252
grad AddEdge W: 1.4086351473102352e-17
grad ChooseDest W: 1.767469048500061
grad AddEdge W: 7.24560254887454e-17
grad ChooseDest W: 1.8771647214889526
grad AddEdge W: 1.7555722655311856e-16
grad ChooseDest W: 1.3579021692276
grad AddEdge W: 1.3334308175450048e-15
grad ChooseDest W: 2.744283676147461
grad AddEdge W: 3.1981356815031614e-17
grad ChooseDest W: 2.0290584564208984
grad AddEdge W: 7.10605215027872e-16
grad ChooseDest W: 2.5491883754730225
=== Epoch 13: Train Loss: 6.0029, Train Log Prob: 0.0070 ===
Total mismatches: 91677
Predicted valid destination but wrong order: 20173
Epoch 13: Validation Loss: 6.2202, Validation Log Prob: 0.0040
Epoch 13: Edge Precision: 0.3609, Recall: 0.3588, F1: 0.3598, Jaccard: 0.2373
Epoch 13: TP: 2.5123836793128134, FP: 4.471438797423049, FN: 4.509090909090909
Epoch 13: Current Learning Rate: 3e-05
[Epoch 13] ‚è±Ô∏è Total: 1898.22s | Current time: 2025-07-14 18:27:14 | üèãÔ∏è Train: 1662.60s | ‚úÖ Val: 235.62s
grad AddEdge W: 1.9276149686858757e-13
grad ChooseDest W: 5.179023742675781
grad AddEdge W: 4.9228757492309435e-17
grad ChooseDest W: 3.167736291885376
grad AddEdge W: 3.43654733201442e-17
grad ChooseDest W: 1.3069438934326172
grad AddEdge W: 3.3186308298708547e-15
grad ChooseDest W: 1.9016433954238892
grad AddEdge W: 7.751119752729787e-17
grad ChooseDest W: 1.8956786394119263
grad AddEdge W: 2.262372856650421e-16
grad ChooseDest W: 3.1818437576293945
grad AddEdge W: 1.4012333697529882e-17
grad ChooseDest W: 2.2502834796905518
grad AddEdge W: 4.816206574077494e-16
grad ChooseDest W: 2.558671236038208
grad AddEdge W: 2.115403587637176e-15
grad ChooseDest W: 2.0798628330230713
grad AddEdge W: 9.036619966874766e-17
grad ChooseDest W: 2.1427221298217773
grad AddEdge W: 4.759392765582698e-17
grad ChooseDest W: 2.4625649452209473
grad AddEdge W: 3.355640028677e-15
grad ChooseDest W: 2.492771625518799
grad AddEdge W: 1.0919523254295343e-16
grad ChooseDest W: 2.3623580932617188
grad AddEdge W: 1.8689486843123578e-16
grad ChooseDest W: 2.16585373878479
grad AddEdge W: 1.674465234471781e-15
grad ChooseDest W: 1.7815064191818237
grad AddEdge W: 6.91956164743557e-15
grad ChooseDest W: 1.933388352394104
grad AddEdge W: 1.1625157217073911e-13
grad ChooseDest W: 1.9534482955932617
grad AddEdge W: 3.825410006938392e-15
grad ChooseDest W: 1.8173367977142334
grad AddEdge W: 6.045460915318811e-15
grad ChooseDest W: 1.958203673362732
grad AddEdge W: 6.649415718561202e-13
grad ChooseDest W: 1.6000491380691528
grad AddEdge W: 4.5515242628815924e-17
grad ChooseDest W: 1.5922765731811523
grad AddEdge W: 1.882392082919876e-13
grad ChooseDest W: 1.102845549583435
grad AddEdge W: 1.0177796520155576e-17
grad ChooseDest W: 1.9190536737442017
grad AddEdge W: 1.284050993711786e-16
grad ChooseDest W: 2.214731454849243
grad AddEdge W: 5.840849360179017e-17
grad ChooseDest W: 2.3267335891723633
grad AddEdge W: 1.7175581415424619e-15
grad ChooseDest W: 1.7315855026245117
grad AddEdge W: 8.638860229926403e-15
grad ChooseDest W: 2.6926565170288086
grad AddEdge W: 1.979748720035558e-15
grad ChooseDest W: 1.5988352298736572
grad AddEdge W: 1.7495358587627983e-15
grad ChooseDest W: 2.293668270111084
grad AddEdge W: 1.2660049197128417e-14
grad ChooseDest W: 2.2558605670928955
grad AddEdge W: 1.5753181221715338e-15
grad ChooseDest W: 1.8450673818588257
grad AddEdge W: 1.5683865537791313e-17
grad ChooseDest W: 1.8987140655517578
grad AddEdge W: 1.4276147402908154e-14
grad ChooseDest W: 1.9885367155075073
grad AddEdge W: 2.729176154953614e-15
grad ChooseDest W: 2.6079370975494385
grad AddEdge W: 3.9055527836315764e-17
grad ChooseDest W: 1.4991421699523926
grad AddEdge W: 3.594606988694523e-17
grad ChooseDest W: 1.8262073993682861
grad AddEdge W: 2.6497335701053436e-15
grad ChooseDest W: 1.956253170967102
grad AddEdge W: 2.1381387989744286e-15
grad ChooseDest W: 2.681246280670166
grad AddEdge W: 2.702932744907071e-15
grad ChooseDest W: 4.454259872436523
grad AddEdge W: 3.198150565989627e-13
grad ChooseDest W: 2.444455146789551
grad AddEdge W: 2.249950959660259e-15
grad ChooseDest W: 2.0098776817321777
grad AddEdge W: 3.8556228383688956e-17
grad ChooseDest W: 2.5504870414733887
grad AddEdge W: 1.6326951811064744e-15
grad ChooseDest W: 1.942149043083191
grad AddEdge W: 1.5079949244372473e-17
grad ChooseDest W: 1.9949513673782349
grad AddEdge W: 8.251618286372553e-17
grad ChooseDest W: 2.760615825653076
grad AddEdge W: 2.4027452742517717e-17
grad ChooseDest W: 1.3999370336532593
grad AddEdge W: 5.825465786019e-17
grad ChooseDest W: 2.7963716983795166
grad AddEdge W: 9.444078548706547e-15
grad ChooseDest W: 1.506670594215393
grad AddEdge W: 2.771620512719384e-17
grad ChooseDest W: 2.6198036670684814
grad AddEdge W: 3.2456853318351596e-17
grad ChooseDest W: 2.6895034313201904
grad AddEdge W: 2.3266699696236372e-17
grad ChooseDest W: 2.9756152629852295
grad AddEdge W: 9.571650180892517e-16
grad ChooseDest W: 3.2935540676116943
grad AddEdge W: 7.838070993487891e-17
grad ChooseDest W: 2.381652355194092
grad AddEdge W: 2.485097046696946e-15
grad ChooseDest W: 1.4347361326217651
grad AddEdge W: 2.3788751578792064e-17
grad ChooseDest W: 1.5485725402832031
grad AddEdge W: 6.7239692197088e-17
grad ChooseDest W: 2.179598808288574
grad AddEdge W: 8.873342454890679e-16
grad ChooseDest W: 2.630866289138794
grad AddEdge W: 5.110177067745408e-18
grad ChooseDest W: 1.3383558988571167
grad AddEdge W: 8.490171219332435e-16
grad ChooseDest W: 1.9521844387054443
grad AddEdge W: 1.1602329086222345e-16
grad ChooseDest W: 3.341641426086426
grad AddEdge W: 2.1629039247697763e-15
grad ChooseDest W: 2.8816030025482178
grad AddEdge W: 8.849591862807905e-15
grad ChooseDest W: 1.452182412147522
grad AddEdge W: 1.4101888902856303e-16
grad ChooseDest W: 2.508450746536255
grad AddEdge W: 7.709045058671045e-15
grad ChooseDest W: 2.139216661453247
grad AddEdge W: 3.979764868350388e-12
grad ChooseDest W: 1.066680669784546
grad AddEdge W: 2.938176637779386e-17
grad ChooseDest W: 2.5477001667022705
=== Epoch 14: Train Loss: 5.9938, Train Log Prob: 0.0070 ===
Total mismatches: 91214
Predicted valid destination but wrong order: 20054
Epoch 14: Validation Loss: 6.2583, Validation Log Prob: 0.0038
Epoch 14: Edge Precision: 0.3601, Recall: 0.3573, F1: 0.3586, Jaccard: 0.2354
Epoch 14: TP: 2.5017895490336435, FP: 4.468718682891911, FN: 4.519685039370079
Epoch 14: Current Learning Rate: 3e-05
[Epoch 14] ‚è±Ô∏è Total: 1923.93s | Current time: 2025-07-14 18:59:18 | üèãÔ∏è Train: 1688.89s | ‚úÖ Val: 235.04s
grad AddEdge W: 2.4802737446337486e-13
grad ChooseDest W: 3.5688936710357666
grad AddEdge W: 2.33209960301677e-15
grad ChooseDest W: 2.683058023452759
grad AddEdge W: 2.319946586047802e-15
grad ChooseDest W: 1.764746069908142
grad AddEdge W: 3.1480151538273483e-17
grad ChooseDest W: 1.6184871196746826
grad AddEdge W: 1.763235790827513e-14
grad ChooseDest W: 1.7254801988601685
grad AddEdge W: 4.164036798887047e-17
grad ChooseDest W: 1.4177868366241455
grad AddEdge W: 2.7082291956041377e-17
grad ChooseDest W: 2.2726192474365234
grad AddEdge W: 1.953404221900716e-17
grad ChooseDest W: 1.8435317277908325
grad AddEdge W: 5.754354326072348e-15
grad ChooseDest W: 2.4894096851348877
grad AddEdge W: 4.294351163460896e-17
grad ChooseDest W: 2.4853975772857666
grad AddEdge W: 1.3292149228082834e-15
grad ChooseDest W: 2.2185869216918945
grad AddEdge W: 1.4552116031079526e-16
grad ChooseDest W: 1.934654712677002
grad AddEdge W: 5.610621834648357e-17
grad ChooseDest W: 2.103358745574951
grad AddEdge W: 2.5613545097308545e-17
grad ChooseDest W: 1.8832757472991943
grad AddEdge W: 8.065705763872499e-13
grad ChooseDest W: 1.479134440422058
grad AddEdge W: 4.0652390082682033e-17
grad ChooseDest W: 2.6865713596343994
grad AddEdge W: 4.4037395128983784e-17
grad ChooseDest W: 1.7385544776916504
grad AddEdge W: 2.2361914652974845e-17
grad ChooseDest W: 3.2557342052459717
grad AddEdge W: 6.449610404123465e-15
grad ChooseDest W: 1.7024750709533691
grad AddEdge W: 6.59605070106115e-17
grad ChooseDest W: 2.828672170639038
grad AddEdge W: 4.200025111233514e-17
grad ChooseDest W: 3.30202579498291
grad AddEdge W: 6.1224857648978226e-12
grad ChooseDest W: 1.091891884803772
grad AddEdge W: 2.8518239449125255e-17
grad ChooseDest W: 2.3544375896453857
grad AddEdge W: 1.8431384416210846e-14
grad ChooseDest W: 2.4410107135772705
grad AddEdge W: 1.6808007232798879e-15
grad ChooseDest W: 2.0834035873413086
grad AddEdge W: 5.171901013738874e-15
grad ChooseDest W: 2.116516590118408
grad AddEdge W: 2.1218615785852796e-15
grad ChooseDest W: 1.7608845233917236
grad AddEdge W: 1.769374429178678e-15
grad ChooseDest W: 2.186725378036499
grad AddEdge W: 1.3956205853373464e-16
grad ChooseDest W: 1.9939724206924438
grad AddEdge W: 1.596912532764172e-11
grad ChooseDest W: 1.8114789724349976
grad AddEdge W: 1.4175550329020885e-16
grad ChooseDest W: 2.771681785583496
grad AddEdge W: 2.932674126465565e-15
grad ChooseDest W: 2.7913076877593994
grad AddEdge W: 5.55123754585644e-17
grad ChooseDest W: 1.6820132732391357
grad AddEdge W: 1.2240437121871636e-15
grad ChooseDest W: 1.2778717279434204
grad AddEdge W: 1.1383181757493397e-14
grad ChooseDest W: 1.9676443338394165
grad AddEdge W: 1.2500015927211433e-16
grad ChooseDest W: 1.8124557733535767
grad AddEdge W: 2.6795247776514614e-15
grad ChooseDest W: 1.5490862131118774
grad AddEdge W: 1.368159644838464e-16
grad ChooseDest W: 2.390004873275757
grad AddEdge W: 9.37729169488144e-13
grad ChooseDest W: 1.9010545015335083
grad AddEdge W: 6.292768277895702e-15
grad ChooseDest W: 1.7332607507705688
grad AddEdge W: 6.120356839064457e-15
grad ChooseDest W: 1.7445363998413086
grad AddEdge W: 7.266163611924649e-17
grad ChooseDest W: 2.259551763534546
grad AddEdge W: 6.876535735377061e-17
grad ChooseDest W: 4.1494975090026855
grad AddEdge W: 1.4905483618295239e-16
grad ChooseDest W: 2.189237117767334
grad AddEdge W: 1.484194456672259e-17
grad ChooseDest W: 1.7472360134124756
grad AddEdge W: 4.3281487706730776e-17
grad ChooseDest W: 2.816652536392212
grad AddEdge W: 5.4426224448555726e-17
grad ChooseDest W: 1.5219289064407349
grad AddEdge W: 6.123676572942984e-16
grad ChooseDest W: 2.035421371459961
grad AddEdge W: 2.788608817267753e-17
grad ChooseDest W: 1.7294673919677734
grad AddEdge W: 5.494948897788942e-17
grad ChooseDest W: 2.8778998851776123
grad AddEdge W: 1.7193803211702427e-15
grad ChooseDest W: 2.2488818168640137
grad AddEdge W: 2.3199258337405944e-15
grad ChooseDest W: 1.4143860340118408
grad AddEdge W: 6.767534754620437e-18
grad ChooseDest W: 1.235101342201233
grad AddEdge W: 1.536225804650294e-16
grad ChooseDest W: 1.7796181440353394
grad AddEdge W: 1.9514117092411982e-17
grad ChooseDest W: 3.773719310760498
grad AddEdge W: 1.2432038092122927e-15
grad ChooseDest W: 1.894578456878662
grad AddEdge W: 2.8249414882119484e-15
grad ChooseDest W: 2.6484436988830566
grad AddEdge W: 4.708035702152792e-16
grad ChooseDest W: 2.9514400959014893
grad AddEdge W: 7.619412746876644e-17
grad ChooseDest W: 2.1580989360809326
grad AddEdge W: 4.061009680118334e-15
grad ChooseDest W: 2.222074270248413
grad AddEdge W: 6.0150543416452355e-18
grad ChooseDest W: 3.7313766479492188
grad AddEdge W: 2.4822736741255107e-15
grad ChooseDest W: 2.898301601409912
grad AddEdge W: 8.174542093203229e-18
grad ChooseDest W: 2.71439790725708
grad AddEdge W: 3.85428874824208e-15
grad ChooseDest W: 1.9758497476577759
grad AddEdge W: 1.508161750223187e-16
grad ChooseDest W: 1.9733338356018066
grad AddEdge W: 1.945129603361103e-17
grad ChooseDest W: 2.543342351913452
=== Epoch 15: Train Loss: 5.9940, Train Log Prob: 0.0071 ===
Total mismatches: 91572
Predicted valid destination but wrong order: 19805
Epoch 15: Validation Loss: 6.2754, Validation Log Prob: 0.0039
Epoch 15: Edge Precision: 0.3603, Recall: 0.3584, F1: 0.3593, Jaccard: 0.2361
Epoch 15: TP: 2.5086614173228345, FP: 4.480458124552613, FN: 4.512813171080888
Epoch 15: Current Learning Rate: 3e-05
[Epoch 15] ‚è±Ô∏è Total: 1902.26s | Current time: 2025-07-14 19:31:01 | üèãÔ∏è Train: 1667.53s | ‚úÖ Val: 234.73s
grad AddEdge W: 8.955078804960114e-15
grad ChooseDest W: 5.8002519607543945
grad AddEdge W: 6.342900922377825e-13
grad ChooseDest W: 1.0775139331817627
grad AddEdge W: 2.308267748710619e-16
grad ChooseDest W: 2.5649795532226562
grad AddEdge W: 8.276604434827574e-17
grad ChooseDest W: 1.8364697694778442
grad AddEdge W: 1.8727576591272624e-15
grad ChooseDest W: 2.0134122371673584
grad AddEdge W: 2.478180010213545e-17
grad ChooseDest W: 2.2156078815460205
grad AddEdge W: 1.0081697985311615e-17
grad ChooseDest W: 1.949183464050293
grad AddEdge W: 2.519332000842597e-15
grad ChooseDest W: 1.6911704540252686
grad AddEdge W: 5.410568534374653e-17
grad ChooseDest W: 2.2363691329956055
grad AddEdge W: 1.8025555949286056e-17
grad ChooseDest W: 3.3597497940063477
grad AddEdge W: 5.927999468391268e-15
grad ChooseDest W: 2.1979968547821045
grad AddEdge W: 1.5053147666083469e-15
grad ChooseDest W: 2.4136345386505127
grad AddEdge W: 1.1716374131636256e-16
grad ChooseDest W: 2.7042884826660156
grad AddEdge W: 2.7287947254295535e-17
grad ChooseDest W: 2.036115884780884
grad AddEdge W: 4.5813676158935256e-17
grad ChooseDest W: 2.167506694793701
grad AddEdge W: 7.910380310223194e-18
grad ChooseDest W: 3.327584743499756
grad AddEdge W: 2.6634970084652787e-15
grad ChooseDest W: 2.648697853088379
grad AddEdge W: 9.523267658945352e-16
grad ChooseDest W: 3.4718379974365234
grad AddEdge W: 2.0810176498686772e-15
grad ChooseDest W: 2.0193378925323486
grad AddEdge W: 5.003109290798382e-17
grad ChooseDest W: 2.188493490219116
grad AddEdge W: 3.686902332370422e-15
grad ChooseDest W: 2.4505503177642822
grad AddEdge W: 1.9718494686463833e-13
grad ChooseDest W: 2.4016971588134766
grad AddEdge W: 9.600418596154825e-16
grad ChooseDest W: 2.1047592163085938
grad AddEdge W: 2.6415325971100275e-16
grad ChooseDest W: 2.1455719470977783
grad AddEdge W: 2.698900087219642e-17
grad ChooseDest W: 3.352949619293213
grad AddEdge W: 1.9805015205674303e-15
grad ChooseDest W: 2.704594373703003
grad AddEdge W: 1.1066114170843466e-11
grad ChooseDest W: 0.8889861106872559
grad AddEdge W: 2.029198450556705e-17
grad ChooseDest W: 1.8085455894470215
grad AddEdge W: 4.675496468262875e-17
grad ChooseDest W: 2.4105372428894043
grad AddEdge W: 1.2350287766009862e-15
grad ChooseDest W: 2.728412628173828
grad AddEdge W: 4.5264418306032695e-17
grad ChooseDest W: 2.10606050491333
grad AddEdge W: 4.66427162735053e-15
grad ChooseDest W: 2.2919888496398926
grad AddEdge W: 7.188681273074561e-17
grad ChooseDest W: 2.403677225112915
grad AddEdge W: 9.538975884952183e-16
grad ChooseDest W: 1.6713628768920898
grad AddEdge W: 1.3064084283545633e-16
grad ChooseDest W: 2.184581995010376
grad AddEdge W: 1.3351700938230731e-15
grad ChooseDest W: 2.5331695079803467
grad AddEdge W: 2.9365075858266013e-15
grad ChooseDest W: 1.7443618774414062
grad AddEdge W: 2.2487985250134052e-17
grad ChooseDest W: 2.280402183532715
grad AddEdge W: 1.6526218429488686e-15
grad ChooseDest W: 3.250152826309204
grad AddEdge W: 2.6898920374093804e-15
grad ChooseDest W: 2.808225154876709
grad AddEdge W: 6.529694404671044e-15
grad ChooseDest W: 2.9775936603546143
grad AddEdge W: 1.922341439229757e-17
grad ChooseDest W: 2.194939613342285
grad AddEdge W: 1.0791031830355438e-17
grad ChooseDest W: 1.6655285358428955
grad AddEdge W: 9.706163115732338e-17
grad ChooseDest W: 1.694396734237671
grad AddEdge W: 3.529108511686859e-17
grad ChooseDest W: 1.952378273010254
grad AddEdge W: 1.775904629685535e-16
grad ChooseDest W: 1.7151508331298828
grad AddEdge W: 1.812690105914487e-15
grad ChooseDest W: 1.9260406494140625
grad AddEdge W: 2.8610923382400597e-17
grad ChooseDest W: 1.9463311433792114
grad AddEdge W: 1.5295406500536487e-15
grad ChooseDest W: 1.985464096069336
grad AddEdge W: 1.888790086994655e-15
grad ChooseDest W: 2.8914942741394043
grad AddEdge W: 8.304327955540106e-14
grad ChooseDest W: 3.989182710647583
grad AddEdge W: 1.0916068948057321e-16
grad ChooseDest W: 2.8871521949768066
grad AddEdge W: 2.3841133618262597e-17
grad ChooseDest W: 1.7765953540802002
grad AddEdge W: 1.1879991780288225e-16
grad ChooseDest W: 1.1675724983215332
grad AddEdge W: 2.3829552494116814e-15
grad ChooseDest W: 2.781125783920288
grad AddEdge W: 3.58433765388624e-15
grad ChooseDest W: 2.9593849182128906
grad AddEdge W: 6.44723164444e-17
grad ChooseDest W: 2.215770959854126
grad AddEdge W: 6.743387450024605e-17
grad ChooseDest W: 1.4799150228500366
grad AddEdge W: 4.696776516701414e-17
grad ChooseDest W: 2.1396868228912354
grad AddEdge W: 2.128501681617043e-15
grad ChooseDest W: 3.4622135162353516
grad AddEdge W: 2.951617330116638e-17
grad ChooseDest W: 3.2088310718536377
grad AddEdge W: 1.0033485578020505e-14
grad ChooseDest W: 1.9045466184616089
grad AddEdge W: 1.7857487738066306e-17
grad ChooseDest W: 2.8263418674468994
grad AddEdge W: 1.685677886050619e-16
grad ChooseDest W: 3.235186815261841
grad AddEdge W: 1.2962501706676568e-17
grad ChooseDest W: 3.1528573036193848
grad AddEdge W: 1.0502888367499262e-14
grad ChooseDest W: 2.2491469383239746
=== Epoch 16: Train Loss: 5.9972, Train Log Prob: 0.0071 ===
Total mismatches: 91434
Predicted valid destination but wrong order: 19714
Epoch 16: Validation Loss: 6.2653, Validation Log Prob: 0.0039
Epoch 16: Edge Precision: 0.3580, Recall: 0.3551, F1: 0.3564, Jaccard: 0.2347
Epoch 16: TP: 2.4871868289191124, FP: 4.479312813171081, FN: 4.53428775948461
Epoch 16: Current Learning Rate: 3e-05
[Epoch 16] ‚è±Ô∏è Total: 1916.20s | Current time: 2025-07-14 20:02:57 | üèãÔ∏è Train: 1681.32s | ‚úÖ Val: 234.88s
grad AddEdge W: 1.4408799538711756e-14
grad ChooseDest W: 5.689366340637207
grad AddEdge W: 3.49656407648525e-15
grad ChooseDest W: 1.6881091594696045
grad AddEdge W: 1.907231000108006e-17
grad ChooseDest W: 1.5688083171844482
grad AddEdge W: 1.0333189842849175e-17
grad ChooseDest W: 1.6664938926696777
grad AddEdge W: 5.99490829496078e-15
grad ChooseDest W: 2.406381607055664
grad AddEdge W: 2.85798015390339e-17
grad ChooseDest W: 2.290426731109619
grad AddEdge W: 2.821620272025764e-17
grad ChooseDest W: 3.140255928039551
grad AddEdge W: 3.0385559977294313e-15
grad ChooseDest W: 2.2688355445861816
grad AddEdge W: 2.238200283341237e-15
grad ChooseDest W: 1.9574651718139648
grad AddEdge W: 1.843511623161821e-17
grad ChooseDest W: 3.3604485988616943
grad AddEdge W: 5.802196864515639e-15
grad ChooseDest W: 2.2657079696655273
grad AddEdge W: 3.3077678837852245e-17
grad ChooseDest W: 2.87127685546875
grad AddEdge W: 4.22584567978581e-15
grad ChooseDest W: 1.3311742544174194
grad AddEdge W: 2.0286283576785333e-17
grad ChooseDest W: 1.7785885334014893
grad AddEdge W: 5.587247695771079e-17
grad ChooseDest W: 1.852953314781189
grad AddEdge W: 2.9669319505007917e-15
grad ChooseDest W: 2.1355836391448975
grad AddEdge W: 2.522513664962589e-12
grad ChooseDest W: 0.8318526148796082
grad AddEdge W: 4.115489992252783e-15
grad ChooseDest W: 2.0402097702026367
grad AddEdge W: 1.2248634911875955e-17
grad ChooseDest W: 1.8136093616485596
grad AddEdge W: 1.6252656433678701e-15
grad ChooseDest W: 1.9763634204864502
grad AddEdge W: 8.032981215584987e-17
grad ChooseDest W: 2.3555078506469727
grad AddEdge W: 5.1246657979187645e-17
grad ChooseDest W: 1.5291916131973267
grad AddEdge W: 1.0318895500119769e-16
grad ChooseDest W: 2.7915868759155273
grad AddEdge W: 9.76467654877536e-17
grad ChooseDest W: 1.4036157131195068
grad AddEdge W: 8.939334791811262e-16
grad ChooseDest W: 1.5598257780075073
grad AddEdge W: 4.707259211168176e-17
grad ChooseDest W: 2.839641571044922
grad AddEdge W: 5.651950160331671e-15
grad ChooseDest W: 1.1986455917358398
grad AddEdge W: 1.446874893082839e-15
grad ChooseDest W: 1.6864763498306274
grad AddEdge W: 5.877258044389294e-14
grad ChooseDest W: 1.8344171047210693
grad AddEdge W: 1.0804175730288906e-17
grad ChooseDest W: 1.6895984411239624
grad AddEdge W: 4.051275868656063e-17
grad ChooseDest W: 2.6583633422851562
grad AddEdge W: 8.958153613967986e-17
grad ChooseDest W: 2.352961540222168
grad AddEdge W: 5.574488600258571e-17
grad ChooseDest W: 2.833906412124634
grad AddEdge W: 6.26332557146782e-17
grad ChooseDest W: 2.027329683303833
grad AddEdge W: 6.945020996140531e-17
grad ChooseDest W: 2.2771477699279785
grad AddEdge W: 8.326005646242711e-16
grad ChooseDest W: 2.229806661605835
grad AddEdge W: 4.603758746565736e-15
grad ChooseDest W: 2.6014277935028076
grad AddEdge W: 3.019107697743999e-15
grad ChooseDest W: 2.437218189239502
grad AddEdge W: 2.673902384705824e-15
grad ChooseDest W: 2.400317907333374
grad AddEdge W: 2.039314617985322e-15
grad ChooseDest W: 2.2728641033172607
grad AddEdge W: 3.3940302485291945e-17
grad ChooseDest W: 3.464906692504883
grad AddEdge W: 3.0196006285676535e-14
grad ChooseDest W: 1.398356556892395
grad AddEdge W: 4.574301481759073e-15
grad ChooseDest W: 1.3649203777313232
grad AddEdge W: 8.295403106864577e-18
grad ChooseDest W: 3.422908306121826
grad AddEdge W: 1.1947424536440283e-17
grad ChooseDest W: 2.579439401626587
grad AddEdge W: 6.0277405625457775e-15
grad ChooseDest W: 1.4829515218734741
grad AddEdge W: 1.168945171880337e-16
grad ChooseDest W: 1.148393988609314
grad AddEdge W: 6.655966370678571e-17
grad ChooseDest W: 4.2986226081848145
grad AddEdge W: 2.5691585022065942e-15
grad ChooseDest W: 2.7600018978118896
grad AddEdge W: 6.606189103104633e-15
grad ChooseDest W: 2.1964457035064697
grad AddEdge W: 4.9071278317896e-15
grad ChooseDest W: 1.7051599025726318
grad AddEdge W: 2.537785321665136e-17
grad ChooseDest W: 3.3745477199554443
grad AddEdge W: 3.729717677332199e-17
grad ChooseDest W: 1.9163408279418945
grad AddEdge W: 1.3275461355532944e-16
grad ChooseDest W: 3.380251169204712
grad AddEdge W: 1.3310864685750213e-17
grad ChooseDest W: 1.913927435874939
grad AddEdge W: 2.155611818126864e-15
grad ChooseDest W: 1.8535665273666382
grad AddEdge W: 9.605662788889514e-16
grad ChooseDest W: 2.5377602577209473
grad AddEdge W: 7.665093366070967e-15
grad ChooseDest W: 2.679914951324463
grad AddEdge W: 1.2769429314464444e-16
grad ChooseDest W: 1.8530248403549194
grad AddEdge W: 7.096719435386754e-15
grad ChooseDest W: 2.568737268447876
grad AddEdge W: 2.9608619006425306e-17
grad ChooseDest W: 1.9818979501724243
grad AddEdge W: 7.596003535541393e-17
grad ChooseDest W: 1.843900203704834
grad AddEdge W: 4.120766981044398e-17
grad ChooseDest W: 2.2703609466552734
grad AddEdge W: 3.197997879830555e-15
grad ChooseDest W: 1.6852610111236572
grad AddEdge W: 1.8350564344032103e-15
grad ChooseDest W: 4.942592144012451
grad AddEdge W: 4.794772603870571e-17
grad ChooseDest W: 2.1844074726104736
=== Epoch 17: Train Loss: 5.9989, Train Log Prob: 0.0072 ===
Total mismatches: 91412
Predicted valid destination but wrong order: 20043
Epoch 17: Validation Loss: 6.1892, Validation Log Prob: 0.0040
Epoch 17: Edge Precision: 0.3611, Recall: 0.3577, F1: 0.3593, Jaccard: 0.2361
Epoch 17: TP: 2.504509663564782, FP: 4.454545454545454, FN: 4.51696492483894
Epoch 17: Current Learning Rate: 1.5e-05
[Epoch 17] ‚è±Ô∏è Total: 1913.13s | Current time: 2025-07-14 20:34:50 | üèãÔ∏è Train: 1677.66s | ‚úÖ Val: 235.47s
grad AddEdge W: 2.2726936652062157e-11
grad ChooseDest W: 3.5896310806274414
grad AddEdge W: 2.6615094456545465e-15
grad ChooseDest W: 1.8490822315216064
grad AddEdge W: 3.6268756352624614e-17
grad ChooseDest W: 1.615534782409668
grad AddEdge W: 6.369463431970214e-17
grad ChooseDest W: 4.0554399490356445
grad AddEdge W: 3.635627616424856e-15
grad ChooseDest W: 1.983283281326294
grad AddEdge W: 3.158149108947858e-17
grad ChooseDest W: 1.8216451406478882
grad AddEdge W: 2.1230240585134922e-17
grad ChooseDest W: 2.118230104446411
grad AddEdge W: 3.7206299402504467e-17
grad ChooseDest W: 4.244904041290283
grad AddEdge W: 3.4272743064754556e-17
grad ChooseDest W: 1.6869144439697266
grad AddEdge W: 5.196397565715438e-12
grad ChooseDest W: 2.08879017829895
grad AddEdge W: 6.888540759808276e-15
grad ChooseDest W: 2.443834066390991
grad AddEdge W: 6.535984815444489e-17
grad ChooseDest W: 2.313847064971924
grad AddEdge W: 1.5463339186484676e-17
grad ChooseDest W: 2.531228542327881
grad AddEdge W: 3.234785407467426e-17
grad ChooseDest W: 1.553814172744751
grad AddEdge W: 6.150127559001163e-17
grad ChooseDest W: 1.762081265449524
grad AddEdge W: 2.923979902362261e-17
grad ChooseDest W: 1.6347466707229614
grad AddEdge W: 5.059060449175959e-17
grad ChooseDest W: 1.8842560052871704
grad AddEdge W: 6.6754972629938054e-18
grad ChooseDest W: 4.219260215759277
grad AddEdge W: 3.898860468959549e-15
grad ChooseDest W: 1.7725845575332642
grad AddEdge W: 4.4513321766222294e-17
grad ChooseDest W: 4.918527126312256
grad AddEdge W: 3.695843969503663e-17
grad ChooseDest W: 2.382260322570801
grad AddEdge W: 6.600629311187753e-17
grad ChooseDest W: 2.262925863265991
grad AddEdge W: 1.6346164900588636e-16
grad ChooseDest W: 2.700723171234131
grad AddEdge W: 2.9694139687944836e-15
grad ChooseDest W: 1.9295812845230103
grad AddEdge W: 1.4552862387384735e-11
grad ChooseDest W: 1.3917373418807983
grad AddEdge W: 2.2611484969949444e-17
grad ChooseDest W: 1.6134947538375854
grad AddEdge W: 6.188437271018699e-17
grad ChooseDest W: 1.879972219467163
grad AddEdge W: 2.1916130477257767e-15
grad ChooseDest W: 1.8858829736709595
grad AddEdge W: 5.777020927740873e-16
grad ChooseDest W: 2.3056459426879883
grad AddEdge W: 1.0922222510070226e-16
grad ChooseDest W: 2.8607993125915527
grad AddEdge W: 9.530977776347734e-16
grad ChooseDest W: 3.1752426624298096
grad AddEdge W: 2.9912661482842237e-16
grad ChooseDest W: 2.0502853393554688
grad AddEdge W: 1.0946690678025667e-17
grad ChooseDest W: 1.3849841356277466
grad AddEdge W: 4.780150816732602e-15
grad ChooseDest W: 1.787482500076294
grad AddEdge W: 7.266793592679169e-17
grad ChooseDest W: 2.0860185623168945
grad AddEdge W: 7.493984372745023e-17
grad ChooseDest W: 3.6274185180664062
grad AddEdge W: 1.56411648202101e-17
grad ChooseDest W: 1.9738272428512573
grad AddEdge W: 3.76731201708622e-15
grad ChooseDest W: 1.7970020771026611
grad AddEdge W: 2.0514373667613155e-17
grad ChooseDest W: 3.3294458389282227
grad AddEdge W: 1.0363328995648157e-17
grad ChooseDest W: 1.8803991079330444
grad AddEdge W: 2.301635004277585e-15
grad ChooseDest W: 2.419492244720459
grad AddEdge W: 6.657156187271667e-17
grad ChooseDest W: 2.285874605178833
grad AddEdge W: 5.17740054720249e-17
grad ChooseDest W: 1.9503294229507446
grad AddEdge W: 4.8977149002305505e-17
grad ChooseDest W: 3.2120282649993896
grad AddEdge W: 1.1017050495493305e-15
grad ChooseDest W: 3.235743999481201
grad AddEdge W: 1.7299725475849965e-16
grad ChooseDest W: 2.9825756549835205
grad AddEdge W: 5.776333904611311e-17
grad ChooseDest W: 2.358163595199585
grad AddEdge W: 1.9219732379780526e-15
grad ChooseDest W: 2.389284133911133
grad AddEdge W: 6.965803081850313e-17
grad ChooseDest W: 1.5095340013504028
grad AddEdge W: 1.2243896986763374e-16
grad ChooseDest W: 2.54718017578125
grad AddEdge W: 1.7567913312307418e-17
grad ChooseDest W: 2.786471128463745
grad AddEdge W: 3.100493296763932e-17
grad ChooseDest W: 3.1348302364349365
grad AddEdge W: 2.9087763227035365e-17
grad ChooseDest W: 1.6009001731872559
grad AddEdge W: 4.482798630049559e-15
grad ChooseDest W: 1.7608364820480347
grad AddEdge W: 2.20090542267363e-15
grad ChooseDest W: 2.483541488647461
grad AddEdge W: 7.326598750966753e-17
grad ChooseDest W: 2.5497965812683105
grad AddEdge W: 2.6534691442214127e-16
grad ChooseDest W: 3.805612802505493
grad AddEdge W: 9.627436862677094e-18
grad ChooseDest W: 4.397311210632324
grad AddEdge W: 5.551416666855005e-15
grad ChooseDest W: 1.661103367805481
grad AddEdge W: 7.815700721002007e-17
grad ChooseDest W: 2.4093918800354004
grad AddEdge W: 4.013054909809532e-15
grad ChooseDest W: 2.888113498687744
grad AddEdge W: 8.208121821042064e-17
grad ChooseDest W: 1.9051687717437744
grad AddEdge W: 1.4379841176311026e-15
grad ChooseDest W: 2.1199848651885986
grad AddEdge W: 1.4223242783813835e-15
grad ChooseDest W: 1.4642550945281982
grad AddEdge W: 7.106237968131524e-16
grad ChooseDest W: 2.529081106185913
grad AddEdge W: 1.2471642970456374e-16
grad ChooseDest W: 2.057650327682495
=== Epoch 18: Train Loss: 5.9923, Train Log Prob: 0.0071 ===
Total mismatches: 91329
Predicted valid destination but wrong order: 19876
Epoch 18: Validation Loss: 6.2533, Validation Log Prob: 0.0039
Epoch 18: Edge Precision: 0.3576, Recall: 0.3543, F1: 0.3558, Jaccard: 0.2335
Epoch 18: TP: 2.4807444523979956, FP: 4.481460272011453, FN: 4.540730136005727
Epoch 18: Current Learning Rate: 1.5e-05
[Epoch 18] ‚è±Ô∏è Total: 1910.88s | Current time: 2025-07-14 21:06:41 | üèãÔ∏è Train: 1675.66s | ‚úÖ Val: 235.22s
grad AddEdge W: 1.099267924001843e-14
grad ChooseDest W: 4.577112674713135
grad AddEdge W: 9.132637107146452e-17
grad ChooseDest W: 2.5179967880249023
grad AddEdge W: 5.613522260748213e-17
grad ChooseDest W: 2.170403003692627
grad AddEdge W: 7.542355750914977e-15
grad ChooseDest W: 2.5923521518707275
grad AddEdge W: 8.847326632009151e-17
grad ChooseDest W: 3.3346314430236816
grad AddEdge W: 3.922204273469404e-15
grad ChooseDest W: 2.8400778770446777
grad AddEdge W: 1.3729246816228022e-15
grad ChooseDest W: 2.24045991897583
grad AddEdge W: 4.427557682328475e-15
grad ChooseDest W: 1.9799761772155762
grad AddEdge W: 1.8539493190032602e-17
grad ChooseDest W: 1.7104356288909912
grad AddEdge W: 4.007332725794796e-17
grad ChooseDest W: 1.5290480852127075
grad AddEdge W: 3.089919281557544e-17
grad ChooseDest W: 1.8704464435577393
grad AddEdge W: 3.9095940572322655e-17
grad ChooseDest W: 2.0267488956451416
grad AddEdge W: 2.0485923617625006e-17
grad ChooseDest W: 2.8043153285980225
grad AddEdge W: 2.2216291425193905e-15
grad ChooseDest W: 1.4753508567810059
grad AddEdge W: 2.9022480677254802e-15
grad ChooseDest W: 3.9315428733825684
grad AddEdge W: 1.316902240128758e-15
grad ChooseDest W: 2.873408555984497
grad AddEdge W: 1.0258227982882541e-14
grad ChooseDest W: 2.0335752964019775
grad AddEdge W: 4.090366108299604e-17
grad ChooseDest W: 1.900858998298645
grad AddEdge W: 1.1139155754232055e-17
grad ChooseDest W: 1.765542984008789
grad AddEdge W: 9.278108371922698e-15
grad ChooseDest W: 2.12680983543396
grad AddEdge W: 4.9688299335974494e-17
grad ChooseDest W: 1.9575124979019165
grad AddEdge W: 2.5981738077206924e-17
grad ChooseDest W: 2.4568021297454834
grad AddEdge W: 5.037878802143089e-15
grad ChooseDest W: 2.450531482696533
grad AddEdge W: 6.060809258202177e-17
grad ChooseDest W: 2.974320888519287
grad AddEdge W: 1.9962587897819073e-14
grad ChooseDest W: 1.2914069890975952
grad AddEdge W: 1.0373627063402197e-16
grad ChooseDest W: 1.9771385192871094
grad AddEdge W: 3.1793718370786697e-15
grad ChooseDest W: 1.7439225912094116
grad AddEdge W: 7.307147433426446e-17
grad ChooseDest W: 3.8325021266937256
grad AddEdge W: 2.1412241164848024e-15
grad ChooseDest W: 4.735435962677002
grad AddEdge W: 4.823769916551985e-17
grad ChooseDest W: 2.186967372894287
grad AddEdge W: 4.575369894708026e-17
grad ChooseDest W: 2.996249198913574
grad AddEdge W: 5.343617528211346e-17
grad ChooseDest W: 1.9985566139221191
grad AddEdge W: 2.666189937962837e-15
grad ChooseDest W: 1.8600746393203735
grad AddEdge W: 1.6382527929723056e-13
grad ChooseDest W: 1.7696902751922607
grad AddEdge W: 2.8367083772709765e-17
grad ChooseDest W: 2.057568311691284
grad AddEdge W: 4.182097791253775e-17
grad ChooseDest W: 2.347001075744629
grad AddEdge W: 2.531950462416081e-15
grad ChooseDest W: 2.0300073623657227
grad AddEdge W: 4.619761700333542e-17
grad ChooseDest W: 4.743004322052002
grad AddEdge W: 6.812243287702499e-17
grad ChooseDest W: 2.003939628601074
grad AddEdge W: 4.119019644718441e-17
grad ChooseDest W: 2.1318585872650146
grad AddEdge W: 4.8349474427332917e-17
grad ChooseDest W: 3.1147449016571045
grad AddEdge W: 1.1835320056743421e-17
grad ChooseDest W: 2.1550142765045166
grad AddEdge W: 3.705331731129646e-17
grad ChooseDest W: 2.6295628547668457
grad AddEdge W: 5.19229939352355e-17
grad ChooseDest W: 1.8865488767623901
grad AddEdge W: 4.1044683995431104e-15
grad ChooseDest W: 2.595681667327881
grad AddEdge W: 8.996017971943723e-17
grad ChooseDest W: 1.967310905456543
grad AddEdge W: 5.1364255798419696e-15
grad ChooseDest W: 2.3297979831695557
grad AddEdge W: 8.596443413966308e-17
grad ChooseDest W: 2.0964341163635254
grad AddEdge W: 5.416696076594209e-15
grad ChooseDest W: 2.750096559524536
grad AddEdge W: 6.6956213827745834e-15
grad ChooseDest W: 2.38163161277771
grad AddEdge W: 1.3582575649872652e-15
grad ChooseDest W: 1.7392425537109375
grad AddEdge W: 1.7133383756004737e-17
grad ChooseDest W: 3.4043095111846924
grad AddEdge W: 1.2022579685355013e-17
grad ChooseDest W: 2.5728561878204346
grad AddEdge W: 9.06111259158307e-14
grad ChooseDest W: 1.0924010276794434
grad AddEdge W: 2.6850090777569166e-17
grad ChooseDest W: 1.8944456577301025
grad AddEdge W: 5.6579335878361855e-18
grad ChooseDest W: 2.323272466659546
grad AddEdge W: 3.3518559089851414e-15
grad ChooseDest W: 1.8183413743972778
grad AddEdge W: 8.459171136951908e-17
grad ChooseDest W: 3.45509934425354
grad AddEdge W: 3.8767715305261614e-17
grad ChooseDest W: 3.372286319732666
grad AddEdge W: 1.563943502011313e-16
grad ChooseDest W: 2.029710531234741
grad AddEdge W: 1.0819828402845798e-15
grad ChooseDest W: 2.173159122467041
grad AddEdge W: 1.0773289633396788e-16
grad ChooseDest W: 2.7438032627105713
grad AddEdge W: 3.7128259874793764e-17
grad ChooseDest W: 3.021265983581543
grad AddEdge W: 2.4056748171091895e-17
grad ChooseDest W: 1.6250946521759033
grad AddEdge W: 2.1054788959767524e-17
grad ChooseDest W: 1.6674998998641968
grad AddEdge W: 9.558835927653275e-14
grad ChooseDest W: 2.031442403793335
=== Epoch 19: Train Loss: 5.9876, Train Log Prob: 0.0071 ===
Total mismatches: 91088
Predicted valid destination but wrong order: 20202
Epoch 19: Validation Loss: 6.2072, Validation Log Prob: 0.0041
Epoch 19: Edge Precision: 0.3615, Recall: 0.3594, F1: 0.3604, Jaccard: 0.2374
Epoch 19: TP: 2.5169649248389407, FP: 4.4661417322834644, FN: 4.504509663564781
Epoch 19: Current Learning Rate: 1.5e-05
[Epoch 19] ‚è±Ô∏è Total: 1945.05s | Current time: 2025-07-14 21:39:06 | üèãÔ∏è Train: 1707.70s | ‚úÖ Val: 237.35s
grad AddEdge W: 1.33759092669182e-13
grad ChooseDest W: 3.780604362487793
grad AddEdge W: 1.8254464948920305e-17
grad ChooseDest W: 1.9356095790863037
grad AddEdge W: 2.6279442489306193e-13
grad ChooseDest W: 2.0982956886291504
grad AddEdge W: 3.1218428416088153e-15
grad ChooseDest W: 2.0340147018432617
grad AddEdge W: 2.253939214292406e-15
grad ChooseDest W: 2.6148786544799805
grad AddEdge W: 6.5014589848907955e-15
grad ChooseDest W: 2.4280478954315186
grad AddEdge W: 1.3869622410202925e-15
grad ChooseDest W: 2.762180805206299
grad AddEdge W: 9.889521528964559e-16
grad ChooseDest W: 1.8664860725402832
grad AddEdge W: 8.91218082134642e-15
grad ChooseDest W: 2.4510738849639893
grad AddEdge W: 8.102341964308783e-17
grad ChooseDest W: 4.076144695281982
grad AddEdge W: 1.670048672335899e-16
grad ChooseDest W: 2.5516726970672607
grad AddEdge W: 6.329609975936528e-15
grad ChooseDest W: 2.2742068767547607
grad AddEdge W: 2.2721582577131354e-15
grad ChooseDest W: 1.9041882753372192
grad AddEdge W: 8.370185031886923e-17
grad ChooseDest W: 3.001455545425415
grad AddEdge W: 1.1727425926364455e-17
grad ChooseDest W: 2.8716092109680176
grad AddEdge W: 5.997697362697852e-16
grad ChooseDest W: 1.630364179611206
grad AddEdge W: 6.120732868753479e-17
grad ChooseDest W: 3.2523293495178223
grad AddEdge W: 2.217481963330515e-17
grad ChooseDest W: 2.394780397415161
grad AddEdge W: 2.0669251656785134e-17
grad ChooseDest W: 2.514671564102173
grad AddEdge W: 1.8013163129348786e-17
grad ChooseDest W: 2.190122365951538
grad AddEdge W: 9.52128666063996e-16
grad ChooseDest W: 2.0366151332855225
grad AddEdge W: 6.161178427287076e-16
grad ChooseDest W: 1.591003179550171
grad AddEdge W: 5.770507244376487e-17
grad ChooseDest W: 1.6153414249420166
grad AddEdge W: 7.990935929451382e-15
grad ChooseDest W: 2.1097943782806396
grad AddEdge W: 3.3973570366039848e-15
grad ChooseDest W: 2.0205771923065186
grad AddEdge W: 4.548445827513915e-17
grad ChooseDest W: 2.3009750843048096
grad AddEdge W: 4.231825308876952e-15
grad ChooseDest W: 2.0175859928131104
grad AddEdge W: 4.4850421026894805e-17
grad ChooseDest W: 3.021353006362915
grad AddEdge W: 5.978357218231872e-17
grad ChooseDest W: 2.120090961456299
grad AddEdge W: 1.3811031023658977e-15
grad ChooseDest W: 2.0137462615966797
grad AddEdge W: 4.814815520984976e-17
grad ChooseDest W: 2.748508930206299
grad AddEdge W: 1.7664422128735205e-15
grad ChooseDest W: 3.7040719985961914
grad AddEdge W: 3.200902104343784e-17
grad ChooseDest W: 2.5906457901000977
grad AddEdge W: 3.974445017256423e-17
grad ChooseDest W: 6.108303546905518
grad AddEdge W: 5.1357850905849473e-17
grad ChooseDest W: 5.50790548324585
grad AddEdge W: 2.336595442142559e-15
grad ChooseDest W: 4.318707466125488
grad AddEdge W: 5.273366733148442e-17
grad ChooseDest W: 2.259627103805542
grad AddEdge W: 8.198945410198646e-16
grad ChooseDest W: 2.3303027153015137
grad AddEdge W: 1.0795537615500912e-15
grad ChooseDest W: 1.938806176185608
grad AddEdge W: 7.839654151005869e-16
grad ChooseDest W: 2.558927059173584
grad AddEdge W: 1.4446711052599853e-17
grad ChooseDest W: 1.8042147159576416
grad AddEdge W: 2.1395160491356757e-13
grad ChooseDest W: 1.7892273664474487
grad AddEdge W: 4.7120568587209836e-17
grad ChooseDest W: 2.9223856925964355
grad AddEdge W: 5.810318322293031e-16
grad ChooseDest W: 1.343818187713623
grad AddEdge W: 1.6453752643268713e-16
grad ChooseDest W: 2.4868574142456055
grad AddEdge W: 1.2070231145076805e-15
grad ChooseDest W: 1.3539016246795654
grad AddEdge W: 2.702934447443295e-13
grad ChooseDest W: 2.2814247608184814
grad AddEdge W: 1.1844689564629034e-15
grad ChooseDest W: 2.3363771438598633
grad AddEdge W: 7.237762200156518e-17
grad ChooseDest W: 3.428776741027832
grad AddEdge W: 1.1612350544779547e-16
grad ChooseDest W: 1.1641929149627686
grad AddEdge W: 9.406802481582457e-17
grad ChooseDest W: 2.405822992324829
grad AddEdge W: 4.795064764062925e-17
grad ChooseDest W: 3.183459997177124
grad AddEdge W: 1.5921431653544843e-13
grad ChooseDest W: 2.346637725830078
grad AddEdge W: 9.22589940510609e-17
grad ChooseDest W: 1.8742871284484863
grad AddEdge W: 1.7768008302483995e-17
grad ChooseDest W: 2.9406673908233643
grad AddEdge W: 1.2876182359114561e-14
grad ChooseDest W: 2.787709951400757
grad AddEdge W: 1.8996462501992622e-17
grad ChooseDest W: 1.1221730709075928
grad AddEdge W: 2.3463293330141686e-15
grad ChooseDest W: 2.4012768268585205
grad AddEdge W: 1.247565711253297e-16
grad ChooseDest W: 1.9479715824127197
grad AddEdge W: 9.084534238420778e-15
grad ChooseDest W: 2.0350592136383057
grad AddEdge W: 5.4813086894879427e-17
grad ChooseDest W: 1.5779483318328857
grad AddEdge W: 3.11434890289644e-17
grad ChooseDest W: 2.69108247756958
grad AddEdge W: 2.3146831299310088e-17
grad ChooseDest W: 3.271395683288574
grad AddEdge W: 4.990680405786405e-17
grad ChooseDest W: 1.8138542175292969
grad AddEdge W: 6.739499039401116e-17
grad ChooseDest W: 2.090818166732788
grad AddEdge W: 2.4680872486875405e-17
grad ChooseDest W: 1.5071409940719604
=== Epoch 20: Train Loss: 5.9964, Train Log Prob: 0.0071 ===
Total mismatches: 91319
Predicted valid destination but wrong order: 19876
Epoch 20: Validation Loss: 6.2562, Validation Log Prob: 0.0039
Epoch 20: Edge Precision: 0.3566, Recall: 0.3534, F1: 0.3549, Jaccard: 0.2327
Epoch 20: TP: 2.4743020758768792, FP: 4.487186828919112, FN: 4.547172512526843
Epoch 20: Current Learning Rate: 1.5e-05
[Epoch 20] ‚è±Ô∏è Total: 1939.04s | Current time: 2025-07-14 22:11:25 | üèãÔ∏è Train: 1702.48s | ‚úÖ Val: 236.56s
grad AddEdge W: 1.8296155606181697e-13
grad ChooseDest W: 4.19212007522583
grad AddEdge W: 1.6618717603702408e-15
grad ChooseDest W: 2.5269429683685303
grad AddEdge W: 1.0103486286605731e-15
grad ChooseDest W: 1.8337876796722412
grad AddEdge W: 1.2989895049975793e-15
grad ChooseDest W: 1.7583999633789062
grad AddEdge W: 1.3526542895752475e-13
grad ChooseDest W: 3.195627450942993
grad AddEdge W: 3.57490751697045e-17
grad ChooseDest W: 2.026472330093384
grad AddEdge W: 6.451345048190103e-17
grad ChooseDest W: 4.049386978149414
grad AddEdge W: 3.127058367572195e-17
grad ChooseDest W: 3.217534303665161
grad AddEdge W: 3.128043374245623e-17
grad ChooseDest W: 2.169985294342041
grad AddEdge W: 4.62307533292787e-15
grad ChooseDest W: 1.7844597101211548
grad AddEdge W: 2.7738960217335573e-13
grad ChooseDest W: 1.7501908540725708
grad AddEdge W: 1.1701282386796349e-16
grad ChooseDest W: 2.592320442199707
grad AddEdge W: 1.1115356080560455e-15
grad ChooseDest W: 3.055114269256592
grad AddEdge W: 6.845679913295363e-17
grad ChooseDest W: 3.170079469680786
grad AddEdge W: 5.7922183928805094e-15
grad ChooseDest W: 1.8031926155090332
grad AddEdge W: 1.14512501721677e-16
grad ChooseDest W: 1.3949922323226929
grad AddEdge W: 4.477839013915369e-17
grad ChooseDest W: 2.068796396255493
grad AddEdge W: 4.176843870875083e-17
grad ChooseDest W: 3.220028877258301
grad AddEdge W: 3.3242892217167502e-15
grad ChooseDest W: 1.7201913595199585
grad AddEdge W: 3.409725843686595e-12
grad ChooseDest W: 1.7075666189193726
grad AddEdge W: 4.530034772311955e-17
grad ChooseDest W: 2.1543548107147217
grad AddEdge W: 1.1817441573507047e-15
grad ChooseDest W: 3.324540615081787
grad AddEdge W: 6.001120434595943e-15
grad ChooseDest W: 2.3305819034576416
grad AddEdge W: 5.4148837701942194e-17
grad ChooseDest W: 2.953462839126587
grad AddEdge W: 1.651775293075092e-17
grad ChooseDest W: 1.6252118349075317
grad AddEdge W: 1.0521436694978237e-15
grad ChooseDest W: 2.372279167175293
grad AddEdge W: 3.3920976237460256e-17
grad ChooseDest W: 1.4963886737823486
grad AddEdge W: 5.293360019426094e-17
grad ChooseDest W: 1.1874369382858276
grad AddEdge W: 5.527967406743217e-14
grad ChooseDest W: 1.1902295351028442
grad AddEdge W: 8.221963530540282e-17
grad ChooseDest W: 2.4511380195617676
grad AddEdge W: 2.690149508955566e-17
grad ChooseDest W: 2.403801918029785
grad AddEdge W: 2.570552506679538e-15
grad ChooseDest W: 1.772609829902649
grad AddEdge W: 8.86455025289818e-16
grad ChooseDest W: 2.3073582649230957
grad AddEdge W: 3.438368121978772e-17
grad ChooseDest W: 2.1780903339385986
grad AddEdge W: 7.893019608963054e-17
grad ChooseDest W: 2.3839993476867676
grad AddEdge W: 3.5323615946603482e-15
grad ChooseDest W: 1.8805257081985474
grad AddEdge W: 3.4209534686903582e-15
grad ChooseDest W: 1.7473030090332031
grad AddEdge W: 4.9122579761701515e-18
grad ChooseDest W: 2.1854970455169678
grad AddEdge W: 2.481822748195106e-17
grad ChooseDest W: 2.02976655960083
grad AddEdge W: 9.9963287837233e-15
grad ChooseDest W: 3.2376906871795654
grad AddEdge W: 1.5872527237541183e-17
grad ChooseDest W: 3.083064556121826
grad AddEdge W: 8.16625126192361e-18
grad ChooseDest W: 3.2554919719696045
grad AddEdge W: 1.0824207563183102e-15
grad ChooseDest W: 1.5853087902069092
grad AddEdge W: 7.355841901726217e-17
grad ChooseDest W: 2.7586519718170166
grad AddEdge W: 2.73318931235441e-14
grad ChooseDest W: 2.5444223880767822
grad AddEdge W: 1.0167782175351641e-16
grad ChooseDest W: 4.311544895172119
grad AddEdge W: 1.4996940592582747e-13
grad ChooseDest W: 1.5508060455322266
grad AddEdge W: 2.0050557616286912e-16
grad ChooseDest W: 2.0247766971588135
grad AddEdge W: 2.6454784471552524e-17
grad ChooseDest W: 3.265911817550659
grad AddEdge W: 5.949020099754821e-17
grad ChooseDest W: 1.9957969188690186
grad AddEdge W: 3.841541325902377e-15
grad ChooseDest W: 1.868780255317688
grad AddEdge W: 1.5567427955173654e-15
grad ChooseDest W: 3.5507700443267822
grad AddEdge W: 5.161194755513596e-17
grad ChooseDest W: 2.521122694015503
grad AddEdge W: 1.0124413426707731e-16
grad ChooseDest W: 2.5849955081939697
grad AddEdge W: 5.010611818954238e-17
grad ChooseDest W: 2.149195909500122
grad AddEdge W: 5.350725739369198e-18
grad ChooseDest W: 3.650144577026367
grad AddEdge W: 5.701022431914282e-10
grad ChooseDest W: 1.0223995447158813
grad AddEdge W: 3.579215142728381e-17
grad ChooseDest W: 2.235309362411499
grad AddEdge W: 7.422927832297088e-14
grad ChooseDest W: 1.6306419372558594
grad AddEdge W: 3.153704832952733e-17
grad ChooseDest W: 1.8468700647354126
grad AddEdge W: 7.077458964957375e-16
grad ChooseDest W: 2.0547609329223633
grad AddEdge W: 9.097783686600407e-17
grad ChooseDest W: 5.644143581390381
grad AddEdge W: 2.0641018328117474e-17
grad ChooseDest W: 2.295633316040039
grad AddEdge W: 2.0127636290998073e-16
grad ChooseDest W: 2.1881911754608154
grad AddEdge W: 1.7057900502260442e-13
grad ChooseDest W: 1.1661030054092407
grad AddEdge W: 1.6885526629340408e-15
grad ChooseDest W: 2.019061326980591
=== Epoch 21: Train Loss: 5.9920, Train Log Prob: 0.0071 ===
Total mismatches: 91411
Predicted valid destination but wrong order: 19907
Epoch 21: Validation Loss: 6.2703, Validation Log Prob: 0.0040
Epoch 21: Edge Precision: 0.3614, Recall: 0.3588, F1: 0.3600, Jaccard: 0.2364
Epoch 21: TP: 2.511667859699356, FP: 4.461130994989262, FN: 4.509806728704366
Epoch 21: Current Learning Rate: 7.5e-06
[Epoch 21] ‚è±Ô∏è Total: 1959.19s | Current time: 2025-07-14 22:44:04 | üèãÔ∏è Train: 1724.12s | ‚úÖ Val: 235.07s
grad AddEdge W: 1.1033901818932235e-16
grad ChooseDest W: 3.4718480110168457
grad AddEdge W: 2.5144565122854256e-17
grad ChooseDest W: 3.107736349105835
grad AddEdge W: 3.3253694004827363e-15
grad ChooseDest W: 2.8502352237701416
grad AddEdge W: 3.260124358379869e-15
grad ChooseDest W: 2.5026440620422363
grad AddEdge W: 4.529717055547396e-15
grad ChooseDest W: 2.5542027950286865
grad AddEdge W: 2.2923808516914766e-17
grad ChooseDest W: 1.8378363847732544
grad AddEdge W: 1.309787163371822e-15
grad ChooseDest W: 2.321054220199585
grad AddEdge W: 2.7288517413348156e-15
grad ChooseDest W: 3.0889785289764404
grad AddEdge W: 2.11594759454755e-15
grad ChooseDest W: 2.663965940475464
grad AddEdge W: 1.147187224805979e-15
grad ChooseDest W: 1.5443897247314453
grad AddEdge W: 9.233563729789762e-17
grad ChooseDest W: 3.01562237739563
grad AddEdge W: 1.6653206667732235e-15
grad ChooseDest W: 2.6459758281707764
grad AddEdge W: 2.8070440261436e-17
grad ChooseDest W: 1.7441444396972656
grad AddEdge W: 7.238928326296871e-16
grad ChooseDest W: 1.7196099758148193
grad AddEdge W: 1.8745904531366635e-17
grad ChooseDest W: 3.285200595855713
grad AddEdge W: 1.1971966852865833e-15
grad ChooseDest W: 1.1228713989257812
grad AddEdge W: 2.847326729358197e-17
grad ChooseDest W: 2.2836759090423584
grad AddEdge W: 8.799625442188933e-17
grad ChooseDest W: 2.015254259109497
grad AddEdge W: 6.866091422090721e-16
grad ChooseDest W: 2.441592216491699
grad AddEdge W: 3.683266019927859e-15
grad ChooseDest W: 3.262455701828003
grad AddEdge W: 6.605797773883001e-15
grad ChooseDest W: 2.357560873031616
grad AddEdge W: 1.6260456746855076e-17
grad ChooseDest W: 2.2819809913635254
grad AddEdge W: 8.824129178910714e-17
grad ChooseDest W: 2.5992984771728516
grad AddEdge W: 2.3726273766858097e-15
grad ChooseDest W: 3.7147414684295654
grad AddEdge W: 1.2270467616225353e-15
grad ChooseDest W: 1.9665091037750244
grad AddEdge W: 7.035430513603597e-17
grad ChooseDest W: 3.047307252883911
grad AddEdge W: 1.515844571988844e-14
grad ChooseDest W: 1.236720323562622
grad AddEdge W: 3.058657385528185e-13
grad ChooseDest W: 1.8712482452392578
grad AddEdge W: 2.4852777161776175e-17
grad ChooseDest W: 1.8887656927108765
grad AddEdge W: 4.342239957844041e-17
grad ChooseDest W: 1.4668169021606445
grad AddEdge W: 5.0850736250795264e-17
grad ChooseDest W: 1.756089687347412
grad AddEdge W: 9.289248205137851e-17
grad ChooseDest W: 2.087268590927124
grad AddEdge W: 1.466949097476733e-16
grad ChooseDest W: 2.0582363605499268
grad AddEdge W: 5.48737165868016e-15
grad ChooseDest W: 2.3095242977142334
grad AddEdge W: 2.002818721145213e-15
grad ChooseDest W: 1.9700061082839966
grad AddEdge W: 1.0649837360661332e-15
grad ChooseDest W: 2.2907345294952393
grad AddEdge W: 3.600116183627693e-15
grad ChooseDest W: 1.9096834659576416
grad AddEdge W: 3.0984442049505155e-17
grad ChooseDest W: 2.375786542892456
grad AddEdge W: 5.713606535595259e-15
grad ChooseDest W: 3.403536796569824
grad AddEdge W: 3.003333985502433e-17
grad ChooseDest W: 2.0447795391082764
grad AddEdge W: 9.06849821066336e-15
grad ChooseDest W: 1.8972312211990356
grad AddEdge W: 1.2264692333832736e-14
grad ChooseDest W: 2.52105450630188
grad AddEdge W: 7.557636913497714e-17
grad ChooseDest W: 1.7759015560150146
grad AddEdge W: 5.419139343130564e-15
grad ChooseDest W: 2.255887746810913
grad AddEdge W: 3.7150104060410064e-17
grad ChooseDest W: 2.309875249862671
grad AddEdge W: 1.1140265003433489e-17
grad ChooseDest W: 1.5295649766921997
grad AddEdge W: 1.618036323042173e-15
grad ChooseDest W: 2.527672529220581
grad AddEdge W: 4.255484428018867e-18
grad ChooseDest W: 1.8336949348449707
grad AddEdge W: 1.39244838077543e-17
grad ChooseDest W: 2.5465967655181885
grad AddEdge W: 3.563579840964353e-15
grad ChooseDest W: 1.8544169664382935
grad AddEdge W: 1.588638085844022e-16
grad ChooseDest W: 2.0527358055114746
grad AddEdge W: 2.5612474725595902e-17
grad ChooseDest W: 1.3756654262542725
grad AddEdge W: 2.7313360889691125e-15
grad ChooseDest W: 2.8333189487457275
grad AddEdge W: 2.2708536416859065e-17
grad ChooseDest W: 1.7695930004119873
grad AddEdge W: 1.169071379789478e-15
grad ChooseDest W: 2.7479751110076904
grad AddEdge W: 2.529830457533716e-13
grad ChooseDest W: 1.9494353532791138
grad AddEdge W: 2.705259967929652e-15
grad ChooseDest W: 3.7903804779052734
grad AddEdge W: 2.106391368836627e-15
grad ChooseDest W: 2.468153238296509
grad AddEdge W: 2.157375975997758e-15
grad ChooseDest W: 2.29015851020813
grad AddEdge W: 7.826604946708926e-17
grad ChooseDest W: 3.2671456336975098
grad AddEdge W: 9.228288435064042e-16
grad ChooseDest W: 2.336451530456543
grad AddEdge W: 2.564292986138888e-17
grad ChooseDest W: 2.2547988891601562
grad AddEdge W: 4.709334441888949e-17
grad ChooseDest W: 2.0457324981689453
grad AddEdge W: 5.130086808781192e-17
grad ChooseDest W: 2.200347900390625
grad AddEdge W: 4.1803176985755606e-17
grad ChooseDest W: 2.3080766201019287
grad AddEdge W: 9.879683108933302e-17
grad ChooseDest W: 6.163858413696289
=== Epoch 22: Train Loss: 5.9908, Train Log Prob: 0.0071 ===
Total mismatches: 91452
Predicted valid destination but wrong order: 19673
Epoch 22: Validation Loss: 6.2863, Validation Log Prob: 0.0038
Epoch 22: Edge Precision: 0.3637, Recall: 0.3607, F1: 0.3621, Jaccard: 0.2386
Epoch 22: TP: 2.526700071581961, FP: 4.440944881889764, FN: 4.494774516821761
Epoch 22: Current Learning Rate: 7.5e-06
[Epoch 22] ‚è±Ô∏è Total: 1909.98s | Current time: 2025-07-14 23:15:54 | üèãÔ∏è Train: 1673.88s | ‚úÖ Val: 236.10s
grad AddEdge W: 1.157902333043248e-11
grad ChooseDest W: 4.167633533477783
grad AddEdge W: 5.302049055452596e-17
grad ChooseDest W: 1.8622126579284668
grad AddEdge W: 8.449720763889612e-17
grad ChooseDest W: 2.0811944007873535
grad AddEdge W: 4.3156348514941304e-17
grad ChooseDest W: 3.4837965965270996
grad AddEdge W: 2.0564295708999647e-13
grad ChooseDest W: 1.483014702796936
grad AddEdge W: 1.4877470650542763e-16
grad ChooseDest W: 2.363743782043457
grad AddEdge W: 2.7472190155213147e-15
grad ChooseDest W: 1.9915690422058105
grad AddEdge W: 7.38837309542058e-18
grad ChooseDest W: 1.8469583988189697
grad AddEdge W: 8.890807877216369e-17
grad ChooseDest W: 3.029937505722046
grad AddEdge W: 3.271746484214255e-17
grad ChooseDest W: 6.901187896728516
grad AddEdge W: 2.987853796646871e-17
grad ChooseDest W: 2.2159008979797363
grad AddEdge W: 7.757433456909282e-17
grad ChooseDest W: 2.542466878890991
grad AddEdge W: 1.0923821390637287e-14
grad ChooseDest W: 2.1345407962799072
grad AddEdge W: 1.0866198078048755e-13
grad ChooseDest W: 2.851515531539917
grad AddEdge W: 3.5178898249982717e-15
grad ChooseDest W: 1.8934468030929565
grad AddEdge W: 2.9002868026407823e-17
grad ChooseDest W: 2.580411195755005
grad AddEdge W: 7.705515472379836e-15
grad ChooseDest W: 2.754087209701538
grad AddEdge W: 4.6321743726055126e-17
grad ChooseDest W: 2.18673038482666
grad AddEdge W: 3.836844184102717e-17
grad ChooseDest W: 1.8723820447921753
grad AddEdge W: 7.614900972943535e-17
grad ChooseDest W: 1.9887566566467285
grad AddEdge W: 3.786209811830386e-17
grad ChooseDest W: 2.318406105041504
grad AddEdge W: 3.482439378573311e-15
grad ChooseDest W: 2.8294882774353027
grad AddEdge W: 1.6631975786909306e-15
grad ChooseDest W: 1.6138439178466797
grad AddEdge W: 3.40955808298804e-17
grad ChooseDest W: 1.9800238609313965
grad AddEdge W: 5.516284069543502e-16
grad ChooseDest W: 2.2727909088134766
grad AddEdge W: 1.5075774298384796e-16
grad ChooseDest W: 2.1047611236572266
grad AddEdge W: 1.5274803218187915e-16
grad ChooseDest W: 2.4187123775482178
grad AddEdge W: 3.4425768169354416e-17
grad ChooseDest W: 3.065110206604004
grad AddEdge W: 9.806645542386707e-18
grad ChooseDest W: 1.6882282495498657
grad AddEdge W: 2.3061024545647512e-17
grad ChooseDest W: 2.3094539642333984
grad AddEdge W: 1.757897556259856e-15
grad ChooseDest W: 5.150476932525635
grad AddEdge W: 2.2530224070061214e-16
grad ChooseDest W: 1.6265060901641846
grad AddEdge W: 1.2924273289969634e-15
grad ChooseDest W: 3.1345431804656982
grad AddEdge W: 2.3371676128984293e-15
grad ChooseDest W: 2.7835776805877686
grad AddEdge W: 5.498256296750174e-18
grad ChooseDest W: 3.6116645336151123
grad AddEdge W: 4.173050116183449e-15
grad ChooseDest W: 1.9744032621383667
grad AddEdge W: 2.7887778400453997e-15
grad ChooseDest W: 1.8896089792251587
grad AddEdge W: 2.950170227265813e-15
grad ChooseDest W: 2.254523277282715
grad AddEdge W: 1.2559041881361992e-17
grad ChooseDest W: 2.459982395172119
grad AddEdge W: 1.7139865212412457e-15
grad ChooseDest W: 2.9500951766967773
grad AddEdge W: 2.519898533535412e-17
grad ChooseDest W: 2.684053659439087
grad AddEdge W: 4.734532586808801e-15
grad ChooseDest W: 2.998732566833496
grad AddEdge W: 9.649184527565566e-14
grad ChooseDest W: 2.1494600772857666
grad AddEdge W: 3.4023285244343265e-17
grad ChooseDest W: 2.2457423210144043
grad AddEdge W: 1.9038135861253044e-17
grad ChooseDest W: 2.1581931114196777
grad AddEdge W: 5.472504642269071e-16
grad ChooseDest W: 1.6592884063720703
grad AddEdge W: 2.3405140672783098e-12
grad ChooseDest W: 2.43005108833313
grad AddEdge W: 1.9074632724240108e-17
grad ChooseDest W: 2.2704696655273438
grad AddEdge W: 4.415749619527277e-15
grad ChooseDest W: 2.8550713062286377
grad AddEdge W: 4.447482014830265e-15
grad ChooseDest W: 3.535304307937622
grad AddEdge W: 2.751813533985459e-12
grad ChooseDest W: 1.6427675485610962
grad AddEdge W: 3.3207234247670464e-15
grad ChooseDest W: 1.7379571199417114
grad AddEdge W: 8.446477406387487e-10
grad ChooseDest W: 1.435475468635559
grad AddEdge W: 4.226982927396617e-17
grad ChooseDest W: 2.3388822078704834
grad AddEdge W: 8.645691762404245e-17
grad ChooseDest W: 3.4264371395111084
grad AddEdge W: 1.8395723905614966e-15
grad ChooseDest W: 2.078772783279419
grad AddEdge W: 9.158476905993628e-17
grad ChooseDest W: 2.303103446960449
grad AddEdge W: 4.0113598380633983e-16
grad ChooseDest W: 2.087428331375122
grad AddEdge W: 2.607529094717214e-15
grad ChooseDest W: 1.674651861190796
grad AddEdge W: 3.5593972187405906e-17
grad ChooseDest W: 1.949635624885559
grad AddEdge W: 1.596781182666239e-17
grad ChooseDest W: 2.5525200366973877
grad AddEdge W: 1.981113925388295e-15
grad ChooseDest W: 1.7921589612960815
grad AddEdge W: 9.915149569574922e-16
grad ChooseDest W: 2.0761590003967285
grad AddEdge W: 6.22088988255487e-17
grad ChooseDest W: 2.135256767272949
grad AddEdge W: 2.0989743248339948e-15
grad ChooseDest W: 1.9594415426254272
grad AddEdge W: 1.3398602211311386e-15
grad ChooseDest W: 2.3540472984313965
=== Epoch 23: Train Loss: 5.9888, Train Log Prob: 0.0071 ===
Total mismatches: 91238
Predicted valid destination but wrong order: 19789
Epoch 23: Validation Loss: 6.2780, Validation Log Prob: 0.0038
Epoch 23: Edge Precision: 0.3566, Recall: 0.3543, F1: 0.3554, Jaccard: 0.2332
Epoch 23: TP: 2.480887616320687, FP: 4.49663564781675, FN: 4.540586972083035
Epoch 23: Current Learning Rate: 7.5e-06
[Epoch 23] ‚è±Ô∏è Total: 1899.07s | Current time: 2025-07-14 23:47:33 | üèãÔ∏è Train: 1663.41s | ‚úÖ Val: 235.66s
grad AddEdge W: 3.398711704866858e-13
grad ChooseDest W: 5.377237796783447
grad AddEdge W: 1.4628481345230421e-15
grad ChooseDest W: 2.5790281295776367
grad AddEdge W: 8.806166124728513e-17
grad ChooseDest W: 2.5451173782348633
grad AddEdge W: 7.801226807287783e-15
grad ChooseDest W: 2.1801490783691406
grad AddEdge W: 2.6883808313698053e-17
grad ChooseDest W: 3.588186264038086
grad AddEdge W: 1.8211439976538972e-17
grad ChooseDest W: 2.487846851348877
grad AddEdge W: 7.954946981830204e-17
grad ChooseDest W: 1.6062270402908325
grad AddEdge W: 5.1474688513011363e-17
grad ChooseDest W: 3.3982794284820557
grad AddEdge W: 1.7604573957055768e-17
grad ChooseDest W: 2.449782133102417
grad AddEdge W: 1.5481509035820016e-17
grad ChooseDest W: 2.6513500213623047
grad AddEdge W: 4.804344936442382e-16
grad ChooseDest W: 2.709486246109009
grad AddEdge W: 1.7275462310673007e-11
grad ChooseDest W: 1.301700234413147
grad AddEdge W: 2.4311761998241583e-16
grad ChooseDest W: 3.1131200790405273
grad AddEdge W: 3.9393130022800707e-17
grad ChooseDest W: 2.2749485969543457
grad AddEdge W: 1.9661090543650404e-17
grad ChooseDest W: 2.7049360275268555
grad AddEdge W: 1.881805453311596e-17
grad ChooseDest W: 2.1381990909576416
grad AddEdge W: 5.010649455672109e-18
grad ChooseDest W: 3.386955976486206
grad AddEdge W: 3.620531879942185e-13
grad ChooseDest W: 1.4944342374801636
grad AddEdge W: 4.3340300248283296e-17
grad ChooseDest W: 1.1786421537399292
grad AddEdge W: 1.4856197682770267e-15
grad ChooseDest W: 2.3651504516601562
grad AddEdge W: 6.204655304980659e-17
grad ChooseDest W: 2.07871413230896
grad AddEdge W: 1.8149515581522027e-17
grad ChooseDest W: 2.4242963790893555
grad AddEdge W: 9.229242041961417e-18
grad ChooseDest W: 2.7683780193328857
grad AddEdge W: 1.4229758584760588e-15
grad ChooseDest W: 2.454676389694214
grad AddEdge W: 9.020973680152203e-17
grad ChooseDest W: 2.683856725692749
grad AddEdge W: 2.9724376646579447e-15
grad ChooseDest W: 2.4789912700653076
grad AddEdge W: 1.4884435180428214e-17
grad ChooseDest W: 2.8437459468841553
grad AddEdge W: 3.233974770467124e-17
grad ChooseDest W: 5.559911727905273
grad AddEdge W: 8.466769287186575e-17
grad ChooseDest W: 3.895808696746826
grad AddEdge W: 1.3743925169285e-17
grad ChooseDest W: 2.4739279747009277
grad AddEdge W: 1.537645709802578e-17
grad ChooseDest W: 2.053067207336426
grad AddEdge W: 2.2872814486512097e-17
grad ChooseDest W: 3.366229772567749
grad AddEdge W: 1.18015417062959e-15
grad ChooseDest W: 2.4764578342437744
grad AddEdge W: 3.860008219104407e-17
grad ChooseDest W: 1.5806742906570435
grad AddEdge W: 2.9848007457978316e-15
grad ChooseDest W: 2.13216233253479
grad AddEdge W: 2.4889464938048617e-16
grad ChooseDest W: 3.127635955810547
grad AddEdge W: 1.8835211913381535e-17
grad ChooseDest W: 2.0891425609588623
grad AddEdge W: 7.494427344506658e-16
grad ChooseDest W: 2.3041481971740723
grad AddEdge W: 3.161882009616187e-17
grad ChooseDest W: 1.8149380683898926
grad AddEdge W: 5.49577973123108e-15
grad ChooseDest W: 1.9306575059890747
grad AddEdge W: 4.7856223319345096e-17
grad ChooseDest W: 1.5646122694015503
grad AddEdge W: 3.7623231518455695e-17
grad ChooseDest W: 2.5777342319488525
grad AddEdge W: 1.0248200806852945e-14
grad ChooseDest W: 3.7707154750823975
grad AddEdge W: 4.454489690456467e-17
grad ChooseDest W: 3.0097267627716064
grad AddEdge W: 4.85728167756716e-12
grad ChooseDest W: 1.388466715812683
grad AddEdge W: 1.6887661483239734e-17
grad ChooseDest W: 2.895358085632324
grad AddEdge W: 2.379258638811186e-17
grad ChooseDest W: 1.52232825756073
grad AddEdge W: 1.5013303302419076e-17
grad ChooseDest W: 2.155167818069458
grad AddEdge W: 7.887494910679971e-13
grad ChooseDest W: 1.659725546836853
grad AddEdge W: 3.801997711873818e-17
grad ChooseDest W: 1.7991104125976562
grad AddEdge W: 1.8438830272568574e-17
grad ChooseDest W: 2.736565351486206
grad AddEdge W: 9.201373830816138e-17
grad ChooseDest W: 4.603259086608887
grad AddEdge W: 1.8359215462099326e-16
grad ChooseDest W: 2.87971568107605
grad AddEdge W: 7.537200920156224e-17
grad ChooseDest W: 1.2454324960708618
grad AddEdge W: 2.8276388646324797e-15
grad ChooseDest W: 2.632587432861328
grad AddEdge W: 2.7755304565085792e-15
grad ChooseDest W: 1.9306890964508057
grad AddEdge W: 1.922954380063659e-17
grad ChooseDest W: 2.7701807022094727
grad AddEdge W: 9.145965633792641e-18
grad ChooseDest W: 2.935465097427368
grad AddEdge W: 5.974570186864257e-15
grad ChooseDest W: 3.389482259750366
grad AddEdge W: 2.2679085478329727e-17
grad ChooseDest W: 1.926392912864685
grad AddEdge W: 1.0602374928253121e-15
grad ChooseDest W: 2.1629645824432373
grad AddEdge W: 3.2843093724854657e-17
grad ChooseDest W: 2.425813913345337
grad AddEdge W: 1.2564755449463467e-14
grad ChooseDest W: 2.599815607070923
grad AddEdge W: 4.162602811812015e-15
grad ChooseDest W: 2.8427934646606445
grad AddEdge W: 5.671287260772997e-17
grad ChooseDest W: 3.916842222213745
grad AddEdge W: 1.1777615937211722e-17
grad ChooseDest W: 1.7492902278900146
=== Epoch 24: Train Loss: 5.9941, Train Log Prob: 0.0071 ===
Total mismatches: 91502
Predicted valid destination but wrong order: 19878
Epoch 24: Validation Loss: 6.2468, Validation Log Prob: 0.0039
Epoch 24: Edge Precision: 0.3624, Recall: 0.3596, F1: 0.3609, Jaccard: 0.2370
Epoch 24: TP: 2.518539727988547, FP: 4.451968503937008, FN: 4.502934860415175
Epoch 24: Current Learning Rate: 7.5e-06
[Epoch 24] ‚è±Ô∏è Total: 1899.41s | Current time: 2025-07-15 00:19:13 | üèãÔ∏è Train: 1664.53s | ‚úÖ Val: 234.88s
grad AddEdge W: 1.3776368847270555e-16
grad ChooseDest W: 4.399015426635742
grad AddEdge W: 7.890947686964731e-17
grad ChooseDest W: 2.1929922103881836
grad AddEdge W: 7.0038621779945805e-15
grad ChooseDest W: 2.514941692352295
grad AddEdge W: 3.494365178954178e-15
grad ChooseDest W: 1.623305082321167
grad AddEdge W: 2.1235175279971617e-15
grad ChooseDest W: 3.097651720046997
grad AddEdge W: 1.7005891069936429e-16
grad ChooseDest W: 3.10331130027771
grad AddEdge W: 1.5717381374199635e-15
grad ChooseDest W: 2.203571319580078
grad AddEdge W: 2.1301764775120014e-15
grad ChooseDest W: 2.8313119411468506
grad AddEdge W: 2.849180275674806e-17
grad ChooseDest W: 2.6486377716064453
grad AddEdge W: 4.854246889913134e-17
grad ChooseDest W: 2.495884656906128
grad AddEdge W: 7.008465749123348e-17
grad ChooseDest W: 2.362531900405884
grad AddEdge W: 4.351132864113035e-15
grad ChooseDest W: 2.172664165496826
grad AddEdge W: 1.5246471289591238e-15
grad ChooseDest W: 3.784313201904297
grad AddEdge W: 1.1271640223834214e-13
grad ChooseDest W: 1.4813302755355835
grad AddEdge W: 1.4825201613208328e-13
grad ChooseDest W: 1.9960321187973022
grad AddEdge W: 9.051750675230247e-15
grad ChooseDest W: 2.1826212406158447
grad AddEdge W: 3.013865410833442e-15
grad ChooseDest W: 1.4402958154678345
grad AddEdge W: 7.958153213293798e-15
grad ChooseDest W: 2.5108070373535156
grad AddEdge W: 1.0804135529311136e-16
grad ChooseDest W: 4.586313247680664
grad AddEdge W: 8.359813179622244e-18
grad ChooseDest W: 3.1036014556884766
grad AddEdge W: 5.962149524027569e-18
grad ChooseDest W: 1.962928295135498
grad AddEdge W: 3.1062921637301736e-15
grad ChooseDest W: 1.830359697341919
grad AddEdge W: 2.4011640357928153e-17
grad ChooseDest W: 2.327770948410034
grad AddEdge W: 3.7752147328401486e-13
grad ChooseDest W: 1.8246475458145142
grad AddEdge W: 6.935960205294413e-15
grad ChooseDest W: 3.389305353164673
grad AddEdge W: 3.3013261320469065e-17
grad ChooseDest W: 3.209383964538574
grad AddEdge W: 3.067012450757266e-14
grad ChooseDest W: 2.254256010055542
grad AddEdge W: 1.7023656527213903e-15
grad ChooseDest W: 1.528244972229004
grad AddEdge W: 3.29069156721968e-17
grad ChooseDest W: 1.6808658838272095
grad AddEdge W: 1.514109933216162e-15
grad ChooseDest W: 2.7769360542297363
grad AddEdge W: 4.5031692695052125e-17
grad ChooseDest W: 2.4143123626708984
grad AddEdge W: 6.093999714844745e-17
grad ChooseDest W: 2.676311731338501
grad AddEdge W: 2.126990720422929e-17
grad ChooseDest W: 3.756674289703369
grad AddEdge W: 4.9755575060159065e-15
grad ChooseDest W: 4.868751525878906
grad AddEdge W: 6.51743016168819e-17
grad ChooseDest W: 2.578510046005249
grad AddEdge W: 6.823590882217747e-17
grad ChooseDest W: 1.8223739862442017
grad AddEdge W: 6.477093167367443e-13
grad ChooseDest W: 1.6970220804214478
grad AddEdge W: 2.1643483277030817e-15
grad ChooseDest W: 2.4770026206970215
grad AddEdge W: 2.687419759994582e-15
grad ChooseDest W: 2.0390207767486572
grad AddEdge W: 6.19297220600896e-17
grad ChooseDest W: 2.9964051246643066
grad AddEdge W: 4.8483937598987087e-17
grad ChooseDest W: 2.137526273727417
grad AddEdge W: 4.2114465431989604e-15
grad ChooseDest W: 2.235938787460327
grad AddEdge W: 5.004267264246617e-15
grad ChooseDest W: 2.986499547958374
grad AddEdge W: 1.040098966866986e-15
grad ChooseDest W: 2.2578506469726562
grad AddEdge W: 8.086877491884859e-18
grad ChooseDest W: 3.538801670074463
grad AddEdge W: 2.9249477036789482e-15
grad ChooseDest W: 1.9416791200637817
grad AddEdge W: 2.93986325084275e-13
grad ChooseDest W: 1.5637167692184448
grad AddEdge W: 1.7947319486415116e-15
grad ChooseDest W: 2.1399080753326416
grad AddEdge W: 9.289921728679817e-16
grad ChooseDest W: 2.1922988891601562
grad AddEdge W: 1.4872311690498392e-16
grad ChooseDest W: 2.022082567214966
grad AddEdge W: 2.4065256352349268e-15
grad ChooseDest W: 2.146453380584717
grad AddEdge W: 6.311112788071744e-17
grad ChooseDest W: 3.7764289379119873
grad AddEdge W: 1.1957439212116462e-17
grad ChooseDest W: 2.1940155029296875
grad AddEdge W: 8.336765611650801e-17
grad ChooseDest W: 2.9975955486297607
grad AddEdge W: 3.051884524375621e-17
grad ChooseDest W: 6.80081033706665
grad AddEdge W: 1.4172459050516145e-13
grad ChooseDest W: 2.2340033054351807
grad AddEdge W: 2.3207392500677545e-17
grad ChooseDest W: 2.561005115509033
grad AddEdge W: 2.806391943123112e-16
grad ChooseDest W: 2.9122681617736816
grad AddEdge W: 5.881268713630808e-17
grad ChooseDest W: 1.948341965675354
grad AddEdge W: 6.916070998190155e-17
grad ChooseDest W: 4.181639671325684
grad AddEdge W: 3.626325924114583e-15
grad ChooseDest W: 1.148498773574829
grad AddEdge W: 2.8618100795488495e-15
grad ChooseDest W: 3.125352144241333
grad AddEdge W: 1.259750491957787e-15
grad ChooseDest W: 1.7750298976898193
grad AddEdge W: 8.375997954306134e-15
grad ChooseDest W: 2.2285983562469482
grad AddEdge W: 2.3245636369118322e-17
grad ChooseDest W: 2.4485158920288086
grad AddEdge W: 2.3457366217093274e-15
grad ChooseDest W: 4.017948150634766
=== Epoch 25: Train Loss: 5.9858, Train Log Prob: 0.0072 ===
Total mismatches: 91017
Predicted valid destination but wrong order: 19933
Epoch 25: Validation Loss: 6.2809, Validation Log Prob: 0.0039
Epoch 25: Edge Precision: 0.3618, Recall: 0.3594, F1: 0.3605, Jaccard: 0.2376
Epoch 25: TP: 2.5159627773801003, FP: 4.46370794559771, FN: 4.505511811023622
Epoch 25: Current Learning Rate: 3.75e-06
üíæ Checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/checkpoint_epoch_25.pth
[Epoch 25] ‚è±Ô∏è Total: 1886.82s | Current time: 2025-07-15 00:50:39 | üèãÔ∏è Train: 1652.14s | ‚úÖ Val: 234.68s
grad AddEdge W: 9.469194134383821e-16
grad ChooseDest W: 7.229525566101074
grad AddEdge W: 4.3786732933600615e-17
grad ChooseDest W: 2.3564395904541016
grad AddEdge W: 1.2263935933410838e-15
grad ChooseDest W: 2.863175630569458
grad AddEdge W: 1.5023242109089472e-15
grad ChooseDest W: 1.316895604133606
grad AddEdge W: 7.001989917543793e-17
grad ChooseDest W: 2.3618555068969727
grad AddEdge W: 2.1370111863633964e-15
grad ChooseDest W: 1.6624804735183716
grad AddEdge W: 4.812092429173562e-15
grad ChooseDest W: 1.9628112316131592
grad AddEdge W: 7.045437519661137e-15
grad ChooseDest W: 1.7225522994995117
grad AddEdge W: 4.415744537329593e-15
grad ChooseDest W: 2.416623115539551
grad AddEdge W: 5.758429825098062e-15
grad ChooseDest W: 2.4233217239379883
grad AddEdge W: 6.367964580700268e-17
grad ChooseDest W: 2.2829291820526123
grad AddEdge W: 2.7755183532018563e-17
grad ChooseDest W: 2.687058448791504
grad AddEdge W: 3.481530737214034e-17
grad ChooseDest W: 2.7899320125579834
grad AddEdge W: 5.473866988885611e-15
grad ChooseDest W: 1.911523461341858
grad AddEdge W: 6.318578589408402e-17
grad ChooseDest W: 1.6327465772628784
grad AddEdge W: 2.3287561522157205e-15
grad ChooseDest W: 1.2981548309326172
grad AddEdge W: 1.6707077698479813e-16
grad ChooseDest W: 2.326267957687378
grad AddEdge W: 2.6147382649231893e-17
grad ChooseDest W: 3.1663453578948975
grad AddEdge W: 3.8458872534313915e-17
grad ChooseDest W: 2.4510622024536133
grad AddEdge W: 8.494051477263807e-15
grad ChooseDest W: 2.9259345531463623
grad AddEdge W: 1.0822011894965142e-16
grad ChooseDest W: 2.1813223361968994
grad AddEdge W: 1.563735158376068e-15
grad ChooseDest W: 2.648253917694092
grad AddEdge W: 2.756762529120584e-17
grad ChooseDest W: 4.105279922485352
grad AddEdge W: 3.0582927124334037e-15
grad ChooseDest W: 1.413292407989502
grad AddEdge W: 5.24658163229724e-17
grad ChooseDest W: 2.5931551456451416
grad AddEdge W: 1.015954504463739e-15
grad ChooseDest W: 2.6615476608276367
grad AddEdge W: 3.198833689591258e-15
grad ChooseDest W: 2.150237798690796
grad AddEdge W: 3.019151333175672e-17
grad ChooseDest W: 1.8423126935958862
grad AddEdge W: 8.059564550541175e-16
grad ChooseDest W: 2.9111602306365967
grad AddEdge W: 7.205013788833298e-17
grad ChooseDest W: 2.3440914154052734
grad AddEdge W: 1.3614815510555274e-17
grad ChooseDest W: 2.3699052333831787
grad AddEdge W: 2.0299090317901124e-16
grad ChooseDest W: 1.6968413591384888
grad AddEdge W: 2.003336470034222e-15
grad ChooseDest W: 2.3845458030700684
grad AddEdge W: 2.3877702611399077e-16
grad ChooseDest W: 2.640425205230713
grad AddEdge W: 2.390565205168051e-16
grad ChooseDest W: 2.1702632904052734
grad AddEdge W: 1.719510552485883e-15
grad ChooseDest W: 2.058804512023926
grad AddEdge W: 3.288995185219456e-17
grad ChooseDest W: 2.2582790851593018
grad AddEdge W: 9.126938825342696e-16
grad ChooseDest W: 1.6740565299987793
grad AddEdge W: 2.6777699900825465e-16
grad ChooseDest W: 2.015890598297119
grad AddEdge W: 6.289770628295369e-15
grad ChooseDest W: 2.848341226577759
grad AddEdge W: 4.698066918456997e-17
grad ChooseDest W: 1.4426043033599854
grad AddEdge W: 1.5434647599757662e-16
grad ChooseDest W: 5.5945353507995605
grad AddEdge W: 1.1328223930534933e-15
grad ChooseDest W: 2.1349480152130127
grad AddEdge W: 4.4198562999533616e-16
grad ChooseDest W: 1.365756869316101
grad AddEdge W: 8.195202159846492e-15
grad ChooseDest W: 2.6461751461029053
grad AddEdge W: 7.437566353629722e-18
grad ChooseDest W: 1.8632997274398804
grad AddEdge W: 2.4010057332759073e-15
grad ChooseDest W: 2.1631970405578613
grad AddEdge W: 1.52378944192558e-17
grad ChooseDest W: 2.9884719848632812
grad AddEdge W: 3.291680213487803e-17
grad ChooseDest W: 2.254443407058716
grad AddEdge W: 9.213923815069793e-17
grad ChooseDest W: 1.9435327053070068
grad AddEdge W: 1.0869571802336735e-17
grad ChooseDest W: 3.0776500701904297
grad AddEdge W: 9.563825846391154e-18
grad ChooseDest W: 3.042109966278076
grad AddEdge W: 2.54862176488394e-15
grad ChooseDest W: 4.040571212768555
grad AddEdge W: 2.5582851733499153e-17
grad ChooseDest W: 2.3433444499969482
grad AddEdge W: 3.4659284565000586e-17
grad ChooseDest W: 1.8815871477127075
grad AddEdge W: 4.0988528760346754e-13
grad ChooseDest W: 2.8398382663726807
grad AddEdge W: 3.279088208459031e-17
grad ChooseDest W: 2.8097310066223145
grad AddEdge W: 6.491481042651258e-16
grad ChooseDest W: 1.395472526550293
grad AddEdge W: 4.591176495650747e-15
grad ChooseDest W: 2.3130686283111572
grad AddEdge W: 3.638440612842688e-17
grad ChooseDest W: 2.365978240966797
grad AddEdge W: 8.270101471723927e-17
grad ChooseDest W: 3.3318324089050293
grad AddEdge W: 2.4555716751473682e-17
grad ChooseDest W: 2.168712615966797
grad AddEdge W: 1.900925739788204e-15
grad ChooseDest W: 3.501768112182617
grad AddEdge W: 2.4800522508119975e-17
grad ChooseDest W: 1.8525662422180176
grad AddEdge W: 1.2393313863356828e-15
grad ChooseDest W: 1.8289583921432495
grad AddEdge W: 3.231419258830368e-15
grad ChooseDest W: 1.9422863721847534
=== Epoch 26: Train Loss: 5.9836, Train Log Prob: 0.0072 ===
Total mismatches: 91319
Predicted valid destination but wrong order: 19967
Epoch 26: Validation Loss: 6.2637, Validation Log Prob: 0.0038
Epoch 26: Edge Precision: 0.3576, Recall: 0.3547, F1: 0.3560, Jaccard: 0.2333
Epoch 26: TP: 2.4830350751610593, FP: 4.487186828919112, FN: 4.538439513242663
Epoch 26: Current Learning Rate: 3.75e-06
[Epoch 26] ‚è±Ô∏è Total: 1880.81s | Current time: 2025-07-15 01:22:00 | üèãÔ∏è Train: 1642.78s | ‚úÖ Val: 238.03s
grad AddEdge W: 2.0408638158348619e-13
grad ChooseDest W: 4.716241359710693
grad AddEdge W: 2.882356173938593e-17
grad ChooseDest W: 2.573594093322754
grad AddEdge W: 3.446096582938418e-15
grad ChooseDest W: 1.8858529329299927
grad AddEdge W: 7.787759056218334e-18
grad ChooseDest W: 2.8084306716918945
grad AddEdge W: 1.051199403123925e-17
grad ChooseDest W: 2.699934482574463
grad AddEdge W: 5.287034814127362e-15
grad ChooseDest W: 3.01224946975708
grad AddEdge W: 9.000529745876833e-17
grad ChooseDest W: 1.887017011642456
grad AddEdge W: 8.734931553836933e-15
grad ChooseDest W: 1.8239349126815796
grad AddEdge W: 3.153800455031544e-17
grad ChooseDest W: 3.381683588027954
grad AddEdge W: 3.3909403385120598e-15
grad ChooseDest W: 2.194495439529419
grad AddEdge W: 1.8048124030434903e-13
grad ChooseDest W: 1.5093843936920166
grad AddEdge W: 2.211614671984003e-15
grad ChooseDest W: 2.3382158279418945
grad AddEdge W: 1.86137184225027e-15
grad ChooseDest W: 2.117431879043579
grad AddEdge W: 2.610763066509831e-15
grad ChooseDest W: 1.5602136850357056
grad AddEdge W: 1.4621155833725651e-16
grad ChooseDest W: 2.006833791732788
grad AddEdge W: 5.029197984983153e-15
grad ChooseDest W: 2.2386090755462646
grad AddEdge W: 4.5560004879006966e-14
grad ChooseDest W: 2.5321664810180664
grad AddEdge W: 5.849403184043922e-10
grad ChooseDest W: 1.8547900915145874
grad AddEdge W: 4.066709788336882e-15
grad ChooseDest W: 2.493851661682129
grad AddEdge W: 3.744583436428757e-17
grad ChooseDest W: 2.2522594928741455
grad AddEdge W: 7.081107427028775e-17
grad ChooseDest W: 3.3181724548339844
grad AddEdge W: 9.659831737006508e-17
grad ChooseDest W: 3.142238140106201
grad AddEdge W: 6.143338722277818e-17
grad ChooseDest W: 6.268401145935059
grad AddEdge W: 3.699346940431482e-15
grad ChooseDest W: 3.4710497856140137
grad AddEdge W: 7.130511285421872e-17
grad ChooseDest W: 2.308135986328125
grad AddEdge W: 8.23644448521588e-17
grad ChooseDest W: 1.4024182558059692
grad AddEdge W: 1.294954557674215e-17
grad ChooseDest W: 1.6799273490905762
grad AddEdge W: 9.06180665038005e-15
grad ChooseDest W: 3.2929890155792236
grad AddEdge W: 4.775668741892206e-15
grad ChooseDest W: 2.209256649017334
grad AddEdge W: 2.131717561315302e-17
grad ChooseDest W: 1.8650805950164795
grad AddEdge W: 1.1808930162936562e-14
grad ChooseDest W: 3.1780223846435547
grad AddEdge W: 3.297541284436109e-17
grad ChooseDest W: 2.2417330741882324
grad AddEdge W: 1.985385936057772e-15
grad ChooseDest W: 2.058831214904785
grad AddEdge W: 2.0432706853654734e-15
grad ChooseDest W: 2.4082248210906982
grad AddEdge W: 3.5377161531410857e-17
grad ChooseDest W: 2.14957332611084
grad AddEdge W: 1.5743336515111426e-17
grad ChooseDest W: 1.2720961570739746
grad AddEdge W: 2.2776965826387576e-15
grad ChooseDest W: 1.5450847148895264
grad AddEdge W: 1.4178260040358711e-14
grad ChooseDest W: 1.6907979249954224
grad AddEdge W: 5.8290825372944374e-15
grad ChooseDest W: 1.8724161386489868
grad AddEdge W: 3.632374586390926e-15
grad ChooseDest W: 2.2652223110198975
grad AddEdge W: 3.890492140782701e-17
grad ChooseDest W: 3.6238136291503906
grad AddEdge W: 5.124256839823918e-17
grad ChooseDest W: 2.384080410003662
grad AddEdge W: 1.3823931070747864e-16
grad ChooseDest W: 1.9688612222671509
grad AddEdge W: 2.7506180660944176e-17
grad ChooseDest W: 1.8524647951126099
grad AddEdge W: 9.094096181604095e-16
grad ChooseDest W: 2.342275857925415
grad AddEdge W: 1.0148215714270067e-16
grad ChooseDest W: 2.662208080291748
grad AddEdge W: 3.251056711860834e-17
grad ChooseDest W: 3.0998098850250244
grad AddEdge W: 7.089957597838603e-17
grad ChooseDest W: 2.8040802478790283
grad AddEdge W: 4.453398473792387e-17
grad ChooseDest W: 3.1736247539520264
grad AddEdge W: 4.02609709961488e-15
grad ChooseDest W: 2.3130428791046143
grad AddEdge W: 4.008904871884459e-15
grad ChooseDest W: 1.7425422668457031
grad AddEdge W: 1.0006801578492491e-16
grad ChooseDest W: 2.9049508571624756
grad AddEdge W: 1.9618315760978888e-13
grad ChooseDest W: 2.2726714611053467
grad AddEdge W: 1.6024656340101525e-16
grad ChooseDest W: 1.9267879724502563
grad AddEdge W: 1.2117005363215371e-15
grad ChooseDest W: 1.8632361888885498
grad AddEdge W: 1.082604588937644e-16
grad ChooseDest W: 1.8781194686889648
grad AddEdge W: 1.0739535370448704e-16
grad ChooseDest W: 2.747380018234253
grad AddEdge W: 1.3660762912040757e-17
grad ChooseDest W: 4.005878925323486
grad AddEdge W: 5.216284984309628e-17
grad ChooseDest W: 4.267683982849121
grad AddEdge W: 1.8068133913264365e-15
grad ChooseDest W: 1.7906173467636108
grad AddEdge W: 9.309920022404494e-12
grad ChooseDest W: 1.4242504835128784
grad AddEdge W: 6.413686492750769e-17
grad ChooseDest W: 2.1407063007354736
grad AddEdge W: 1.3248647468796425e-17
grad ChooseDest W: 1.6862242221832275
grad AddEdge W: 1.0755829769724815e-15
grad ChooseDest W: 3.2693278789520264
grad AddEdge W: 9.63919894085368e-16
grad ChooseDest W: 1.6364331245422363
grad AddEdge W: 1.5852487629149228e-16
grad ChooseDest W: 2.0541939735412598
=== Epoch 27: Train Loss: 5.9851, Train Log Prob: 0.0072 ===
Total mismatches: 91114
Predicted valid destination but wrong order: 20106
Epoch 27: Validation Loss: 6.2620, Validation Log Prob: 0.0039
Epoch 27: Edge Precision: 0.3580, Recall: 0.3553, F1: 0.3565, Jaccard: 0.2343
Epoch 27: TP: 2.4866141732283467, FP: 4.483750894774516, FN: 4.534860415175376
Epoch 27: Current Learning Rate: 3.75e-06
[Epoch 27] ‚è±Ô∏è Total: 1877.60s | Current time: 2025-07-15 01:53:18 | üèãÔ∏è Train: 1643.26s | ‚úÖ Val: 234.35s
grad AddEdge W: 1.6436342939829085e-15
grad ChooseDest W: 3.85140323638916
grad AddEdge W: 6.025000940357002e-17
grad ChooseDest W: 3.3770601749420166
grad AddEdge W: 1.0042789229094468e-16
grad ChooseDest W: 2.8164939880371094
grad AddEdge W: 1.8163245125329183e-16
grad ChooseDest W: 2.0626494884490967
grad AddEdge W: 2.215275541764591e-17
grad ChooseDest W: 2.650052070617676
grad AddEdge W: 4.663917906391757e-14
grad ChooseDest W: 1.2476108074188232
grad AddEdge W: 3.4452211743874308e-15
grad ChooseDest W: 3.0293538570404053
grad AddEdge W: 6.3768722968109495e-15
grad ChooseDest W: 3.180957555770874
grad AddEdge W: 5.647252039150166e-17
grad ChooseDest W: 2.501321315765381
grad AddEdge W: 1.9319292897098593e-16
grad ChooseDest W: 3.7116308212280273
grad AddEdge W: 1.316779800854488e-17
grad ChooseDest W: 2.077301263809204
grad AddEdge W: 4.570079826536566e-18
grad ChooseDest W: 3.7314562797546387
grad AddEdge W: 2.7235935725564977e-15
grad ChooseDest W: 2.61275577545166
grad AddEdge W: 1.573562328750994e-15
grad ChooseDest W: 2.2830076217651367
grad AddEdge W: 1.502310605442232e-16
grad ChooseDest W: 1.344780445098877
grad AddEdge W: 9.701066339129483e-12
grad ChooseDest W: 1.8907310962677002
grad AddEdge W: 1.6221647616270905e-15
grad ChooseDest W: 2.657371759414673
grad AddEdge W: 8.484437653312471e-15
grad ChooseDest W: 3.072155714035034
grad AddEdge W: 9.732051963081117e-15
grad ChooseDest W: 1.3666867017745972
grad AddEdge W: 3.5802686399565286e-17
grad ChooseDest W: 2.276515007019043
grad AddEdge W: 5.905558045137815e-17
grad ChooseDest W: 2.4528050422668457
grad AddEdge W: 5.18098586561465e-15
grad ChooseDest W: 2.8965814113616943
grad AddEdge W: 9.459304627677933e-17
grad ChooseDest W: 2.4752309322357178
grad AddEdge W: 9.9954668879948e-18
grad ChooseDest W: 1.8891404867172241
grad AddEdge W: 3.0339815962977844e-15
grad ChooseDest W: 3.8580822944641113
grad AddEdge W: 3.221948887849583e-17
grad ChooseDest W: 0.9894574284553528
grad AddEdge W: 1.5256023915412135e-14
grad ChooseDest W: 2.5390572547912598
grad AddEdge W: 1.0089280749986889e-17
grad ChooseDest W: 1.4472442865371704
grad AddEdge W: 1.2800973880211378e-15
grad ChooseDest W: 2.4899234771728516
grad AddEdge W: 5.413320729708739e-17
grad ChooseDest W: 2.0303964614868164
grad AddEdge W: 6.788930194754427e-18
grad ChooseDest W: 1.9588916301727295
grad AddEdge W: 8.733711058769278e-17
grad ChooseDest W: 3.4408316612243652
grad AddEdge W: 5.015960368795006e-17
grad ChooseDest W: 4.518805027008057
grad AddEdge W: 4.2483104165623453e-13
grad ChooseDest W: 2.11358380317688
grad AddEdge W: 3.280201495625343e-14
grad ChooseDest W: 1.2790135145187378
grad AddEdge W: 4.9907261720353365e-15
grad ChooseDest W: 1.7181980609893799
grad AddEdge W: 1.2421970258511862e-14
grad ChooseDest W: 2.67829966545105
grad AddEdge W: 1.0004297868214416e-17
grad ChooseDest W: 1.4344772100448608
grad AddEdge W: 1.4447505344511251e-15
grad ChooseDest W: 2.2780039310455322
grad AddEdge W: 2.8695299113603456e-17
grad ChooseDest W: 2.6353535652160645
grad AddEdge W: 1.1620062779764298e-15
grad ChooseDest W: 2.430941343307495
grad AddEdge W: 1.8998597488240846e-15
grad ChooseDest W: 3.67164945602417
grad AddEdge W: 2.0257896392523738e-17
grad ChooseDest W: 3.255265235900879
grad AddEdge W: 1.5932269530102212e-17
grad ChooseDest W: 1.702891230583191
grad AddEdge W: 2.432357599027341e-15
grad ChooseDest W: 2.333705186843872
grad AddEdge W: 1.4159267020582428e-14
grad ChooseDest W: 1.7753702402114868
grad AddEdge W: 2.2395890269454848e-17
grad ChooseDest W: 1.7252745628356934
grad AddEdge W: 2.0762566894303976e-15
grad ChooseDest W: 3.0380616188049316
grad AddEdge W: 3.2820849597412187e-18
grad ChooseDest W: 2.428068161010742
grad AddEdge W: 1.2285280215004746e-12
grad ChooseDest W: 2.7122890949249268
grad AddEdge W: 2.5848558689863436e-17
grad ChooseDest W: 1.7209105491638184
grad AddEdge W: 4.4673318084326955e-15
grad ChooseDest W: 2.117772102355957
grad AddEdge W: 5.753876612724986e-18
grad ChooseDest W: 2.536646842956543
grad AddEdge W: 3.958715205144327e-15
grad ChooseDest W: 3.4911396503448486
grad AddEdge W: 1.3658917670617498e-15
grad ChooseDest W: 2.1004269123077393
grad AddEdge W: 9.327329615306323e-17
grad ChooseDest W: 3.99648380279541
grad AddEdge W: 2.0040497776549287e-16
grad ChooseDest W: 2.769035577774048
grad AddEdge W: 2.4405625954291568e-15
grad ChooseDest W: 1.7080256938934326
grad AddEdge W: 3.473654654293549e-17
grad ChooseDest W: 1.2908978462219238
grad AddEdge W: 1.2881100020648083e-15
grad ChooseDest W: 4.181601047515869
grad AddEdge W: 1.115892057322452e-16
grad ChooseDest W: 2.2637038230895996
grad AddEdge W: 6.360018352863839e-17
grad ChooseDest W: 2.36702299118042
grad AddEdge W: 1.6085689637929113e-13
grad ChooseDest W: 1.428317666053772
grad AddEdge W: 1.3281190654624436e-13
grad ChooseDest W: 1.4758896827697754
grad AddEdge W: 1.180231991781619e-14
grad ChooseDest W: 1.9453513622283936
grad AddEdge W: 5.719114182046323e-17
grad ChooseDest W: 2.360755681991577
=== Epoch 28: Train Loss: 5.9901, Train Log Prob: 0.0072 ===
Total mismatches: 91128
Predicted valid destination but wrong order: 19956
Epoch 28: Validation Loss: 6.2614, Validation Log Prob: 0.0039
Epoch 28: Edge Precision: 0.3598, Recall: 0.3571, F1: 0.3584, Jaccard: 0.2356
Epoch 28: TP: 2.4993557623478884, FP: 4.470722977809592, FN: 4.522118826055834
Epoch 28: Current Learning Rate: 3.75e-06
[Epoch 28] ‚è±Ô∏è Total: 1882.69s | Current time: 2025-07-15 02:24:41 | üèãÔ∏è Train: 1648.26s | ‚úÖ Val: 234.42s
grad AddEdge W: 9.58414229056126e-13
grad ChooseDest W: 3.3537795543670654
grad AddEdge W: 1.545137815482716e-17
grad ChooseDest W: 3.2253565788269043
grad AddEdge W: 1.8314021225105184e-15
grad ChooseDest W: 1.7958077192306519
grad AddEdge W: 2.1511655039587097e-16
grad ChooseDest W: 1.3121999502182007
grad AddEdge W: 8.656241623936747e-18
grad ChooseDest W: 2.093682289123535
grad AddEdge W: 1.6957594640947417e-17
grad ChooseDest W: 2.5902578830718994
grad AddEdge W: 5.001315963230367e-17
grad ChooseDest W: 2.255152702331543
grad AddEdge W: 3.877121791884741e-15
grad ChooseDest W: 1.9816557168960571
grad AddEdge W: 3.023814315724756e-17
grad ChooseDest W: 1.7508100271224976
grad AddEdge W: 1.4561554095694307e-15
grad ChooseDest W: 2.1137607097625732
grad AddEdge W: 1.4860507492285015e-16
grad ChooseDest W: 1.9626129865646362
grad AddEdge W: 1.2588013882316661e-17
grad ChooseDest W: 3.734745740890503
grad AddEdge W: 9.113097248193377e-16
grad ChooseDest W: 2.0427820682525635
grad AddEdge W: 4.7000567841385543e-17
grad ChooseDest W: 2.713420867919922
grad AddEdge W: 3.300117866017428e-15
grad ChooseDest W: 1.708433747291565
grad AddEdge W: 1.3481732903343455e-17
grad ChooseDest W: 1.9881654977798462
grad AddEdge W: 8.030980100247098e-17
grad ChooseDest W: 2.8716928958892822
grad AddEdge W: 3.759808152206494e-15
grad ChooseDest W: 1.5621711015701294
grad AddEdge W: 7.80540736211588e-12
grad ChooseDest W: 0.8573533892631531
grad AddEdge W: 5.761524036454382e-15
grad ChooseDest W: 3.053647756576538
grad AddEdge W: 1.0605472057902642e-17
grad ChooseDest W: 2.459300994873047
grad AddEdge W: 1.7212282294237964e-15
grad ChooseDest W: 1.8320780992507935
grad AddEdge W: 1.6661785883290148e-13
grad ChooseDest W: 2.247969388961792
grad AddEdge W: 5.782158579612664e-17
grad ChooseDest W: 1.638147234916687
grad AddEdge W: 5.742432337339743e-15
grad ChooseDest W: 2.1961026191711426
grad AddEdge W: 8.990482876331213e-16
grad ChooseDest W: 2.3840088844299316
grad AddEdge W: 3.868813887597279e-18
grad ChooseDest W: 2.1559462547302246
grad AddEdge W: 7.952249049544301e-17
grad ChooseDest W: 2.8806538581848145
grad AddEdge W: 5.2353716806359215e-17
grad ChooseDest W: 2.8108954429626465
grad AddEdge W: 7.615256329734688e-17
grad ChooseDest W: 3.7162067890167236
grad AddEdge W: 3.042389483560247e-17
grad ChooseDest W: 2.5180647373199463
grad AddEdge W: 1.4850385117693082e-17
grad ChooseDest W: 1.883880615234375
grad AddEdge W: 3.5963989927735576e-17
grad ChooseDest W: 5.518954753875732
grad AddEdge W: 3.0450766691156317e-15
grad ChooseDest W: 3.1712114810943604
grad AddEdge W: 6.473386433664436e-17
grad ChooseDest W: 3.6247940063476562
grad AddEdge W: 8.49117574746832e-17
grad ChooseDest W: 2.0156702995300293
grad AddEdge W: 2.3872761696164175e-17
grad ChooseDest W: 2.3202524185180664
grad AddEdge W: 1.9956021791056186e-17
grad ChooseDest W: 2.432451009750366
grad AddEdge W: 1.039317764261601e-16
grad ChooseDest W: 2.8119142055511475
grad AddEdge W: 5.328218403055814e-16
grad ChooseDest W: 11.179231643676758
grad AddEdge W: 4.490241085040609e-15
grad ChooseDest W: 2.009453058242798
grad AddEdge W: 4.9465847308201834e-17
grad ChooseDest W: 3.0913965702056885
grad AddEdge W: 1.769616601192254e-16
grad ChooseDest W: 2.8093907833099365
grad AddEdge W: 8.3642719478217e-16
grad ChooseDest W: 2.413041114807129
grad AddEdge W: 3.827283299710584e-17
grad ChooseDest W: 2.1924991607666016
grad AddEdge W: 6.1239878575511e-16
grad ChooseDest W: 3.341113567352295
grad AddEdge W: 3.001314923653975e-15
grad ChooseDest W: 2.4173994064331055
grad AddEdge W: 1.0439471552839419e-12
grad ChooseDest W: 2.1731300354003906
grad AddEdge W: 6.6420684128987e-17
grad ChooseDest W: 1.9410984516143799
grad AddEdge W: 6.377894242061812e-15
grad ChooseDest W: 2.800105333328247
grad AddEdge W: 8.280683427864196e-17
grad ChooseDest W: 2.3947770595550537
grad AddEdge W: 2.4282147608823204e-15
grad ChooseDest W: 1.8766669034957886
grad AddEdge W: 1.241012587917305e-16
grad ChooseDest W: 2.499366521835327
grad AddEdge W: 3.624435783327675e-17
grad ChooseDest W: 3.9354898929595947
grad AddEdge W: 3.1054032026520302e-15
grad ChooseDest W: 2.0858969688415527
grad AddEdge W: 2.7484927413157483e-15
grad ChooseDest W: 2.9094278812408447
grad AddEdge W: 9.850274892628241e-15
grad ChooseDest W: 1.9404460191726685
grad AddEdge W: 1.50179636379902e-17
grad ChooseDest W: 1.766530156135559
grad AddEdge W: 1.7331572061782155e-15
grad ChooseDest W: 4.211083889007568
grad AddEdge W: 2.1136027955913133e-15
grad ChooseDest W: 2.597006320953369
grad AddEdge W: 2.8267488447631522e-15
grad ChooseDest W: 3.2404932975769043
grad AddEdge W: 1.080191894996729e-15
grad ChooseDest W: 1.3686044216156006
grad AddEdge W: 2.8236793561809803e-16
grad ChooseDest W: 0.9096227884292603
grad AddEdge W: 1.7424299934891635e-15
grad ChooseDest W: 2.8342299461364746
grad AddEdge W: 9.033177075729198e-18
grad ChooseDest W: 2.4706666469573975
grad AddEdge W: 6.0137367256475e-17
grad ChooseDest W: 2.701357364654541
=== Epoch 29: Train Loss: 5.9886, Train Log Prob: 0.0071 ===
Total mismatches: 91334
Predicted valid destination but wrong order: 19702
Epoch 29: Validation Loss: 6.2791, Validation Log Prob: 0.0038
Epoch 29: Edge Precision: 0.3591, Recall: 0.3563, F1: 0.3576, Jaccard: 0.2346
Epoch 29: TP: 2.495490336435218, FP: 4.478453829634932, FN: 4.525984251968504
Epoch 29: Current Learning Rate: 1.875e-06
[Epoch 29] ‚è±Ô∏è Total: 1877.75s | Current time: 2025-07-15 02:55:58 | üèãÔ∏è Train: 1644.37s | ‚úÖ Val: 233.38s
grad AddEdge W: 1.2054445627313537e-15
grad ChooseDest W: 4.588631629943848
grad AddEdge W: 3.5924956928990424e-17
grad ChooseDest W: 1.6435425281524658
grad AddEdge W: 2.998685230459885e-17
grad ChooseDest W: 2.412374496459961
grad AddEdge W: 3.9583210337301105e-18
grad ChooseDest W: 3.5169625282287598
grad AddEdge W: 2.953971287616617e-15
grad ChooseDest W: 1.8753432035446167
grad AddEdge W: 2.7464289455397632e-15
grad ChooseDest W: 2.2623183727264404
grad AddEdge W: 4.676306112646442e-17
grad ChooseDest W: 3.208117961883545
grad AddEdge W: 3.440465190267716e-17
grad ChooseDest W: 5.044606685638428
grad AddEdge W: 9.013409278886528e-17
grad ChooseDest W: 1.9304722547531128
grad AddEdge W: 1.0126510494996675e-16
grad ChooseDest W: 2.173557758331299
grad AddEdge W: 1.632726316184731e-17
grad ChooseDest W: 6.131523132324219
grad AddEdge W: 4.265486821717311e-15
grad ChooseDest W: 2.7600972652435303
grad AddEdge W: 8.684579097528733e-12
grad ChooseDest W: 1.5558280944824219
grad AddEdge W: 1.1869479042621615e-15
grad ChooseDest W: 2.346724510192871
grad AddEdge W: 4.250922196068392e-17
grad ChooseDest W: 1.8934446573257446
grad AddEdge W: 2.361665168926673e-17
grad ChooseDest W: 2.1916327476501465
grad AddEdge W: 4.358274688582259e-17
grad ChooseDest W: 2.049964427947998
grad AddEdge W: 6.523179999634587e-12
grad ChooseDest W: 1.133325219154358
grad AddEdge W: 1.9119583439261914e-15
grad ChooseDest W: 2.727341651916504
grad AddEdge W: 1.0635473071539328e-17
grad ChooseDest W: 1.6815433502197266
grad AddEdge W: 5.797880305207092e-17
grad ChooseDest W: 3.0875742435455322
grad AddEdge W: 6.0406908492763485e-15
grad ChooseDest W: 3.3893680572509766
grad AddEdge W: 1.489222981043703e-11
grad ChooseDest W: 1.571128487586975
grad AddEdge W: 1.3045827745682098e-17
grad ChooseDest W: 2.2546541690826416
grad AddEdge W: 6.425725610258111e-17
grad ChooseDest W: 2.384726047515869
grad AddEdge W: 5.086991466490788e-15
grad ChooseDest W: 2.525614023208618
grad AddEdge W: 2.6892705468366673e-17
grad ChooseDest W: 2.238429069519043
grad AddEdge W: 9.767108459776265e-17
grad ChooseDest W: 2.0307095050811768
grad AddEdge W: 1.15600806921759e-13
grad ChooseDest W: 1.175640344619751
grad AddEdge W: 2.2283522746323194e-17
grad ChooseDest W: 2.9276034832000732
grad AddEdge W: 3.439014540466425e-15
grad ChooseDest W: 1.824905276298523
grad AddEdge W: 1.2865220173907677e-11
grad ChooseDest W: 1.7155630588531494
grad AddEdge W: 4.823206721168982e-13
grad ChooseDest W: 1.7714943885803223
grad AddEdge W: 2.0963350760494688e-17
grad ChooseDest W: 5.281640529632568
grad AddEdge W: 1.885470988390839e-15
grad ChooseDest W: 1.9147745370864868
grad AddEdge W: 3.488398321531694e-17
grad ChooseDest W: 2.626783609390259
grad AddEdge W: 8.924193548483499e-17
grad ChooseDest W: 1.7148841619491577
grad AddEdge W: 2.739691010202592e-17
grad ChooseDest W: 1.575691819190979
grad AddEdge W: 2.0834932624757055e-17
grad ChooseDest W: 3.0175139904022217
grad AddEdge W: 2.3753321646446295e-15
grad ChooseDest W: 2.021649122238159
grad AddEdge W: 2.662515224487517e-17
grad ChooseDest W: 3.2058050632476807
grad AddEdge W: 1.4225109366496897e-17
grad ChooseDest W: 1.5888078212738037
grad AddEdge W: 3.482109101898331e-17
grad ChooseDest W: 2.791891098022461
grad AddEdge W: 1.0414669250768016e-15
grad ChooseDest W: 2.605004072189331
grad AddEdge W: 4.843754097225454e-15
grad ChooseDest W: 3.6866352558135986
grad AddEdge W: 7.708846323565795e-16
grad ChooseDest W: 2.9657607078552246
grad AddEdge W: 2.376148605144109e-17
grad ChooseDest W: 2.527830123901367
grad AddEdge W: 1.0957241366738781e-16
grad ChooseDest W: 2.77321457862854
grad AddEdge W: 5.013372868429601e-15
grad ChooseDest W: 2.2713232040405273
grad AddEdge W: 1.9868066220685545e-15
grad ChooseDest W: 1.587772011756897
grad AddEdge W: 2.344792179973139e-15
grad ChooseDest W: 1.4706642627716064
grad AddEdge W: 4.508081716492153e-15
grad ChooseDest W: 3.483968496322632
grad AddEdge W: 5.89188494813566e-15
grad ChooseDest W: 1.989669680595398
grad AddEdge W: 2.0978883558037208e-17
grad ChooseDest W: 2.222574472427368
grad AddEdge W: 3.25357101005075e-17
grad ChooseDest W: 1.3875832557678223
grad AddEdge W: 1.4579244908193305e-16
grad ChooseDest W: 2.8079137802124023
grad AddEdge W: 1.2518101404124292e-16
grad ChooseDest W: 1.6763062477111816
grad AddEdge W: 1.6434637822801593e-17
grad ChooseDest W: 2.699978828430176
grad AddEdge W: 1.1220982941967638e-16
grad ChooseDest W: 2.537268877029419
grad AddEdge W: 1.5918289118958882e-15
grad ChooseDest W: 1.8934860229492188
grad AddEdge W: 4.2109832691163715e-16
grad ChooseDest W: 3.121483087539673
grad AddEdge W: 5.367344707774062e-17
grad ChooseDest W: 2.0066616535186768
grad AddEdge W: 4.036467350457894e-17
grad ChooseDest W: 2.698741912841797
grad AddEdge W: 1.1203809613659853e-15
grad ChooseDest W: 2.5195698738098145
grad AddEdge W: 1.1037385903672308e-16
grad ChooseDest W: 2.953073263168335
grad AddEdge W: 1.0491821246768674e-15
grad ChooseDest W: 2.9085519313812256
=== Epoch 30: Train Loss: 5.9916, Train Log Prob: 0.0071 ===
Total mismatches: 91601
Predicted valid destination but wrong order: 19759
Epoch 30: Validation Loss: 6.2810, Validation Log Prob: 0.0037
Epoch 30: Edge Precision: 0.3584, Recall: 0.3558, F1: 0.3570, Jaccard: 0.2341
Epoch 30: TP: 2.4909090909090907, FP: 4.484323550465283, FN: 4.530565497494631
Epoch 30: Current Learning Rate: 1.875e-06
[Epoch 30] ‚è±Ô∏è Total: 1886.79s | Current time: 2025-07-15 03:27:25 | üèãÔ∏è Train: 1651.07s | ‚úÖ Val: 235.72s
grad AddEdge W: 1.7378105934321938e-15
grad ChooseDest W: 5.845142364501953
grad AddEdge W: 2.872956291980887e-16
grad ChooseDest W: 2.430119276046753
grad AddEdge W: 4.978171118818888e-17
grad ChooseDest W: 2.410907745361328
grad AddEdge W: 6.710293345285612e-12
grad ChooseDest W: 1.3429203033447266
grad AddEdge W: 4.422745688155124e-15
grad ChooseDest W: 2.070091485977173
grad AddEdge W: 2.2095514850129487e-16
grad ChooseDest W: 2.2846267223358154
grad AddEdge W: 4.752995019852968e-17
grad ChooseDest W: 2.652986764907837
grad AddEdge W: 4.0700283840148854e-17
grad ChooseDest W: 2.2700419425964355
grad AddEdge W: 5.113679357076403e-15
grad ChooseDest W: 2.156564950942993
grad AddEdge W: 1.382465766619793e-15
grad ChooseDest W: 3.8098301887512207
grad AddEdge W: 8.109433297673416e-15
grad ChooseDest W: 1.7249391078948975
grad AddEdge W: 6.544451836194582e-15
grad ChooseDest W: 1.4523338079452515
grad AddEdge W: 1.9525125873748328e-15
grad ChooseDest W: 2.08254075050354
grad AddEdge W: 2.52787941031334e-15
grad ChooseDest W: 1.8157917261123657
grad AddEdge W: 3.408701190047884e-16
grad ChooseDest W: 1.9517699480056763
grad AddEdge W: 2.6321409075491086e-15
grad ChooseDest W: 1.819191336631775
grad AddEdge W: 2.036047175156399e-17
grad ChooseDest W: 2.065434694290161
grad AddEdge W: 4.982129674358322e-17
grad ChooseDest W: 2.402953624725342
grad AddEdge W: 1.0249561055739451e-17
grad ChooseDest W: 1.673643946647644
grad AddEdge W: 1.699014022758444e-16
grad ChooseDest W: 2.890678882598877
grad AddEdge W: 5.070475723317758e-12
grad ChooseDest W: 1.155815839767456
grad AddEdge W: 8.232565736061945e-16
grad ChooseDest W: 3.5557761192321777
grad AddEdge W: 1.62128425764353e-17
grad ChooseDest W: 2.232483148574829
grad AddEdge W: 5.970895387362153e-17
grad ChooseDest W: 2.9970786571502686
grad AddEdge W: 2.3554266786259166e-15
grad ChooseDest W: 2.9719016551971436
grad AddEdge W: 9.705322483177792e-12
grad ChooseDest W: 1.9161019325256348
grad AddEdge W: 1.888952183689771e-13
grad ChooseDest W: 1.8000670671463013
grad AddEdge W: 1.8624359273902583e-15
grad ChooseDest W: 3.1597635746002197
grad AddEdge W: 1.7370831244794004e-16
grad ChooseDest W: 2.136775016784668
grad AddEdge W: 1.4513114134325405e-16
grad ChooseDest W: 3.4043145179748535
grad AddEdge W: 1.35817815564846e-16
grad ChooseDest W: 1.5120959281921387
grad AddEdge W: 6.593169359672387e-15
grad ChooseDest W: 1.8978253602981567
grad AddEdge W: 1.0528449135164767e-17
grad ChooseDest W: 3.8961782455444336
grad AddEdge W: 3.9965568785189454e-17
grad ChooseDest W: 1.9079997539520264
grad AddEdge W: 7.906963888857228e-17
grad ChooseDest W: 2.0709104537963867
grad AddEdge W: 2.7149068592531557e-17
grad ChooseDest W: 2.5751442909240723
grad AddEdge W: 4.856045815839535e-15
grad ChooseDest W: 2.7780067920684814
grad AddEdge W: 1.5915156817589715e-17
grad ChooseDest W: 2.9968128204345703
grad AddEdge W: 4.3810291037446126e-17
grad ChooseDest W: 2.8588156700134277
grad AddEdge W: 4.9798109216652134e-17
grad ChooseDest W: 2.623810052871704
grad AddEdge W: 1.0313017800855917e-11
grad ChooseDest W: 2.1892049312591553
grad AddEdge W: 2.8615021830725226e-15
grad ChooseDest W: 1.9010602235794067
grad AddEdge W: 5.320620941035416e-15
grad ChooseDest W: 1.7622095346450806
grad AddEdge W: 2.692837680510241e-17
grad ChooseDest W: 2.1046860218048096
grad AddEdge W: 3.5688965608951496e-16
grad ChooseDest W: 2.002143383026123
grad AddEdge W: 3.5899102571764467e-17
grad ChooseDest W: 2.4229001998901367
grad AddEdge W: 3.0863959950738805e-15
grad ChooseDest W: 3.2945404052734375
grad AddEdge W: 4.8598859455850304e-17
grad ChooseDest W: 3.4162135124206543
grad AddEdge W: 8.593360180951078e-18
grad ChooseDest W: 3.372849464416504
grad AddEdge W: 9.046824331609016e-16
grad ChooseDest W: 4.7138519287109375
grad AddEdge W: 7.381237008276085e-17
grad ChooseDest W: 1.5405710935592651
grad AddEdge W: 2.906835426114242e-17
grad ChooseDest W: 3.921098232269287
grad AddEdge W: 4.437907154394502e-15
grad ChooseDest W: 2.5465266704559326
grad AddEdge W: 2.2920911730409605e-17
grad ChooseDest W: 1.320780873298645
grad AddEdge W: 2.0114110233621606e-15
grad ChooseDest W: 2.229461669921875
grad AddEdge W: 1.2678480475241993e-16
grad ChooseDest W: 3.132533550262451
grad AddEdge W: 1.1777448912902435e-15
grad ChooseDest W: 2.060757875442505
grad AddEdge W: 3.110699911429448e-15
grad ChooseDest W: 2.320157527923584
grad AddEdge W: 2.242195949511672e-15
grad ChooseDest W: 2.2188398838043213
grad AddEdge W: 6.8995022131786935e-15
grad ChooseDest W: 2.027758836746216
grad AddEdge W: 1.255126298033149e-11
grad ChooseDest W: 1.7500081062316895
grad AddEdge W: 8.809844762348659e-17
grad ChooseDest W: 1.7247503995895386
grad AddEdge W: 2.3181796753198296e-15
grad ChooseDest W: 1.560053825378418
grad AddEdge W: 1.4453062145343036e-16
grad ChooseDest W: 2.1625702381134033
grad AddEdge W: 8.180931236254589e-18
grad ChooseDest W: 1.683029055595398
grad AddEdge W: 3.80409679451299e-12
grad ChooseDest W: 0.759968638420105
=== Epoch 31: Train Loss: 5.9912, Train Log Prob: 0.0072 ===
Total mismatches: 91339
Predicted valid destination but wrong order: 19836
Epoch 31: Validation Loss: 6.2670, Validation Log Prob: 0.0039
Epoch 31: Edge Precision: 0.3599, Recall: 0.3569, F1: 0.3583, Jaccard: 0.2352
Epoch 31: TP: 2.498353614889048, FP: 4.467716535433071, FN: 4.523120973514675
Epoch 31: Current Learning Rate: 1.875e-06
[Epoch 31] ‚è±Ô∏è Total: 1878.67s | Current time: 2025-07-15 03:58:44 | üèãÔ∏è Train: 1642.46s | ‚úÖ Val: 236.21s
grad AddEdge W: 2.3168676212845327e-15
grad ChooseDest W: 5.904949188232422
grad AddEdge W: 4.263187630191328e-17
grad ChooseDest W: 3.4871551990509033
grad AddEdge W: 2.953117174003319e-17
grad ChooseDest W: 2.8036019802093506
grad AddEdge W: 3.937480631787143e-17
grad ChooseDest W: 3.251821994781494
grad AddEdge W: 6.766765559368824e-17
grad ChooseDest W: 1.6539896726608276
grad AddEdge W: 3.705756240220008e-17
grad ChooseDest W: 3.8320562839508057
grad AddEdge W: 3.681704938206069e-15
grad ChooseDest W: 2.8570473194122314
grad AddEdge W: 9.672595332381548e-16
grad ChooseDest W: 2.733232259750366
grad AddEdge W: 5.719935406958465e-17
grad ChooseDest W: 2.352926254272461
grad AddEdge W: 6.586958993512457e-17
grad ChooseDest W: 3.2673206329345703
grad AddEdge W: 9.971382816393714e-15
grad ChooseDest W: 2.065136671066284
grad AddEdge W: 7.335726192717908e-17
grad ChooseDest W: 3.1507129669189453
grad AddEdge W: 1.034652134734557e-16
grad ChooseDest W: 1.7354400157928467
grad AddEdge W: 5.913240236922717e-17
grad ChooseDest W: 2.850041389465332
grad AddEdge W: 1.4543378886228597e-15
grad ChooseDest W: 3.497710943222046
grad AddEdge W: 3.969673693899429e-15
grad ChooseDest W: 2.1074934005737305
grad AddEdge W: 8.718885996958857e-17
grad ChooseDest W: 2.40761137008667
grad AddEdge W: 4.9507226191164186e-17
grad ChooseDest W: 2.656040668487549
grad AddEdge W: 3.689409496954735e-17
grad ChooseDest W: 1.5575735569000244
grad AddEdge W: 3.3783695225418587e-15
grad ChooseDest W: 1.368162989616394
grad AddEdge W: 2.503656597362472e-15
grad ChooseDest W: 1.635815978050232
grad AddEdge W: 3.583130631936403e-15
grad ChooseDest W: 3.713073968887329
grad AddEdge W: 3.08237076215963e-17
grad ChooseDest W: 2.654665470123291
grad AddEdge W: 1.1421484905005592e-16
grad ChooseDest W: 4.318098545074463
grad AddEdge W: 4.680037875114248e-15
grad ChooseDest W: 2.454169750213623
grad AddEdge W: 1.6444845231560497e-17
grad ChooseDest W: 2.922771692276001
grad AddEdge W: 3.0976166934657174e-17
grad ChooseDest W: 3.0942349433898926
grad AddEdge W: 4.367609587231042e-17
grad ChooseDest W: 1.7156987190246582
grad AddEdge W: 9.286122785911381e-16
grad ChooseDest W: 3.2024788856506348
grad AddEdge W: 8.955213483198728e-15
grad ChooseDest W: 2.748155355453491
grad AddEdge W: 3.786617446436252e-17
grad ChooseDest W: 3.516174554824829
grad AddEdge W: 2.3527231810688524e-17
grad ChooseDest W: 1.394761323928833
grad AddEdge W: 3.5457673733583052e-15
grad ChooseDest W: 1.5658825635910034
grad AddEdge W: 5.667948759820733e-17
grad ChooseDest W: 3.042832612991333
grad AddEdge W: 3.1217008841808114e-17
grad ChooseDest W: 2.2183713912963867
grad AddEdge W: 1.03361507788652e-14
grad ChooseDest W: 1.472385287284851
grad AddEdge W: 2.619750086080199e-15
grad ChooseDest W: 1.971678376197815
grad AddEdge W: 6.9015753263170984e-15
grad ChooseDest W: 3.1134910583496094
grad AddEdge W: 3.2349382704446257e-17
grad ChooseDest W: 2.0537657737731934
grad AddEdge W: 3.2292013030579825e-15
grad ChooseDest W: 2.1043078899383545
grad AddEdge W: 7.013653825925281e-17
grad ChooseDest W: 3.3197269439697266
grad AddEdge W: 4.1883924239570665e-15
grad ChooseDest W: 2.8005337715148926
grad AddEdge W: 9.523680587507138e-16
grad ChooseDest W: 3.077877998352051
grad AddEdge W: 1.634950406328539e-15
grad ChooseDest W: 1.8002586364746094
grad AddEdge W: 3.2895007580098485e-17
grad ChooseDest W: 3.029261589050293
grad AddEdge W: 7.204772913838923e-17
grad ChooseDest W: 2.9213879108428955
grad AddEdge W: 4.221172175499334e-15
grad ChooseDest W: 3.852734088897705
grad AddEdge W: 1.996473597337331e-15
grad ChooseDest W: 2.087113380432129
grad AddEdge W: 1.652668535640086e-15
grad ChooseDest W: 4.544128894805908
grad AddEdge W: 2.3544155330451318e-15
grad ChooseDest W: 1.9042108058929443
grad AddEdge W: 3.1881992571129294e-17
grad ChooseDest W: 2.8227107524871826
grad AddEdge W: 7.400729353974775e-17
grad ChooseDest W: 2.8283956050872803
grad AddEdge W: 8.619847331345638e-18
grad ChooseDest W: 2.464614152908325
grad AddEdge W: 6.721113262838675e-16
grad ChooseDest W: 2.7890331745147705
grad AddEdge W: 4.9954456278592007e-17
grad ChooseDest W: 2.5256597995758057
grad AddEdge W: 1.8644336016567984e-16
grad ChooseDest W: 1.4269709587097168
grad AddEdge W: 3.3769777547652913e-17
grad ChooseDest W: 2.8499722480773926
grad AddEdge W: 9.044704664745737e-18
grad ChooseDest W: 2.565436363220215
grad AddEdge W: 7.558769820064666e-17
grad ChooseDest W: 2.5076208114624023
grad AddEdge W: 1.6778438371401415e-15
grad ChooseDest W: 2.5959317684173584
grad AddEdge W: 2.325576105981597e-17
grad ChooseDest W: 2.137395143508911
grad AddEdge W: 3.698460096935707e-15
grad ChooseDest W: 3.7234044075012207
grad AddEdge W: 3.0540495008841333e-15
grad ChooseDest W: 2.3110227584838867
grad AddEdge W: 1.5654358099517415e-14
grad ChooseDest W: 2.6906533241271973
grad AddEdge W: 7.489777027747113e-15
grad ChooseDest W: 1.5530024766921997
grad AddEdge W: 2.18910477141072e-15
grad ChooseDest W: 1.724772572517395
=== Epoch 32: Train Loss: 5.9916, Train Log Prob: 0.0071 ===
Total mismatches: 91438
Predicted valid destination but wrong order: 19714
Epoch 32: Validation Loss: 6.2796, Validation Log Prob: 0.0038
Epoch 32: Edge Precision: 0.3589, Recall: 0.3564, F1: 0.3575, Jaccard: 0.2352
Epoch 32: TP: 2.495490336435218, FP: 4.479599141016464, FN: 4.525984251968504
Epoch 32: Current Learning Rate: 1.875e-06
[Epoch 32] ‚è±Ô∏è Total: 1882.41s | Current time: 2025-07-15 04:30:06 | üèãÔ∏è Train: 1645.66s | ‚úÖ Val: 236.75s
grad AddEdge W: 3.0909873371644725e-15
grad ChooseDest W: 6.36037015914917
grad AddEdge W: 2.475475336868871e-13
grad ChooseDest W: 1.7375024557113647
grad AddEdge W: 2.3520731825435082e-17
grad ChooseDest W: 2.269822597503662
grad AddEdge W: 3.7173834217822985e-17
grad ChooseDest W: 2.5347135066986084
grad AddEdge W: 2.3627939340319093e-13
grad ChooseDest W: 3.1328125
grad AddEdge W: 1.5514615450912348e-15
grad ChooseDest W: 2.8660120964050293
grad AddEdge W: 2.7057749507266928e-17
grad ChooseDest W: 2.1097657680511475
grad AddEdge W: 5.0654133021594844e-15
grad ChooseDest W: 1.1390787363052368
grad AddEdge W: 1.3507569188327389e-14
grad ChooseDest W: 1.7642085552215576
grad AddEdge W: 7.541860924855102e-17
grad ChooseDest W: 1.8419994115829468
grad AddEdge W: 1.8211972680853456e-17
grad ChooseDest W: 1.7130814790725708
grad AddEdge W: 1.1494744255218024e-15
grad ChooseDest W: 1.7472925186157227
grad AddEdge W: 2.4949143703138603e-15
grad ChooseDest W: 1.9301180839538574
grad AddEdge W: 3.378366981443017e-15
grad ChooseDest W: 1.8855023384094238
grad AddEdge W: 3.4794462420704e-17
grad ChooseDest W: 1.6427680253982544
grad AddEdge W: 3.2243262049300603e-17
grad ChooseDest W: 3.5246541500091553
grad AddEdge W: 8.9452734192138e-18
grad ChooseDest W: 3.240546941757202
grad AddEdge W: 6.133419781177013e-15
grad ChooseDest W: 3.3646788597106934
grad AddEdge W: 3.2679803772074157e-15
grad ChooseDest W: 2.3555941581726074
grad AddEdge W: 3.791447373885284e-15
grad ChooseDest W: 1.804688572883606
grad AddEdge W: 2.003271883771994e-14
grad ChooseDest W: 2.3413941860198975
grad AddEdge W: 3.438559035264149e-17
grad ChooseDest W: 2.7640492916107178
grad AddEdge W: 3.758631941080113e-17
grad ChooseDest W: 1.9085445404052734
grad AddEdge W: 1.822029941177166e-16
grad ChooseDest W: 3.593496799468994
grad AddEdge W: 5.1625533169516534e-17
grad ChooseDest W: 2.2713310718536377
grad AddEdge W: 4.707507577110179e-15
grad ChooseDest W: 2.00010347366333
grad AddEdge W: 4.6953673318098687e-17
grad ChooseDest W: 3.710659980773926
grad AddEdge W: 4.03119629089691e-16
grad ChooseDest W: 2.3249709606170654
grad AddEdge W: 3.064576214594373e-15
grad ChooseDest W: 4.244173049926758
grad AddEdge W: 3.439041857278974e-15
grad ChooseDest W: 1.904445767402649
grad AddEdge W: 2.3246122751318503e-17
grad ChooseDest W: 2.0427803993225098
grad AddEdge W: 1.703735470433223e-17
grad ChooseDest W: 3.7234854698181152
grad AddEdge W: 6.613215664918581e-15
grad ChooseDest W: 1.722888469696045
grad AddEdge W: 7.633952265939611e-18
grad ChooseDest W: 1.9225417375564575
grad AddEdge W: 2.0704775755371836e-17
grad ChooseDest W: 3.6551737785339355
grad AddEdge W: 3.211603346545827e-15
grad ChooseDest W: 2.4293572902679443
grad AddEdge W: 3.536571004301067e-17
grad ChooseDest W: 2.8507614135742188
grad AddEdge W: 4.537208214932913e-15
grad ChooseDest W: 1.7785680294036865
grad AddEdge W: 1.826818996639515e-14
grad ChooseDest W: 2.8991360664367676
grad AddEdge W: 1.155545838573713e-17
grad ChooseDest W: 3.578953981399536
grad AddEdge W: 8.352743266926753e-18
grad ChooseDest W: 2.0483736991882324
grad AddEdge W: 4.2738188862961047e-17
grad ChooseDest W: 2.084014892578125
grad AddEdge W: 5.954719122712405e-15
grad ChooseDest W: 1.4957592487335205
grad AddEdge W: 1.305063234155205e-16
grad ChooseDest W: 2.414252281188965
grad AddEdge W: 3.2355193218115523e-15
grad ChooseDest W: 2.7910566329956055
grad AddEdge W: 1.0212801149317595e-17
grad ChooseDest W: 2.0739548206329346
grad AddEdge W: 4.13741217107468e-17
grad ChooseDest W: 2.3498623371124268
grad AddEdge W: 9.535073875479423e-18
grad ChooseDest W: 2.362311840057373
grad AddEdge W: 1.8359761732175856e-17
grad ChooseDest W: 1.7072134017944336
grad AddEdge W: 4.97445211478485e-17
grad ChooseDest W: 2.0985188484191895
grad AddEdge W: 9.448480473054309e-17
grad ChooseDest W: 2.7359533309936523
grad AddEdge W: 2.0134396672708346e-17
grad ChooseDest W: 3.0115225315093994
grad AddEdge W: 1.1246996647266798e-15
grad ChooseDest W: 2.0551023483276367
grad AddEdge W: 2.2029679479001942e-15
grad ChooseDest W: 2.5313093662261963
grad AddEdge W: 4.530162171361178e-15
grad ChooseDest W: 2.680267810821533
grad AddEdge W: 6.428541994807731e-17
grad ChooseDest W: 2.770845413208008
grad AddEdge W: 8.580243577977825e-18
grad ChooseDest W: 2.8233792781829834
grad AddEdge W: 1.511574022451201e-15
grad ChooseDest W: 1.803483247756958
grad AddEdge W: 1.0485306703136452e-17
grad ChooseDest W: 1.8443117141723633
grad AddEdge W: 8.827743627315326e-17
grad ChooseDest W: 2.9529051780700684
grad AddEdge W: 1.225817213890257e-16
grad ChooseDest W: 2.1329498291015625
grad AddEdge W: 1.4897774294986245e-15
grad ChooseDest W: 2.8323490619659424
grad AddEdge W: 6.774725683839544e-17
grad ChooseDest W: 2.6112868785858154
grad AddEdge W: 9.339042397488867e-14
grad ChooseDest W: 3.2800323963165283
grad AddEdge W: 5.077673667319627e-17
grad ChooseDest W: 5.050863265991211
grad AddEdge W: 1.3270961493000655e-17
grad ChooseDest W: 3.359875202178955
=== Epoch 33: Train Loss: 5.9887, Train Log Prob: 0.0072 ===
Total mismatches: 91558
Predicted valid destination but wrong order: 20077
Epoch 33: Validation Loss: 6.2522, Validation Log Prob: 0.0039
Epoch 33: Edge Precision: 0.3610, Recall: 0.3584, F1: 0.3596, Jaccard: 0.2368
Epoch 33: TP: 2.5092340730136007, FP: 4.464996420901933, FN: 4.512240515390122
Epoch 33: Current Learning Rate: 1e-06
[Epoch 33] ‚è±Ô∏è Total: 1881.99s | Current time: 2025-07-15 05:01:28 | üèãÔ∏è Train: 1645.04s | ‚úÖ Val: 236.95s
grad AddEdge W: 1.4205421210603604e-15
grad ChooseDest W: 4.5787272453308105
grad AddEdge W: 6.566929311287853e-17
grad ChooseDest W: 2.059274911880493
grad AddEdge W: 1.2084071536379998e-13
grad ChooseDest W: 2.477848768234253
grad AddEdge W: 1.1790344566007908e-14
grad ChooseDest W: 2.3594887256622314
grad AddEdge W: 1.9922248800739034e-15
grad ChooseDest W: 2.1605846881866455
grad AddEdge W: 5.085952157064507e-16
grad ChooseDest W: 1.5859088897705078
grad AddEdge W: 4.7865405024144435e-17
grad ChooseDest W: 1.9425331354141235
grad AddEdge W: 3.1721512914849106e-17
grad ChooseDest W: 3.0033278465270996
grad AddEdge W: 5.871644750271576e-12
grad ChooseDest W: 1.4048540592193604
grad AddEdge W: 9.368454057512254e-18
grad ChooseDest W: 3.201148271560669
grad AddEdge W: 5.779514248630455e-17
grad ChooseDest W: 1.7656461000442505
grad AddEdge W: 1.1013671628127148e-17
grad ChooseDest W: 1.943934679031372
grad AddEdge W: 4.1466903251334424e-18
grad ChooseDest W: 1.9905428886413574
grad AddEdge W: 1.626988991456063e-17
grad ChooseDest W: 1.6499433517456055
grad AddEdge W: 3.0858396267764325e-17
grad ChooseDest W: 2.3500540256500244
grad AddEdge W: 6.527041338661566e-17
grad ChooseDest W: 2.237037181854248
grad AddEdge W: 1.7523938029839758e-13
grad ChooseDest W: 2.3424770832061768
grad AddEdge W: 4.310103858697458e-15
grad ChooseDest W: 1.609761357307434
grad AddEdge W: 9.818874745998814e-16
grad ChooseDest W: 2.7066633701324463
grad AddEdge W: 1.3215964735613842e-17
grad ChooseDest W: 3.066347122192383
grad AddEdge W: 1.8925858872708562e-13
grad ChooseDest W: 1.5277479887008667
grad AddEdge W: 1.1733781188097924e-15
grad ChooseDest W: 1.9349976778030396
grad AddEdge W: 3.8569761058510323e-17
grad ChooseDest W: 2.673448324203491
grad AddEdge W: 2.891452513698716e-17
grad ChooseDest W: 2.2187111377716064
grad AddEdge W: 5.998539525205659e-15
grad ChooseDest W: 1.9682838916778564
grad AddEdge W: 2.2755859419703224e-17
grad ChooseDest W: 2.564594268798828
grad AddEdge W: 1.1068176757056403e-11
grad ChooseDest W: 1.0121076107025146
grad AddEdge W: 3.1846738133422284e-17
grad ChooseDest W: 4.040200233459473
grad AddEdge W: 7.179983197618325e-15
grad ChooseDest W: 4.47738790512085
grad AddEdge W: 6.313499038702836e-17
grad ChooseDest W: 2.103506565093994
grad AddEdge W: 8.28043461193594e-17
grad ChooseDest W: 2.851442575454712
grad AddEdge W: 1.5043133619064555e-15
grad ChooseDest W: 2.0790748596191406
grad AddEdge W: 1.4502236378398088e-17
grad ChooseDest W: 2.568328857421875
grad AddEdge W: 8.595608821815467e-16
grad ChooseDest W: 1.540130615234375
grad AddEdge W: 2.4698102659034885e-17
grad ChooseDest W: 1.7580362558364868
grad AddEdge W: 5.0048199003051416e-17
grad ChooseDest W: 1.9191009998321533
grad AddEdge W: 1.3665190809859753e-17
grad ChooseDest W: 2.1511576175689697
grad AddEdge W: 3.729103247573195e-17
grad ChooseDest W: 1.652085304260254
grad AddEdge W: 5.921259521350847e-16
grad ChooseDest W: 2.569610357284546
grad AddEdge W: 7.556771616402534e-16
grad ChooseDest W: 1.1679459810256958
grad AddEdge W: 3.358753721791107e-15
grad ChooseDest W: 2.256490468978882
grad AddEdge W: 4.688256225519873e-17
grad ChooseDest W: 2.0366108417510986
grad AddEdge W: 2.91031520951513e-15
grad ChooseDest W: 2.0526108741760254
grad AddEdge W: 3.769489315277137e-15
grad ChooseDest W: 2.5978574752807617
grad AddEdge W: 4.476572090854293e-15
grad ChooseDest W: 2.823251247406006
grad AddEdge W: 2.142343682282836e-15
grad ChooseDest W: 2.382906436920166
grad AddEdge W: 4.9583154753951654e-17
grad ChooseDest W: 4.190065383911133
grad AddEdge W: 9.622068758286645e-16
grad ChooseDest W: 3.4785032272338867
grad AddEdge W: 2.943726688817372e-16
grad ChooseDest W: 2.343085289001465
grad AddEdge W: 2.59352234614318e-15
grad ChooseDest W: 2.969846725463867
grad AddEdge W: 1.7593261832045187e-15
grad ChooseDest W: 1.8740687370300293
grad AddEdge W: 2.7638719810493547e-17
grad ChooseDest W: 2.3134849071502686
grad AddEdge W: 6.312997436379384e-17
grad ChooseDest W: 2.0989339351654053
grad AddEdge W: 2.616351631077137e-16
grad ChooseDest W: 1.4360593557357788
grad AddEdge W: 3.309078627566431e-15
grad ChooseDest W: 1.7420727014541626
grad AddEdge W: 4.830283798439718e-17
grad ChooseDest W: 1.3607449531555176
grad AddEdge W: 2.367987118442514e-15
grad ChooseDest W: 3.0006651878356934
grad AddEdge W: 8.69029168667188e-18
grad ChooseDest W: 2.1693716049194336
grad AddEdge W: 7.573695969963386e-15
grad ChooseDest W: 1.9035208225250244
grad AddEdge W: 4.3785449678685525e-15
grad ChooseDest W: 1.8735084533691406
grad AddEdge W: 1.9762680234272737e-16
grad ChooseDest W: 1.4295341968536377
grad AddEdge W: 1.1254209132812668e-15
grad ChooseDest W: 2.591620922088623
grad AddEdge W: 9.978898460325363e-18
grad ChooseDest W: 1.8178285360336304
grad AddEdge W: 2.0989519446353415e-17
grad ChooseDest W: 1.6444357633590698
grad AddEdge W: 8.491083897333102e-16
grad ChooseDest W: 3.8342130184173584
grad AddEdge W: 5.921263888864482e-17
grad ChooseDest W: 1.975524663925171
=== Epoch 34: Train Loss: 5.9938, Train Log Prob: 0.0071 ===
Total mismatches: 91529
Predicted valid destination but wrong order: 20076
Epoch 34: Validation Loss: 6.2604, Validation Log Prob: 0.0039
Epoch 34: Edge Precision: 0.3587, Recall: 0.3560, F1: 0.3572, Jaccard: 0.2351
Epoch 34: TP: 2.4920544022906226, FP: 4.479312813171081, FN: 4.529420186113099
Epoch 34: Current Learning Rate: 1e-06
[Epoch 34] ‚è±Ô∏è Total: 1882.18s | Current time: 2025-07-15 05:32:50 | üèãÔ∏è Train: 1648.67s | ‚úÖ Val: 233.51s
grad AddEdge W: 9.575687465179783e-12
grad ChooseDest W: 3.726529121398926
grad AddEdge W: 1.633897934804351e-17
grad ChooseDest W: 2.556504487991333
grad AddEdge W: 5.978055793616657e-18
grad ChooseDest W: 2.091146945953369
grad AddEdge W: 6.121880995443703e-17
grad ChooseDest W: 2.209083080291748
grad AddEdge W: 2.3025906692003247e-15
grad ChooseDest W: 1.2524945735931396
grad AddEdge W: 4.111575853003521e-15
grad ChooseDest W: 2.1123428344726562
grad AddEdge W: 4.971198250952862e-15
grad ChooseDest W: 4.762247085571289
grad AddEdge W: 1.2161014018781152e-15
grad ChooseDest W: 1.9194886684417725
grad AddEdge W: 9.281787863193236e-18
grad ChooseDest W: 2.175116777420044
grad AddEdge W: 1.245692974346477e-16
grad ChooseDest W: 2.5159547328948975
grad AddEdge W: 8.777386856856568e-17
grad ChooseDest W: 3.3441102504730225
grad AddEdge W: 2.1052855011495375e-17
grad ChooseDest W: 2.0247209072113037
grad AddEdge W: 2.243709173871942e-15
grad ChooseDest W: 4.066235542297363
grad AddEdge W: 2.694820597874653e-17
grad ChooseDest W: 2.396897792816162
grad AddEdge W: 3.461864683586708e-17
grad ChooseDest W: 3.4469494819641113
grad AddEdge W: 2.4229643808366503e-17
grad ChooseDest W: 3.2252206802368164
grad AddEdge W: 1.2737149583676297e-17
grad ChooseDest W: 2.535979986190796
grad AddEdge W: 3.9169725744706875e-16
grad ChooseDest W: 3.022043228149414
grad AddEdge W: 2.228581569098119e-17
grad ChooseDest W: 2.1614086627960205
grad AddEdge W: 3.8531422891479716e-15
grad ChooseDest W: 4.01069450378418
grad AddEdge W: 9.439747100146974e-18
grad ChooseDest W: 2.970137596130371
grad AddEdge W: 1.569129551228128e-14
grad ChooseDest W: 2.407139301300049
grad AddEdge W: 2.493273773374147e-16
grad ChooseDest W: 2.040485382080078
grad AddEdge W: 5.23431378942436e-15
grad ChooseDest W: 3.268739938735962
grad AddEdge W: 5.4374433016042556e-14
grad ChooseDest W: 2.0161538124084473
grad AddEdge W: 7.507256399646653e-15
grad ChooseDest W: 1.8850675821304321
grad AddEdge W: 4.1176379222232325e-17
grad ChooseDest W: 3.191457986831665
grad AddEdge W: 1.629429075001421e-16
grad ChooseDest W: 2.042980432510376
grad AddEdge W: 1.6851978830673218e-15
grad ChooseDest W: 2.9817698001861572
grad AddEdge W: 2.726708561631014e-13
grad ChooseDest W: 1.6238809823989868
grad AddEdge W: 2.7720230188054523e-17
grad ChooseDest W: 2.736889123916626
grad AddEdge W: 3.860902381494242e-15
grad ChooseDest W: 1.9454551935195923
grad AddEdge W: 1.5754164309329745e-17
grad ChooseDest W: 3.2429418563842773
grad AddEdge W: 4.439815506389776e-17
grad ChooseDest W: 2.732849359512329
grad AddEdge W: 6.690349290892195e-17
grad ChooseDest W: 4.363229274749756
grad AddEdge W: 7.052719911894935e-17
grad ChooseDest W: 2.2316267490386963
grad AddEdge W: 3.4278500241817926e-17
grad ChooseDest W: 1.9628554582595825
grad AddEdge W: 1.0625929723376181e-16
grad ChooseDest W: 2.2184271812438965
grad AddEdge W: 3.611675033453942e-17
grad ChooseDest W: 2.7385036945343018
grad AddEdge W: 7.425482568369302e-17
grad ChooseDest W: 3.878312826156616
grad AddEdge W: 1.4680010726925534e-17
grad ChooseDest W: 3.2151429653167725
grad AddEdge W: 1.0271844884542601e-14
grad ChooseDest W: 2.352085590362549
grad AddEdge W: 5.1489528133200565e-17
grad ChooseDest W: 2.1510021686553955
grad AddEdge W: 1.9014236958995162e-17
grad ChooseDest W: 2.4596707820892334
grad AddEdge W: 9.847639773129333e-15
grad ChooseDest W: 1.436682939529419
grad AddEdge W: 1.6888283523060374e-17
grad ChooseDest W: 1.6453760862350464
grad AddEdge W: 1.2479932511334237e-15
grad ChooseDest W: 2.9429829120635986
grad AddEdge W: 2.693873145201035e-16
grad ChooseDest W: 2.1831531524658203
grad AddEdge W: 8.48313588278655e-18
grad ChooseDest W: 3.6393017768859863
grad AddEdge W: 3.4777559216497052e-15
grad ChooseDest W: 1.8442959785461426
grad AddEdge W: 3.7638216722432705e-17
grad ChooseDest W: 2.3798561096191406
grad AddEdge W: 3.422600947772768e-15
grad ChooseDest W: 1.756426215171814
grad AddEdge W: 7.791319909936697e-15
grad ChooseDest W: 1.4631829261779785
grad AddEdge W: 1.3175293078759864e-09
grad ChooseDest W: 0.8016452193260193
grad AddEdge W: 4.145384537818466e-17
grad ChooseDest W: 2.0116143226623535
grad AddEdge W: 5.123044788615957e-16
grad ChooseDest W: 1.434425711631775
grad AddEdge W: 2.4716330411013103e-17
grad ChooseDest W: 2.026048421859741
grad AddEdge W: 3.923455949937429e-17
grad ChooseDest W: 1.6628038883209229
grad AddEdge W: 7.772294364095239e-14
grad ChooseDest W: 1.3332051038742065
grad AddEdge W: 1.0403649087426442e-16
grad ChooseDest W: 2.549443006515503
grad AddEdge W: 9.213153015087791e-16
grad ChooseDest W: 2.4237828254699707
grad AddEdge W: 3.8213510912295985e-17
grad ChooseDest W: 3.2390482425689697
grad AddEdge W: 2.7708311177172124e-16
grad ChooseDest W: 1.9579123258590698
grad AddEdge W: 3.2228789697303376e-17
grad ChooseDest W: 1.6690139770507812
grad AddEdge W: 2.0823668079175308e-17
grad ChooseDest W: 1.3338971138000488
grad AddEdge W: 5.105956957696285e-17
grad ChooseDest W: 1.9356460571289062
=== Epoch 35: Train Loss: 5.9836, Train Log Prob: 0.0072 ===
Total mismatches: 91420
Predicted valid destination but wrong order: 19918
Epoch 35: Validation Loss: 6.2541, Validation Log Prob: 0.0039
Epoch 35: Edge Precision: 0.3606, Recall: 0.3578, F1: 0.3591, Jaccard: 0.2362
Epoch 35: TP: 2.5047959914101647, FP: 4.463851109520401, FN: 4.5166785969935574
Epoch 35: Current Learning Rate: 1e-06
[Epoch 35] ‚è±Ô∏è Total: 1890.57s | Current time: 2025-07-15 06:04:21 | üèãÔ∏è Train: 1656.75s | ‚úÖ Val: 233.82s
grad AddEdge W: 4.704496798499163e-15
grad ChooseDest W: 4.9590582847595215
grad AddEdge W: 1.3300421563603956e-15
grad ChooseDest W: 3.5897319316864014
grad AddEdge W: 7.741050516220404e-16
grad ChooseDest W: 4.2625813484191895
grad AddEdge W: 5.0121014719757724e-15
grad ChooseDest W: 3.8248417377471924
grad AddEdge W: 7.2000559991139e-17
grad ChooseDest W: 2.581096887588501
grad AddEdge W: 1.0767192801415526e-13
grad ChooseDest W: 3.2314939498901367
grad AddEdge W: 1.380402728541249e-17
grad ChooseDest W: 3.1429057121276855
grad AddEdge W: 1.621177385908388e-17
grad ChooseDest W: 4.440179824829102
grad AddEdge W: 1.3214779385796054e-16
grad ChooseDest W: 1.9746092557907104
grad AddEdge W: 1.087286642962931e-15
grad ChooseDest W: 1.8865904808044434
grad AddEdge W: 7.462940482879255e-15
grad ChooseDest W: 2.779636859893799
grad AddEdge W: 9.522515784855765e-17
grad ChooseDest W: 2.8354945182800293
grad AddEdge W: 9.204295432739675e-17
grad ChooseDest W: 1.4538708925247192
grad AddEdge W: 3.582303504263409e-17
grad ChooseDest W: 2.2299435138702393
grad AddEdge W: 5.000050840114304e-16
grad ChooseDest W: 1.8700095415115356
grad AddEdge W: 3.7940653806716793e-17
grad ChooseDest W: 2.4556596279144287
grad AddEdge W: 1.183661187727919e-13
grad ChooseDest W: 2.21297025680542
grad AddEdge W: 1.3584841463006557e-14
grad ChooseDest W: 1.913355827331543
grad AddEdge W: 2.480606461822408e-17
grad ChooseDest W: 2.479410171508789
grad AddEdge W: 4.4239697037034653e-17
grad ChooseDest W: 3.007153034210205
grad AddEdge W: 1.3820082364793777e-16
grad ChooseDest W: 1.974657654762268
grad AddEdge W: 3.3611813182179377e-16
grad ChooseDest W: 2.05320405960083
grad AddEdge W: 8.644133857056008e-15
grad ChooseDest W: 5.376460552215576
grad AddEdge W: 1.9002030089259594e-15
grad ChooseDest W: 2.011094331741333
grad AddEdge W: 1.0325060311789004e-17
grad ChooseDest W: 2.2984280586242676
grad AddEdge W: 8.391369061199958e-16
grad ChooseDest W: 3.133134603500366
grad AddEdge W: 7.171297907065637e-17
grad ChooseDest W: 3.250852584838867
grad AddEdge W: 1.2234801143311618e-17
grad ChooseDest W: 1.782133936882019
grad AddEdge W: 8.010090811929929e-17
grad ChooseDest W: 2.079342842102051
grad AddEdge W: 2.2190526800520798e-15
grad ChooseDest W: 2.720632553100586
grad AddEdge W: 1.1262803539377094e-17
grad ChooseDest W: 1.8112791776657104
grad AddEdge W: 1.095148167504635e-15
grad ChooseDest W: 2.9692840576171875
grad AddEdge W: 3.184795905200641e-17
grad ChooseDest W: 2.0253164768218994
grad AddEdge W: 9.31638257208392e-14
grad ChooseDest W: 2.8550069332122803
grad AddEdge W: 1.864334406157741e-17
grad ChooseDest W: 3.1489346027374268
grad AddEdge W: 1.3360807369594327e-17
grad ChooseDest W: 3.3814876079559326
grad AddEdge W: 7.75132092305476e-17
grad ChooseDest W: 2.619781255722046
grad AddEdge W: 2.4164041768346148e-17
grad ChooseDest W: 3.3334224224090576
grad AddEdge W: 3.874555017356764e-17
grad ChooseDest W: 3.0300803184509277
grad AddEdge W: 4.523110198558812e-15
grad ChooseDest W: 1.7222874164581299
grad AddEdge W: 2.653520019137807e-15
grad ChooseDest W: 2.398226737976074
grad AddEdge W: 9.415411115653419e-17
grad ChooseDest W: 2.4603164196014404
grad AddEdge W: 8.871643624435842e-16
grad ChooseDest W: 2.2890424728393555
grad AddEdge W: 4.314270413765002e-15
grad ChooseDest W: 2.194810628890991
grad AddEdge W: 3.695712745571287e-15
grad ChooseDest W: 0.96383136510849
grad AddEdge W: 3.2806426462661406e-17
grad ChooseDest W: 2.1565825939178467
grad AddEdge W: 5.305031045531317e-13
grad ChooseDest W: 2.0142927169799805
grad AddEdge W: 3.3793262462557825e-15
grad ChooseDest W: 2.0975708961486816
grad AddEdge W: 1.614518595212226e-15
grad ChooseDest W: 2.870250940322876
grad AddEdge W: 5.152803808910079e-15
grad ChooseDest W: 2.8192861080169678
grad AddEdge W: 4.3715889877219124e-17
grad ChooseDest W: 2.2677948474884033
grad AddEdge W: 4.83023248677196e-15
grad ChooseDest W: 2.397972583770752
grad AddEdge W: 1.1696504326880446e-15
grad ChooseDest W: 1.5358465909957886
grad AddEdge W: 1.9415197871679216e-17
grad ChooseDest W: 2.860398530960083
grad AddEdge W: 4.380038141370774e-17
grad ChooseDest W: 2.283757209777832
grad AddEdge W: 1.5192440540193544e-14
grad ChooseDest W: 1.6347583532333374
grad AddEdge W: 1.6941219773541318e-17
grad ChooseDest W: 2.1579461097717285
grad AddEdge W: 7.850499878499868e-16
grad ChooseDest W: 1.421639323234558
grad AddEdge W: 3.084462218855299e-15
grad ChooseDest W: 1.3048746585845947
grad AddEdge W: 7.861516600770095e-16
grad ChooseDest W: 2.3083348274230957
grad AddEdge W: 5.0592563255450113e-17
grad ChooseDest W: 2.87648344039917
grad AddEdge W: 5.4376120464492164e-17
grad ChooseDest W: 2.4101409912109375
grad AddEdge W: 5.378561607752526e-17
grad ChooseDest W: 2.0942635536193848
grad AddEdge W: 3.244097806803548e-17
grad ChooseDest W: 2.592956304550171
grad AddEdge W: 5.221868651967708e-16
grad ChooseDest W: 2.531240940093994
grad AddEdge W: 6.301563815080431e-17
grad ChooseDest W: 3.0566930770874023
=== Epoch 36: Train Loss: 5.9906, Train Log Prob: 0.0073 ===
Total mismatches: 91129
Predicted valid destination but wrong order: 19803
Epoch 36: Validation Loss: 6.2552, Validation Log Prob: 0.0038
Epoch 36: Edge Precision: 0.3621, Recall: 0.3592, F1: 0.3605, Jaccard: 0.2375
Epoch 36: TP: 2.5151037938439513, FP: 4.45311381531854, FN: 4.506370794559771
Epoch 36: Current Learning Rate: 1e-06
[Epoch 36] ‚è±Ô∏è Total: 1880.29s | Current time: 2025-07-15 06:35:41 | üèãÔ∏è Train: 1647.23s | ‚úÖ Val: 233.06s
grad AddEdge W: 4.299823843333106e-15
grad ChooseDest W: 5.414471626281738
grad AddEdge W: 1.2490099024283657e-16
grad ChooseDest W: 2.233591079711914
grad AddEdge W: 4.461779136886529e-17
grad ChooseDest W: 3.2233057022094727
grad AddEdge W: 1.5834720120463834e-17
grad ChooseDest W: 1.7018965482711792
grad AddEdge W: 3.9561580391463455e-17
grad ChooseDest W: 2.586656093597412
grad AddEdge W: 3.4370810289456393e-17
grad ChooseDest W: 4.833487510681152
grad AddEdge W: 2.5550714112340243e-17
grad ChooseDest W: 2.4533121585845947
grad AddEdge W: 2.620540579578224e-15
grad ChooseDest W: 1.8889003992080688
grad AddEdge W: 1.34937617638112e-17
grad ChooseDest W: 2.032942771911621
grad AddEdge W: 5.387102862999512e-15
grad ChooseDest W: 3.6672708988189697
grad AddEdge W: 4.674332128832645e-17
grad ChooseDest W: 1.9443405866622925
grad AddEdge W: 3.175431889794296e-17
grad ChooseDest W: 2.298351526260376
grad AddEdge W: 2.063329696105876e-15
grad ChooseDest W: 2.477952241897583
grad AddEdge W: 1.455191697833692e-15
grad ChooseDest W: 1.9135637283325195
grad AddEdge W: 7.94569049990349e-17
grad ChooseDest W: 2.566852569580078
grad AddEdge W: 3.281863233978024e-17
grad ChooseDest W: 2.0340676307678223
grad AddEdge W: 2.5389352681527073e-17
grad ChooseDest W: 2.508436918258667
grad AddEdge W: 5.060255295027179e-16
grad ChooseDest W: 1.5565053224563599
grad AddEdge W: 1.3415702383765797e-17
grad ChooseDest W: 3.3227627277374268
grad AddEdge W: 1.0721168122731985e-15
grad ChooseDest W: 1.9918594360351562
grad AddEdge W: 3.167849224380696e-15
grad ChooseDest W: 2.515969753265381
grad AddEdge W: 3.873225572676269e-17
grad ChooseDest W: 3.2288379669189453
grad AddEdge W: 3.595940072969713e-17
grad ChooseDest W: 1.6545106172561646
grad AddEdge W: 2.9607176932832605e-15
grad ChooseDest W: 2.0485947132110596
grad AddEdge W: 1.6984837007241239e-15
grad ChooseDest W: 1.8332465887069702
grad AddEdge W: 6.954779742135187e-17
grad ChooseDest W: 1.9733686447143555
grad AddEdge W: 5.209534687585383e-15
grad ChooseDest W: 1.866007924079895
grad AddEdge W: 9.275716192060974e-17
grad ChooseDest W: 1.7919249534606934
grad AddEdge W: 2.378700655857182e-16
grad ChooseDest W: 2.847783088684082
grad AddEdge W: 2.848993001984124e-17
grad ChooseDest W: 2.3708243370056152
grad AddEdge W: 6.941621118386816e-18
grad ChooseDest W: 4.51333475112915
grad AddEdge W: 6.518851853550597e-16
grad ChooseDest W: 2.2833855152130127
grad AddEdge W: 2.6039552509545113e-15
grad ChooseDest W: 2.7083945274353027
grad AddEdge W: 2.909253440480857e-17
grad ChooseDest W: 2.378408670425415
grad AddEdge W: 2.7063990154854722e-15
grad ChooseDest W: 2.799058675765991
grad AddEdge W: 4.2837992505204755e-15
grad ChooseDest W: 2.3528215885162354
grad AddEdge W: 2.381169260590061e-17
grad ChooseDest W: 3.1888394355773926
grad AddEdge W: 5.6009968933895884e-15
grad ChooseDest W: 1.714945673942566
grad AddEdge W: 9.119003681726475e-18
grad ChooseDest W: 2.703763484954834
grad AddEdge W: 1.5859235040667496e-13
grad ChooseDest W: 2.271599292755127
grad AddEdge W: 3.142836553206513e-15
grad ChooseDest W: 2.1341211795806885
grad AddEdge W: 5.495321790809081e-17
grad ChooseDest W: 1.8247638940811157
grad AddEdge W: 5.155454783806968e-17
grad ChooseDest W: 2.4956705570220947
grad AddEdge W: 9.194507569987458e-17
grad ChooseDest W: 1.5967905521392822
grad AddEdge W: 2.9415609843899205e-15
grad ChooseDest W: 1.2529661655426025
grad AddEdge W: 1.089993512108172e-17
grad ChooseDest W: 1.7931466102600098
grad AddEdge W: 2.976715313390477e-17
grad ChooseDest W: 2.9872348308563232
grad AddEdge W: 9.475345975861052e-17
grad ChooseDest W: 2.3307485580444336
grad AddEdge W: 9.80601771230178e-17
grad ChooseDest W: 2.9552664756774902
grad AddEdge W: 7.659202119573915e-17
grad ChooseDest W: 1.537368655204773
grad AddEdge W: 4.58209669288543e-18
grad ChooseDest W: 1.6158713102340698
grad AddEdge W: 6.75704254757663e-17
grad ChooseDest W: 2.1597084999084473
grad AddEdge W: 8.800308572143312e-12
grad ChooseDest W: 0.7806057929992676
grad AddEdge W: 5.3640141477556783e-17
grad ChooseDest W: 2.803579568862915
grad AddEdge W: 4.936897651753799e-16
grad ChooseDest W: 3.1383302211761475
grad AddEdge W: 2.0652043917763867e-15
grad ChooseDest W: 3.5726544857025146
grad AddEdge W: 4.189853436677072e-17
grad ChooseDest W: 2.1634838581085205
grad AddEdge W: 3.781012775008058e-15
grad ChooseDest W: 3.0211021900177
grad AddEdge W: 1.1324311697109802e-16
grad ChooseDest W: 2.441678524017334
grad AddEdge W: 1.3040555296457685e-17
grad ChooseDest W: 1.7805185317993164
grad AddEdge W: 7.865078936796115e-18
grad ChooseDest W: 2.1786978244781494
grad AddEdge W: 1.9391861121365625e-16
grad ChooseDest W: 3.7351341247558594
grad AddEdge W: 1.1129426455867206e-17
grad ChooseDest W: 1.5054097175598145
grad AddEdge W: 8.033907740456758e-13
grad ChooseDest W: 1.655486822128296
grad AddEdge W: 3.2656040262738937e-15
grad ChooseDest W: 1.678452968597412
grad AddEdge W: 4.089938621359037e-17
grad ChooseDest W: 2.385953426361084
=== Epoch 37: Train Loss: 5.9939, Train Log Prob: 0.0072 ===
Total mismatches: 91172
Predicted valid destination but wrong order: 19832
Epoch 37: Validation Loss: 6.2604, Validation Log Prob: 0.0040
Epoch 37: Edge Precision: 0.3620, Recall: 0.3592, F1: 0.3605, Jaccard: 0.2377
Epoch 37: TP: 2.515676449534717, FP: 4.453400143163923, FN: 4.505798138869005
Epoch 37: Current Learning Rate: 1e-06
[Epoch 37] ‚è±Ô∏è Total: 1883.68s | Current time: 2025-07-15 07:07:05 | üèãÔ∏è Train: 1649.10s | ‚úÖ Val: 234.58s
grad AddEdge W: 1.5020487134428527e-15
grad ChooseDest W: 8.77393627166748
grad AddEdge W: 1.4555874739782967e-15
grad ChooseDest W: 3.347494125366211
grad AddEdge W: 4.9622042168908997e-17
grad ChooseDest W: 2.51419734954834
grad AddEdge W: 2.0322673038641664e-15
grad ChooseDest W: 2.088006019592285
grad AddEdge W: 1.287739061190915e-17
grad ChooseDest W: 2.195112705230713
grad AddEdge W: 1.232104712987946e-17
grad ChooseDest W: 2.0338656902313232
grad AddEdge W: 1.2463400545786201e-15
grad ChooseDest W: 1.464868187904358
grad AddEdge W: 3.7559098551203233e-17
grad ChooseDest W: 2.145764112472534
grad AddEdge W: 7.23692075233276e-15
grad ChooseDest W: 1.6669600009918213
grad AddEdge W: 2.9041585505380123e-15
grad ChooseDest W: 1.3661885261535645
grad AddEdge W: 2.5063569251634288e-17
grad ChooseDest W: 2.891941547393799
grad AddEdge W: 4.625006687161618e-17
grad ChooseDest W: 2.393568515777588
grad AddEdge W: 2.2074257631875854e-17
grad ChooseDest W: 3.265108108520508
grad AddEdge W: 7.005387313755671e-17
grad ChooseDest W: 1.8816324472427368
grad AddEdge W: 1.4043786413141598e-17
grad ChooseDest W: 2.873819589614868
grad AddEdge W: 2.5713547924643756e-17
grad ChooseDest W: 3.6557536125183105
grad AddEdge W: 1.0713684983689687e-16
grad ChooseDest W: 1.8776354789733887
grad AddEdge W: 9.246264591787146e-18
grad ChooseDest W: 1.9994847774505615
grad AddEdge W: 9.833342465784611e-17
grad ChooseDest W: 2.3363869190216064
grad AddEdge W: 1.6682778747854898e-13
grad ChooseDest W: 1.2223010063171387
grad AddEdge W: 2.441619904305567e-15
grad ChooseDest W: 1.9662617444992065
grad AddEdge W: 8.658884300557731e-17
grad ChooseDest W: 1.7819370031356812
grad AddEdge W: 4.305296523205316e-15
grad ChooseDest W: 1.6533480882644653
grad AddEdge W: 8.739312725877487e-16
grad ChooseDest W: 2.094381332397461
grad AddEdge W: 5.741185346022707e-17
grad ChooseDest W: 2.355397939682007
grad AddEdge W: 1.3865514929066007e-17
grad ChooseDest W: 2.5927133560180664
grad AddEdge W: 3.396676233872629e-17
grad ChooseDest W: 1.9809082746505737
grad AddEdge W: 3.0852536520305e-17
grad ChooseDest W: 3.255481004714966
grad AddEdge W: 2.5201635754385637e-15
grad ChooseDest W: 1.9137005805969238
grad AddEdge W: 5.44670899501427e-15
grad ChooseDest W: 2.3473384380340576
grad AddEdge W: 1.1058378334132674e-13
grad ChooseDest W: 1.0342373847961426
grad AddEdge W: 1.1402863415055798e-15
grad ChooseDest W: 2.9042768478393555
grad AddEdge W: 1.7347081235046381e-15
grad ChooseDest W: 2.9263808727264404
grad AddEdge W: 8.136747780881763e-16
grad ChooseDest W: 2.497323751449585
grad AddEdge W: 9.504283731538362e-18
grad ChooseDest W: 1.6644572019577026
grad AddEdge W: 4.449442896103158e-16
grad ChooseDest W: 2.7859320640563965
grad AddEdge W: 7.742701039327468e-17
grad ChooseDest W: 2.5848302841186523
grad AddEdge W: 1.2094972977466103e-15
grad ChooseDest W: 2.6762869358062744
grad AddEdge W: 5.283139375777389e-17
grad ChooseDest W: 3.799251079559326
grad AddEdge W: 5.940048366376294e-12
grad ChooseDest W: 1.110995888710022
grad AddEdge W: 5.19535705011424e-16
grad ChooseDest W: 1.3990262746810913
grad AddEdge W: 6.609896381026307e-17
grad ChooseDest W: 2.0741100311279297
grad AddEdge W: 1.56041187092963e-17
grad ChooseDest W: 2.625504493713379
grad AddEdge W: 8.407778074621743e-18
grad ChooseDest W: 4.269055366516113
grad AddEdge W: 2.2556887608449596e-15
grad ChooseDest W: 2.1390063762664795
grad AddEdge W: 3.4393140857272875e-17
grad ChooseDest W: 1.6608681678771973
grad AddEdge W: 4.005256171585043e-17
grad ChooseDest W: 1.8671281337738037
grad AddEdge W: 3.8481443712426976e-15
grad ChooseDest W: 3.2941229343414307
grad AddEdge W: 1.1719382686785785e-14
grad ChooseDest W: 1.3899612426757812
grad AddEdge W: 6.049355149293813e-15
grad ChooseDest W: 2.2427120208740234
grad AddEdge W: 4.7045421147618414e-15
grad ChooseDest W: 1.3868345022201538
grad AddEdge W: 1.152904237287525e-17
grad ChooseDest W: 2.235809564590454
grad AddEdge W: 3.174230347088836e-15
grad ChooseDest W: 2.2358784675598145
grad AddEdge W: 1.9152245426754734e-17
grad ChooseDest W: 3.386474370956421
grad AddEdge W: 4.424118596213725e-17
grad ChooseDest W: 2.1736366748809814
grad AddEdge W: 6.3164339549255134e-15
grad ChooseDest W: 2.06652569770813
grad AddEdge W: 1.5851189286459765e-16
grad ChooseDest W: 2.234929323196411
grad AddEdge W: 1.3449211635816442e-16
grad ChooseDest W: 3.7388250827789307
grad AddEdge W: 1.6046970067844224e-13
grad ChooseDest W: 1.5423208475112915
grad AddEdge W: 3.0261073662618716e-15
grad ChooseDest W: 4.048257827758789
grad AddEdge W: 2.4707956034491616e-17
grad ChooseDest W: 3.11344838142395
grad AddEdge W: 4.617656016131337e-15
grad ChooseDest W: 1.320180058479309
grad AddEdge W: 1.967414146848302e-16
grad ChooseDest W: 2.1450259685516357
grad AddEdge W: 8.255930213469669e-17
grad ChooseDest W: 2.482569694519043
grad AddEdge W: 1.3071647493671226e-15
grad ChooseDest W: 2.0343029499053955
grad AddEdge W: 4.841843151191779e-17
grad ChooseDest W: 2.895362377166748
=== Epoch 38: Train Loss: 5.9870, Train Log Prob: 0.0073 ===
Total mismatches: 91321
Predicted valid destination but wrong order: 20012
Epoch 38: Validation Loss: 6.2746, Validation Log Prob: 0.0038
Epoch 38: Edge Precision: 0.3600, Recall: 0.3573, F1: 0.3585, Jaccard: 0.2359
Epoch 38: TP: 2.5017895490336435, FP: 4.469148174659986, FN: 4.519685039370079
Epoch 38: Current Learning Rate: 1e-06
[Epoch 38] ‚è±Ô∏è Total: 1885.85s | Current time: 2025-07-15 07:38:31 | üèãÔ∏è Train: 1652.53s | ‚úÖ Val: 233.32s
grad AddEdge W: 4.781158785939835e-15
grad ChooseDest W: 6.16060209274292
grad AddEdge W: 1.2530260297384331e-16
grad ChooseDest W: 2.3794310092926025
grad AddEdge W: 5.337897739711664e-17
grad ChooseDest W: 2.3191962242126465
grad AddEdge W: 2.5620845793394938e-17
grad ChooseDest W: 2.4079322814941406
grad AddEdge W: 1.2169632579019465e-15
grad ChooseDest W: 2.498544454574585
grad AddEdge W: 3.1003995672743622e-15
grad ChooseDest W: 1.9994230270385742
grad AddEdge W: 3.453755666605728e-17
grad ChooseDest W: 2.2757439613342285
grad AddEdge W: 2.3430048014880894e-17
grad ChooseDest W: 2.045138120651245
grad AddEdge W: 2.471403382676041e-16
grad ChooseDest W: 2.6576454639434814
grad AddEdge W: 4.067351627552664e-17
grad ChooseDest W: 1.488150954246521
grad AddEdge W: 1.3772013245037096e-16
grad ChooseDest W: 2.0564634799957275
grad AddEdge W: 1.3157360643575685e-17
grad ChooseDest W: 1.8736882209777832
grad AddEdge W: 1.0450342043341631e-15
grad ChooseDest W: 3.205932140350342
grad AddEdge W: 2.938674600508143e-17
grad ChooseDest W: 2.5274133682250977
grad AddEdge W: 1.707874649131214e-15
grad ChooseDest W: 2.9870259761810303
grad AddEdge W: 5.859799763610141e-15
grad ChooseDest W: 1.9800784587860107
grad AddEdge W: 3.5835446060544836e-17
grad ChooseDest W: 0.9027212262153625
grad AddEdge W: 1.0080445084616283e-14
grad ChooseDest W: 2.8843090534210205
grad AddEdge W: 6.897664151683152e-15
grad ChooseDest W: 2.02492356300354
grad AddEdge W: 1.5099299315005804e-16
grad ChooseDest W: 3.767770528793335
grad AddEdge W: 2.979911539277382e-17
grad ChooseDest W: 2.6623361110687256
grad AddEdge W: 2.1642911066570277e-17
grad ChooseDest W: 2.0622541904449463
grad AddEdge W: 3.0371793574319062e-15
grad ChooseDest W: 2.02996826171875
grad AddEdge W: 4.61466726037695e-16
grad ChooseDest W: 1.6685123443603516
grad AddEdge W: 3.655911937929229e-15
grad ChooseDest W: 2.5585336685180664
grad AddEdge W: 9.719321904916832e-17
grad ChooseDest W: 1.931648850440979
grad AddEdge W: 3.3823589545498023e-13
grad ChooseDest W: 1.1031825542449951
grad AddEdge W: 9.144726517235037e-17
grad ChooseDest W: 4.55401611328125
grad AddEdge W: 1.5068234712537497e-17
grad ChooseDest W: 2.1896190643310547
grad AddEdge W: 2.3313489994766047e-17
grad ChooseDest W: 2.6651759147644043
grad AddEdge W: 1.3456534500343252e-16
grad ChooseDest W: 2.2728662490844727
grad AddEdge W: 4.174420191975633e-15
grad ChooseDest W: 2.004833936691284
grad AddEdge W: 1.1941026294402185e-17
grad ChooseDest W: 2.1674015522003174
grad AddEdge W: 2.6680023767117244e-15
grad ChooseDest W: 1.7234920263290405
grad AddEdge W: 2.3528042646212173e-15
grad ChooseDest W: 1.4712095260620117
grad AddEdge W: 1.922548585107475e-15
grad ChooseDest W: 3.551692247390747
grad AddEdge W: 4.095341196019972e-15
grad ChooseDest W: 2.3454668521881104
grad AddEdge W: 1.2896997273968494e-17
grad ChooseDest W: 1.6666889190673828
grad AddEdge W: 4.02974357645281e-15
grad ChooseDest W: 2.1087863445281982
grad AddEdge W: 4.193140043626427e-15
grad ChooseDest W: 2.045877695083618
grad AddEdge W: 2.6916195743401954e-17
grad ChooseDest W: 2.547206163406372
grad AddEdge W: 3.41884533603354e-17
grad ChooseDest W: 1.7223364114761353
grad AddEdge W: 3.104392957043752e-17
grad ChooseDest W: 4.1673102378845215
grad AddEdge W: 4.8294384198536885e-17
grad ChooseDest W: 2.034329414367676
grad AddEdge W: 3.4544789533333446e-17
grad ChooseDest W: 1.8168267011642456
grad AddEdge W: 3.6022795851843196e-17
grad ChooseDest W: 2.246675729751587
grad AddEdge W: 1.5470140927225577e-16
grad ChooseDest W: 2.648829221725464
grad AddEdge W: 5.964307959191797e-14
grad ChooseDest W: 1.7189102172851562
grad AddEdge W: 6.055844851037879e-17
grad ChooseDest W: 1.7682156562805176
grad AddEdge W: 4.186174891701155e-15
grad ChooseDest W: 1.5592344999313354
grad AddEdge W: 1.1825657793095414e-15
grad ChooseDest W: 2.4889941215515137
grad AddEdge W: 4.160800325700258e-15
grad ChooseDest W: 2.602180242538452
grad AddEdge W: 3.8018535971587764e-15
grad ChooseDest W: 1.701407790184021
grad AddEdge W: 1.3471078320745404e-16
grad ChooseDest W: 2.198925733566284
grad AddEdge W: 1.593046131328317e-17
grad ChooseDest W: 2.3784749507904053
grad AddEdge W: 8.617158663482596e-17
grad ChooseDest W: 2.691864490509033
grad AddEdge W: 3.793402908262698e-16
grad ChooseDest W: 1.984770655632019
grad AddEdge W: 2.2216289506134884e-17
grad ChooseDest W: 3.732069492340088
grad AddEdge W: 4.056075832314586e-17
grad ChooseDest W: 2.440497636795044
grad AddEdge W: 4.916080625934943e-17
grad ChooseDest W: 1.951736330986023
grad AddEdge W: 2.6547950154816617e-15
grad ChooseDest W: 2.529175043106079
grad AddEdge W: 1.225982888240784e-15
grad ChooseDest W: 3.351522207260132
grad AddEdge W: 1.5575928394020494e-17
grad ChooseDest W: 3.9868760108947754
grad AddEdge W: 3.72641472529394e-15
grad ChooseDest W: 3.4608662128448486
grad AddEdge W: 2.922395593408871e-15
grad ChooseDest W: 2.6913769245147705
grad AddEdge W: 2.7125427770624792e-17
grad ChooseDest W: 2.7126660346984863
=== Epoch 39: Train Loss: 5.9883, Train Log Prob: 0.0071 ===
Total mismatches: 91349
Predicted valid destination but wrong order: 19911
Epoch 39: Validation Loss: 6.2561, Validation Log Prob: 0.0039
Epoch 39: Edge Precision: 0.3591, Recall: 0.3564, F1: 0.3576, Jaccard: 0.2353
Epoch 39: TP: 2.494774516821761, FP: 4.4773085182534, FN: 4.526700071581962
Epoch 39: Current Learning Rate: 1e-06
[Epoch 39] ‚è±Ô∏è Total: 1875.16s | Current time: 2025-07-15 08:09:46 | üèãÔ∏è Train: 1641.24s | ‚úÖ Val: 233.92s
grad AddEdge W: 1.9833613917885629e-10
grad ChooseDest W: 4.946030139923096
grad AddEdge W: 1.1803270116729442e-17
grad ChooseDest W: 1.9013071060180664
grad AddEdge W: 6.053343456865519e-17
grad ChooseDest W: 2.9417216777801514
grad AddEdge W: 6.680556795928547e-17
grad ChooseDest W: 2.9183502197265625
grad AddEdge W: 1.0137080208863878e-17
grad ChooseDest W: 6.36260461807251
grad AddEdge W: 3.670700325860011e-17
grad ChooseDest W: 3.544539213180542
grad AddEdge W: 2.1548898681231175e-17
grad ChooseDest W: 2.1548333168029785
grad AddEdge W: 2.4918892053776314e-17
grad ChooseDest W: 4.164449691772461
grad AddEdge W: 2.5612109111765153e-17
grad ChooseDest W: 2.766991376876831
grad AddEdge W: 2.0066132765476795e-17
grad ChooseDest W: 1.6843465566635132
grad AddEdge W: 6.833455507330809e-17
grad ChooseDest W: 2.2869346141815186
grad AddEdge W: 3.5221177899544915e-15
grad ChooseDest W: 1.1830288171768188
grad AddEdge W: 3.9108826391905006e-18
grad ChooseDest W: 2.36936354637146
grad AddEdge W: 2.5668920537979785e-15
grad ChooseDest W: 1.3730300664901733
grad AddEdge W: 2.5213972789262396e-15
grad ChooseDest W: 2.3257832527160645
grad AddEdge W: 3.52858010871156e-17
grad ChooseDest W: 2.10351824760437
grad AddEdge W: 1.0677315109470262e-15
grad ChooseDest W: 1.560994029045105
grad AddEdge W: 3.2699872364874775e-17
grad ChooseDest W: 2.4758501052856445
grad AddEdge W: 2.8534617625253805e-17
grad ChooseDest W: 2.856454372406006
grad AddEdge W: 3.4497666708197526e-17
grad ChooseDest W: 3.664451837539673
grad AddEdge W: 3.851482528087827e-15
grad ChooseDest W: 2.819810628890991
grad AddEdge W: 1.4339500173406856e-15
grad ChooseDest W: 2.417739152908325
grad AddEdge W: 7.875169266702287e-12
grad ChooseDest W: 0.9684399366378784
grad AddEdge W: 4.7593805233096323e-17
grad ChooseDest W: 1.6146358251571655
grad AddEdge W: 3.045790850239065e-17
grad ChooseDest W: 1.6179299354553223
grad AddEdge W: 5.0047583580675677e-17
grad ChooseDest W: 2.141122817993164
grad AddEdge W: 1.5898029149651743e-15
grad ChooseDest W: 2.562278985977173
grad AddEdge W: 4.444215855785652e-15
grad ChooseDest W: 3.111795425415039
grad AddEdge W: 4.106246321699397e-15
grad ChooseDest W: 3.673478603363037
grad AddEdge W: 2.255868768581141e-17
grad ChooseDest W: 1.930821180343628
grad AddEdge W: 1.826310395706336e-15
grad ChooseDest W: 2.3743770122528076
grad AddEdge W: 4.471623578792645e-17
grad ChooseDest W: 2.174482822418213
grad AddEdge W: 7.189643449563083e-17
grad ChooseDest W: 1.82254958152771
grad AddEdge W: 4.970429039157637e-17
grad ChooseDest W: 2.7592291831970215
grad AddEdge W: 5.862099299243259e-17
grad ChooseDest W: 3.5884735584259033
grad AddEdge W: 1.485549378515621e-17
grad ChooseDest W: 2.475494384765625
grad AddEdge W: 9.817786308661592e-16
grad ChooseDest W: 2.012646198272705
grad AddEdge W: 2.0518550929706548e-17
grad ChooseDest W: 1.6111716032028198
grad AddEdge W: 2.226868464814632e-15
grad ChooseDest W: 2.197662353515625
grad AddEdge W: 4.3304238482298434e-17
grad ChooseDest W: 2.5964291095733643
grad AddEdge W: 4.921561524673719e-17
grad ChooseDest W: 4.570858955383301
grad AddEdge W: 1.4442047275957381e-15
grad ChooseDest W: 3.2350034713745117
grad AddEdge W: 1.4884996008883525e-17
grad ChooseDest W: 2.711361885070801
grad AddEdge W: 8.663128729716863e-17
grad ChooseDest W: 4.133584976196289
grad AddEdge W: 3.1079722535810525e-15
grad ChooseDest W: 3.773343086242676
grad AddEdge W: 5.690797208875121e-15
grad ChooseDest W: 1.6253321170806885
grad AddEdge W: 7.371810458015431e-17
grad ChooseDest W: 2.945401191711426
grad AddEdge W: 3.059643094709564e-15
grad ChooseDest W: 1.7208918333053589
grad AddEdge W: 4.137912780781397e-17
grad ChooseDest W: 3.0038912296295166
grad AddEdge W: 3.7145451996645066e-17
grad ChooseDest W: 2.8579204082489014
grad AddEdge W: 5.2054707558533546e-17
grad ChooseDest W: 2.5709598064422607
grad AddEdge W: 1.0785446276725561e-15
grad ChooseDest W: 1.9497560262680054
grad AddEdge W: 1.6711468770217938e-15
grad ChooseDest W: 2.470869302749634
grad AddEdge W: 1.0820189958859333e-13
grad ChooseDest W: 1.6710578203201294
grad AddEdge W: 5.4388743240639723e-17
grad ChooseDest W: 1.8828731775283813
grad AddEdge W: 7.424111433785934e-17
grad ChooseDest W: 2.3248186111450195
grad AddEdge W: 1.785397718354663e-17
grad ChooseDest W: 2.785543918609619
grad AddEdge W: 8.707637611177557e-15
grad ChooseDest W: 2.9522225856781006
grad AddEdge W: 2.925691173613511e-17
grad ChooseDest W: 2.3129138946533203
grad AddEdge W: 2.0052738461428295e-15
grad ChooseDest W: 2.2608253955841064
grad AddEdge W: 5.790581925226414e-15
grad ChooseDest W: 1.9611210823059082
grad AddEdge W: 1.3092121338797546e-15
grad ChooseDest W: 2.3392200469970703
grad AddEdge W: 2.5293828937947163e-15
grad ChooseDest W: 1.573500156402588
grad AddEdge W: 6.928207392137533e-17
grad ChooseDest W: 1.9032561779022217
grad AddEdge W: 6.243146334988466e-17
grad ChooseDest W: 2.044914484024048
grad AddEdge W: 8.280120600940525e-15
grad ChooseDest W: 2.375716209411621
=== Epoch 40: Train Loss: 5.9881, Train Log Prob: 0.0071 ===
Total mismatches: 91457
Predicted valid destination but wrong order: 19934
Epoch 40: Validation Loss: 6.2593, Validation Log Prob: 0.0039
Epoch 40: Edge Precision: 0.3598, Recall: 0.3574, F1: 0.3585, Jaccard: 0.2357
Epoch 40: TP: 2.50050107372942, FP: 4.474158911954188, FN: 4.520973514674302
Epoch 40: Current Learning Rate: 1e-06
[Epoch 40] ‚è±Ô∏è Total: 1892.02s | Current time: 2025-07-15 08:41:18 | üèãÔ∏è Train: 1655.07s | ‚úÖ Val: 236.95s
grad AddEdge W: 4.2788136315335956e-13
grad ChooseDest W: 8.214308738708496
grad AddEdge W: 8.168345683234594e-18
grad ChooseDest W: 3.0020790100097656
grad AddEdge W: 4.734519196409045e-18
grad ChooseDest W: 2.3451621532440186
grad AddEdge W: 1.423208580778317e-15
grad ChooseDest W: 2.7604424953460693
grad AddEdge W: 2.0293084060211703e-15
grad ChooseDest W: 4.187833786010742
grad AddEdge W: 7.10692274132982e-17
grad ChooseDest W: 2.4717507362365723
grad AddEdge W: 1.6303168184697027e-15
grad ChooseDest W: 2.4293100833892822
grad AddEdge W: 1.5304712745648852e-16
grad ChooseDest W: 1.5615525245666504
grad AddEdge W: 1.2385793037965823e-17
grad ChooseDest W: 2.205735206604004
grad AddEdge W: 7.253597878152131e-16
grad ChooseDest W: 1.9780433177947998
grad AddEdge W: 1.7415427529466943e-17
grad ChooseDest W: 2.3925089836120605
grad AddEdge W: 8.252634725909258e-17
grad ChooseDest W: 2.2200536727905273
grad AddEdge W: 8.921719451271353e-18
grad ChooseDest W: 2.326497793197632
grad AddEdge W: 1.4085211353520458e-15
grad ChooseDest W: 2.762653112411499
grad AddEdge W: 2.4749601799006803e-15
grad ChooseDest W: 2.6956937313079834
grad AddEdge W: 1.2512031883661623e-13
grad ChooseDest W: 1.8104617595672607
grad AddEdge W: 1.0272820878256075e-16
grad ChooseDest W: 3.227987289428711
grad AddEdge W: 4.727237939067047e-17
grad ChooseDest W: 2.2311031818389893
grad AddEdge W: 4.422611433432984e-15
grad ChooseDest W: 3.5095152854919434
grad AddEdge W: 1.5544701002407637e-15
grad ChooseDest W: 2.0134665966033936
grad AddEdge W: 2.744230842102079e-16
grad ChooseDest W: 2.2379848957061768
grad AddEdge W: 1.364929537633669e-16
grad ChooseDest W: 1.6413174867630005
grad AddEdge W: 3.0885463999687224e-15
grad ChooseDest W: 3.232145309448242
grad AddEdge W: 3.996356925803834e-15
grad ChooseDest W: 3.593531608581543
grad AddEdge W: 2.1927756004458832e-15
grad ChooseDest W: 1.5282835960388184
grad AddEdge W: 5.936854350821865e-15
grad ChooseDest W: 2.827669858932495
grad AddEdge W: 3.241833978903113e-17
grad ChooseDest W: 2.223375082015991
grad AddEdge W: 1.3933966209249914e-15
grad ChooseDest W: 2.273176670074463
grad AddEdge W: 1.0744268322260275e-11
grad ChooseDest W: 3.0388832092285156
grad AddEdge W: 2.4869426653145643e-17
grad ChooseDest W: 2.215604782104492
grad AddEdge W: 9.079485789706244e-17
grad ChooseDest W: 4.009230613708496
grad AddEdge W: 1.7176981137369957e-16
grad ChooseDest W: 2.583355665206909
grad AddEdge W: 1.3164715238750792e-15
grad ChooseDest W: 1.7967299222946167
grad AddEdge W: 1.1878087014948087e-15
grad ChooseDest W: 1.5557917356491089
grad AddEdge W: 8.048064357746523e-17
grad ChooseDest W: 1.480505108833313
grad AddEdge W: 3.856743449723223e-15
grad ChooseDest W: 2.284411668777466
grad AddEdge W: 1.5273378349951955e-15
grad ChooseDest W: 2.636986494064331
grad AddEdge W: 2.4026811644455763e-15
grad ChooseDest W: 3.34546160697937
grad AddEdge W: 2.036587595411637e-15
grad ChooseDest W: 3.5417561531066895
grad AddEdge W: 1.3471291402471198e-17
grad ChooseDest W: 2.0744383335113525
grad AddEdge W: 6.741834335706476e-17
grad ChooseDest W: 3.066776752471924
grad AddEdge W: 7.533324420933555e-17
grad ChooseDest W: 4.4207844734191895
grad AddEdge W: 1.685039368792177e-17
grad ChooseDest W: 2.1461143493652344
grad AddEdge W: 5.045217747061006e-18
grad ChooseDest W: 2.1143648624420166
grad AddEdge W: 3.454032937547056e-17
grad ChooseDest W: 2.2977774143218994
grad AddEdge W: 8.610348783284468e-16
grad ChooseDest W: 2.3472979068756104
grad AddEdge W: 5.4193167965330136e-14
grad ChooseDest W: 2.575835704803467
grad AddEdge W: 2.829139343793871e-17
grad ChooseDest W: 2.1774239540100098
grad AddEdge W: 7.575203688609498e-15
grad ChooseDest W: 4.642665863037109
grad AddEdge W: 3.949140768225038e-15
grad ChooseDest W: 1.577128291130066
grad AddEdge W: 1.5599855420419203e-17
grad ChooseDest W: 1.3693299293518066
grad AddEdge W: 8.978839746726712e-17
grad ChooseDest W: 1.3148167133331299
grad AddEdge W: 3.3464531601530567e-13
grad ChooseDest W: 1.2324129343032837
grad AddEdge W: 8.588732767168334e-17
grad ChooseDest W: 2.007483959197998
grad AddEdge W: 4.648593047496853e-15
grad ChooseDest W: 2.4210610389709473
grad AddEdge W: 4.3077309753050637e-17
grad ChooseDest W: 2.608783483505249
grad AddEdge W: 2.566072761179747e-15
grad ChooseDest W: 1.3736917972564697
grad AddEdge W: 2.8431177035292824e-17
grad ChooseDest W: 1.5754214525222778
grad AddEdge W: 1.4686677802662712e-15
grad ChooseDest W: 2.447071075439453
grad AddEdge W: 1.5156466759756078e-17
grad ChooseDest W: 2.8970158100128174
grad AddEdge W: 2.0627715609506396e-17
grad ChooseDest W: 2.234070301055908
grad AddEdge W: 3.588957795097039e-15
grad ChooseDest W: 2.864698886871338
grad AddEdge W: 9.413799238424574e-16
grad ChooseDest W: 3.141955614089966
grad AddEdge W: 1.1150790876728226e-17
grad ChooseDest W: 3.340407133102417
grad AddEdge W: 9.306372829051169e-17
grad ChooseDest W: 2.31790828704834
grad AddEdge W: 3.718546106851303e-17
grad ChooseDest W: 1.9239007234573364
=== Epoch 41: Train Loss: 5.9869, Train Log Prob: 0.0072 ===
Total mismatches: 91359
Predicted valid destination but wrong order: 19822
Epoch 41: Validation Loss: 6.2830, Validation Log Prob: 0.0037
Epoch 41: Edge Precision: 0.3596, Recall: 0.3570, F1: 0.3582, Jaccard: 0.2355
Epoch 41: TP: 2.498067287043665, FP: 4.477594846098783, FN: 4.5234073013600575
Epoch 41: Current Learning Rate: 1e-06
[Epoch 41] ‚è±Ô∏è Total: 1881.14s | Current time: 2025-07-15 09:12:39 | üèãÔ∏è Train: 1643.73s | ‚úÖ Val: 237.41s
grad AddEdge W: 5.759295069253682e-15
grad ChooseDest W: 5.074617385864258
grad AddEdge W: 1.2388193019795084e-16
grad ChooseDest W: 1.9937663078308105
grad AddEdge W: 3.3945904152400154e-17
grad ChooseDest W: 2.071098804473877
grad AddEdge W: 3.765861168761581e-17
grad ChooseDest W: 2.4922454357147217
grad AddEdge W: 7.325395699483856e-17
grad ChooseDest W: 2.7348854541778564
grad AddEdge W: 7.450288722323032e-17
grad ChooseDest W: 4.613954067230225
grad AddEdge W: 1.079441000288988e-15
grad ChooseDest W: 1.6268947124481201
grad AddEdge W: 8.708734836481607e-17
grad ChooseDest W: 2.4414288997650146
grad AddEdge W: 1.7190388080738216e-17
grad ChooseDest W: 2.220041513442993
grad AddEdge W: 1.0237409607104925e-14
grad ChooseDest W: 2.8963325023651123
grad AddEdge W: 1.0487466438628603e-15
grad ChooseDest W: 4.304862976074219
grad AddEdge W: 2.2179898654615124e-15
grad ChooseDest W: 2.2537145614624023
grad AddEdge W: 2.641321897664398e-15
grad ChooseDest W: 2.7221274375915527
grad AddEdge W: 3.45581435371425e-17
grad ChooseDest W: 1.8480278253555298
grad AddEdge W: 1.2594678476512e-16
grad ChooseDest W: 2.228564500808716
grad AddEdge W: 2.5813187189556138e-15
grad ChooseDest W: 2.513413190841675
grad AddEdge W: 1.81591836023471e-15
grad ChooseDest W: 2.82867693901062
grad AddEdge W: 1.0119361166344935e-14
grad ChooseDest W: 3.1312899589538574
grad AddEdge W: 6.667235217599503e-17
grad ChooseDest W: 7.056216716766357
grad AddEdge W: 7.262836360628715e-17
grad ChooseDest W: 1.9130773544311523
grad AddEdge W: 3.744800157749246e-17
grad ChooseDest W: 2.349574327468872
grad AddEdge W: 1.655729183315871e-15
grad ChooseDest W: 1.991507649421692
grad AddEdge W: 2.3680331361543514e-17
grad ChooseDest W: 2.455690860748291
grad AddEdge W: 2.4563288431928747e-15
grad ChooseDest W: 1.7031813859939575
grad AddEdge W: 8.132536968342725e-15
grad ChooseDest W: 3.2829911708831787
grad AddEdge W: 5.547442441206047e-17
grad ChooseDest W: 2.848198175430298
grad AddEdge W: 1.9140138811309408e-15
grad ChooseDest W: 5.663165092468262
grad AddEdge W: 1.2196021890491173e-15
grad ChooseDest W: 1.9405113458633423
grad AddEdge W: 1.2617802475972043e-16
grad ChooseDest W: 1.9460541009902954
grad AddEdge W: 7.265302682343103e-18
grad ChooseDest W: 3.2255096435546875
grad AddEdge W: 5.470610928061917e-17
grad ChooseDest W: 3.5251660346984863
grad AddEdge W: 1.0637501649273553e-16
grad ChooseDest W: 2.323070764541626
grad AddEdge W: 2.928451309881478e-17
grad ChooseDest W: 1.902036428451538
grad AddEdge W: 1.7337947043501428e-15
grad ChooseDest W: 3.0278642177581787
grad AddEdge W: 1.1530713774020974e-16
grad ChooseDest W: 2.4711923599243164
grad AddEdge W: 4.5619612969785415e-17
grad ChooseDest W: 2.432974338531494
grad AddEdge W: 4.669479291789368e-16
grad ChooseDest W: 2.074768543243408
grad AddEdge W: 3.1751582584476634e-17
grad ChooseDest W: 2.9862067699432373
grad AddEdge W: 4.306536857372782e-17
grad ChooseDest W: 1.604422688484192
grad AddEdge W: 8.444025790808307e-17
grad ChooseDest W: 2.5724453926086426
grad AddEdge W: 5.326958970942365e-16
grad ChooseDest W: 2.8529958724975586
grad AddEdge W: 5.0167050960440995e-15
grad ChooseDest W: 2.5842645168304443
grad AddEdge W: 4.049318759326763e-17
grad ChooseDest W: 1.917438268661499
grad AddEdge W: 3.6557022178654444e-17
grad ChooseDest W: 3.421973466873169
grad AddEdge W: 2.8413002355222707e-15
grad ChooseDest W: 2.5343210697174072
grad AddEdge W: 3.518093112905613e-17
grad ChooseDest W: 1.7588008642196655
grad AddEdge W: 5.333902457353033e-17
grad ChooseDest W: 1.8303147554397583
grad AddEdge W: 4.598505143824399e-17
grad ChooseDest W: 1.8804811239242554
grad AddEdge W: 1.0483479031029403e-15
grad ChooseDest W: 2.319535970687866
grad AddEdge W: 7.107323096746296e-17
grad ChooseDest W: 1.989285945892334
grad AddEdge W: 6.226926729383343e-18
grad ChooseDest W: 1.5228674411773682
grad AddEdge W: 4.130800112774405e-15
grad ChooseDest W: 2.010586738586426
grad AddEdge W: 6.721926414468039e-15
grad ChooseDest W: 3.487689733505249
grad AddEdge W: 1.205867205702254e-17
grad ChooseDest W: 2.2391726970672607
grad AddEdge W: 1.4199850579311978e-17
grad ChooseDest W: 2.354769229888916
grad AddEdge W: 6.396890755849002e-17
grad ChooseDest W: 2.48484468460083
grad AddEdge W: 3.639142723746623e-17
grad ChooseDest W: 2.701831579208374
grad AddEdge W: 8.137132850000518e-18
grad ChooseDest W: 2.094136953353882
grad AddEdge W: 1.6775333664777483e-17
grad ChooseDest W: 3.193908214569092
grad AddEdge W: 2.6613896963716284e-16
grad ChooseDest W: 1.7853164672851562
grad AddEdge W: 6.590848065880436e-17
grad ChooseDest W: 2.2766387462615967
grad AddEdge W: 3.699525915846259e-17
grad ChooseDest W: 2.0450406074523926
grad AddEdge W: 1.4208599066463466e-14
grad ChooseDest W: 2.0548243522644043
grad AddEdge W: 2.2419678329510648e-17
grad ChooseDest W: 1.4300674200057983
grad AddEdge W: 8.368446629111582e-17
grad ChooseDest W: 5.288882732391357
grad AddEdge W: 3.832110037551174e-15
grad ChooseDest W: 1.9442172050476074
=== Epoch 42: Train Loss: 5.9919, Train Log Prob: 0.0072 ===
Total mismatches: 91449
Predicted valid destination but wrong order: 19729
Epoch 42: Validation Loss: 6.2649, Validation Log Prob: 0.0039
Epoch 42: Edge Precision: 0.3614, Recall: 0.3587, F1: 0.3599, Jaccard: 0.2367
Epoch 42: TP: 2.511667859699356, FP: 4.46184681460272, FN: 4.509806728704366
Epoch 42: Current Learning Rate: 1e-06
[Epoch 42] ‚è±Ô∏è Total: 1873.71s | Current time: 2025-07-15 09:43:53 | üèãÔ∏è Train: 1637.43s | ‚úÖ Val: 236.28s
grad AddEdge W: 7.60130162074735e-15
grad ChooseDest W: 4.7584614753723145
grad AddEdge W: 1.8018004385863488e-15
grad ChooseDest W: 2.485102653503418
grad AddEdge W: 4.714302898929526e-15
grad ChooseDest W: 1.6385043859481812
grad AddEdge W: 1.2851588333974559e-15
grad ChooseDest W: 2.3955812454223633
grad AddEdge W: 1.1949198607243638e-15
grad ChooseDest W: 1.4415546655654907
grad AddEdge W: 4.277216533970889e-15
grad ChooseDest W: 2.832507610321045
grad AddEdge W: 1.0669678644229621e-17
grad ChooseDest W: 3.322624921798706
grad AddEdge W: 7.978635846386947e-16
grad ChooseDest W: 3.182953357696533
grad AddEdge W: 1.1177592355755557e-17
grad ChooseDest W: 3.169893503189087
grad AddEdge W: 2.1122778243035708e-15
grad ChooseDest W: 1.2629876136779785
grad AddEdge W: 1.7507341986249263e-15
grad ChooseDest W: 2.0702426433563232
grad AddEdge W: 2.334942272057535e-17
grad ChooseDest W: 2.5121734142303467
grad AddEdge W: 1.0710493390014213e-16
grad ChooseDest W: 2.05387282371521
grad AddEdge W: 4.153746010322397e-17
grad ChooseDest W: 7.351563930511475
grad AddEdge W: 3.2272973847507916e-15
grad ChooseDest W: 2.4153246879577637
grad AddEdge W: 5.561249822708844e-18
grad ChooseDest W: 2.4037723541259766
grad AddEdge W: 2.938298398765554e-17
grad ChooseDest W: 1.7481633424758911
grad AddEdge W: 2.1468035158909216e-17
grad ChooseDest W: 2.1139824390411377
grad AddEdge W: 2.0079887984970164e-15
grad ChooseDest W: 2.5717735290527344
grad AddEdge W: 7.721749811726031e-16
grad ChooseDest W: 2.63950777053833
grad AddEdge W: 4.305607807813432e-15
grad ChooseDest W: 2.393831253051758
grad AddEdge W: 1.4953033268627237e-17
grad ChooseDest W: 2.867811679840088
grad AddEdge W: 3.887654249537154e-17
grad ChooseDest W: 6.258433818817139
grad AddEdge W: 1.112535176416977e-17
grad ChooseDest W: 2.1574079990386963
grad AddEdge W: 1.9312840358925087e-15
grad ChooseDest W: 1.415130853652954
grad AddEdge W: 4.153260289866706e-17
grad ChooseDest W: 3.509079933166504
grad AddEdge W: 7.47015466249102e-15
grad ChooseDest W: 3.0837275981903076
grad AddEdge W: 3.458043440028958e-17
grad ChooseDest W: 1.369488000869751
grad AddEdge W: 8.595436916478821e-14
grad ChooseDest W: 2.5392017364501953
grad AddEdge W: 1.7590788919355678e-14
grad ChooseDest W: 1.85719633102417
grad AddEdge W: 2.51680525920929e-14
grad ChooseDest W: 2.4464492797851562
grad AddEdge W: 6.72022440763965e-17
grad ChooseDest W: 1.9005680084228516
grad AddEdge W: 4.21857400781275e-16
grad ChooseDest W: 1.58010995388031
grad AddEdge W: 3.113889625750571e-15
grad ChooseDest W: 2.2348132133483887
grad AddEdge W: 2.3406927437963354e-13
grad ChooseDest W: 0.7208747267723083
grad AddEdge W: 4.608980281169085e-15
grad ChooseDest W: 1.3071515560150146
grad AddEdge W: 1.6956287695579584e-17
grad ChooseDest W: 1.8490289449691772
grad AddEdge W: 5.335166389329014e-17
grad ChooseDest W: 1.9837740659713745
grad AddEdge W: 4.6025053892667056e-17
grad ChooseDest W: 2.368440628051758
grad AddEdge W: 1.0220588723912536e-16
grad ChooseDest W: 1.9293937683105469
grad AddEdge W: 1.4903058171214512e-13
grad ChooseDest W: 3.1925153732299805
grad AddEdge W: 3.547477109362338e-15
grad ChooseDest W: 1.7791721820831299
grad AddEdge W: 3.097045867374651e-14
grad ChooseDest W: 1.6776952743530273
grad AddEdge W: 1.0892743778272306e-16
grad ChooseDest W: 3.396498203277588
grad AddEdge W: 9.9289075022207e-15
grad ChooseDest W: 2.7926928997039795
grad AddEdge W: 7.864295265883783e-17
grad ChooseDest W: 1.453713059425354
grad AddEdge W: 2.224137842350921e-15
grad ChooseDest W: 3.7028250694274902
grad AddEdge W: 6.598248592740597e-15
grad ChooseDest W: 1.787916898727417
grad AddEdge W: 3.3882879607168514e-17
grad ChooseDest W: 2.309297561645508
grad AddEdge W: 4.305806860556037e-15
grad ChooseDest W: 1.5612300634384155
grad AddEdge W: 1.8934326348738864e-17
grad ChooseDest W: 1.8907248973846436
grad AddEdge W: 8.912769357043529e-18
grad ChooseDest W: 1.986617922782898
grad AddEdge W: 4.915261783302964e-16
grad ChooseDest W: 1.4749256372451782
grad AddEdge W: 4.627304568433511e-15
grad ChooseDest W: 2.630760669708252
grad AddEdge W: 1.3493267936985507e-17
grad ChooseDest W: 2.6853208541870117
grad AddEdge W: 5.3629221899377695e-15
grad ChooseDest W: 1.6408113241195679
grad AddEdge W: 8.775257945422762e-14
grad ChooseDest W: 1.2913823127746582
grad AddEdge W: 3.355510101763825e-17
grad ChooseDest W: 2.4203920364379883
grad AddEdge W: 6.016408188153801e-17
grad ChooseDest W: 2.730778932571411
grad AddEdge W: 8.754042496481341e-18
grad ChooseDest W: 3.0629892349243164
grad AddEdge W: 1.958270293833794e-15
grad ChooseDest W: 2.1132287979125977
grad AddEdge W: 1.6872897963666888e-17
grad ChooseDest W: 1.8747276067733765
grad AddEdge W: 8.933939390460599e-18
grad ChooseDest W: 3.761890411376953
grad AddEdge W: 2.2060403481581224e-15
grad ChooseDest W: 2.620440721511841
grad AddEdge W: 1.8093374833348053e-17
grad ChooseDest W: 1.7755661010742188
grad AddEdge W: 7.292945761183667e-12
grad ChooseDest W: 2.11622953414917
=== Epoch 43: Train Loss: 5.9842, Train Log Prob: 0.0072 ===
Total mismatches: 91302
Predicted valid destination but wrong order: 19874
Epoch 43: Validation Loss: 6.2570, Validation Log Prob: 0.0040
Epoch 43: Edge Precision: 0.3590, Recall: 0.3561, F1: 0.3574, Jaccard: 0.2350
Epoch 43: TP: 2.493915533285612, FP: 4.476020042949177, FN: 4.52755905511811
Epoch 43: Current Learning Rate: 1e-06
[Epoch 43] ‚è±Ô∏è Total: 1876.74s | Current time: 2025-07-15 10:15:09 | üèãÔ∏è Train: 1641.81s | ‚úÖ Val: 234.93s
grad AddEdge W: 1.409613003172704e-14
grad ChooseDest W: 4.495230674743652
grad AddEdge W: 1.3788934729273838e-13
grad ChooseDest W: 1.826149582862854
grad AddEdge W: 1.1862068596211549e-17
grad ChooseDest W: 2.516622543334961
grad AddEdge W: 4.4716186421787495e-15
grad ChooseDest W: 1.7238097190856934
grad AddEdge W: 8.547455528159652e-16
grad ChooseDest W: 1.5709004402160645
grad AddEdge W: 1.48081674735896e-17
grad ChooseDest W: 2.420147657394409
grad AddEdge W: 1.0302324425472371e-17
grad ChooseDest W: 2.007441759109497
grad AddEdge W: 2.287118130111067e-16
grad ChooseDest W: 2.189206838607788
grad AddEdge W: 3.5411042587779606e-12
grad ChooseDest W: 1.2935419082641602
grad AddEdge W: 1.9800586282151347e-16
grad ChooseDest W: 2.126643180847168
grad AddEdge W: 4.5773757878411325e-15
grad ChooseDest W: 2.0494487285614014
grad AddEdge W: 2.4276730833125513e-15
grad ChooseDest W: 2.3144960403442383
grad AddEdge W: 7.168429244701303e-16
grad ChooseDest W: 2.775574207305908
grad AddEdge W: 1.0207897126338013e-16
grad ChooseDest W: 1.6979278326034546
grad AddEdge W: 5.476007864659797e-15
grad ChooseDest W: 3.220745801925659
grad AddEdge W: 5.52244537396694e-17
grad ChooseDest W: 1.8428630828857422
grad AddEdge W: 1.6966391806023592e-15
grad ChooseDest W: 2.2779579162597656
grad AddEdge W: 5.646477798096816e-17
grad ChooseDest W: 3.0792856216430664
grad AddEdge W: 4.563587362009253e-15
grad ChooseDest W: 4.964167594909668
grad AddEdge W: 9.0753434227233e-14
grad ChooseDest W: 1.1565731763839722
grad AddEdge W: 1.1298094703903302e-16
grad ChooseDest W: 2.170778512954712
grad AddEdge W: 3.1809151046387874e-17
grad ChooseDest W: 2.0889053344726562
grad AddEdge W: 2.315131627259135e-17
grad ChooseDest W: 2.595768928527832
grad AddEdge W: 2.1353321751560362e-17
grad ChooseDest W: 2.633600950241089
grad AddEdge W: 9.019064547298431e-17
grad ChooseDest W: 2.6778738498687744
grad AddEdge W: 1.8874458457073624e-15
grad ChooseDest W: 1.6821712255477905
grad AddEdge W: 2.1864358897079298e-17
grad ChooseDest W: 2.1180715560913086
grad AddEdge W: 4.050940695071857e-17
grad ChooseDest W: 1.9617207050323486
grad AddEdge W: 5.506245207106211e-17
grad ChooseDest W: 2.0770280361175537
grad AddEdge W: 1.9171596005607833e-13
grad ChooseDest W: 1.3057183027267456
grad AddEdge W: 6.437036147081916e-17
grad ChooseDest W: 2.083467960357666
grad AddEdge W: 4.281804911446165e-15
grad ChooseDest W: 1.3107560873031616
grad AddEdge W: 2.671239498408114e-17
grad ChooseDest W: 2.028195381164551
grad AddEdge W: 3.285158721538435e-17
grad ChooseDest W: 2.2679805755615234
grad AddEdge W: 2.9511760788906776e-15
grad ChooseDest W: 1.8473416566848755
grad AddEdge W: 1.0384373793920486e-16
grad ChooseDest W: 2.023287296295166
grad AddEdge W: 3.2555810854090336e-15
grad ChooseDest W: 1.7884752750396729
grad AddEdge W: 5.971432723888068e-17
grad ChooseDest W: 2.168408155441284
grad AddEdge W: 2.848460628541885e-17
grad ChooseDest W: 2.582211971282959
grad AddEdge W: 4.747864514821669e-17
grad ChooseDest W: 3.5662755966186523
grad AddEdge W: 1.6006579598215934e-15
grad ChooseDest W: 2.150374412536621
grad AddEdge W: 2.695963781333536e-15
grad ChooseDest W: 1.324570894241333
grad AddEdge W: 1.8982570830785407e-16
grad ChooseDest W: 1.6230615377426147
grad AddEdge W: 2.9278841948535115e-17
grad ChooseDest W: 1.8667757511138916
grad AddEdge W: 1.0782165612241727e-16
grad ChooseDest W: 3.189100980758667
grad AddEdge W: 3.195446682767874e-17
grad ChooseDest W: 1.92189621925354
grad AddEdge W: 2.397948685490148e-16
grad ChooseDest W: 3.9264540672302246
grad AddEdge W: 1.1780024951853272e-15
grad ChooseDest W: 2.5641579627990723
grad AddEdge W: 2.907077002557777e-15
grad ChooseDest W: 1.8023985624313354
grad AddEdge W: 2.1007588114956227e-15
grad ChooseDest W: 2.157493829727173
grad AddEdge W: 4.227948015560895e-15
grad ChooseDest W: 2.658079147338867
grad AddEdge W: 8.732767980967893e-12
grad ChooseDest W: 1.6315817832946777
grad AddEdge W: 4.2010484990873646e-17
grad ChooseDest W: 3.1878609657287598
grad AddEdge W: 8.499551447478787e-17
grad ChooseDest W: 2.788611888885498
grad AddEdge W: 5.716624699274783e-17
grad ChooseDest W: 1.7894501686096191
grad AddEdge W: 8.903944167088201e-17
grad ChooseDest W: 1.4150797128677368
grad AddEdge W: 1.2975317612953547e-15
grad ChooseDest W: 3.046030044555664
grad AddEdge W: 7.279756009186242e-18
grad ChooseDest W: 2.231215238571167
grad AddEdge W: 2.1838038274685657e-15
grad ChooseDest W: 1.2036727666854858
grad AddEdge W: 8.89902244244351e-18
grad ChooseDest W: 1.7398066520690918
grad AddEdge W: 1.6495895708768167e-15
grad ChooseDest W: 2.7765543460845947
grad AddEdge W: 5.613187418036252e-17
grad ChooseDest W: 5.118206977844238
grad AddEdge W: 8.256078925985427e-14
grad ChooseDest W: 1.317558765411377
grad AddEdge W: 3.0165665591975666e-17
grad ChooseDest W: 2.8388257026672363
grad AddEdge W: 3.9196572056923406e-17
grad ChooseDest W: 2.0651328563690186
grad AddEdge W: 9.39396490317343e-16
grad ChooseDest W: 1.5877976417541504
=== Epoch 44: Train Loss: 5.9907, Train Log Prob: 0.0071 ===
Total mismatches: 91266
Predicted valid destination but wrong order: 19542
Epoch 44: Validation Loss: 6.2751, Validation Log Prob: 0.0038
Epoch 44: Edge Precision: 0.3600, Recall: 0.3574, F1: 0.3586, Jaccard: 0.2351
Epoch 44: TP: 2.500787401574803, FP: 4.474588403722262, FN: 4.520687186828919
Epoch 44: Current Learning Rate: 1e-06
[Epoch 44] ‚è±Ô∏è Total: 1870.56s | Current time: 2025-07-15 10:46:20 | üèãÔ∏è Train: 1635.89s | ‚úÖ Val: 234.67s
grad AddEdge W: 9.06279717070857e-14
grad ChooseDest W: 3.4121432304382324
grad AddEdge W: 6.95622499210144e-18
grad ChooseDest W: 2.4512722492218018
grad AddEdge W: 7.173492913539108e-17
grad ChooseDest W: 2.997084140777588
grad AddEdge W: 6.83714110650299e-15
grad ChooseDest W: 2.435173273086548
grad AddEdge W: 4.442780121705166e-17
grad ChooseDest W: 1.5902646780014038
grad AddEdge W: 4.5970112556381284e-17
grad ChooseDest W: 2.0509843826293945
grad AddEdge W: 2.9602223907673536e-15
grad ChooseDest W: 2.581791400909424
grad AddEdge W: 6.403972268925404e-15
grad ChooseDest W: 1.7207156419754028
grad AddEdge W: 2.968168089208191e-16
grad ChooseDest W: 1.4512264728546143
grad AddEdge W: 1.0045424295653817e-16
grad ChooseDest W: 2.732670783996582
grad AddEdge W: 3.2838319238359e-17
grad ChooseDest W: 4.599174499511719
grad AddEdge W: 4.989787050724848e-17
grad ChooseDest W: 2.5567262172698975
grad AddEdge W: 7.780766567628178e-17
grad ChooseDest W: 1.8938283920288086
grad AddEdge W: 5.759541476431994e-17
grad ChooseDest W: 3.7864606380462646
grad AddEdge W: 5.4016247267194844e-17
grad ChooseDest W: 2.9132182598114014
grad AddEdge W: 1.9377622365173382e-17
grad ChooseDest W: 2.4463324546813965
grad AddEdge W: 9.471337127740375e-17
grad ChooseDest W: 2.6478922367095947
grad AddEdge W: 1.6019828252302176e-17
grad ChooseDest W: 1.7676159143447876
grad AddEdge W: 1.387845947847185e-17
grad ChooseDest W: 2.86154842376709
grad AddEdge W: 2.961336959070998e-14
grad ChooseDest W: 1.6285393238067627
grad AddEdge W: 5.338622508746938e-15
grad ChooseDest W: 2.2751259803771973
grad AddEdge W: 3.180145559324339e-14
grad ChooseDest W: 1.8007920980453491
grad AddEdge W: 4.6426712945788105e-17
grad ChooseDest W: 1.8100615739822388
grad AddEdge W: 4.6288527461377447e-17
grad ChooseDest W: 2.5683062076568604
grad AddEdge W: 1.7627768101431755e-17
grad ChooseDest W: 2.068418264389038
grad AddEdge W: 9.209477553841198e-17
grad ChooseDest W: 1.1277352571487427
grad AddEdge W: 1.0427243454870006e-15
grad ChooseDest W: 2.1601979732513428
grad AddEdge W: 4.685122865359522e-17
grad ChooseDest W: 3.101879835128784
grad AddEdge W: 1.0714840786615995e-15
grad ChooseDest W: 2.418043851852417
grad AddEdge W: 3.078202102744608e-17
grad ChooseDest W: 4.297138214111328
grad AddEdge W: 6.245274293510206e-15
grad ChooseDest W: 2.75116229057312
grad AddEdge W: 2.620118477649619e-13
grad ChooseDest W: 1.4681042432785034
grad AddEdge W: 1.2816714928744915e-15
grad ChooseDest W: 3.0921878814697266
grad AddEdge W: 5.969267495916649e-17
grad ChooseDest W: 2.610499620437622
grad AddEdge W: 9.228346205358022e-18
grad ChooseDest W: 4.814614772796631
grad AddEdge W: 4.974229040717257e-16
grad ChooseDest W: 2.3242483139038086
grad AddEdge W: 1.5831962300301582e-17
grad ChooseDest W: 2.324057102203369
grad AddEdge W: 2.51169594588354e-16
grad ChooseDest W: 2.405921459197998
grad AddEdge W: 2.971769355662561e-15
grad ChooseDest W: 1.891601324081421
grad AddEdge W: 1.2394997903822313e-17
grad ChooseDest W: 3.4536542892456055
grad AddEdge W: 2.573279476313664e-17
grad ChooseDest W: 1.8270671367645264
grad AddEdge W: 3.933529527490667e-15
grad ChooseDest W: 1.9600857496261597
grad AddEdge W: 4.714352946665308e-18
grad ChooseDest W: 1.8934723138809204
grad AddEdge W: 1.175551863432798e-17
grad ChooseDest W: 1.700666069984436
grad AddEdge W: 2.1843234821817062e-14
grad ChooseDest W: 3.114811897277832
grad AddEdge W: 5.222698003102188e-15
grad ChooseDest W: 1.6775643825531006
grad AddEdge W: 3.595236638576798e-17
grad ChooseDest W: 2.9119138717651367
grad AddEdge W: 2.484745839043746e-17
grad ChooseDest W: 1.7879055738449097
grad AddEdge W: 8.082667564759949e-13
grad ChooseDest W: 1.093120813369751
grad AddEdge W: 1.2972106299292269e-13
grad ChooseDest W: 1.6242635250091553
grad AddEdge W: 9.042515944844922e-18
grad ChooseDest W: 1.8330045938491821
grad AddEdge W: 2.4649164999635022e-17
grad ChooseDest W: 4.652018070220947
grad AddEdge W: 8.024676851630546e-16
grad ChooseDest W: 2.7100565433502197
grad AddEdge W: 1.1793315957586877e-14
grad ChooseDest W: 1.8684736490249634
grad AddEdge W: 2.5070825279967603e-17
grad ChooseDest W: 3.021435260772705
grad AddEdge W: 6.1893652849479225e-18
grad ChooseDest W: 1.8682588338851929
grad AddEdge W: 3.189985371666003e-15
grad ChooseDest W: 1.7771023511886597
grad AddEdge W: 2.8629931992877065e-17
grad ChooseDest W: 1.4074608087539673
grad AddEdge W: 4.1444690143164925e-17
grad ChooseDest W: 3.406961679458618
grad AddEdge W: 2.2487428789192375e-15
grad ChooseDest W: 2.5449585914611816
grad AddEdge W: 6.867716004813775e-17
grad ChooseDest W: 3.1438024044036865
grad AddEdge W: 4.198055428758903e-17
grad ChooseDest W: 4.386298656463623
grad AddEdge W: 9.012076525483583e-17
grad ChooseDest W: 2.7788050174713135
grad AddEdge W: 7.947890800332882e-17
grad ChooseDest W: 2.1995818614959717
grad AddEdge W: 2.500403143812068e-15
grad ChooseDest W: 1.8890589475631714
grad AddEdge W: 4.9949936563725017e-17
grad ChooseDest W: 1.9299407005310059
=== Epoch 45: Train Loss: 5.9893, Train Log Prob: 0.0071 ===
Total mismatches: 91215
Predicted valid destination but wrong order: 19986
Epoch 45: Validation Loss: 6.2638, Validation Log Prob: 0.0039
Epoch 45: Edge Precision: 0.3577, Recall: 0.3548, F1: 0.3562, Jaccard: 0.2338
Epoch 45: TP: 2.4847530422333572, FP: 4.4850393700787405, FN: 4.536721546170365
Epoch 45: Current Learning Rate: 1e-06
[Epoch 45] ‚è±Ô∏è Total: 1897.69s | Current time: 2025-07-15 11:17:58 | üèãÔ∏è Train: 1660.62s | ‚úÖ Val: 237.07s
grad AddEdge W: 1.66953489391383e-16
grad ChooseDest W: 4.671994686126709
grad AddEdge W: 2.146785483353568e-17
grad ChooseDest W: 2.4508652687072754
grad AddEdge W: 1.5259943083919522e-16
grad ChooseDest W: 3.3599886894226074
grad AddEdge W: 7.396657938918416e-12
grad ChooseDest W: 1.7855242490768433
grad AddEdge W: 4.496702252362265e-15
grad ChooseDest W: 1.4963377714157104
grad AddEdge W: 8.458838941217907e-17
grad ChooseDest W: 1.790355920791626
grad AddEdge W: 2.481367798858202e-17
grad ChooseDest W: 2.2485318183898926
grad AddEdge W: 4.243676993874934e-15
grad ChooseDest W: 3.0809831619262695
grad AddEdge W: 1.5847122866568454e-17
grad ChooseDest W: 2.549556255340576
grad AddEdge W: 2.4001626509432586e-17
grad ChooseDest W: 3.2697863578796387
grad AddEdge W: 5.5793704547977366e-18
grad ChooseDest W: 2.9529306888580322
grad AddEdge W: 3.501113411035614e-17
grad ChooseDest W: 2.8938372135162354
grad AddEdge W: 2.366176102708978e-13
grad ChooseDest W: 1.375982403755188
grad AddEdge W: 1.0543591503458163e-16
grad ChooseDest W: 4.575196743011475
grad AddEdge W: 1.650816127524e-16
grad ChooseDest W: 1.8911569118499756
grad AddEdge W: 1.1139752747023747e-15
grad ChooseDest W: 2.3880455493927
grad AddEdge W: 1.7282211089725968e-13
grad ChooseDest W: 2.1692934036254883
grad AddEdge W: 1.0432449861994313e-17
grad ChooseDest W: 3.546748161315918
grad AddEdge W: 1.8815450568547643e-17
grad ChooseDest W: 4.956506729125977
grad AddEdge W: 5.753847694490771e-16
grad ChooseDest W: 2.0320889949798584
grad AddEdge W: 5.3261120703440085e-17
grad ChooseDest W: 2.5381627082824707
grad AddEdge W: 5.754288548670037e-17
grad ChooseDest W: 2.6095235347747803
grad AddEdge W: 6.148583047361404e-17
grad ChooseDest W: 3.0967299938201904
grad AddEdge W: 7.076748909096153e-12
grad ChooseDest W: 1.5893422365188599
grad AddEdge W: 1.6303691823111998e-17
grad ChooseDest W: 2.532898426055908
grad AddEdge W: 2.8197822767046714e-17
grad ChooseDest W: 2.2495720386505127
grad AddEdge W: 2.5962702996950854e-17
grad ChooseDest W: 3.1998291015625
grad AddEdge W: 4.790038814661053e-17
grad ChooseDest W: 2.060991048812866
grad AddEdge W: 1.5837760637872232e-15
grad ChooseDest W: 2.488399028778076
grad AddEdge W: 6.485837156244584e-17
grad ChooseDest W: 1.5320621728897095
grad AddEdge W: 1.202209638026671e-15
grad ChooseDest W: 2.8641555309295654
grad AddEdge W: 2.1755848550231204e-15
grad ChooseDest W: 2.5031492710113525
grad AddEdge W: 1.692517577838024e-17
grad ChooseDest W: 2.207378387451172
grad AddEdge W: 4.441065541731466e-17
grad ChooseDest W: 2.8800976276397705
grad AddEdge W: 1.1269736093768787e-14
grad ChooseDest W: 2.5732102394104004
grad AddEdge W: 8.979442595957141e-17
grad ChooseDest W: 1.9107317924499512
grad AddEdge W: 2.819762755242215e-17
grad ChooseDest W: 2.033306121826172
grad AddEdge W: 1.2336338276863063e-13
grad ChooseDest W: 1.6504534482955933
grad AddEdge W: 3.777032506202538e-12
grad ChooseDest W: 2.1060564517974854
grad AddEdge W: 2.5018966746563143e-15
grad ChooseDest W: 2.7507412433624268
grad AddEdge W: 3.57259015394077e-15
grad ChooseDest W: 1.9381603002548218
grad AddEdge W: 1.8052777205930645e-15
grad ChooseDest W: 2.0913219451904297
grad AddEdge W: 6.542569173120411e-17
grad ChooseDest W: 4.425637722015381
grad AddEdge W: 1.4817293459502877e-15
grad ChooseDest W: 1.5074913501739502
grad AddEdge W: 2.0326920908872144e-15
grad ChooseDest W: 1.9281185865402222
grad AddEdge W: 2.9034949002238386e-15
grad ChooseDest W: 1.8037067651748657
grad AddEdge W: 1.8847704921434597e-15
grad ChooseDest W: 1.7985951900482178
grad AddEdge W: 1.3354110515355098e-17
grad ChooseDest W: 4.288522243499756
grad AddEdge W: 1.6757110081789552e-15
grad ChooseDest W: 1.4517967700958252
grad AddEdge W: 4.015460933365987e-17
grad ChooseDest W: 3.070500135421753
grad AddEdge W: 1.2940886209844571e-11
grad ChooseDest W: 0.9561949968338013
grad AddEdge W: 6.830746363505058e-13
grad ChooseDest W: 1.8667123317718506
grad AddEdge W: 6.175268555666855e-17
grad ChooseDest W: 4.694464206695557
grad AddEdge W: 2.1755494913975725e-15
grad ChooseDest W: 2.3570122718811035
grad AddEdge W: 3.282720656313772e-15
grad ChooseDest W: 2.7292981147766113
grad AddEdge W: 3.2400065714938606e-17
grad ChooseDest W: 3.400089979171753
grad AddEdge W: 3.241132185636533e-15
grad ChooseDest W: 2.3600847721099854
grad AddEdge W: 2.7026414251302195e-17
grad ChooseDest W: 2.777569532394409
grad AddEdge W: 2.9176685142884816e-15
grad ChooseDest W: 2.7368249893188477
grad AddEdge W: 5.2543879011740256e-17
grad ChooseDest W: 2.624880313873291
grad AddEdge W: 6.287873645070434e-17
grad ChooseDest W: 1.9350334405899048
grad AddEdge W: 2.7334060919084142e-17
grad ChooseDest W: 2.410433769226074
grad AddEdge W: 4.576666821264281e-15
grad ChooseDest W: 2.2262392044067383
grad AddEdge W: 2.391506205832891e-16
grad ChooseDest W: 2.6512458324432373
grad AddEdge W: 3.152687122866607e-15
grad ChooseDest W: 2.756474733352661
grad AddEdge W: 3.6191861642881684e-17
grad ChooseDest W: 2.110633373260498
=== Epoch 46: Train Loss: 5.9833, Train Log Prob: 0.0072 ===
Total mismatches: 91140
Predicted valid destination but wrong order: 20163
Epoch 46: Validation Loss: 6.2687, Validation Log Prob: 0.0039
Epoch 46: Edge Precision: 0.3612, Recall: 0.3584, F1: 0.3597, Jaccard: 0.2361
Epoch 46: TP: 2.5092340730136007, FP: 4.461417322834646, FN: 4.512240515390122
Epoch 46: Current Learning Rate: 1e-06
[Epoch 46] ‚è±Ô∏è Total: 1874.52s | Current time: 2025-07-15 11:49:12 | üèãÔ∏è Train: 1638.42s | ‚úÖ Val: 236.10s
grad AddEdge W: 5.4315890333893075e-15
grad ChooseDest W: 3.5429062843322754
grad AddEdge W: 1.1572979394599983e-16
grad ChooseDest W: 2.2484290599823
grad AddEdge W: 1.9745684782883875e-15
grad ChooseDest W: 3.2148313522338867
grad AddEdge W: 1.8272933774416246e-15
grad ChooseDest W: 2.9840660095214844
grad AddEdge W: 2.924628742834748e-17
grad ChooseDest W: 2.435575246810913
grad AddEdge W: 2.6890194148026962e-17
grad ChooseDest W: 1.6981457471847534
grad AddEdge W: 3.837468367975504e-15
grad ChooseDest W: 3.2709591388702393
grad AddEdge W: 1.1960502261924746e-16
grad ChooseDest W: 4.177950859069824
grad AddEdge W: 3.06069002743237e-16
grad ChooseDest W: 1.9609875679016113
grad AddEdge W: 3.7786851152341134e-17
grad ChooseDest W: 2.9833245277404785
grad AddEdge W: 3.7668786009150215e-17
grad ChooseDest W: 2.632150650024414
grad AddEdge W: 1.2147709778157747e-17
grad ChooseDest W: 1.8243329524993896
grad AddEdge W: 5.975976261556699e-17
grad ChooseDest W: 3.543524742126465
grad AddEdge W: 5.1577487210817003e-17
grad ChooseDest W: 3.7565512657165527
grad AddEdge W: 5.397584895721784e-15
grad ChooseDest W: 2.1121368408203125
grad AddEdge W: 4.841724037183571e-17
grad ChooseDest W: 2.1981513500213623
grad AddEdge W: 3.357412286300452e-17
grad ChooseDest W: 4.297879695892334
grad AddEdge W: 9.368075870536195e-17
grad ChooseDest W: 2.1812541484832764
grad AddEdge W: 7.084005047331993e-15
grad ChooseDest W: 2.632798194885254
grad AddEdge W: 9.507333275141604e-15
grad ChooseDest W: 2.68257212638855
grad AddEdge W: 6.114305344521697e-17
grad ChooseDest W: 5.124396800994873
grad AddEdge W: 1.8780536267508515e-16
grad ChooseDest W: 2.678232431411743
grad AddEdge W: 3.733094850632461e-15
grad ChooseDest W: 1.690341830253601
grad AddEdge W: 2.4681413529170464e-15
grad ChooseDest W: 3.06551456451416
grad AddEdge W: 1.3286578434690277e-11
grad ChooseDest W: 2.707899570465088
grad AddEdge W: 6.978100279708772e-17
grad ChooseDest W: 2.5312905311584473
grad AddEdge W: 4.309079795864227e-15
grad ChooseDest W: 2.5121500492095947
grad AddEdge W: 1.3104933936352074e-17
grad ChooseDest W: 3.0838582515716553
grad AddEdge W: 3.656996987134661e-15
grad ChooseDest W: 2.6853535175323486
grad AddEdge W: 1.6532776582082803e-15
grad ChooseDest W: 2.470224142074585
grad AddEdge W: 2.046180031781055e-15
grad ChooseDest W: 3.093167781829834
grad AddEdge W: 1.971795557116873e-16
grad ChooseDest W: 2.4192821979522705
grad AddEdge W: 2.8097999272468305e-15
grad ChooseDest W: 1.4491455554962158
grad AddEdge W: 6.337831383601696e-17
grad ChooseDest W: 2.0496301651000977
grad AddEdge W: 9.211300494475142e-18
grad ChooseDest W: 2.3414595127105713
grad AddEdge W: 2.28198153703046e-17
grad ChooseDest W: 1.5901129245758057
grad AddEdge W: 8.107571069269098e-17
grad ChooseDest W: 4.484011650085449
grad AddEdge W: 2.08076692811629e-15
grad ChooseDest W: 2.2856838703155518
grad AddEdge W: 4.506632592011911e-18
grad ChooseDest W: 2.324871778488159
grad AddEdge W: 5.5003973710477064e-17
grad ChooseDest W: 2.939049482345581
grad AddEdge W: 2.3770439454391365e-17
grad ChooseDest W: 1.9750932455062866
grad AddEdge W: 9.60299145873211e-16
grad ChooseDest W: 2.529062032699585
grad AddEdge W: 1.749353323162665e-17
grad ChooseDest W: 2.6806962490081787
grad AddEdge W: 1.5826014408476183e-14
grad ChooseDest W: 1.7968531847000122
grad AddEdge W: 2.6197361100365694e-15
grad ChooseDest W: 2.748659133911133
grad AddEdge W: 4.000749029863364e-17
grad ChooseDest W: 2.3781988620758057
grad AddEdge W: 3.197596028871532e-17
grad ChooseDest W: 2.6345980167388916
grad AddEdge W: 6.552859878967e-18
grad ChooseDest W: 2.8557493686676025
grad AddEdge W: 4.9261992868167986e-14
grad ChooseDest W: 2.2636547088623047
grad AddEdge W: 1.3216627445244988e-11
grad ChooseDest W: 1.7983275651931763
grad AddEdge W: 3.9917870242347196e-17
grad ChooseDest W: 1.8953418731689453
grad AddEdge W: 2.9382451283341057e-17
grad ChooseDest W: 3.3905792236328125
grad AddEdge W: 2.1050297633739157e-15
grad ChooseDest W: 2.035508871078491
grad AddEdge W: 2.0793380364738311e-16
grad ChooseDest W: 2.0069262981414795
grad AddEdge W: 2.797744861697279e-17
grad ChooseDest W: 5.14705753326416
grad AddEdge W: 3.3877036138623643e-15
grad ChooseDest W: 2.3890163898468018
grad AddEdge W: 2.2075796187815203e-15
grad ChooseDest W: 1.554616093635559
grad AddEdge W: 2.8019095506453607e-17
grad ChooseDest W: 2.453157424926758
grad AddEdge W: 9.512931077661991e-17
grad ChooseDest W: 2.4036500453948975
grad AddEdge W: 5.752659806845806e-11
grad ChooseDest W: 2.8003716468811035
grad AddEdge W: 2.7842068102019296e-18
grad ChooseDest W: 2.235100746154785
grad AddEdge W: 7.211716598772938e-17
grad ChooseDest W: 3.4544057846069336
grad AddEdge W: 2.0382361332852306e-15
grad ChooseDest W: 2.1846439838409424
grad AddEdge W: 1.00975072927893e-15
grad ChooseDest W: 1.7714147567749023
grad AddEdge W: 1.7106876624897044e-13
grad ChooseDest W: 2.0429420471191406
grad AddEdge W: 5.556017988052507e-17
grad ChooseDest W: 2.281496286392212
=== Epoch 47: Train Loss: 5.9924, Train Log Prob: 0.0071 ===
Total mismatches: 91180
Predicted valid destination but wrong order: 19987
Epoch 47: Validation Loss: 6.2848, Validation Log Prob: 0.0039
Epoch 47: Edge Precision: 0.3582, Recall: 0.3556, F1: 0.3568, Jaccard: 0.2345
Epoch 47: TP: 2.48790264853257, FP: 4.486184681460272, FN: 4.533571939871153
Epoch 47: Current Learning Rate: 1e-06
[Epoch 47] ‚è±Ô∏è Total: 1886.15s | Current time: 2025-07-15 12:20:38 | üèãÔ∏è Train: 1650.75s | ‚úÖ Val: 235.40s
grad AddEdge W: 6.2017952452946955e-15
grad ChooseDest W: 5.334691524505615
grad AddEdge W: 1.4836965799702858e-15
grad ChooseDest W: 1.9783554077148438
grad AddEdge W: 3.588013776877324e-15
grad ChooseDest W: 3.43475604057312
grad AddEdge W: 3.1232308374417895e-17
grad ChooseDest W: 2.787860155105591
grad AddEdge W: 8.145016708418761e-18
grad ChooseDest W: 2.005404233932495
grad AddEdge W: 5.405710482784794e-15
grad ChooseDest W: 2.7798383235931396
grad AddEdge W: 5.943502474196847e-17
grad ChooseDest W: 2.047551393508911
grad AddEdge W: 5.846382702168629e-18
grad ChooseDest W: 1.8747129440307617
grad AddEdge W: 3.885159141937449e-17
grad ChooseDest W: 1.9637526273727417
grad AddEdge W: 1.7781117460831736e-17
grad ChooseDest W: 4.0362467765808105
grad AddEdge W: 3.600511544726154e-13
grad ChooseDest W: 2.0538182258605957
grad AddEdge W: 4.732841062013134e-15
grad ChooseDest W: 1.281239628791809
grad AddEdge W: 1.0269663266483653e-13
grad ChooseDest W: 3.981515407562256
grad AddEdge W: 1.8229771940039483e-14
grad ChooseDest W: 2.418428897857666
grad AddEdge W: 2.89814572834325e-17
grad ChooseDest W: 2.854215621948242
grad AddEdge W: 4.848114503723911e-17
grad ChooseDest W: 3.824503183364868
grad AddEdge W: 9.41885549572409e-17
grad ChooseDest W: 2.2171530723571777
grad AddEdge W: 3.318453376468405e-15
grad ChooseDest W: 2.0087084770202637
grad AddEdge W: 2.448165265378691e-17
grad ChooseDest W: 2.2433269023895264
grad AddEdge W: 2.2512043566639585e-15
grad ChooseDest W: 6.0412397384643555
grad AddEdge W: 1.4091601962997607e-13
grad ChooseDest W: 2.3069000244140625
grad AddEdge W: 3.0010370968472757e-15
grad ChooseDest W: 1.984342098236084
grad AddEdge W: 2.8349583939670593e-17
grad ChooseDest W: 2.4664251804351807
grad AddEdge W: 2.5797345555860114e-15
grad ChooseDest W: 1.6589176654815674
grad AddEdge W: 4.3578185481052726e-16
grad ChooseDest W: 3.1533148288726807
grad AddEdge W: 8.70079936199314e-18
grad ChooseDest W: 1.7259304523468018
grad AddEdge W: 2.7657688716300613e-17
grad ChooseDest W: 3.8395960330963135
grad AddEdge W: 3.0644394187733914e-15
grad ChooseDest W: 2.956467390060425
grad AddEdge W: 6.54947738854976e-16
grad ChooseDest W: 2.6709446907043457
grad AddEdge W: 2.1864504480867107e-17
grad ChooseDest W: 3.2064969539642334
grad AddEdge W: 2.876131805265254e-17
grad ChooseDest W: 2.540844440460205
grad AddEdge W: 4.457444048732261e-17
grad ChooseDest W: 2.0240209102630615
grad AddEdge W: 3.6488745510342147e-13
grad ChooseDest W: 1.5959430932998657
grad AddEdge W: 9.819648821616041e-18
grad ChooseDest W: 5.248563289642334
grad AddEdge W: 6.820798320469768e-17
grad ChooseDest W: 2.5221376419067383
grad AddEdge W: 2.862239101736634e-15
grad ChooseDest W: 2.8146393299102783
grad AddEdge W: 8.230441800946705e-17
grad ChooseDest W: 3.1139161586761475
grad AddEdge W: 2.0724293909105637e-17
grad ChooseDest W: 2.7385480403900146
grad AddEdge W: 1.1427061756695424e-17
grad ChooseDest W: 2.2799108028411865
grad AddEdge W: 9.69914388604734e-14
grad ChooseDest W: 1.7915595769882202
grad AddEdge W: 5.687793815332615e-17
grad ChooseDest W: 3.1280267238616943
grad AddEdge W: 6.710161920924065e-17
grad ChooseDest W: 2.312471866607666
grad AddEdge W: 1.6732930467518994e-15
grad ChooseDest W: 3.3199715614318848
grad AddEdge W: 7.227931324012448e-17
grad ChooseDest W: 2.0119130611419678
grad AddEdge W: 4.609073865074867e-17
grad ChooseDest W: 3.1447315216064453
grad AddEdge W: 8.984350357793091e-16
grad ChooseDest W: 2.4154226779937744
grad AddEdge W: 1.2035353504682425e-15
grad ChooseDest W: 2.5968029499053955
grad AddEdge W: 1.4903288280949523e-17
grad ChooseDest W: 2.4030652046203613
grad AddEdge W: 1.4757600930127499e-15
grad ChooseDest W: 2.5725276470184326
grad AddEdge W: 3.05241822130684e-17
grad ChooseDest W: 2.959592580795288
grad AddEdge W: 6.761260176083916e-18
grad ChooseDest W: 2.9369258880615234
grad AddEdge W: 6.214525224049642e-17
grad ChooseDest W: 1.993392825126648
grad AddEdge W: 4.588355028903443e-15
grad ChooseDest W: 4.98207426071167
grad AddEdge W: 1.490570146458136e-15
grad ChooseDest W: 2.7837424278259277
grad AddEdge W: 3.732713262289723e-15
grad ChooseDest W: 1.7177584171295166
grad AddEdge W: 3.4656503385257836e-15
grad ChooseDest W: 3.0658154487609863
grad AddEdge W: 1.298050032962511e-17
grad ChooseDest W: 2.572666883468628
grad AddEdge W: 3.0385457406898356e-17
grad ChooseDest W: 3.462505578994751
grad AddEdge W: 1.4029405011307082e-14
grad ChooseDest W: 2.741534471511841
grad AddEdge W: 1.7074344037568785e-15
grad ChooseDest W: 2.237597703933716
grad AddEdge W: 2.5113403574818158e-17
grad ChooseDest W: 3.035604953765869
grad AddEdge W: 3.0745803089761567e-15
grad ChooseDest W: 2.4850542545318604
grad AddEdge W: 1.1591668117789966e-15
grad ChooseDest W: 2.289100170135498
grad AddEdge W: 1.5910272163871357e-14
grad ChooseDest W: 2.243934392929077
grad AddEdge W: 1.0753165935808995e-13
grad ChooseDest W: 1.557615041732788
grad AddEdge W: 2.7581465677215077e-17
grad ChooseDest W: 2.6628222465515137
=== Epoch 48: Train Loss: 5.9837, Train Log Prob: 0.0073 ===
Total mismatches: 91247
Predicted valid destination but wrong order: 19842
Epoch 48: Validation Loss: 6.2793, Validation Log Prob: 0.0039
Epoch 48: Edge Precision: 0.3594, Recall: 0.3567, F1: 0.3579, Jaccard: 0.2356
Epoch 48: TP: 2.4960629921259843, FP: 4.478883321403006, FN: 4.525411596277738
Epoch 48: Current Learning Rate: 1e-06
[Epoch 48] ‚è±Ô∏è Total: 1889.05s | Current time: 2025-07-15 12:52:07 | üèãÔ∏è Train: 1653.56s | ‚úÖ Val: 235.48s
grad AddEdge W: 2.394284484487797e-14
grad ChooseDest W: 5.421799182891846
grad AddEdge W: 2.116622468048275e-15
grad ChooseDest W: 1.539116621017456
grad AddEdge W: 2.7091591120487698e-17
grad ChooseDest W: 2.47969913482666
grad AddEdge W: 2.1133900513552096e-17
grad ChooseDest W: 6.420285224914551
grad AddEdge W: 7.493866502816457e-15
grad ChooseDest W: 1.6188994646072388
grad AddEdge W: 3.7901690291143096e-17
grad ChooseDest W: 1.8112746477127075
grad AddEdge W: 2.8751749227326525e-17
grad ChooseDest W: 2.178618907928467
grad AddEdge W: 7.45540863944249e-18
grad ChooseDest W: 1.4130089282989502
grad AddEdge W: 1.9153858428949212e-17
grad ChooseDest W: 2.6146512031555176
grad AddEdge W: 3.040319215923149e-17
grad ChooseDest W: 1.9943454265594482
grad AddEdge W: 1.4858049773248998e-17
grad ChooseDest W: 1.9878135919570923
grad AddEdge W: 2.1283876630414085e-17
grad ChooseDest W: 5.591672897338867
grad AddEdge W: 3.0385193635544625e-15
grad ChooseDest W: 2.550279140472412
grad AddEdge W: 5.143730656676887e-17
grad ChooseDest W: 1.6899166107177734
grad AddEdge W: 4.311296123745167e-17
grad ChooseDest W: 2.9567999839782715
grad AddEdge W: 6.5800009156357835e-18
grad ChooseDest W: 1.9897099733352661
grad AddEdge W: 6.155586951044013e-17
grad ChooseDest W: 1.9937727451324463
grad AddEdge W: 8.572413486299398e-17
grad ChooseDest W: 1.7982220649719238
grad AddEdge W: 1.0591643867844357e-14
grad ChooseDest W: 4.890554428100586
grad AddEdge W: 2.2049849451058435e-15
grad ChooseDest W: 1.5446685552597046
grad AddEdge W: 1.4981527986368464e-17
grad ChooseDest W: 2.756420612335205
grad AddEdge W: 1.0110584210945486e-14
grad ChooseDest W: 3.2923264503479004
grad AddEdge W: 5.058927316772474e-14
grad ChooseDest W: 2.0112321376800537
grad AddEdge W: 2.6378899319203604e-15
grad ChooseDest W: 1.4995511770248413
grad AddEdge W: 2.437804232636423e-15
grad ChooseDest W: 3.2271740436553955
grad AddEdge W: 1.0624413666749494e-15
grad ChooseDest W: 2.0955004692077637
grad AddEdge W: 2.2836569086431248e-17
grad ChooseDest W: 1.9292874336242676
grad AddEdge W: 1.7954029641718595e-17
grad ChooseDest W: 2.5477488040924072
grad AddEdge W: 1.4660044969218669e-15
grad ChooseDest W: 2.1668171882629395
grad AddEdge W: 1.2046435574221043e-17
grad ChooseDest W: 2.464665174484253
grad AddEdge W: 1.7976770491118903e-17
grad ChooseDest W: 2.452465534210205
grad AddEdge W: 3.1929667243524307e-13
grad ChooseDest W: 2.1524879932403564
grad AddEdge W: 2.7932963373025277e-15
grad ChooseDest W: 3.9088971614837646
grad AddEdge W: 1.5072262023329446e-15
grad ChooseDest W: 4.100977897644043
grad AddEdge W: 1.8612867419088506e-17
grad ChooseDest W: 1.8649572134017944
grad AddEdge W: 5.3878240453796497e-17
grad ChooseDest W: 2.237765073776245
grad AddEdge W: 1.2900651757914753e-17
grad ChooseDest W: 1.5672016143798828
grad AddEdge W: 2.1115821984956382e-15
grad ChooseDest W: 2.1303462982177734
grad AddEdge W: 2.35654158574274e-15
grad ChooseDest W: 2.7749133110046387
grad AddEdge W: 4.244281920983061e-17
grad ChooseDest W: 2.278773307800293
grad AddEdge W: 1.9842171960266838e-17
grad ChooseDest W: 1.4324814081192017
grad AddEdge W: 1.5211127516378073e-16
grad ChooseDest W: 2.007255792617798
grad AddEdge W: 1.5500811990245655e-15
grad ChooseDest W: 4.031698226928711
grad AddEdge W: 1.2699197704697796e-13
grad ChooseDest W: 2.6005425453186035
grad AddEdge W: 8.456332749397994e-18
grad ChooseDest W: 3.8637826442718506
grad AddEdge W: 6.32981315796475e-16
grad ChooseDest W: 3.3660101890563965
grad AddEdge W: 5.729201815052529e-17
grad ChooseDest W: 2.6627490520477295
grad AddEdge W: 4.112881977808187e-15
grad ChooseDest W: 1.8714091777801514
grad AddEdge W: 9.525273009447976e-16
grad ChooseDest W: 2.9765870571136475
grad AddEdge W: 2.2693747380418683e-13
grad ChooseDest W: 3.847888469696045
grad AddEdge W: 5.54062184074718e-17
grad ChooseDest W: 1.6064940690994263
grad AddEdge W: 1.671685563506468e-16
grad ChooseDest W: 2.028275728225708
grad AddEdge W: 2.3185521845281643e-17
grad ChooseDest W: 3.302234411239624
grad AddEdge W: 2.2071653667307537e-17
grad ChooseDest W: 2.6392717361450195
grad AddEdge W: 2.869007464085457e-17
grad ChooseDest W: 3.2299678325653076
grad AddEdge W: 6.848196858463239e-18
grad ChooseDest W: 2.210216760635376
grad AddEdge W: 2.5810705250671785e-17
grad ChooseDest W: 3.26867413520813
grad AddEdge W: 4.3343860433639724e-17
grad ChooseDest W: 6.396667957305908
grad AddEdge W: 1.4382959316348106e-15
grad ChooseDest W: 1.4542559385299683
grad AddEdge W: 6.116789533337316e-17
grad ChooseDest W: 2.4732606410980225
grad AddEdge W: 3.310297719735767e-15
grad ChooseDest W: 1.9436225891113281
grad AddEdge W: 1.7824840904522309e-16
grad ChooseDest W: 3.192032814025879
grad AddEdge W: 8.639231150947961e-17
grad ChooseDest W: 3.0477898120880127
grad AddEdge W: 7.240307190055882e-15
grad ChooseDest W: 4.435324668884277
grad AddEdge W: 1.47065756653849e-15
grad ChooseDest W: 2.8147833347320557
grad AddEdge W: 1.043138875470453e-16
grad ChooseDest W: 1.3714689016342163
=== Epoch 49: Train Loss: 5.9887, Train Log Prob: 0.0071 ===
Total mismatches: 91222
Predicted valid destination but wrong order: 20028
Epoch 49: Validation Loss: 6.2774, Validation Log Prob: 0.0039
Epoch 49: Edge Precision: 0.3589, Recall: 0.3566, F1: 0.3577, Jaccard: 0.2348
Epoch 49: TP: 2.49663564781675, FP: 4.4833214030064426, FN: 4.524838940586972
Epoch 49: Current Learning Rate: 1e-06
[Epoch 49] ‚è±Ô∏è Total: 1889.25s | Current time: 2025-07-15 13:23:37 | üèãÔ∏è Train: 1652.90s | ‚úÖ Val: 236.34s
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:3638: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
grad AddEdge W: 3.2935763491503955e-13
grad ChooseDest W: 5.2414069175720215
grad AddEdge W: 1.1097359806804853e-14
grad ChooseDest W: 1.7530885934829712
grad AddEdge W: 1.3610735524901917e-16
grad ChooseDest W: 1.8100701570510864
grad AddEdge W: 3.69197905080957e-17
grad ChooseDest W: 2.701598644256592
grad AddEdge W: 1.5739634054689638e-17
grad ChooseDest W: 2.140028715133667
grad AddEdge W: 4.2317516170105405e-15
grad ChooseDest W: 3.108232259750366
grad AddEdge W: 2.3509995021084144e-17
grad ChooseDest W: 2.205937623977661
grad AddEdge W: 2.8661904177913465e-17
grad ChooseDest W: 2.4303553104400635
grad AddEdge W: 4.576473684517417e-17
grad ChooseDest W: 2.537193536758423
grad AddEdge W: 1.6229440319385645e-15
grad ChooseDest W: 1.9260458946228027
grad AddEdge W: 1.6340471515694107e-15
grad ChooseDest W: 2.051017999649048
grad AddEdge W: 5.950751885085262e-17
grad ChooseDest W: 1.7259482145309448
grad AddEdge W: 1.331130487818499e-15
grad ChooseDest W: 3.1614577770233154
grad AddEdge W: 8.219926680999931e-17
grad ChooseDest W: 3.8083508014678955
grad AddEdge W: 6.846929948637053e-17
grad ChooseDest W: 1.7633063793182373
grad AddEdge W: 6.254249745786888e-17
grad ChooseDest W: 2.9834442138671875
grad AddEdge W: 3.33029023838981e-15
grad ChooseDest W: 1.9020026922225952
grad AddEdge W: 4.5205517355416305e-15
grad ChooseDest W: 1.4678535461425781
grad AddEdge W: 2.9418472815260924e-15
grad ChooseDest W: 2.3115830421447754
grad AddEdge W: 1.801647516052145e-17
grad ChooseDest W: 2.2929675579071045
grad AddEdge W: 4.8741020171761545e-15
grad ChooseDest W: 3.025111436843872
grad AddEdge W: 7.952232340495927e-18
grad ChooseDest W: 2.287569999694824
grad AddEdge W: 3.1245366181393208e-15
grad ChooseDest W: 2.5793850421905518
grad AddEdge W: 1.3709093255434882e-16
grad ChooseDest W: 2.6563422679901123
grad AddEdge W: 1.0558386786766531e-16
grad ChooseDest W: 2.1695826053619385
grad AddEdge W: 3.6240410527393647e-17
grad ChooseDest W: 2.0898754596710205
grad AddEdge W: 2.9415508729341126e-16
grad ChooseDest W: 2.3056702613830566
grad AddEdge W: 2.602786769003774e-15
grad ChooseDest W: 2.5062334537506104
grad AddEdge W: 9.835508736665347e-14
grad ChooseDest W: 2.5460941791534424
grad AddEdge W: 1.1623367267049774e-15
grad ChooseDest W: 1.9734970331192017
grad AddEdge W: 1.9673886035109864e-15
grad ChooseDest W: 0.9208841323852539
grad AddEdge W: 1.660677655672849e-15
grad ChooseDest W: 1.5056958198547363
grad AddEdge W: 1.2531230414806734e-16
grad ChooseDest W: 1.9558515548706055
grad AddEdge W: 4.5798985431256314e-17
grad ChooseDest W: 1.5349901914596558
grad AddEdge W: 3.2929470968442098e-15
grad ChooseDest W: 2.108217716217041
grad AddEdge W: 4.619295024884274e-15
grad ChooseDest W: 3.024153470993042
grad AddEdge W: 5.136172211111622e-17
grad ChooseDest W: 6.514043807983398
grad AddEdge W: 1.3219212246698725e-17
grad ChooseDest W: 2.0331668853759766
grad AddEdge W: 6.460921258584626e-15
grad ChooseDest W: 2.870482921600342
grad AddEdge W: 2.2968763731799104e-15
grad ChooseDest W: 2.2620060443878174
grad AddEdge W: 5.558518058735887e-17
grad ChooseDest W: 3.0911858081817627
grad AddEdge W: 2.633029868627252e-15
grad ChooseDest W: 2.8888585567474365
grad AddEdge W: 1.6833534092676714e-17
grad ChooseDest W: 1.7598416805267334
grad AddEdge W: 3.366379916757262e-17
grad ChooseDest W: 1.5279359817504883
grad AddEdge W: 2.1133152742278348e-17
grad ChooseDest W: 2.4007246494293213
grad AddEdge W: 2.7864618402093696e-15
grad ChooseDest W: 2.108649730682373
grad AddEdge W: 1.8160356610630147e-17
grad ChooseDest W: 3.801535129547119
grad AddEdge W: 3.866700441132206e-17
grad ChooseDest W: 2.7107491493225098
grad AddEdge W: 4.446085204560683e-17
grad ChooseDest W: 2.7299864292144775
grad AddEdge W: 7.107256922297292e-17
grad ChooseDest W: 2.0540339946746826
grad AddEdge W: 2.933731858858449e-15
grad ChooseDest W: 2.3533730506896973
grad AddEdge W: 4.0316888934952976e-17
grad ChooseDest W: 2.3811075687408447
grad AddEdge W: 1.6507933635135426e-16
grad ChooseDest W: 5.42465353012085
grad AddEdge W: 9.536239968532551e-16
grad ChooseDest W: 1.660179615020752
grad AddEdge W: 6.202937416284509e-17
grad ChooseDest W: 2.8978936672210693
grad AddEdge W: 1.0184487915002762e-16
grad ChooseDest W: 2.1604559421539307
grad AddEdge W: 1.8388931429296925e-17
grad ChooseDest W: 1.5751044750213623
grad AddEdge W: 1.2653076104269265e-16
grad ChooseDest W: 2.2931625843048096
grad AddEdge W: 2.1894082209640738e-15
grad ChooseDest W: 2.5100457668304443
grad AddEdge W: 1.0741987133784311e-16
grad ChooseDest W: 1.7448972463607788
grad AddEdge W: 4.7037125915737937e-17
grad ChooseDest W: 2.690382957458496
grad AddEdge W: 1.891365345086994e-17
grad ChooseDest W: 3.1670820713043213
grad AddEdge W: 2.106365322573499e-15
grad ChooseDest W: 2.487570285797119
grad AddEdge W: 3.5101140625424772e-15
grad ChooseDest W: 1.7395362854003906
grad AddEdge W: 7.601119482193911e-17
grad ChooseDest W: 2.3340587615966797
grad AddEdge W: 6.096681421155752e-15
grad ChooseDest W: 2.826240301132202
=== Epoch 50: Train Loss: 5.9898, Train Log Prob: 0.0071 ===
Total mismatches: 91086
Predicted valid destination but wrong order: 19955
Epoch 50: Validation Loss: 6.2564, Validation Log Prob: 0.0039
Epoch 50: Edge Precision: 0.3626, Recall: 0.3601, F1: 0.3612, Jaccard: 0.2375
Epoch 50: TP: 2.520687186828919, FP: 4.456406585540444, FN: 4.5007874015748035
Epoch 50: Current Learning Rate: 1e-06
üíæ Checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/checkpoint_epoch_50.pth
[Epoch 50] ‚è±Ô∏è Total: 1889.28s | Current time: 2025-07-15 13:55:06 | üèãÔ∏è Train: 1651.34s | ‚úÖ Val: 237.94s
Training finished at: 2025-07-15 13:55:06
Training time: 94944.82076525688
‚úÖ Model saved to: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/model.pth
üìà Metrics saved to: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM
Device for model: cuda:5

Epoch-wise Validation Metrics:

Epoch 1:
  Validation Loss: 7.1535, Validation Log Prob: 0.0015
  Edge Precision: 0.3599, Recall: 0.3566, F1: 0.3581, Jaccard: 0.2349
  TP: 2.497208303507516, FP: 4.465282748747316, FN: 4.524266284896206

Epoch 2:
  Validation Loss: 6.8048, Validation Log Prob: 0.0022
  Edge Precision: 0.3608, Recall: 0.3587, F1: 0.3596, Jaccard: 0.2367
  TP: 2.5110952040085897, FP: 4.4747315676449535, FN: 4.510379384395132

Epoch 3:
  Validation Loss: 6.3654, Validation Log Prob: 0.0033
  Edge Precision: 0.3615, Recall: 0.3591, F1: 0.3602, Jaccard: 0.2369
  TP: 2.5139584824624195, FP: 4.467430207587688, FN: 4.507516105941303

Epoch 4:
  Validation Loss: 6.3021, Validation Log Prob: 0.0035
  Edge Precision: 0.3617, Recall: 0.3583, F1: 0.3599, Jaccard: 0.2367
  TP: 2.509663564781675, FP: 4.45125268432355, FN: 4.511811023622047

Epoch 5:
  Validation Loss: 6.2796, Validation Log Prob: 0.0037
  Edge Precision: 0.3604, Recall: 0.3567, F1: 0.3584, Jaccard: 0.2354
  TP: 2.4979241231209737, FP: 4.454831782390838, FN: 4.5235504652827485

Epoch 6:
  Validation Loss: 6.3617, Validation Log Prob: 0.0036
  Edge Precision: 0.3589, Recall: 0.3558, F1: 0.3572, Jaccard: 0.2342
  TP: 2.490622763063708, FP: 4.473586256263421, FN: 4.530851825340014

Epoch 7:
  Validation Loss: 6.2792, Validation Log Prob: 0.0037
  Edge Precision: 0.3592, Recall: 0.3561, F1: 0.3575, Jaccard: 0.2347
  TP: 2.4937723693629206, FP: 4.4728704366499645, FN: 4.527702219040802

Epoch 8:
  Validation Loss: 6.2557, Validation Log Prob: 0.0039
  Edge Precision: 0.3596, Recall: 0.3572, F1: 0.3583, Jaccard: 0.2353
  TP: 2.501646385110952, FP: 4.477451682176092, FN: 4.519828203292771

Epoch 9:
  Validation Loss: 6.1673, Validation Log Prob: 0.0042
  Edge Precision: 0.3592, Recall: 0.3570, F1: 0.3580, Jaccard: 0.2346
  TP: 2.4997852541159626, FP: 4.480458124552613, FN: 4.5216893342877595

Epoch 10:
  Validation Loss: 6.3129, Validation Log Prob: 0.0039
  Edge Precision: 0.3625, Recall: 0.3592, F1: 0.3607, Jaccard: 0.2370
  TP: 2.517108088761632, FP: 4.448389405869721, FN: 4.5043664996420905

Epoch 11:
  Validation Loss: 6.3024, Validation Log Prob: 0.0038
  Edge Precision: 0.3608, Recall: 0.3591, F1: 0.3599, Jaccard: 0.2367
  TP: 2.514674302075877, FP: 4.4773085182534, FN: 4.506800286327845

Epoch 12:
  Validation Loss: 6.2412, Validation Log Prob: 0.0040
  Edge Precision: 0.3599, Recall: 0.3567, F1: 0.3582, Jaccard: 0.2356
  TP: 2.498783106657122, FP: 4.46571224051539, FN: 4.5226914817466

Epoch 13:
  Validation Loss: 6.2202, Validation Log Prob: 0.0040
  Edge Precision: 0.3609, Recall: 0.3588, F1: 0.3598, Jaccard: 0.2373
  TP: 2.5123836793128134, FP: 4.471438797423049, FN: 4.509090909090909

Epoch 14:
  Validation Loss: 6.2583, Validation Log Prob: 0.0038
  Edge Precision: 0.3601, Recall: 0.3573, F1: 0.3586, Jaccard: 0.2354
  TP: 2.5017895490336435, FP: 4.468718682891911, FN: 4.519685039370079

Epoch 15:
  Validation Loss: 6.2754, Validation Log Prob: 0.0039
  Edge Precision: 0.3603, Recall: 0.3584, F1: 0.3593, Jaccard: 0.2361
  TP: 2.5086614173228345, FP: 4.480458124552613, FN: 4.512813171080888

Epoch 16:
  Validation Loss: 6.2653, Validation Log Prob: 0.0039
  Edge Precision: 0.3580, Recall: 0.3551, F1: 0.3564, Jaccard: 0.2347
  TP: 2.4871868289191124, FP: 4.479312813171081, FN: 4.53428775948461

Epoch 17:
  Validation Loss: 6.1892, Validation Log Prob: 0.0040
  Edge Precision: 0.3611, Recall: 0.3577, F1: 0.3593, Jaccard: 0.2361
  TP: 2.504509663564782, FP: 4.454545454545454, FN: 4.51696492483894

Epoch 18:
  Validation Loss: 6.2533, Validation Log Prob: 0.0039
  Edge Precision: 0.3576, Recall: 0.3543, F1: 0.3558, Jaccard: 0.2335
  TP: 2.4807444523979956, FP: 4.481460272011453, FN: 4.540730136005727

Epoch 19:
  Validation Loss: 6.2072, Validation Log Prob: 0.0041
  Edge Precision: 0.3615, Recall: 0.3594, F1: 0.3604, Jaccard: 0.2374
  TP: 2.5169649248389407, FP: 4.4661417322834644, FN: 4.504509663564781

Epoch 20:
  Validation Loss: 6.2562, Validation Log Prob: 0.0039
  Edge Precision: 0.3566, Recall: 0.3534, F1: 0.3549, Jaccard: 0.2327
  TP: 2.4743020758768792, FP: 4.487186828919112, FN: 4.547172512526843

Epoch 21:
  Validation Loss: 6.2703, Validation Log Prob: 0.0040
  Edge Precision: 0.3614, Recall: 0.3588, F1: 0.3600, Jaccard: 0.2364
  TP: 2.511667859699356, FP: 4.461130994989262, FN: 4.509806728704366

Epoch 22:
  Validation Loss: 6.2863, Validation Log Prob: 0.0038
  Edge Precision: 0.3637, Recall: 0.3607, F1: 0.3621, Jaccard: 0.2386
  TP: 2.526700071581961, FP: 4.440944881889764, FN: 4.494774516821761

Epoch 23:
  Validation Loss: 6.2780, Validation Log Prob: 0.0038
  Edge Precision: 0.3566, Recall: 0.3543, F1: 0.3554, Jaccard: 0.2332
  TP: 2.480887616320687, FP: 4.49663564781675, FN: 4.540586972083035

Epoch 24:
  Validation Loss: 6.2468, Validation Log Prob: 0.0039
  Edge Precision: 0.3624, Recall: 0.3596, F1: 0.3609, Jaccard: 0.2370
  TP: 2.518539727988547, FP: 4.451968503937008, FN: 4.502934860415175

Epoch 25:
  Validation Loss: 6.2809, Validation Log Prob: 0.0039
  Edge Precision: 0.3618, Recall: 0.3594, F1: 0.3605, Jaccard: 0.2376
  TP: 2.5159627773801003, FP: 4.46370794559771, FN: 4.505511811023622

Epoch 26:
  Validation Loss: 6.2637, Validation Log Prob: 0.0038
  Edge Precision: 0.3576, Recall: 0.3547, F1: 0.3560, Jaccard: 0.2333
  TP: 2.4830350751610593, FP: 4.487186828919112, FN: 4.538439513242663

Epoch 27:
  Validation Loss: 6.2620, Validation Log Prob: 0.0039
  Edge Precision: 0.3580, Recall: 0.3553, F1: 0.3565, Jaccard: 0.2343
  TP: 2.4866141732283467, FP: 4.483750894774516, FN: 4.534860415175376

Epoch 28:
  Validation Loss: 6.2614, Validation Log Prob: 0.0039
  Edge Precision: 0.3598, Recall: 0.3571, F1: 0.3584, Jaccard: 0.2356
  TP: 2.4993557623478884, FP: 4.470722977809592, FN: 4.522118826055834

Epoch 29:
  Validation Loss: 6.2791, Validation Log Prob: 0.0038
  Edge Precision: 0.3591, Recall: 0.3563, F1: 0.3576, Jaccard: 0.2346
  TP: 2.495490336435218, FP: 4.478453829634932, FN: 4.525984251968504

Epoch 30:
  Validation Loss: 6.2810, Validation Log Prob: 0.0037
  Edge Precision: 0.3584, Recall: 0.3558, F1: 0.3570, Jaccard: 0.2341
  TP: 2.4909090909090907, FP: 4.484323550465283, FN: 4.530565497494631

Epoch 31:
  Validation Loss: 6.2670, Validation Log Prob: 0.0039
  Edge Precision: 0.3599, Recall: 0.3569, F1: 0.3583, Jaccard: 0.2352
  TP: 2.498353614889048, FP: 4.467716535433071, FN: 4.523120973514675

Epoch 32:
  Validation Loss: 6.2796, Validation Log Prob: 0.0038
  Edge Precision: 0.3589, Recall: 0.3564, F1: 0.3575, Jaccard: 0.2352
  TP: 2.495490336435218, FP: 4.479599141016464, FN: 4.525984251968504

Epoch 33:
  Validation Loss: 6.2522, Validation Log Prob: 0.0039
  Edge Precision: 0.3610, Recall: 0.3584, F1: 0.3596, Jaccard: 0.2368
  TP: 2.5092340730136007, FP: 4.464996420901933, FN: 4.512240515390122

Epoch 34:
  Validation Loss: 6.2604, Validation Log Prob: 0.0039
  Edge Precision: 0.3587, Recall: 0.3560, F1: 0.3572, Jaccard: 0.2351
  TP: 2.4920544022906226, FP: 4.479312813171081, FN: 4.529420186113099

Epoch 35:
  Validation Loss: 6.2541, Validation Log Prob: 0.0039
  Edge Precision: 0.3606, Recall: 0.3578, F1: 0.3591, Jaccard: 0.2362
  TP: 2.5047959914101647, FP: 4.463851109520401, FN: 4.5166785969935574

Epoch 36:
  Validation Loss: 6.2552, Validation Log Prob: 0.0038
  Edge Precision: 0.3621, Recall: 0.3592, F1: 0.3605, Jaccard: 0.2375
  TP: 2.5151037938439513, FP: 4.45311381531854, FN: 4.506370794559771

Epoch 37:
  Validation Loss: 6.2604, Validation Log Prob: 0.0040
  Edge Precision: 0.3620, Recall: 0.3592, F1: 0.3605, Jaccard: 0.2377
  TP: 2.515676449534717, FP: 4.453400143163923, FN: 4.505798138869005

Epoch 38:
  Validation Loss: 6.2746, Validation Log Prob: 0.0038
  Edge Precision: 0.3600, Recall: 0.3573, F1: 0.3585, Jaccard: 0.2359
  TP: 2.5017895490336435, FP: 4.469148174659986, FN: 4.519685039370079

Epoch 39:
  Validation Loss: 6.2561, Validation Log Prob: 0.0039
  Edge Precision: 0.3591, Recall: 0.3564, F1: 0.3576, Jaccard: 0.2353
  TP: 2.494774516821761, FP: 4.4773085182534, FN: 4.526700071581962

Epoch 40:
  Validation Loss: 6.2593, Validation Log Prob: 0.0039
  Edge Precision: 0.3598, Recall: 0.3574, F1: 0.3585, Jaccard: 0.2357
  TP: 2.50050107372942, FP: 4.474158911954188, FN: 4.520973514674302

Epoch 41:
  Validation Loss: 6.2830, Validation Log Prob: 0.0037
  Edge Precision: 0.3596, Recall: 0.3570, F1: 0.3582, Jaccard: 0.2355
  TP: 2.498067287043665, FP: 4.477594846098783, FN: 4.5234073013600575

Epoch 42:
  Validation Loss: 6.2649, Validation Log Prob: 0.0039
  Edge Precision: 0.3614, Recall: 0.3587, F1: 0.3599, Jaccard: 0.2367
  TP: 2.511667859699356, FP: 4.46184681460272, FN: 4.509806728704366

Epoch 43:
  Validation Loss: 6.2570, Validation Log Prob: 0.0040
  Edge Precision: 0.3590, Recall: 0.3561, F1: 0.3574, Jaccard: 0.2350
  TP: 2.493915533285612, FP: 4.476020042949177, FN: 4.52755905511811

Epoch 44:
  Validation Loss: 6.2751, Validation Log Prob: 0.0038
  Edge Precision: 0.3600, Recall: 0.3574, F1: 0.3586, Jaccard: 0.2351
  TP: 2.500787401574803, FP: 4.474588403722262, FN: 4.520687186828919

Epoch 45:
  Validation Loss: 6.2638, Validation Log Prob: 0.0039
  Edge Precision: 0.3577, Recall: 0.3548, F1: 0.3562, Jaccard: 0.2338
  TP: 2.4847530422333572, FP: 4.4850393700787405, FN: 4.536721546170365

Epoch 46:
  Validation Loss: 6.2687, Validation Log Prob: 0.0039
  Edge Precision: 0.3612, Recall: 0.3584, F1: 0.3597, Jaccard: 0.2361
  TP: 2.5092340730136007, FP: 4.461417322834646, FN: 4.512240515390122

Epoch 47:
  Validation Loss: 6.2848, Validation Log Prob: 0.0039
  Edge Precision: 0.3582, Recall: 0.3556, F1: 0.3568, Jaccard: 0.2345
  TP: 2.48790264853257, FP: 4.486184681460272, FN: 4.533571939871153

Epoch 48:
  Validation Loss: 6.2793, Validation Log Prob: 0.0039
  Edge Precision: 0.3594, Recall: 0.3567, F1: 0.3579, Jaccard: 0.2356
  TP: 2.4960629921259843, FP: 4.478883321403006, FN: 4.525411596277738

Epoch 49:
  Validation Loss: 6.2774, Validation Log Prob: 0.0039
  Edge Precision: 0.3589, Recall: 0.3566, F1: 0.3577, Jaccard: 0.2348/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4434: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4449: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))

  TP: 2.49663564781675, FP: 4.4833214030064426, FN: 4.524838940586972

Epoch 50:
  Validation Loss: 6.2564, Validation Log Prob: 0.0039
  Edge Precision: 0.3626, Recall: 0.3601, F1: 0.3612, Jaccard: 0.2375
  TP: 2.520687186828919, FP: 4.456406585540444, FN: 4.5007874015748035
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 73.50%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 26.50%
  ‚ùå False Discovery rate (FP/TP+FP): 21.58%
  üéØ Precision (TP/TP+FP): 78.42%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 20.64%
  ‚ö†Ô∏è Std. False Negative rate: 20.64%
  ‚ùå Std. False Discovery rate: 18.07%
  üéØ Std. Precision: 18.07%
üìâ  Average detailed edge-metrics
  F1: 0.76
  Jaccard: 0.65
  TP: 4.25
  FP: 1.10
  FN: 1.50

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 54.56%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 45.44%
  ‚ùå False Discovery rate (FP/TP+FP): 43.95%
  üéØ Precision (TP/TP+FP): 56.05%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 21.97%
  ‚ö†Ô∏è Std. False Negative rate: 21.97%
  ‚ùå Std. False Discovery rate: 21.90%
  üéØ Std. Precision: 21.90%
üìâ  Average detailed edge-metrics
  F1: 0.55
  Jaccard: 0.42
  TP: 3.34
  FP: 2.53
  FN: 2.71

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 45.61%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 54.39%
  ‚ùå False Discovery rate (FP/TP+FP): 53.72%
  üéØ Precision (TP/TP+FP): 46.28%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.26%
  ‚ö†Ô∏è Std. False Negative rate: 19.26%
  ‚ùå Std. False Discovery rate: 19.38%
  üéØ Std. Precision: 19.38%
üìâ  Average detailed edge-metrics
  F1: 0.46
  Jaccard: 0.32
  TP: 2.99
  FP: 3.45
  FN: 3.55

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 38.89%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 61.11%
  ‚ùå False Discovery rate (FP/TP+FP): 60.82%
  üéØ Precision (TP/TP+FP): 39.18%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.01%
  ‚ö†Ô∏è Std. False Negative rate: 18.01%
  ‚ùå Std. False Discovery rate: 18.12%
  üéØ Std. Precision: 18.12%
üìâ  Average detailed edge-metrics
  F1: 0.39
  Jaccard: 0.26
  TP: 2.73
  FP: 4.23
  FN: 4.29

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 31.83%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 68.17%
  ‚ùå False Discovery rate (FP/TP+FP): 68.05%
  üéØ Precision (TP/TP+FP): 31.95%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 16.77%
  ‚ö†Ô∏è Std. False Negative rate: 16.77%
  ‚ùå Std. False Discovery rate: 16.83%
  üéØ Std. Precision: 16.83%
üìâ  Average detailed edge-metrics
  F1: 0.32
  Jaccard: 0.20
  TP: 2.27
  FP: 4.82
  FN: 4.85

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 35.76%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 64.24%
  ‚ùå False Discovery rate (FP/TP+FP): 63.99%
  üéØ Precision (TP/TP+FP): 36.01%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.35%
  ‚ö†Ô∏è Std. False Negative rate: 18.35%
  ‚ùå Std. False Discovery rate: 18.49%
  üéØ Std. Precision: 18.49%
üìâ  Average detailed edge-metrics
  F1: 0.36
  Jaccard: 0.24
  TP: 2.50
  FP: 4.47
  FN: 4.52
[6, 7, 8, 9, 10, 999]
[0.735, 0.545578231292517, 0.45613240418118467, 0.38889585073795596, 0.3182594014097502, 0.35755632818624944]
[np.float64(0.18069887474285318), np.float64(0.21902925689617522), np.float64(0.19382011930384294), np.float64(0.18117130375641669), np.float64(0.16829709772766988), np.float64(0.18487123826313365)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.FIXED

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 76.00%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 24.00%
  ‚ùå False Discovery rate (FP/TP+FP): 20.17%
  üéØ Precision (TP/TP+FP): 79.83%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 21.54%
  ‚ö†Ô∏è Std. False Negative rate: 21.54%
  ‚ùå Std. False Discovery rate: 20.01%
  üéØ Std. Precision: 20.01%
üìâ  Average detailed edge-metrics
  F1: 0.78
  Jaccard: 0.68
  TP: 4.40
  FP: 1.05
  FN: 1.35

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 61.28%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 38.72%
  ‚ùå False Discovery rate (FP/TP+FP): 38.61%
  üéØ Precision (TP/TP+FP): 61.39%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 21.51%
  ‚ö†Ô∏è Std. False Negative rate: 21.51%
  ‚ùå Std. False Discovery rate: 21.38%
  üéØ Std. Precision: 21.38%
üìâ  Average detailed edge-metrics
  F1: 0.61
  Jaccard: 0.48
  TP: 3.73
  FP: 2.29
  FN: 2.31

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 47.30%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 52.70%
  ‚ùå False Discovery rate (FP/TP+FP): 52.68%
  üéØ Precision (TP/TP+FP): 47.32%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.38%
  ‚ö†Ô∏è Std. False Negative rate: 18.38%
  ‚ùå Std. False Discovery rate: 18.35%
  üéØ Std. Precision: 18.35%
üìâ  Average detailed edge-metrics
  F1: 0.47
  Jaccard: 0.33
  TP: 3.10
  FP: 3.43
  FN: 3.44

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 40.84%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 59.16%
  ‚ùå False Discovery rate (FP/TP+FP): 59.10%
  üéØ Precision (TP/TP+FP): 40.90%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.56%
  ‚ö†Ô∏è Std. False Negative rate: 18.56%
  ‚ùå Std. False Discovery rate: 18.58%
  üéØ Std. Precision: 18.58%
üìâ  Average detailed edge-metrics
  F1: 0.41
  Jaccard: 0.28
  TP: 2.87
  FP: 4.14
  FN: 4.15

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 33.35%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 66.65%
  ‚ùå False Discovery rate (FP/TP+FP): 66.65%
  üéØ Precision (TP/TP+FP): 33.35%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.07%
  ‚ö†Ô∏è Std. False Negative rate: 17.07%
  ‚ùå Std. False Discovery rate: 17.07%
  üéØ Std. Precision: 17.07%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.37
  FP: 4.74
  FN: 4.74

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 37.52%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 62.48%
  ‚ùå False Discovery rate (FP/TP+FP): 62.45%
  üéØ Precision (TP/TP+FP): 37.55%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.71%
  ‚ö†Ô∏è Std. False Negative rate: 18.71%
  ‚ùå Std. False Discovery rate: 18.74%
  üéØ Std. Precision: 18.74%
üìâ  Average detailed edge-metrics
  F1: 0.38
  Jaccard: 0.25
  TP: 2.63
  FP: 4.39
  FN: 4.39
[6, 7, 8, 9, 10, 999]
[0.76, 0.612765106042417, 0.4729616724738676, 0.40840295182400443, 0.33352093097636887, 0.3751820227017077]
[np.float64(0.20013190094979302), np.float64(0.2137899024391198), np.float64(0.18352123216017083), np.float64(0.1858039903149577), np.float64(0.17066736238789895), np.float64(0.18739533654570542)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_ASC

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 79.17%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 20.83%
  ‚ùå False Discovery rate (FP/TP+FP): 17.25%
  üéØ Precision (TP/TP+FP): 82.75%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 21.88%
  ‚ö†Ô∏è Std. False Negative rate: 21.88%
  ‚ùå Std. False Discovery rate: 18.98%
  üéØ Std. Precision: 18.98%
üìâ  Average detailed edge-metrics
  F1: 0.81
  Jaccard: 0.73
  TP: 4.55
  FP: 0.90
  FN: 1.20

Face Count 7: 119 graphs evaluated/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4477: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4491: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))

üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 61.27%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 38.73%
  ‚ùå False Discovery rate (FP/TP+FP): 38.53%
  üéØ Precision (TP/TP+FP): 61.47%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 21.33%
  ‚ö†Ô∏è Std. False Negative rate: 21.33%
  ‚ùå Std. False Discovery rate: 21.37%
  üéØ Std. Precision: 21.37%
üìâ  Average detailed edge-metrics
  F1: 0.61
  Jaccard: 0.48
  TP: 3.73
  FP: 2.29
  FN: 2.31

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 47.39%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 52.61%
  ‚ùå False Discovery rate (FP/TP+FP): 52.51%
  üéØ Precision (TP/TP+FP): 47.49%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.97%
  ‚ö†Ô∏è Std. False Negative rate: 18.97%
  ‚ùå Std. False Discovery rate: 18.94%
  üéØ Std. Precision: 18.94%
üìâ  Average detailed edge-metrics
  F1: 0.47
  Jaccard: 0.33
  TP: 3.11
  FP: 3.41
  FN: 3.43

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 39.99%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 60.01%
  ‚ùå False Discovery rate (FP/TP+FP): 59.99%
  üéØ Precision (TP/TP+FP): 40.01%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.57%
  ‚ö†Ô∏è Std. False Negative rate: 18.57%
  ‚ùå Std. False Discovery rate: 18.57%
  üéØ Std. Precision: 18.57%
üìâ  Average detailed edge-metrics
  F1: 0.40
  Jaccard: 0.27
  TP: 2.81
  FP: 4.20
  FN: 4.21

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 32.85%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 67.15%
  ‚ùå False Discovery rate (FP/TP+FP): 67.14%
  üéØ Precision (TP/TP+FP): 32.86%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.52%
  ‚ö†Ô∏è Std. False Negative rate: 17.52%
  ‚ùå Std. False Discovery rate: 17.52%
  üéØ Std. Precision: 17.52%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.34
  FP: 4.77
  FN: 4.77

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.94%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.06%
  ‚ùå False Discovery rate (FP/TP+FP): 63.02%
  üéØ Precision (TP/TP+FP): 36.98%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.00%
  ‚ö†Ô∏è Std. False Negative rate: 19.00%
  ‚ùå Std. False Discovery rate: 19.03%
  üéØ Std. Precision: 19.03%
üìâ  Average detailed edge-metrics
  F1: 0.37
  Jaccard: 0.25
  TP: 2.59
  FP: 4.43
  FN: 4.43
[6, 7, 8, 9, 10, 999]
[0.7916666666666667, 0.6127250900360144, 0.47391405342624854, 0.39985844240230206, 0.3285465180647304, 0.36943347990592085]
[np.float64(0.18982996777818475), np.float64(0.21367935180286154), np.float64(0.18944680380477663), np.float64(0.18574626132056346), np.float64(0.17523936145266364), np.float64(0.19031904873299205)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 68.17%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 31.83%
  ‚ùå False Discovery rate (FP/TP+FP): 27.08%
  üéØ Precision (TP/TP+FP): 72.92%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 22.57%
  ‚ö†Ô∏è Std. False Negative rate: 22.57%
  ‚ùå Std. False Discovery rate: 21.07%
  üéØ Std. Precision: 21.07%
üìâ  Average detailed edge-metrics
  F1: 0.70
  Jaccard: 0.59
  TP: 3.95
  FP: 1.40
  FN: 1.80

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 57.26%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 42.74%
  ‚ùå False Discovery rate (FP/TP+FP): 41.41%
  üéØ Precision (TP/TP+FP): 58.59%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 24.40%
  ‚ö†Ô∏è Std. False Negative rate: 24.40%
  ‚ùå Std. False Discovery rate: 24.61%
  üéØ Std. Precision: 24.61%
üìâ  Average detailed edge-metrics
  F1: 0.58
  Jaccard: 0.45
  TP: 3.49
  FP: 2.42
  FN: 2.55

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 46.98%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 53.02%
  ‚ùå False Discovery rate (FP/TP+FP): 52.69%
  üéØ Precision (TP/TP+FP): 47.31%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 19.76%
  ‚ö†Ô∏è Std. False Negative rate: 19.76%
  ‚ùå Std. False Discovery rate: 19.74%
  üéØ Std. Precision: 19.74%
üìâ  Average detailed edge-metrics
  F1: 0.47
  Jaccard: 0.33
  TP: 3.08
  FP: 3.41
  FN: 3.46

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 40.36%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 59.64%
  ‚ùå False Discovery rate (FP/TP+FP): 59.53%
  üéØ Precision (TP/TP+FP): 40.47%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.51%
  ‚ö†Ô∏è Std. False Negative rate: 17.51%
  ‚ùå Std. False Discovery rate: 17.56%
  üéØ Std. Precision: 17.56%
üìâ  Average detailed edge-metrics
  F1: 0.40
  Jaccard: 0.27
  TP: 2.83
  FP: 4.16
  FN: 4.18

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 33.34%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 66.66%
  ‚ùå False Discovery rate (FP/TP+FP): 66.64%
  üéØ Precision (TP/TP+FP): 33.36%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.08%
  ‚ö†Ô∏è Std. False Negative rate: 17.08%
  ‚ùå Std. False Discovery rate: 17.09%
  üéØ Std. Precision: 17.09%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.37
  FP: 4.74
  FN: 4.74

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 37.22%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 62.78%
  ‚ùå False Discovery rate (FP/TP+FP): 62.67%
  üéØ Precision (TP/TP+FP): 37.33%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.33%
  ‚ö†Ô∏è Std. False Negative rate: 18.33%
  ‚ùå Std. False Discovery rate: 18.41%
  üéØ Std. Precision: 18.41%
üìâ  Average detailed edge-metrics
  F1: 0.37
  Jaccard: 0.25
  TP: 2.61
  FP: 4.40
  FN: 4.41
[6, 7, 8, 9, 10, 999]
[0.6816666666666666, 0.5725890356142457, 0.46980255516840885, 0.40357606980413996, 0.33339022770048343, 0.3722429014554999]
[np.float64(0.21067846749648306), np.float64(0.2460665871760266), np.float64(0.19740628031101948), np.float64(0.175550753600814), np.float64(0.1709346029561858), np.float64(0.1841414290631616)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 74.50%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 25.50%
  ‚ùå False Discovery rate (FP/TP+FP): 22.42%
  üéØ Precision (TP/TP+FP): 77.58%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 22.32%
  ‚ö†Ô∏è Std. False Negative rate: 22.32%
  ‚ùå Std. False Discovery rate: 20.88%
  üéØ Std. Precision: 20.88%
üìâ  Average detailed edge-metrics
  F1: 0.76
  Jaccard: 0.66
  TP: 4.30
  FP: 1.20
  FN: 1.45

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 54.69%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 45.31%
  ‚ùå False Discovery rate (FP/TP+FP): 44.65%
  üéØ Precision (TP/TP+FP): 55.35%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 24.24%
  ‚ö†Ô∏è Std. False Negative rate: 24.24%
  ‚ùå Std. False Discovery rate: 24.27%
  üéØ Std. Precision: 24.27%
üìâ  Average detailed edge-metrics
  F1: 0.55
  Jaccard: 0.42
  TP: 3.34
  FP: 2.62
  FN: 2.70

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 46.74%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 53.26%
  ‚ùå False Discovery rate (FP/TP+FP): 53.15%
  üéØ Precision (TP/TP+FP): 46.85%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.92%
  ‚ö†Ô∏è Std. False Negative rate: 17.92%
  ‚ùå Std. False Discovery rate: 17.90%
  üéØ Std. Precision: 17.90%/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4505: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:4525: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5344: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5361: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5377: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5409: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current.py:5663: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  loaded_model.load_state_dict(torch.load(load_model_path))

üìâ  Average detailed edge-metrics
  F1: 0.47
  Jaccard: 0.32
  TP: 3.06
  FP: 3.46
  FN: 3.47

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 39.14%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 60.86%
  ‚ùå False Discovery rate (FP/TP+FP): 60.82%
  üéØ Precision (TP/TP+FP): 39.18%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.98%
  ‚ö†Ô∏è Std. False Negative rate: 17.98%
  ‚ùå Std. False Discovery rate: 17.98%
  üéØ Std. Precision: 17.98%
üìâ  Average detailed edge-metrics
  F1: 0.39
  Jaccard: 0.26
  TP: 2.75
  FP: 4.26
  FN: 4.27

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 32.46%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 67.54%
  ‚ùå False Discovery rate (FP/TP+FP): 67.52%
  üéØ Precision (TP/TP+FP): 32.48%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.45%
  ‚ö†Ô∏è Std. False Negative rate: 17.45%
  ‚ùå Std. False Discovery rate: 17.47%
  üéØ Std. Precision: 17.47%
üìâ  Average detailed edge-metrics
  F1: 0.32
  Jaccard: 0.21
  TP: 2.31
  FP: 4.80
  FN: 4.80

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.25%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.75%
  ‚ùå False Discovery rate (FP/TP+FP): 63.70%
  üéØ Precision (TP/TP+FP): 36.30%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.58%
  ‚ö†Ô∏è Std. False Negative rate: 18.58%
  ‚ùå Std. False Discovery rate: 18.62%
  üéØ Std. Precision: 18.62%
üìâ  Average detailed edge-metrics
  F1: 0.36
  Jaccard: 0.24
  TP: 2.54
  FP: 4.47
  FN: 4.48
[6, 7, 8, 9, 10, 999]
[0.745, 0.5468587434973989, 0.4673751451800232, 0.3913603453077137, 0.3246161936745765, 0.36249514265262295]
[np.float64(0.20879781554838586), np.float64(0.2426767730752772), np.float64(0.17903395186475057), np.float64(0.17978804579690066), np.float64(0.17465716689156147), np.float64(0.18616092198805373)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM

Face Count 6: 20 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 72.83%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 27.17%
  ‚ùå False Discovery rate (FP/TP+FP): 23.83%
  üéØ Precision (TP/TP+FP): 76.17%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 23.43%
  ‚ö†Ô∏è Std. False Negative rate: 23.43%
  ‚ùå Std. False Discovery rate: 21.40%
  üéØ Std. Precision: 21.40%
üìâ  Average detailed edge-metrics
  F1: 0.74
  Jaccard: 0.64
  TP: 4.20
  FP: 1.25
  FN: 1.55

Face Count 7: 119 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 55.77%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 44.23%
  ‚ùå False Discovery rate (FP/TP+FP): 42.76%
  üéØ Precision (TP/TP+FP): 57.24%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 23.26%
  ‚ö†Ô∏è Std. False Negative rate: 23.26%
  ‚ùå Std. False Discovery rate: 23.31%
  üéØ Std. Precision: 23.31%
üìâ  Average detailed edge-metrics
  F1: 0.56
  Jaccard: 0.43
  TP: 3.39
  FP: 2.49
  FN: 2.65

Face Count 8: 410 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 45.49%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 54.51%
  ‚ùå False Discovery rate (FP/TP+FP): 54.22%
  üéØ Precision (TP/TP+FP): 45.78%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.82%
  ‚ö†Ô∏è Std. False Negative rate: 17.82%
  ‚ùå Std. False Discovery rate: 17.79%
  üéØ Std. Precision: 17.79%
üìâ  Average detailed edge-metrics
  F1: 0.46
  Jaccard: 0.31
  TP: 2.98
  FP: 3.51
  FN: 3.55

Face Count 9: 2565 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 39.20%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 60.80%
  ‚ùå False Discovery rate (FP/TP+FP): 60.67%
  üéØ Precision (TP/TP+FP): 39.33%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 17.68%
  ‚ö†Ô∏è Std. False Negative rate: 17.68%
  ‚ùå Std. False Discovery rate: 17.73%
  üéØ Std. Precision: 17.73%
üìâ  Average detailed edge-metrics
  F1: 0.39
  Jaccard: 0.26
  TP: 2.75
  FP: 4.24
  FN: 4.26

Face Count 10: 3871 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 32.68%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 67.32%
  ‚ùå False Discovery rate (FP/TP+FP): 67.31%
  üéØ Precision (TP/TP+FP): 32.69%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 16.79%
  ‚ö†Ô∏è Std. False Negative rate: 16.79%
  ‚ùå Std. False Discovery rate: 16.79%
  üéØ Std. Precision: 16.79%
üìâ  Average detailed edge-metrics
  F1: 0.33
  Jaccard: 0.21
  TP: 2.33
  FP: 4.78
  FN: 4.79

Face Count 999: 6985 graphs evaluated
üìâ  Average edge-level rates
  ‚úÖ Recall (TP/TP+FN): 36.33%
  ‚ö†Ô∏è False Negative rate (FN/TP+FN): 63.67%
  ‚ùå False Discovery rate (FP/TP+FP): 63.56%
  üéØ Precision (TP/TP+FP): 36.44%
üìä Standard Deviation Across Graphs:
  ‚úÖ Std. Recall: 18.05%
  ‚ö†Ô∏è Std. False Negative rate: 18.05%
  ‚ùå Std. False Discovery rate: 18.13%
  üéØ Std. Precision: 18.13%
üìâ  Average detailed edge-metrics
  F1: 0.36
  Jaccard: 0.24
  TP: 2.54
  FP: 4.46
  FN: 4.48
[6, 7, 8, 9, 10, 999]
[0.7283333333333333, 0.5577430972388955, 0.4549128919860627, 0.3920263622018008, 0.3267581897134984, 0.36333299246685075]
[np.float64(0.21402881425951353), np.float64(0.2331277515506609), np.float64(0.17787631021388892), np.float64(0.17731084923198942), np.float64(0.16791575478460397), np.float64(0.18130318681514607)]
‚úÖ Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/metrics.json
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM
Device for model: cuda:5
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/random
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.FIXED
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/fixed
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_ASC
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/deg_asc
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.DEG_DESC
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/deg_desc
0
0
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MIN_REM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/neigh_min_rem
56074
7501
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.NEIGH_MAX_REM
‚úÖ  Reports written to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/18__constraints_random__trained_dgmg_model-path=6_7_8_9_10-n=3000-mf=50-node_hidden_size=128-num_prop_rounds=2-epochs=50-lr=0.00006/reports/neigh_max_rem
56142
7445
DGMG-Model with constraints config:  True
DGMG-Model node ordering strategy:  NodeOrder.RANDOM
Device for model: cuda:5
‚úÖ Code finished successfully at: 2025-07-15 15:19:59
