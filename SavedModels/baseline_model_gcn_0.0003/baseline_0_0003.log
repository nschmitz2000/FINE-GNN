nohup: ignoring input
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current_baseline.py:618: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  metadata = torch.load(load_path_metadata)
âœ… Using device: cuda:1
/home/nschmitz/GNNs/kirigami-database/data/cleaned
Model without constraints:  False
ğŸ’¿ Folder that model data gets saved into:  22__testing__
Metrics---
Amount unfoldings:  3000
Max files per dir:  50
Batch size:  1
Num prop rounds:  2
Learning rate:  0.0003
Num epochs:  10
Accum steps:  4
Training node ordering strategy:  NodeOrder.RANDOM
Add edge is deterministic:  True
Add edge is deterministic threshold:  0.5
Calc PR-Curve:  False
---------------
6_7_8_9_10
Found existing dataset files; skipping save.
Loading dataset from diskâ€¦
6_7_8_9_10
Max amount of features per node in a graph:  128
Loaded 46571 graphs with actions!
Train size: 32599, Validation size: 6985, Test size: 6987
Graph(num_nodes=10, num_edges=18,
      ndata_schemes={'a': Scheme(shape=(256,), dtype=torch.float32), 'hv': Scheme(shape=(128,), dtype=torch.float32), 'deg_rem': Scheme(shape=(1,), dtype=torch.int16), 'deg_target': Scheme(shape=(1,), dtype=torch.int16)}
      edata_schemes={'he': Scheme(shape=(1,), dtype=torch.float32)})
[[0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
 [1. 0. 1. 0. 0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]
 [0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]]
--------
Graph(num_nodes=10, num_edges=32,
      ndata_schemes={'deg_target': Scheme(shape=(1,), dtype=torch.int16), 'hv': Scheme(shape=(128,), dtype=torch.float32)}
      edata_schemes={})
[[0. 0. 0. 1. 0. 1. 0. 1. 0. 1.]
 [0. 0. 1. 0. 1. 0. 1. 0. 1. 0.]
 [0. 1. 0. 1. 0. 1. 0. 0. 0. 0.]
 [1. 0. 1. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 1. 0. 0. 0. 0. 0. 1.]
 [1. 0. 1. 0. 0. 0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 0. 1. 0. 1. 0. 0.]
 [1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]
 [0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]
 [1. 0. 0. 0. 1. 0. 0. 0. 1. 0.]]
Device for model: cuda:1
Training started at: 2025-10-08 07:39:04
=== Epoch 1: Train Loss: 0.6444 ===
Epoch 1: Validation Loss: 0.6413
Epoch 1: Edge Precision: 0.3218, Recall: 0.6834, F1: 0.4296, Jaccard: 0.2778
Epoch 1: TP: 4.80, FP: 10.51, FN: 2.23
[Epoch 1] â±ï¸ Total: 242.80s | 2025-10-08 07:43:06 | ğŸ‹ï¸ Train: 208.10s | âœ… Val: 34.70s
=== Epoch 2: Train Loss: 0.6329 ===
Epoch 2: Validation Loss: 0.6375
Epoch 2: Edge Precision: 0.3371, Recall: 0.6059, F1: 0.4182, Jaccard: 0.2704
Epoch 2: TP: 4.25, FP: 8.76, FN: 2.77
[Epoch 2] â±ï¸ Total: 241.19s | 2025-10-08 07:47:08 | ğŸ‹ï¸ Train: 206.16s | âœ… Val: 35.03s
ğŸ’¾ Baseline checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/baseline_model_gcn_0.0003/baseline_checkpoint_epoch_2.pth
=== Epoch 3: Train Loss: 0.6298 ===
Epoch 3: Validation Loss: 0.6366
Epoch 3: Edge Precision: 0.3183, Recall: 0.7260, F1: 0.4370, Jaccard: 0.2834
Epoch 3: TP: 5.09, FP: 11.26, FN: 1.93
[Epoch 3] â±ï¸ Total: 234.87s | 2025-10-08 07:51:03 | ğŸ‹ï¸ Train: 199.09s | âœ… Val: 35.78s
=== Epoch 4: Train Loss: 0.6282 ===
Epoch 4: Validation Loss: 0.6355
Epoch 4: Edge Precision: 0.3195, Recall: 0.7172, F1: 0.4363, Jaccard: 0.2831
Epoch 4: TP: 5.03, FP: 11.05, FN: 1.99
[Epoch 4] â±ï¸ Total: 236.63s | 2025-10-08 07:54:59 | ğŸ‹ï¸ Train: 200.43s | âœ… Val: 36.20s
ğŸ’¾ Baseline checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/baseline_model_gcn_0.0003/baseline_checkpoint_epoch_4.pth
=== Epoch 5: Train Loss: 0.6268 ===
Epoch 5: Validation Loss: 0.6376
Epoch 5: Edge Precision: 0.3145, Recall: 0.7633, F1: 0.4414, Jaccard: 0.2867
Epoch 5: TP: 5.36, FP: 12.03, FN: 1.67
[Epoch 5] â±ï¸ Total: 234.15s | 2025-10-08 07:58:53 | ğŸ‹ï¸ Train: 197.81s | âœ… Val: 36.33s
=== Epoch 6: Train Loss: 0.6265 ===
Epoch 6: Validation Loss: 0.6357
Epoch 6: Edge Precision: 0.3152, Recall: 0.7556, F1: 0.4404, Jaccard: 0.2860
Epoch 6: TP: 5.30, FP: 11.87, FN: 1.72
[Epoch 6] â±ï¸ Total: 233.80s | 2025-10-08 08:02:47 | ğŸ‹ï¸ Train: 196.79s | âœ… Val: 37.02s
ğŸ’¾ Baseline checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/baseline_model_gcn_0.0003/baseline_checkpoint_epoch_6.pth
=== Epoch 7: Train Loss: 0.6251 ===
Epoch 7: Validation Loss: 0.6386
Epoch 7: Edge Precision: 0.3166, Recall: 0.7475, F1: 0.4402, Jaccard: 0.2859
Epoch 7: TP: 5.25, FP: 11.67, FN: 1.78
[Epoch 7] â±ï¸ Total: 234.12s | 2025-10-08 08:06:41 | ğŸ‹ï¸ Train: 197.63s | âœ… Val: 36.49s
=== Epoch 8: Train Loss: 0.6250 ===
Epoch 8: Validation Loss: 0.6372
Epoch 8: Edge Precision: 0.3160, Recall: 0.7459, F1: 0.4393, Jaccard: 0.2853
Epoch 8: TP: 5.23, FP: 11.67, FN: 1.79
[Epoch 8] â±ï¸ Total: 235.97s | 2025-10-08 08:10:37 | ğŸ‹ï¸ Train: 199.05s | âœ… Val: 36.92s
ğŸ’¾ Baseline checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/baseline_model_gcn_0.0003/baseline_checkpoint_epoch_8.pth
=== Epoch 9: Train Loss: 0.6241 ===
Epoch 9: Validation Loss: 0.6360
Epoch 9: Edge Precision: 0.3164, Recall: 0.7420, F1: 0.4386, Jaccard: 0.2848
Epoch 9: TP: 5.21, FP: 11.60, FN: 1.81
[Epoch 9] â±ï¸ Total: 237.11s | 2025-10-08 08:14:34 | ğŸ‹ï¸ Train: 202.55s | âœ… Val: 34.56s
/home/nschmitz/GNNs/MasterThesisGNN/DGMG/DGMG_current_baseline.py:1359: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(os.path.join(run_folder, "baseline_model_final.pth"), map_location=device)
=== Epoch 10: Train Loss: 0.6226 ===
Epoch 10: Validation Loss: 0.6381
Epoch 10: Edge Precision: 0.3160, Recall: 0.7360, F1: 0.4372, Jaccard: 0.2837
Epoch 10: TP: 5.16, FP: 11.53, FN: 1.86
[Epoch 10] â±ï¸ Total: 241.75s | 2025-10-08 08:18:36 | ğŸ‹ï¸ Train: 206.86s | âœ… Val: 34.89s
ğŸ’¾ Baseline checkpoint saved: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/baseline_model_gcn_0.0003/baseline_checkpoint_epoch_10.pth
Training finished at: 2025-10-08 08:18:36
Training time (s): 2372.4072077274323
âœ… Baseline model saved to: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/baseline_model_gcn_0.0003/baseline_model_final.pth
ğŸ“ˆ Baseline metrics saved to: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/baseline_model_gcn_0.0003/metrics.json
âœ… Loaded baseline from: /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/baseline_model_gcn_0.0003/baseline_model_final.pth

Face Count 6: 20 graphs evaluated
ğŸ“‰  Average edge-level rates
  âœ… Recall (TP/TP+FN): 67.50%
  âš ï¸ False Negative rate (FN/TP+FN): 32.50%
  âŒ False Discovery rate (FP/TP+FP): 39.88%
  ğŸ¯ Precision (TP/TP+FP): 60.13%
ğŸ“Š Standard Deviation Across Graphs:
  âœ… Std. Recall: 20.76%
  âš ï¸ Std. False Negative rate: 20.76%
  âŒ Std. False Discovery rate: 11.94%
  ğŸ¯ Std. Precision: 11.94%
ğŸ“‰  Average detailed edge-metrics
     F1: 0.63
     Jaccard: 0.48
     TP: 3.85
     FP: 2.45
     FN: 1.90

Face Count 7: 119 graphs evaluated
ğŸ“‰  Average edge-level rates
  âœ… Recall (TP/TP+FN): 69.99%
  âš ï¸ False Negative rate (FN/TP+FN): 30.01%
  âŒ False Discovery rate (FP/TP+FP): 53.52%
  ğŸ¯ Precision (TP/TP+FP): 46.48%
ğŸ“Š Standard Deviation Across Graphs:
  âœ… Std. Recall: 17.22%
  âš ï¸ Std. False Negative rate: 17.22%
  âŒ Std. False Discovery rate: 10.82%
  ğŸ¯ Std. Precision: 10.82%
ğŸ“‰  Average detailed edge-metrics
     F1: 0.55
     Jaccard: 0.39
     TP: 4.23
     FP: 4.97
     FN: 1.82

Face Count 8: 410 graphs evaluated
ğŸ“‰  Average edge-level rates
  âœ… Recall (TP/TP+FN): 72.59%
  âš ï¸ False Negative rate (FN/TP+FN): 27.41%
  âŒ False Discovery rate (FP/TP+FP): 60.10%
  ğŸ¯ Precision (TP/TP+FP): 39.90%
ğŸ“Š Standard Deviation Across Graphs:
  âœ… Std. Recall: 17.37%
  âš ï¸ Std. False Negative rate: 17.37%
  âŒ Std. False Discovery rate: 7.62%
  ğŸ¯ Std. Precision: 7.62%
ğŸ“‰  Average detailed edge-metrics
     F1: 0.51
     Jaccard: 0.35
     TP: 4.73
     FP: 7.16
     FN: 1.81

Face Count 9: 2565 graphs evaluated
ğŸ“‰  Average edge-level rates
  âœ… Recall (TP/TP+FN): 72.24%
  âš ï¸ False Negative rate (FN/TP+FN): 27.76%
  âŒ False Discovery rate (FP/TP+FP): 66.40%
  ğŸ¯ Precision (TP/TP+FP): 33.60%
ğŸ“Š Standard Deviation Across Graphs:
  âœ… Std. Recall: 16.88%
  âš ï¸ Std. False Negative rate: 16.88%
  âŒ Std. False Discovery rate: 6.16%
  ğŸ¯ Std. Precision: 6.16%
ğŸ“‰  Average detailed edge-metrics
     F1: 0.46
     Jaccard: 0.30
     TP: 5.06
     FP: 10.05
     FN: 1.95

Face Count 10: 3871 graphs evaluated
ğŸ“‰  Average edge-level rates
  âœ… Recall (TP/TP+FN): 74.75%
  âš ï¸ False Negative rate (FN/TP+FN): 25.25%
  âŒ False Discovery rate (FP/TP+FP): 71.21%
  ğŸ¯ Precision (TP/TP+FP): 28.79%
ğŸ“Š Standard Deviation Across Graphs:
  âœ… Std. Recall: 16.29%
  âš ï¸ Std. False Negative rate: 16.29%
  âŒ Std. False Discovery rate: 5.15%
  ğŸ¯ Std. Precision: 5.15%
ğŸ“‰  Average detailed edge-metrics
     F1: 0.41
     Jaccard: 0.26
     TP: 5.31
     FP: 13.22
     FN: 1.80

Face Count 999: 6985 graphs evaluated
ğŸ“‰  Average edge-level rates
  âœ… Recall (TP/TP+FN): 73.60%
  âš ï¸ False Negative rate (FN/TP+FN): 26.40%
  âŒ False Discovery rate (FP/TP+FP): 68.40%
  ğŸ¯ Precision (TP/TP+FP): 31.60%
ğŸ“Š Standard Deviation Across Graphs:
  âœ… Std. Recall: 16.66%
  âš ï¸ Std. False Negative rate: 16.66%
  âŒ Std. False Discovery rate: 7.10%
  ğŸ¯ Std. Precision: 7.10%
ğŸ“‰  Average detailed edge-metrics
     F1: 0.44
     Jaccard: 0.28
     TP: 5.16
     FP: 11.53
     FN: 1.86
[6, 7, 8, 9, 10, 999]
[0.675, 0.6998799519807923, 0.7259001161440185, 0.722419474612457, 0.7474797333038098, 0.7359920918976036]
[np.float64(0.11939738796487329), np.float64(0.10815318158519721), np.float64(0.07622411339620001), np.float64(0.06157377557091452), np.float64(0.05154978838135304), np.float64(0.07102911437960681)]
âœ… Stats saved to /home/nschmitz/GNNs/MasterThesisGNN/SavedModels/baseline_model_gcn_0.0003/metrics.json
âœ… Code finished successfully at: 2025-10-08 08:19:38
